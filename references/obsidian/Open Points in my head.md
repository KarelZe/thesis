*This week*


- Study feature importances
- generate Transformer results (even if it is just 10 %)
- think about visualizing self-training
- improve captions of newly added graphics
- resource intensiveness of self-training
- better motivate what FT-Transformer is, why we settle on linear embeddings
- finish chapter on supervised discussion
- Implement pre-training
- fix $\mathbf{X}$ in thesis
- clean up vault
- run Transformer on 100 % for a bit. See if gradient problem is now gone.
- investigate bias in unsupervised / supervised dataset
- think about how to discuss filtering bias in dataset
- think about how add intuition to feature set definition
- try out label smoothing
- retrieve attention maps from model 
- structure chapters for
	- semi-supervised training
	- supervised training
- think about adding further visualizations e. g., random token replacement