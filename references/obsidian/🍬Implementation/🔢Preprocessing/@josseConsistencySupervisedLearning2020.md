*title:* On the consistency of supervised learning with missing values
*authors:* Julie Josse, Nicolas Prost, Erwan Scornet, Ga√´l Varoquaux
*year:* 2020
*tags:* #gradient_boosting #decision-trees #missing-value #imputation #nan 
*status:* #üì¶ 
*related:*
- [[@perez-lebelBenchmarkingMissingvaluesApproaches2022]]
## Notes üìç
- Write if needed.

## Annotations üìñ


‚ÄúA striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative.‚Äù ([Josse et al., 2020, p. 1](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=1&annotation=5QVYVDL4))

‚ÄúThis contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data, through multiple imputation.‚Äù ([Josse et al., 2020, p. 1](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=1&annotation=34875C7U))

‚ÄúThese can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the ‚Äúmissing incorporated in attribute‚Äù method as it can handle both non-informative and informative missing values‚Äù ([Josse et al., 2020, p. 1](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=1&annotation=35WKNVW9))

‚ÄúThe classical literature on missing values, led by Rubin (1976), defines missing-values mechanisms based on the relationship between missingness and observed values: if they are independent, the mechanism is said to be Missing Completely At Random (MCAR); if the missingness only depends on the observed values, then it is Missing At Random (MAR); otherwise it is Missing Not At Random (MNAR). However, this nomenclature has seldom been discussed in the context of supervised learning, accounting for the target variable of the prediction.‚Äù ([Josse et al., 2020, p. 2](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=2&annotation=YI5CZ8V8))

‚ÄúListwise deletion, i.e. removing incomplete observations, may allow to train the model on complete data. Yet it does not suffice for supervised learning, as the test set may also contain incomplete data. Hence the prediction procedure should handle missing values. A popular solution is to impute missing values, that is to replace them with plausible values to produce a completed data set.‚Äù ([Josse et al., 2020, p. 2](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=2&annotation=FUKJZVNZ))

‚ÄúThe benefit of imputation is that it adapts existing pipelines and software to the presence of missing values. The widespread practice of imputing with the mean of the variable on the observed entries is known to have serious drawbacks as it distorts the joint and marginal distributions of the data which induces bias in estimators (Little and Rubin, 2019).‚Äù ([Josse et al., 2020, p. 2](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=2&annotation=CAUT4QUV))

‚ÄúIn Section 5, to compare imputation to learning directly with missing values, we analyze further decision trees. Indeed, their greedy and discrete natures allow adapting them to handle missing values directly.‚Äù ([Josse et al., 2020, p. 3](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=3&annotation=ZHCHXEQP))

‚ÄúWe also show the benefits for prediction of an approach often used in practice, which consists in ‚Äúadding the mask‚Äù, i.e. adding binary variables that code for the missingness of each variables as new covariates, even though this method is not recommended for parameter estimation (Jones, 1996).‚Äù ([Josse et al., 2020, p. 3](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=3&annotation=U7756GTF))

‚ÄúImputation prior to analysis‚Äù ([Josse et al., 2020, p. 6](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=6&annotation=CR7CZ4GE))

‚ÄúMost statistical models and machine-learning procedures are not designed for incomplete data. To use existing pipelines in the presence on missing values, imputing the data is commonly used to form a completed data set‚Äù ([Josse et al., 2020, p. 6](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=6&annotation=B36832XQ))

‚ÄúOne important issue with ‚Äúsingle‚Äù imputation, i.e. predicting only one value for each missing entries, is that it forgets that some values were missing and considers imputed values and observed values in the same way. It leads to underestimation of the variance of the parameters (Little and Rubin, 2019) estimated on the completed data.‚Äù ([Josse et al., 2020, p. 7](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=7&annotation=C8HKB6GT))

‚ÄúSupervised learning typically assumes that the data are i.i.d. In particular, an out-of-sample observation (test set) is supposed to be drawn from the same distribution as the original sample (train set). Hence it has the same missing-values mechanism. An appropriate method should be able to predict on new data with missing values.‚Äù ([Josse et al., 2020, p. 8](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=8&annotation=FDHUY7H9))

‚ÄúThis highlights the fact that the imputation method should be chosen in accordance with the learning algorithm that will be applied later on.‚Äù ([Josse et al., 2020, p. 13](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=13&annotation=DRJRXUK4))

‚ÄúA simple option to extend CART methodology in presence of missing values is to select the best split only on the available cases for each variable. More precisely, for any node A, the best split in presence of missing values is a solution of the new optimization problem‚Äù ([Josse et al., 2020, p. 17](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=17&annotation=RU4ZZ7JV))

‚ÄúThis splitting strategy is described in Algorithm 1. As the missing values were not used to calculate the criterion, it is still necessary to specify to which cell they are sent. The solution consisting in discarding missing data at each step would lead to a drastic reduction of the data set and is therefore not viable.‚Äù ([Josse et al., 2020, p. 17](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=17&annotation=U36IQAGN))

‚ÄúSurrogate splits Once the best split is chosen via Algorithm 1, surrogate splits search for a split on another variable that induces a data partition close to the original one.‚Äù ([Josse et al., 2020, p. 17](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=17&annotation=69AXLE97))

‚ÄúProbabilistic splits Another option is to propagate missing observations according to a Bernoulli distribution B( #L #L+#R ), where #L (resp. #R) is the number of points already on the left (resp. right) (see Algorithm 3 in Appendix A.3)‚Äù ([Josse et al., 2020, p. 18](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=18&annotation=RZ3GTFLX))

‚ÄúBlock propagation A third option to choose the split on the observed values, and then send all incomplete observations as a block, to a side chosen by minimizing the error (see Algorithm 4 in Appendix A.3). This is the method used in LightGBM (Ke et al., 2017).‚Äù ([Josse et al., 2020, p. 18](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=18&annotation=7G6MQTX6))

‚ÄúA second class of methods uses missing values to compute the splitting criterion itself. Consequently, the splitting location depends on the missing values, contrary to all methods presented in Section 5.2. Its most common instance is ‚Äúmissing incorporated in attribute‚Äù (MIA, Twala et al. 2008, )‚Äù ([Josse et al., 2020, p. 19](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=19&annotation=W5PPBJXU))

‚ÄúMore specifically, MIA considers the following splits, for all splits (j, z): ‚Ä¢ { ÃÉ Xj ‚â§ z or ÃÉ Xj = NA} vs { ÃÉ Xj > z}, ‚Ä¢ { ÃÉ Xj ‚â§ z} vs { ÃÉ Xj > z or ÃÉ Xj = NA}, ‚Ä¢ { ÃÉ Xj 6= NA} vs { ÃÉ Xj = NA}. In a nutshell, for each possible split, MIA tries to send all missing values to the left or to the right, and compute for each choice the corresponding error (right-hand side in 9, as well as the error associated to separating the observed values from the missing ones.‚Äù ([Josse et al., 2020, p. 19](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=19&annotation=V5DEHSCS))

‚ÄúFinally, it chooses the split among the previous ones with the lowest error (see Algorithm 5 in Appendix A.3).‚Äù ([Josse et al., 2020, p. 19](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=19&annotation=K7XKH276))

‚ÄúMIA is thought to be a good method to apply when missing pattern is informative, as this procedure allows to cut with respect to missing/ non missing and uses missing data to compute the best splits. Note this latter property implies that the MIA approach does not require a different method to propagate missing data down the tree. Notably, MIA is implemented in the R packages partykit (Hothorn and Zeileis, 2015) and grf (Tibshirani et al., 2020), as well as in XGBoost (Chen and Guestrin, 2016) and for the HistGradientBoosting models in scikit-learn (Pedregosa et al., 2011).‚Äù ([Josse et al., 2020, p. 19](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=19&annotation=TUX46ZNE))

‚ÄúRemark 5 Implementation: A simple way to implement MIA consists in duplicating the incomplete columns, and replacing the missing entries once by +‚àû and once by ‚àí‚àû (or an extreme out-of-range value). This creates two dummy variables for each original one containing missing values. Splitting along a variable and sending all missing data to the left (for example) is the same as splitting along the corresponding dummy variable where missing entries have been completed by ‚àí‚àû. Alternatively, MIA can be with two scans on a feature‚Äôs values in ascending and descending orders (Chen and Guestrin, 2016, Alg 3).‚Äù ([Josse et al., 2020, p. 19](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=19&annotation=WW3JPTT5))

‚ÄúRemark 6 Implicit imputation: Whether it is in the case where the missing values are propagated in the available case method (Section 5.2), or incorporated in the split choice in MIA, missing values are assigned either to the left or the right interval. Consequently, handling missing values in a tree can be seen as implicit imputation by an interval value.‚Äù ([Josse et al., 2020, p. 20](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=20&annotation=UFCPQBWY))

‚ÄúWe have studied procedures for supervised learning with missing data. Unlike in the classic missing data literature, the goal of the procedures is to yield the best possible prediction on test data with missing values. Focusing on simple ways of adapting existing procedures, our theoretical and empirical results outline simple practical recommendations: ‚Ä¢ Given a model suitable for the fully observed data, good prediction can be achieved on a test set by multiple imputation of its missing values with a conditional imputation model fit on the train set (Theorem 3). ‚Ä¢ To train and test on data with missing values, the same imputation model should be used. Single mean imputation is consistent, provided a powerful, non-linear model (Theorem 4). ‚Ä¢ For tree-based models, a good solution for missing values is Missing Incorporated in Attribute (MIA, Twala et al. 2008, see implementation Remark 5): optimizing jointly the split and the handling of the missing values (Proposition 8 and experimental results). Empirically, this approach also performs well outside of MAR settings. ‚Ä¢ Empirically, good imputation methods applied at train and test time reduce the number of samples required to reach good prediction (Figure 5). ‚Ä¢ When missingness is related to the prediction target, imputation does not suffice and it is useful to add indicator variables of missing entries as features (Example 3 and Figure 3). These recommendations hold to minimize the prediction error in an asymptotic regime. More work is needed to establish theoretical results in the finite sample regime. In addition, different practices may be needed to control for the uncertainty associated to a prediction.‚Äù ([Josse et al., 2020, p. 27](zotero://select/library/items/IV3UNULR)) ([pdf](zotero://open-pdf/library/items/HK8RPCRM?page=27&annotation=TBAT5MBI))