
title: Why do tree-based models still outperform deep learning on typical tabular data?
authors: LÃ©o Grinsztajn, Edouard Oyallon, GaÃ«l Varoquaux
year: 2022
tags :  #gradient_boosting  #dt #neural_network #transformer
status : #ğŸ“¥  
related: 
- [[ğŸ„Tree-based Methods/Gradient Boosting/@friedmanGreedyFunctionApproximation2001]]
- [[@hastietrevorElementsStatisticalLearning2009]]
- [[ğŸ„Tree-based Methods/Random Forests/@breimanRandomForests2001]]
- [[ğŸ§ Deep Learning Methods/Transformer/@arikTabNetAttentiveInterpretable2020]]
- [[ğŸ§ Deep Learning Methods/Transformer/@huangTabTransformerTabularData2020]]
Code: 
- https://github.com/LeoGrin/tabular-benchmark
# Notes Sebastian Raschka
-   The main takeaway is that tree-based models (random forests and XGBoost) outperform deep learning methods for tabular data on medium-sized datasets (10k training examples).
-   The gap between tree-based models and deep learning becomes narrower as the dataset size increases (here: 10k -> 50k).
-   Solid experiments and thorough investigation into the role of uninformative features: uninformative features harm deep learning methods more than tree-based methods.
-   Small caveats: some of the recent tabular methods for deep learning were not considered; â€œlargeâ€ datasets are only 50k training examples (small in many industry domains.)
-   Experiments based on 45 tabular datasets; numerical and mixed numerical-categorical; classification and regression datasets; 10k training examples with balanced classes for main experiments; 50k datasets for â€œlargeâ€ dataset experiments.