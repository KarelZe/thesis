*title:* Transfer Learning with Deep Tabular Models
*authors:* Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, Micah Goldblum
*year:* 2022
*tags:* #pretraining #semi-supervised #transformer #tabular-data #transfer-learning
*status:* #üì¶ 
*related:*
- [[@huangTabTransformerTabularData2020]]
- [[@devlinBERTPretrainingDeep2019]]
- [[@somepalliSAINTImprovedNeural2021]]
## Notes üìç

## Annotations üìñ
‚Äúransfer learning plays a central role in industrial computer vision and natural language processing pipelines, where models learn generic features that are useful across many tasks‚Äù ([Levin et al., 2022, p. 1](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=1&annotation=LJ689XWU))

‚ÄúAs tabular data is highly heterogeneous, the problem of downstream tasks whose formats and features differ from those of upstream data is common and has been reported to complicate knowledge transfer [47]. Nonetheless, if our upstream data is missing columns present in downstream data, we still want to leverage pre-training‚Äù ([Levin et al., 2022, p. 2](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=2&annotation=V6JVIM8S))

‚ÄúAn extensive line of work on tabular deep learning aims to challenge the dominance of GBDT models. Numerous tabular neural architectures have been introduced, based on the ideas of creating differentiable learner ensembles [55, 29, 77, 43, 8], incorporating attention mechanisms and transformer architectures [64, 26, 6, 34, 65, 44], as well as a variety of other approaches [70, 71, 10, 42, 23, 61]. However, recent systematic benchmarking of deep tabular models [26, 63] shows that while these models are competitive with GBDT on some tasks, there is still no universal best method. Gorishniy et al. [26] show that transformer-based models are the strongest alternative to GBDT and that ResNet and MLP models coupled with a strong hyperparameter tuning routine [2] offer competitive baselines. Similarly, Kadra et al. [40] find that carefully regularized MLPs are competitive. In a follow-up work, Gorishniy et al. [27] show that transformer architectures equipped with advanced embedding schemes for numerical features bridge the performance gap between deep tabular models and GBDT‚Äù ([Levin et al., 2022, p. 3](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=3&annotation=BNGFTTM2))

‚ÄúTransfer learning [54, 72, 83] has been incredibly successful in domains of computer vision and natural language processing (NLP). Large fine-tuned models excel on a variety of image classification [21, 18] and NLP benchmarks [20, 33].‚Äù ([Levin et al., 2022, p. 3](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=3&annotation=R7TUBUVN))

‚ÄúSelf-supervised learning. Self-supervised learning (SSL) aimed at harnessing unlabelled data through learning its structure and invariances has accumulated a large body of works over the last few years. Prominent SSL methods, such as Masked Language Modeling (MLM) [20] in NLP and contrastive pre-training in computer vision [17] have revolutionized their fields making SSL the pre-training approach of choice [20, 45, 50, 48, 17, 30, 12, 9, 53]. In fact, SSL pre-training in vision has been shown to produce more transferable features than supervised pre-training on ImageNet [30]. Recently, SSL has been adopted in the tabular domain for semi-supervised learning [78, 79, 69, 64, 34]. Contrastive pre-training on auxilary unlabelled data [64] and MLM-like approaches [34] have been shown to provide gains over training from scratch for transformer tabular architectures in cases of limited labelled data.‚Äù ([Levin et al., 2022, p. 3](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=3&annotation=X9KFNK9G))

‚ÄúFor deep models with transfer learning, we tune the hyperparameters on the full upstream data using the available large upstream validation set with the goal to obtain the best performing feature extractor for the pre-training multi-target task. We then fine-tune this feature extractor with a small learning rate on the downstream data. As this strategy offers considerable performance gains over default hyperparameters, we highlight the importance of tuning the feature extractor and present the comparison with default hyperparameters in Appendix B as well as the details on hyperparameter search spaces for each model.‚Äù ([Levin et al., 2022, p. 6](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=6&annotation=PICSZEZU))

‚ÄúIn domains where established SSL methods are increasingly dominant, such as computer vision, self-supervised learners are known to extract more transferable features than models trained on labelled data [30, 31]. In this section, we compare supervised pre-training with unsupervised pre-training and find that the opposite is true in the tabular domain. We use the Masked Language Model (MLM) pre-training recently adapted to tabular data [34] and the tabular version of contrastive learning [64]. Since both methods were proposed for tabular transformer architectures, we conduct the experiments with the FT-Transformer model. The inferior performance of self-supervised pre-training might be a consequence of the fact that SSL is significantly less explored and tuned in the tabular domain than in vision or NLP.‚Äù ([Levin et al., 2022, p. 7](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=7&annotation=YAULJZUP))

‚ÄúMasked Language Modeling (MLM) was first proposed for language models by Devlin et al. [20] as a powerful unsupervised learning strategy. MLM involves training a model to predict tokens in text masked at random so that its learned representations contain information useful for reconstructing these masked tokens. In the tabular domain, instead of masking tokens, a random subset of features is masked for each sample, and the masked values are predicted in a multi-target classification manner [34]. In our experiments, we mask one randomly selected feature for each sample, asking the network to learn the structure of the data and form representations from n ‚àí 1 features that are useful in producing the value in the n-th feature. For more detail, see Appendix A.‚Äù ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=DC87W4J4))

‚ÄúContrastive pre-training uses data augmentations to generate positive pairs, or two different augmented views of a given example, and the loss function encourages a feature extractor to map positive pairs to similar features. Meanwhile, the network is also trained to map negative pairs, or augmented views of different base examples, far apart in feature space. We use the implementation of contrastive learning from Somepalli et al. [64]. In particular, we generate positive pairs by applying two data augmentations: CutMix [80] in the input space and Mixup [81] in the embedding space. For more details, see Appendix A‚Äù ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=J9CNVZ4T))

‚ÄúIn Figure 3, we compare supervised pre-training with contrastive and MLM pre-training strategies and show that supervised pre-training always attains the best average rank. Contrastive pre-training produces better results than training from scratch on the downstream data when using a linear head, but it is still inferior to supervised pre-training. Tabular MLM pretraining also falls behind the supervised strategy and performs comparably to training from scratch in the lower data regimes but leads to a weaker downstream model in the higher data regimes.‚Äù ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=WVB866BS))