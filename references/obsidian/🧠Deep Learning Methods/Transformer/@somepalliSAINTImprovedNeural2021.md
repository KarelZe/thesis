*title:* Saint: Improved Neural Networks for Tabular Data Via Row Attention and Contrastive Pre-Training
*authors:* Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, Tom Goldstein
*year:* 2021
*tags:* #supervised-learning #semi-supervised #deep-learning #tabular-data 
*status:* #ðŸ“¥
*related:*
*code:*Â [https://github.com/somepago/saint](https://github.com/somepago/saint)
# Notes Sebastian Raschka
-   The Self-Attention and Intersample Attention Transformer (SAINT) hybrid architecture is based on self-attention that applies attention across both rows and columns.
-   Also proposes a self-supervised learning technique for pre-training under scarce data regimes.
-   When looking at the average performance across all nine datasets, the proposed SAINT method tends to outperform gradient-boosted trees. The datasets ranged from 200 to 495,000 examples.
# Annotations