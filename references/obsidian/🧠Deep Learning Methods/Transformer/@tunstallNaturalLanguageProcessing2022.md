*title:* Natural Language Processing with Transformers
*authors:* Lewis Tunstall
*year:* 2022
*tags:* #transformer #nlp #tokenization #deep-learning 
*status:* #📦 
*related:*
*code:*
*review:*

## Notes 📍

## Annotations 📖

“Naturally, we want to avoid being so wasteful with our model parameters since models are expensive to train, and larger models are more difficult to maintain. A common approach is to limit the vocabulary and discard rare words by considering, say, the 100,000 most common words in the corpus. Words that are not part of the vocabulary are classified as “unknown” and mapped to a shared UNK token. This means that we lose some potentially important information in the process of word tokenization, since the model has no information about words associated with UNK.” (Tunstall, 2022, p. 32)