*title:* Natural Language Processing with Transformers
*authors:* Lewis Tunstall
*year:* 2022
*tags:* #transformer #nlp #tokenization #deep-learning 
*status:* #ğŸ“¦ 
*related:*
*code:*
*review:*

## Notes ğŸ“

## Annotations ğŸ“–

â€œNaturally, we want to avoid being so wasteful with our model parameters since models are expensive to train, and larger models are more difficult to maintain. A common approach is to limit the vocabulary and discard rare words by considering, say, the 100,000 most common words in the corpus. Words that are not part of the vocabulary are classified as â€œunknownâ€ and mapped to a shared UNK token. This means that we lose some potentially important information in the process of word tokenization, since the model has no information about words associated with UNK.â€ (Tunstall, 2022, p. 32)