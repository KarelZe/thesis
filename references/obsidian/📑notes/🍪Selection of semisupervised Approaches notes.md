
**Semis-supervised gradient-boosting**

For the existing semi-supervised boosting methods [Bennett et al., 2002; Chen and Wang, 2007; d’Alche-Buc ´ et al., 2002; Mallapragada et al., 2009; Saffari et al., 2008; Zheng et al., 2009],A Direct Boosting Approach for Semi-Supervised Classification ([[@zhaiDirectBoostingApproach]]).

“4.3 Boosting Ensemble classifiers consist of multiple base classifiers, which are trained and then used to form combined predictions (Zhou 2012). The simplest form of ensemble learning trains k base classifiers independently and aggregates their predictions. Beyond this simplistic approach, two main branches of supervised ensemble learning exist: bagging and boosting (Zhou 2012). In bagging methods, each base learner is provided with a set of l data points, which are sampled, uniformly at random with replacement, from the original data set (bootstrapping). The base classifiers are trained independently. When training is completed, their outputs are aggregated to form the prediction of the ensemble. In boosting methods, on the other hand, each base learner is dependent on the previous base learners: it is provided with the full data set, but with weights applied to the data points. The weight of a data point xi is based on the performance of the previous base learners on xi , such that larger weights get assigned to data points that were incorrectly classified. The final prediction is obtained as a linear combination of the predictions of the base classifiers. Technically, boosting methods construct a weighted ensemble of classifiers ht in a greedy fashion. Let FT −1(x) = ∑T −1 t=1 αt · ht (x) denote the ensemble of classifiers ht with weight αt at time T − 1. Furthermore, let ( ˆ y, y) denote the loss function for predicting label ˆ y for a data point with true label y. In each iteration of the algorithm, an additional classifier hT is added to the ensemble with a certain weight αT , such that the cost function L(FT ) = l ∑ i =1 (FT (xi ), yi ) = l ∑ i =1 (FT −1(xi ) + αT · hT (xi ), yi ) is minimized. Note that, at time T ,theensembleFT −1 is fixed. With particular choices of loss functions, such as ( ˆ y, y) = exp(−ˆ y · y), the optimization problem yields a weighted classification problem for determining hT , and allows us to express the optimal αT in terms of the loss of hT on the training data. 12” ([[@vanengelenSurveySemisupervisedLearning2020]], 2020, p. 390)

“Machine Learning (2020) 109:373–440 391 By definition, base learners in bagging methods are trained independently. Therefore, the only truly semi-supervised bagging method would apply self-training to individual base learners. Co-training, however, can be seen to be closely related to bagging methods: the only way classifiers interact is by the exchange of pseudo-labelled data; other than that, the classifiers can be trained independently and simultaneously. However, most co-training methods do not use bootstrapping, a defining characteristic of bagging methods. In boosting, on the other hand, there is an inherent dependency between base learners. Consequently, boosting methods can be readily extended to the semi-supervised setting, by introducing pseudo-labelled data after each learning step; this idea gives rise to the class of semi-supervised boosting methods. Semi-supervised boosting methods have been studied extensively over the past two decades. The success achieved by supervised boosting methods, such as AdaBoost (Freund and Schapire 1997), gradient boosting, and XGBoost (Chen and Guestrin 2016), provides ample motivation for bringing boosting to the semi-supervised setting. Furthermore, the pseudo-labelling approach of self-training and co-training can be easily extended to boosting methods. 4.3.1 SSMBoost The first effort towards semi-supervised boosting methods was made by Grandvalet et al., who extended AdaBoost to the semi-supervised setting. They proposed a semi-supervised boosting algorithm (Grandvalet et al. 2001), which they later extended and motivated from the perspective of gradient boosting (d’Alché Buc et al. 2002). A loss function is defined for unlabelled data, based on the predictions of the current ensemble and on the predictions of the base learner under construction. Experiments were conducted with multiple loss functions; the authors reported the strongest results using the expected loss of the new, combined classifier. The weighted error t for base classifier ht is thus adapted to include the unlabelled data points, causing the weight term αt to depend on the unlabelled data as well. Crucially, SSMBoost does not assign pseudo-labels to the unlabelled data points. As a result, it requires semi-supervised base learners to make use of the unlabelled data and is therefore intrinsically semi-supervised, in contrast to most other semi-supervised boosting algorithms, which are wrapper methods. Nevertheless, SSMBoost is included here, because it forms the foundation for all other forms of semi-supervised boosting algorithms, which do not require semi-supervised base learners. 4.3.2 ASSEMBLE The ASSEMBLE algorithm, short for Adaptive Supervised Ensemble, pseudo-labels the unlabelled data points after each iteration, and uses these pseudo-labelled data points in the construction of the next classifier, thus alleviating the need for semi-supervised base learners (Bennett et al. 2002). As shown by its authors, ASSEMBLE effectively maximizes the classification margin in function space. Since pseudo-labels are used in ASSEMBLE, it is not trivial to decide which unlabelled data points to pass to the next base learner. Bennett et al. (2002) proposed to use bootstrapping—i.e. sampling, uniformly at random, with replacement, l data points from the l + u labelled and unlabelled data points. 12” ([[@vanengelenSurveySemisupervisedLearning2020]], 2020, p. 391)

“By definition, base learners in bagging methods are trained independently. Therefore, the only truly semi-supervised bagging method would apply self-training to individual base learners. Co-training, however, can be seen to be closely related to bagging methods: the only way classifiers interact is by the exchange of pseudo-labelled data; other than that, the classifiers can be trained independently and simultaneously. However, most co-training methods do not use bootstrapping, a defining characteristic of bagging methods. In boosting, on the other hand, there is an inherent dependency between base learners. Consequently, boosting methods can be readily extended to the semi-supervised setting, by introducing pseudo-labelled data after each learning step; this idea gives rise to the class of semi-supervised boosting methods.” ([[@vanengelenSurveySemisupervisedLearning2020]],  p. 391)

“Semi-supervised boosting methods have been studied extensively over the past two decades. The success achieved by supervised boosting methods, such as AdaBoost (Freund and Schapire 1997), gradient boosting, and XGBoost (Chen and Guestrin 2016), provides ample motivation for bringing boosting to the semi-supervised setting. Furthermore, the pseudo-labelling approach of self-training and co-training can be easily extended to boosting methods.” (Engelen and Hoos, 2020, p. 391)

“392 Machine Learning (2020) 109:373–440 4.3.3 SemiBoost The semi-supervised boosting algorithm SemiBoost addresses the problem of selecting data points to be used by the base learners by relying on the manifold assumption, utilizing principles from graph-based methods (Mallapragada et al. 2009). Each unlabelled data point is assigned a pseudo-label, and the corresponding prediction confidence is calculated based on a predefined neighbourhood graph that encodes similarity between data points. Then, a subset of these pseudo-labelled data points is added to the set of labelled data points for training the next base learner. The probability of a sample being selected for this subset is proportional to its prediction confidence. SemiBoost was successfully applied to object tracking in videos by Grabner et al. (2008). SemiBoost uses the standard boosting classification model, expressing the final label prediction as a linear combination of the predictions of the individual learners. Its cost function, however, is highly dissimilar from the previously described semi-supervised boosting methods. Mallapragada et al. (2009) argue that a successful labelling of the test data should conform to the following three requirements. Firstly, the predicted labels of the unlabelled data should be consistent for unlabelled data points that are close to each other. Secondly, the predicted labels of the unlabelled data should be consistent with the labels of nearby labelled data points. And, thirdly, the predicted labels for the labelled data points should correspond to their true labels. These requirements are expressed in the form of a constrained optimization problem, where the first two are captured by the objective function, and the last is imposed as a constraint. In other words, the SemiBoost algorithm uses boosting to solve the optimization problem minimize FT LL ( ˆ y, A, FT ) + λ · LU (ˆ y, A, FT ) subject to ˆ yi = yi , i = 1,...,l, (1) where LU and LL are the cost functions expressing the inconsistency across the unlabelled and the combined labelled and unlabelled data, respectively, and λ ∈ R is a constant governing the relative weight of the cost terms; A is an n × n symmetric matrix denoting the pairwise similarities between data points. Lastly, FT denotes the joint prediction function of the ensemble of classifiers at time T . We note that the optimization objective in Eq. 1 is very similar to the cost functions encountered in graph-based methods (see Sects. 6.3 and 7)in that it favours classifiers that consistently label data points on the same manifold. In graphbased methods, however, no distinction is generally made between labelled-unlabelled and unlabelled-unlabelled pairs. 4.3.4 Other semi-supervised boosting methods The three previously discussed methods form the core of semi-supervised boosting research. Further work in the area includes RegBoost, which, like SemiBoost, includes local label consistency in its objective function (Chen and Wang 2011). In RegBoost, this term is also dependent on the estimated local density of the marginal distribution p(x). Several attempts have been made to extend the label consistency regularization to the multiclass setting (Tanha et al. 2012; Valizadegan et al. 2008).” ([[@vanengelenSurveySemisupervisedLearning2020]], , p. 392)

- [[@huangTabTransformerTabularData2020]] compare in their semi-supervised comparsion against GBM (PL) GBMs trained on pseudo labelled data

- “Tuning hyperparameters does not make neural networks state-of-the-art Tree-based models are superior for every random search budget, and the performance gap stays wide even after a large number of random search iterations. This does not take into account that each random search iteration is generally slower for neural networks than for tree-based models (see A.2).” ([Grinsztajn et al., 2022, p. 6](zotero://select/library/items/G3KP2Z9W)) ([pdf](zotero://open-pdf/library/items/A3KU4A43?page=6&annotation=K2FYJND8)) [[@grinsztajnWhyTreebasedModels2022]]
- For overview see [[@zhuSemiSupervisedLearningLiterature]]