

Masked language modeling (MLM) objectives (Devlin et al., 2018; Yang et al., 2019; Gu et al., 2017) for sequences, although recent, have become ubiquitous for many Natural Language Processing (NLP) applications (Liu et al., 2019; Zhang et al., 2019; Rogers et al., 2021) because they are easy to optimize and enable learning of highly expressive and flexible representations by the virtue of conditioning on bidirectional context (Peters et al., 2018; Devlin et al., 2018). (https://arxiv.org/pdf/2106.02736.pdf)

Difference of self-supervised learning and transfer learning: Recently, self-supervised learning (SSL) [2,3], as an unsupervised pretraining approach, has achieved promisingsuccess and outperforms transfer learning in a wide range of applications [2]. Similar to TL, SSLalso solves predictive tasks. But the output labels in SSL are constructed from the input data, ratherthan annotated by human as in TL. The auxiliary predictive tasks in SSL could be predicting whethertwo augmented data examples originate from the same original data example [3], inpainting maskedregions in images [4], etc. Since SSL does not leverage labels provided by human, it does not havethe risk of being biased to labels in a source task. On the other hand, the potential pitfall of not usinghuman-annotated labels is that the learned representations by SSL may not be as discriminative asthose in TL. (https://www.techrxiv.org/articles/preprint/Transfer_Learning_or_Self-supervised_Learning_A_Tale_of_Two_Pretraining_Paradigms/12502298)  (Do not cite but keep in mind when writing.)

“Datasets like these present huge opportunities for self- and semi-supervised learning algorithms, which can leverage the unlabeled data to further improve the performance of a predictive model.” ([Yoon et al., 2020, p. 1](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=1&annotation=5AB6GP26))

“Self-supervised learning & contrastive learning. SSL uses unlabeled data with pretext tasks to learn useful representations and most of them are in CV and NLP [20, 17, 15, 16, 58, 23, 59, 60, 61, 62, 63]. Recent SSL tabular models can be classified into reconstruction and contrastive based methods: TabNet [30] and VIME [2] try to recover the corrupted inputs with auto-encoding loss; SCARF [11] takes a SimCLR-like [64] contrastive loss between the sample and its corrupted version; SubTab [9] takes a combination of both. Nevertheless, all fail to learn transferable models across tables such that cannot benefit from pretraining with scale. Contrastive learning can also be applied to supervised learning by leveraging class labels to build positive samples [26]. Our work extends it to to the tabular domain, which we prove works better than vanilla supervised pretraining. The vertical partition sampling also enjoys high query speed from large databases which are often column-oriented [25]. Another line of research takes table pretraining table semantic parsing [65, 66, 67, 68, 69] or table-to-text generation [70, 71]. But these methods either encode the whole table instead of each row or do not demonstrate to benefit tabular prediction yet.” ([Wang and Sun, p. 9](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=9&annotation=CXBL5HGM))

“Contrastive learning, in which models are pre-trained to be invariant to reordering, cropping, or other label-preserving “views” of the data [5, 12, 15, 32, 43], is a powerful tool in the vision and language domains that has never (to our knowledge) been applied to tabular data.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=HQX7N9D5))

“Existing self-supervised objectives for tabular data include denoising [43], a variation of which was used by VIME [49], masking, and replaced token detection as used by TabTransformer [18]. We find that, while these methods are effective, superior results are achieved by contrastive learning.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=LXSCK4TP))

“It is difficult to craft invariance transforms for tabular data. The authors of VIME [49] use mixup in the non-embedded space as a data augmentation method, but this is limited to continuous data. We instead use CutMix [50] to augment samples in the input space and we use mixup [51] in the embedding space. These two augmentations combined yield a challenging and effective self-supervision task” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=XZYZV3VZ))

“Self-supervised learning: Unsupervised representation learning improves supervised learning especially in small data regime (Raina et al. 2007). Recent work for text (Devlin et al. 2018) and image (Trinh, Luong, and Le 2019) data has shown significant advances – driven by the judicious choice of the unsupervised learning objective (masked input prediction) and attention-based deep learning” ([Arik and Pfister, 2020, p. 3](zotero://select/library/items/EH5DCRUW)) ([pdf](zotero://open-pdf/library/items/TPDKX93V?page=3&annotation=B6TI27FJ))

“Image and tabular data are very different. The spatial correlations between pixels in images or the sequential correlations between words in text data are well-known and consistent across different datasets. By contrast, the correlation structure among features in tabular data is unknown and varies across different datasets. In other words, there is no “common” correlation structure in tabular data (unlike in image and text data). This makes the self- and semi-supervised learning in tabular data more challenging. Note that promising methods for image domain do not guarantee the favorable results on tabular domain (vice versa). Also, most augmentations and pretext tasks used in image data are not applicable to tabular data; because they directly utilize the spatial relationship of the image for augmentation (e.g., rotation) and pretext tasks (e.g., jigsaw puzzle and colorization).” ([Yoon et al., 2020, p. 9](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=9&annotation=7PWW4SF4))



“Standard semi-supervised learning methods also suffer from the same problem, since the regularizers they use for the predictive model are based on some prior knowledge of these data structures” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=ZLC8Z4AD)) 
“The notion of rotation simply does not exist in tabular data.” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=KTJJNLDZ)) [[@yoonVIMEExtendingSuccess2020]]


“A number of self-supervised learning techniques have been proposed in computer vision (Zhai et al., 2019; Tung et al., 2017; Jing & Tian, 2020). One framework involves learning features based on generated images through various methods, including using a GAN (Goodfellow et al., 2014; Donahue et al., 2016; Radford et al., 2015; Chen et al., 2018), predicting pixels (Larsson et al., 2016), predicting colorizations (Zhang et al., 2016; Larsson et al., 2017), ensuring local and global consistency (Iizuka et al., 2017), and learning synthetic artifacts (Jenni & Favaro, 2018). Most related to our approach are contrastive learning ones (Tian et al., 2019; Hassani & Khasahmadi, 2020; Oord et al., 2018; Henaff, 2020; Li et al., 2016; He et al., 2020; Bojanowski & Joulin, 2017; Wang & Gupta, 2015; Gidaris et al., 2018). In particular, our framework is similar to SimCLR (Chen et al., 2020), which involves generating views of a single image via image-based corruptions like random cropping, color distortion and blurring; however, we generate views that are applicable to tabular data. Self-supervised learning has had an especially large impact in language modeling (Qiu et al., 2020). One popular approach is masked language modeling, wherein the model is trained to predict input tokens that have been intentionally masked out (Devlin et al., 2018; Raffel et al., 2019; Song et al., 2019) as well as enhancements to this approach (Liu et al., 2019; Dong et al., 2019; Bao et al., 2020; Lample & Conneau, 2019; Joshi et al., 2020) and variations involving permuting the tokens (Yang et al., 2019; Song et al., 2020). Denoising autoencoders have been used by training them to reconstruct the input from a corrupted version (produced by, for example, token masking, deletion, and infilling) (Lewis et al., 2019; Wang et al., 2019; Freitag & Roy, 2018). Contrastive approaches include randomly replacing words and distinguishing between real and fake phrases (Collobert et al., 2011; Mnih & Kavukcuoglu, 2013), random token replacement (Mikolov et al., 2013; Clark et al., 2020), and adjacent sentences (Joshi et al., 2020; Lan et al., 2019; de Vries et al., 2019).” ([Bahri et al., 2022, p. 3](zotero://select/library/items/JZ2AZEJD)) ([pdf](zotero://open-pdf/library/items/TL73PSVV?page=3&annotation=94UU9FV6)) (proabably the most in-depth overview from [[@bahriSCARFSelfSupervisedContrastive2022]])

“In recent years, the self-supervised learning has successfully been used to learn meaningful representations of the data in natural language processing [34, 41, 11, 28, 10, 21, 9]. A similar success has been achieved in image and audio domains [7, 15, 37, 5, 17, 13, 8]. This progress is mainly enabled by taking advantage of spatial, semantic, or temporal structure in the data through data augmentation [7], pretext task generation [11] and using inductive biases through architectural choices (e.g. CNN for images). However, these methods can be less effective in the lack of such structures and biases in the tabular data commonly used in many fields such as healthcare, advertisement, finance, and law. And some augmentation methods such as cropping, rotation, color transformation etc. are domain specific, and not suitable for tabular setting. The difficulty in designing similarly effective methods tailored for tabular data is one of the reasons why self-supervised learning is under-studied in this domain [46].” ([Ucar et al., 2021, p. 1](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=1&annotation=KZDPXQEV))
 ([[@ucarSubTabSubsettingFeatures2021]])


“Recently, Yao et al. (2020) adapted the contrastive framework to large-scale recommendation systems in a way similar to our approach. The key difference is in the way the methods generate multiple views. Yao et al. (2020) proposes masking random features in a correlated manner and applying a dropout for categorical features, while our approach involves randomizing random features based on the features’ respective empirical marginal distribution (in an uncorrelated way).” ([Bahri et al., 2022, p. 3](zotero://select/library/items/JZ2AZEJD)) ([pdf](zotero://open-pdf/library/items/TL73PSVV?page=3&annotation=8MD5CLIW)) [[@bahriSCARFSelfSupervisedContrastive2022]]

“Lastly, also similar to our work is VIME (Yoon et al., 2020), which proposes the same corruption technique for tabular data that we do. They pre-train an encoder network on unlabeled data by attaching “mask estimator” and “feature estimator” heads on top of the encoder state and teaching the model to recover both the binary mask that was used for corruption as well as the original uncorrupted input, given the corrupted input. The pre-trained encoder network is subsequently used for semisupervised learning via attachment of a task-specific head and minimization of the supervised loss as well as an auto-encoder reconstruction loss. VIME was shown to achieve state-of-art results on genomics and clinical datasets. The key differences with our work is that we pre-train using a contrastive loss, which we show to be more effective than the denoising auto-encoder loss that partly constitutes VIME. Furthermore, after pre-training we fine-tune all model weights, including the encoder (unlike VIME, which only fine-tunes the task head), and we do so using task supervision only” ([Bahri et al., 2022, p. 3](zotero://select/library/items/JZ2AZEJD)) ([pdf](zotero://open-pdf/library/items/TL73PSVV?page=3&annotation=ZGG37Q2J)) [[@bahriSCARFSelfSupervisedContrastive2022]]

“In this paper, we propose novel self- and semi-supervised learning frameworks for tabular data. For self-supervised learning, we introduce a novel pretext task, mask vector estimation in addition to feature vector estimation.” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=IVMX9FSR))
 [[@yoonVIMEExtendingSuccess2020]]

“Recently, SSL has been adopted in the tabular domain for semi-supervised learning (saint, tabtransformer etc.). Contrastive pre-training on auxilary unlabelled data (saint) and MLM-like approaches (tabtransformer) have been shown to provide gains over training from scratch for transformer tabular architectures in cases of limited labelled data.” ([Levin et al., 2022, p. 3](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/TRH7QFZ2?page=3&annotation=CYRD7A54)) [[@levinTransferLearningDeep2022]]

“n this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables.” ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=PP423JV9)) “These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019).” ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=JRUQ8NT6)) [[@yinTaBERTPretrainingJoint2020]]

In domains where established SSL methods are increasingly dominant, such as computer vision, self-supervised learners are known to extract more transferable features than models trained on labelled data [30, 31]. In this section, we compare supervised pre-training with unsupervised pre-training and find that the opposite is true in the tabular domain. We use the Masked Language Model (MLM) pre-training recently adapted to tabular data [34] and the tabular version of contrastive learning [64]. Since both methods were proposed for tabular transformer architectures, we conduct the experiments with the FT-Transformer model. The inferior performance of self-supervised pre-training might be a consequence of the fact that SSL is significantly less explored and tuned in the tabular domain than in vision or NLP.” ([Levin et al., 2022, p. 7](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=7&annotation=YAULJZUP))

“he most common approach in tabular data is to corrupt data through adding noise [43]. An autoencoder maps corrupted examples of data to a latent space, from which it maps back to uncorrupted data. Through this process, it learns a representation robust to the noise in the input. This approach may not be as effective since it treats all features equally as if features are equally informative. However, perturbing uninformative features may not result in the intended goal of the corruption. A recent work takes advantage of self-supervised learning in tabular data setting by introducing a pretext task [46], in which a de-noising autoencoder with a classifier attached to representation layer is trained on  corrupted data. The classifier’s task is to predict the location of corrupted features. However, this framework still relies on noisy data in the input. Additionally, training a classifier on an imbalanced binary mask for a high-dimensional data may not be ideal to learn meaningful representations.” ([Ucar et al., 2021, p. 2](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=2&annotation=3753MUZQ))


“Masked Language Modeling (MLM) was first proposed for language models by Devlin et al. [20] as a powerful unsupervised learning strategy. MLM involves training a model to predict tokens in text masked at random so that its learned representations contain information useful for reconstructing these masked tokens. In the tabular domain, instead of masking tokens, a random subset of features is masked for each sample, and the masked values are predicted in a multi-target classification manner [34]. In our experiments, we mask one randomly selected feature for each sample, asking the network to learn the structure of the data and form representations from n − 1 features that are useful in producing the value in the n-th feature. For more detail, see Appendix A.” ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=DC87W4J4))


“Contrastive pre-training uses data augmentations to generate positive pairs, or two different augmented views of a given example, and the loss function encourages a feature extractor to map positive pairs to similar features. Meanwhile, the network is also trained to map negative pairs, or augmented views of different base examples, far apart in feature space. We use the implementation of contrastive learning from Somepalli et al. [64]. In particular, we generate positive pairs by applying two data augmentations: CutMix [80] in the input space and Mixup [81] in the embedding space. For more details, see Appendix A” ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=J9CNVZ4T))

“Contrastive learning, in which models are pre-trained to be invariant to reordering, cropping, or other label-preserving “views” of the data [5, 12, 15, 32, 43], is a powerful tool in the vision and language domains that has never (to our knowledge) been applied to tabular data.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=HQX7N9D5)) [[@somepalliSAINTImprovedNeural2021]]

“In this work, we turn the problem of learning representation from a single-view of the data into the one learnt from its multiple views by dividing the features into subsets, akin to cropping in image domain or feature bagging in ensemble learning, to generate different views of the data. Each subset can be considered a different view. We show that reconstructing data from the subset of its features forces the encoder to learn better representation than the ones learned through the existing methods such as adding noise.” ([Ucar et al., 2021, p. 2](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=2&annotation=YC5S6D4S)) [[@ucarSubTabSubsettingFeatures2021]] (Some AE like approach inspired by VIME. Requires change in loss etc.)

“Self-Supervised Learning Self-supervision via a ‘pretext task’ on unlabeled data coupled with finetuning on labeled data is widely used for improving model performance in language and computer vision. Some of the tasks previously used for self-supervision on tabular data include masking, denoising, and replaced token detection. Masking (or Masked Language Modeling(MLM)) is when individual features are masked and the model’s objective is to impute their value [1, 18, 32]. Denoising injects various types of noise into the data, and the objective there is to recover the original values [43, 49]. Replaced token detection (RTD) inserts random values into a given feature vector and seeks to detect the location of these replacements [18, 20]. Contrastive pre-training, where the distance between two views of the same point is minimized while maximizing the distance between two different points [5, 12, 15], is another pretext task that applies to tabular data. In this paper, to the best of our knowledge, we are the first to adopt contrastive learning for tabular data. We couple this strategy with denoising to perform pre-training on a plethora of datasets with varied volumes of labeled data, and we show that our method outperforms traditional boosting methods.” ([Somepalli et al., 2021, p. 3](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=3&annotation=NYSIVQZX)) [[@somepalliSAINTImprovedNeural2021]]

“Existing self-supervised objectives for tabular data include denoising [43], a variation of which was used by VIME [49], masking, and replaced token detection as used by TabTransformer [18]. We find that, while these methods are effective, superior results are achieved by contrastive learning.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=LXSCK4TP))

“In Figure 3, we compare supervised pre-training with contrastive and MLM pre-training strategies and show that supervised pre-training always attains the best average rank. Contrastive pre-training produces better results than training from scratch on the downstream data when using a linear head, but it is still inferior to supervised pre-training. Tabular MLM pretraining also falls behind the supervised strategy and performs comparably to training from scratch in the lower data regimes but leads to a weaker downstream model in the higher data regimes.” ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=WVB866BS))

“Semi-supervised setting We perform 3 sets of experiments with 50, 200, and 500 labeled data points (in each case the rest are unlabeled). See Table 3 for numerical results. In all cases, the pre-trained SAINT model (with both self and intersample attention) performs the best. Interestingly, we note that when all the training data samples are labeled, pre-training does not contribute appreciably, hence the results with and without pre-training are fairly close.” ([Somepalli et al., 2021, p. 8](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=8&annotation=ZJHH2RKT))

“Finally, for the first time for tabular data, we show significant performance improvements by using unsupervised pre-training to predict masked features (see Fig. 2” ([Arik and Pfister, 2020, p. 1](zotero://select/library/items/EH5DCRUW)) ([pdf](zotero://open-pdf/library/items/TPDKX93V?page=1&annotation=X5WBP7CA)) [[@arikTabNetAttentiveInterpretable2020]]

“Note, the pre-training is only applied in semi-supervised scenario. We do not find much benefit in using it when the entire data is labeled. Its benefit is evident when there is a large number of unlabeled examples and a few labeled examples.” ([Huang et al., 2020, p. 4](zotero://select/library/items/MH4GW34I)) ([pdf](zotero://open-pdf/library/items/QYWHEUYE?page=4&annotation=VN52BT3H))

“Furthermore, we observe that when the number of unlabeled data is small as shown in Table 4, TabTransformerRTD performs better than TabTransformer-MLM, thanks to its easier pre-training task (a binary classification) than that of MLM (a multi-class classification). This is consistent with the finding of the ELECTRA paper (Clark et al. 2020).” ([Huang et al., 2020, p. 7](zotero://select/library/items/MH4GW34I)) ([pdf](zotero://open-pdf/library/items/QYWHEUYE?page=7&annotation=HU3JVTCV)) ([[@huangTabTransformerTabularData2020]])

“Tabular self-supervised learning: We propose a decoder architecture to reconstruct tabular features from the TabNet encoded representations. The decoder is composed of feature transformers, followed by FC layers at each decision step. The outputs are summed to obtain the reconstructed features.” ([Arik and Pfister, 2020, p. 5](zotero://select/library/items/EH5DCRUW)) ([pdf](zotero://open-pdf/library/items/TPDKX93V?page=5&annotation=3QTDAR2A))


Limitations of all these approaches? “Recent works enhance tabular ML modeling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=3CDUAZDS)) [[@wangTransTabLearningTransferable]]

In this section we introduce BERT, the main model we are going to use in this work. BERT is proposed by Devlin et al. (2018), with the intent of pre-training deep bidirectional representations from unlabeled text and providing a pre-trained model that can be easily fine-tuned on specific downstream tasks. The rough architecture Fig. 3.6: Architecture of BERT used in this work of our used version is depicted in figure 3.6. The authors propose two different version, a base one and a large one. The one we use is the base version, which is composed of a stack of 12 encoders. Each of these itself use 12 attention heads for computing the Multi-Head Attention, and outputting embeddings of dimension dmodel = 768 for a total of about 110 million parameters. Before BERT the prediction of the next token for an input sequence was the commonly used LM objective. BERT instead is trained on two major tasks: Masked Language Modelling (MLM) and Next Sentence Prediction (NSP). Masked Language Modelling The main goal of this task is to predict the masked tokens of an input sequence. Given for example the sequence "The child plays in the park.", we randomly select and mask 15%17 of the input tokens. This is done by replacing them with a [MASK] token, i.e. "The child [MASK] in the park.". We then train the model on predicting which token was replaced by [MASK]. This allows the model to have conditioning on context tokens from both sides (left and right) of the token to be predicted. (https://epub.ub.uni-muenchen.de/92623/1/MA_Rizzo.pdf)

3.3.1 Masked Language Modeling Unlike language modeling (LM), which aims at predicting the next word given the sequence of previous words, masked language modeling is the task of predicting a percentage of input tokens which are randomly masked. The MLM training objective was chosen over the traditional LM objective because of the bidirectionality of BERT (i.e., BERT uses both left and right context in the sequence to predict the target word). For such models, standard conditional language modeling cannot be used as training objective, as the bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. Hence, BERT is trained using masked language modeling, also referred as a Cloze task in the literature (Taylor, 1953). As shown in Figure 3.11, the prediction is given by the final hidden vectors corresponding to the masked tokens, that are fed into an output softmax over the vocabulary, as in a standard LM. 24 Figure 3.11: Illustration of the masked language modeling (MLM) training objective (Jay Alammar, 2019). Formally, given an input sequence x = [x1, x2, ..., xN ] of N tokens, MLM first selects a random set of k positions (integers between 1 and N) to mask out m = [m1, ..., mk]. The tokens in the selected positions are then replaced with a [MASK] token, resulting in the masked input sequence x masked. BERT eventually learns to predict the original identities of the k masked-out tokens by computing an output word distribution yˆ (h) (h = 1, ..., k) for each one of them. More precisely, given the h-th masked word xmh from sequence x, the MLM loss function is the cross-entropy between the predicted probability distribution yˆ (h) , and the true next word distribution y (h) , which is simply the one-hot vector for xmh . Therefore, we have L (h) MLM(θ) = CE(y (h) , yˆ (h) ) = X w∈V −y (h) w log ˆy (h) w = − log ˆy (h) xmh . (3.13) The overall loss of the sequence is simply the average loss for all the k masked-out tokens in x masked , LMLM(θ) = 1 k X k h=1 L (h) MLM(θ) = 1 k X k h=1 − log ˆy (h) xmh = 1 k X i∈m − log p  xi |x masked . (3.14) The masking procedure works as follows: BERT selects 15% of all WordPiece tokens in each training sequence at random. If the i-th token is chosen, it is replaced with: 1. the [MASK] token 80% of the time; 2. a random token 10% of the time; 3. the unchanged i-th token 10% of the time. 25 The selected words are not always replaced with the [MASK] token because it would then create a mismatch between pre-training and fine-tuning, since the masked token would never be seen before fine-tuning. 3.3.2 Next Sentence Prediction Next sentence prediction (NSP) is a binary classification task in which the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original corpus. This training objective helps understand the relationship between pairs of sentences, which is not directly captured by language modeling but still very important for many downstream tasks such as question-answering (QA) and natural language inference (NLI). This prediction task can can be easily generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and the other 50% of the time it is a random sentence from the corpus (labeled as NotNext). In this case, the final hidden vector corresponding to the [CLS] token is fed into an output softmax over the two possible predictions, as shown in Figure 3.12. Figure 3.12: Illustration of the next sentence prediction (NSP) training objective (Jay Alammar, 2019). (https://matheo.uliege.be/bitstream/2268.2/9060/7/Antoine_Louis_Thesis.pdf)

![[Pasted image 20230415084057.png]]
(https://www.cell.com/cell-systems/pdf/S2405-4712%2821%2900203-9.pdf
The success of deep transformers for machine translation inspired their application to contextual text embedding, that is learning contextual vector embeddings of words and sentences, giving rise to the now widely used Bidirectional Encoder Representations from Transformers (BERT) model in NLP (Devlin et al., 2018). BERT is a deep transformer trained as a masked language model on a large text corpus.)

The training task used by BERT and other recent bidirectional language models. Instead of modeling the probability of a sequence autoregressively, masked language models seek to model the probability of each token given all other tokens. For computational convenience, this is achieved by randomly masking some percentage of the tokens in each (Continued on next page) ll OPEN ACCESS Synthesis 656 Cell Systems 12, 654–669, June 16, 2021 can be used to improve protein function prediction. Finally, we will discuss future directions in protein machine learning and large-scale language modeling. Protein language models distill information from massive protein sequence databases Language models for protein sequence representation learning (Figure 2) have seen a surge of interest following the success of large-scale models in the field of natural language processing (NLP). These models draw on the idea that distributed vector representations of proteins can be extracted from generative models of protein sequences, learned from a large and diverse database of sequences across natural protein space, and thus can capture the semantics, or function, of a given sequence. Here, function refers to any and all properties related to what a protein does. These properties are often subject to evolutionary pressures because these functions must be maintained or enhanced in order for an organism to survive and reproduce. These pressures manifest in the distribution over amino acids present in natural protein sequences and, hence, are discoverable from large and diverse enough sets of naturally occurring sequences. The ability to learn semantics emerges from the distributional hypothesis: tokens (e.g., words, amino acids) that occur in similar contexts tend to carry similar meanings. Language models only require sequences to be observed and are trained to model the probability distribution over amino acids using an autoregressive formulation (Figures 2A and 2B) or masked position prediction formulation (also called a cloze task in NLP, Figure 2C). In autoregressive language models, the probability of a sequence is factorized such that the probability of each token Box 1. Continued minibatch and training the model to recover those tokens. An auxiliary token is added to the vocabulary to indicate that this token has been masked.