
> â€œHyperparameters & Evaluation. Hyperparameter tuning is crucial for a fair comparison, therefore, we use Optuna [1] to optimize the model and pretraining hyperparameters for each method on each dataset. We use the validation subset of each dataset for hyperparameter tuning. The exact search spaces for the hyperparameters of each method are provided in Appendix B.â€ ([[@rubachevRevisitingPretrainingObjectives2022]]., 2022, p. 4)

> "â€œTuning. For every dataset, we carefully tune each modelâ€™s hyperparameters. The best hyperparameters are the ones that perform best on the validation set, so the test set is never used for tuning. For most algorithms, we use the Optuna library (Akiba et al., 2019) to run Bayesian optimization (the Tree-Structured Parzen Estimator algorithm), which is reported to be superior to random search ([[@turnerBayesianOptimizationSuperior2021]]). For the rest, we iterate over predefined sets of configurations recommended by corresponding papers. We provide parameter spaces and grids in supplementary. We set the budget for Optuna-based tuning in terms of iterations and provide additional analysis on setting the budget in terms of time in supplementary.â€ ([[@gorishniyRevisitingDeepLearning2021]], p. 6)"

> Implementation. We fix and do not tune the following hyperparameters: â€¢ early-stopping-rounds = 50 â€¢ od-pval = 0.001 â€¢ iterations = 2000 In Table 17, we provide hyperparameter space used for Optuna-driven tuning (Akiba et al., 2019). We set the task_type parameter to â€œGPUâ€ (the tuning was unacceptably slow on CPU). Evaluation. We set the task_type parameter to â€œCPUâ€, since for the used version of the CatBoost library it is crucial for performance in terms of target metrics.

> â€œThese black-box optimization problems are often solved using Bayesian optimization (BO) methods [19]. BO methods rely on a (probabilistic) surrogate model for the objective function that provides a measure of uncertainty. This model is often a Gaussian process (GP) [55], but other models such as Bayesian neural networks are also commonly used as long as they provide a measure of uncertainty. Using this surrogate model, an acquisition function is used to determine the most promising point to evaluate next, where popular options include expected improvement (EI) [35], knowledge gradient (KG) [18], and entropy search (ES) [31]. There are also other surrogate optimization methods that rely on deterministic surrogate models such as radial basis functions [16, 66], see Forrester et al. [17] for an overview. The choice of surrogate model and acquisition function are both problem-dependent and the goal of this challenge is to compare. Using this surrogate model, an acquisition function is used to determine the most promising point to evaluate next, where popular options include expected improvement (EI) [35], knowledge gradient (KG) [18], and entropy search (ES) [31]. There are also other surrogate optimization methods that rely on deterministic surrogate models such as radial basis functions [16, 66], see Forrester et al. [17] for an overview. different approaches over a large number of different problems. This was the first challenge aiming to find the best black box optimizers specially for ML-related problems.â€ ([[@turnerBayesianOptimizationSuperior2021]] p. 2)

**Tree-Parzen Estimator**
> â€œTPE is another variant of BO that performs well in general and can be utilized for both categorical and continuous types of hyperparameters. Unlike BOGP, which has cubical time complexity, TPE runs in linear time. TPE is suggested if you have a huge hyperparameter space and have a very tight budget for evaluating the cross-validation score. The main difference between TPE and BOGP or SMAC is in the way that it models the relationship between hyperparameters and the cross-validation score. Unlike BOGP or SMAC, which approximate the value of the objective function, or the posterior probability, ğ‘ğ‘(ğ‘¦ğ‘¦|ğ‘¥ğ‘¥), TPE works the other way around. It tries to get the optimal hyperparameters based on the condition of the objective function, or the likelihood probability, ğ‘ğ‘(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦) (see the explanation of Bayes Theorem in the Understanding BO GP section)â€ (Owen, 2022, p. 51)

> â€œExploring Bayesian Optimization 52 In other words, unlike BOGP or SMAC, which construct a predictive distribution over the objective function, TPE tries to utilize the information of the objective function to model the hyperparameter distributions. To be more precise, when the optimization problem is in the form of a minimization problem, ğ‘ğ‘(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦) is defined as follows: ğ‘ğ‘(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦) = ğ‘™ğ‘™(ğ‘¥ğ‘¥) ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¦ğ‘¦ < ğ‘¦ğ‘¦âˆ— ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘”ğ‘”(ğ‘¥ğ‘¥) ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¦ğ‘¦ â‰¥ ğ‘¦ğ‘¦âˆ— Here, ğ‘™ğ‘™(ğ‘¥ğ‘¥) and ğ‘”ğ‘”(ğ‘¥ğ‘¥) are utilized when the value of the objective function is lower or higher than the threshold, ğ‘¦ğ‘¦âˆ— , respectively. There is no specific rule on how to choose the threshold, ğ‘¦ğ‘¦âˆ— . However, in the Hyperopt and Microsoft NNI implementations, this threshold is chosen based on the TPEâ€™s hyperparameter, ğ›¾ğ›¾, and the number of observed points in D up to the current trial. The definition of ğ‘ğ‘(ğ‘¥ğ‘¥|ğ‘¦ğ‘¦) tells us that TPE has two models that act as the learning algorithm based on the value of the objective function, which is ruled by the threshold, ğ‘¦ğ‘¦âˆ— . When the distribution of hyperparameters is continuous, TPE will utilize Gaussian mixture models (GMMs), along with the EI acquisition function, to suggest the next set of hyperparameters to be tested. If the continuous distribution is not a Gaussian distribution, then TPE will convert it to mimic the Gaussian distribution. For example, if the specified hyperparameter distribution is the uniform distribution, then it will be converted into a truncated Gaussian distribution. The probabilities of the different possible outcomes for the multinomial distribution within the GMM, and the mean and variance values for the normal distribution within the GMM, are generated by the adaptive Parzen estimator. This estimator is responsible for constructing the two probability distributions, ğ‘™ğ‘™(ğ‘¥ğ‘¥) and ğ‘”ğ‘”(ğ‘¥ğ‘¥), based on the mean and variance of the normal hyperparameter distribution, as well as the hyperparameter value of all observed points in D up to the current trial. When the distribution is categorical or discrete, TPE will convert the categorical distribution into a re-weighted categorical and use weighted random sampling, along with the EI acquisition function, to suggest the expected best set of hyperparameters. The weights in the random sampling procedure are generated based on the historical counts of the hyperparameter value. The EI acquisition function definition in TPE is a bit different from the definition we learned about in the Introducing BO section. In TPE, we are using Bayes Theorem when deriving the EI formula. The simple formulation of the EI acquisition function in TPE is defined as follows: ğ¸ğ¸ğ¸ğ¸(ğ‘¥ğ‘¥) âˆ ğ‘™ğ‘™(ğ‘¥ğ‘¥) ğ‘”ğ‘”(ğ‘¥ğ‘¥) The proportionality defined here tells us that to get a high value of EI, we need to get a high ğ‘™ğ‘™(ğ‘¥ğ‘¥) ğ‘”ğ‘”(ğ‘¥ğ‘¥) ratio. In other words, when the optimization problem is in the form of a minimization problem, the EI acquisition function must suggest more hyperparameters from ğ‘™ğ‘™(ğ‘¥ğ‘¥) over ğ‘”ğ‘”(ğ‘¥ğ‘¥) . It is the other way around when the optimization problem is in the form of a maximization problem. For example, when we use accuracy to measure the performance of our classification model, then we should sample more hyperparameters from ğ‘”ğ‘”(ğ‘¥ğ‘¥) over ğ‘™ğ‘™(ğ‘¥ğ‘¥)â€ (Owen, 2022, p. 52)

> â€œUnderstanding TPE 53 To summarize, TPE works as follows. Note that the following procedure describes how TPE works for the minimization problem. This procedure replaces Steps 7 to 11 in the Introducing BO section: 6. (The first few steps are the same as we saw earlier). 7. Divide pairs of hyperparameter values and cross-validation scores in D into two groups based on the threshold, ğ‘¦ğ‘¦âˆ—, namely below and above groups (see Figure 4.19). 8. Sample the next set of hyperparameters by utilizing the EI acquisition function: I. For each group, calculate the probabilities, means, and variances for the GMM using the adaptive Parzen estimator (if itâ€™s a continuous type) or weights for random sampling (if itâ€™s a categorical type). II. For each group, fit the GMM (if itâ€™s a continuous type), or perform random sampling (if itâ€™s a categorical type), to sample which hyperparameters will be passed to the EI acquisition function. III.For each group, calculate the probability of those samples being good samples (for the below group), or the probability of those samples being bad samples (for the above group). IV. Get the expected optimal set of hyperparameters based on the EI acquisition function. 9. Compute the cross-validation score using the objective function, f, based on the output from Step 8. 10. Add the hyperparameters and cross-validation score pair from Step 8 and Step 9 to set D. 11. Repeat Steps 7 to 10 until the stopping criteria have been met. 12. (The last few steps are the same as we saw earlier): Figure 4.20 â€“ Illustration of groups division in TPâ€ (Owen, 2022, p. 53)

 > Based on the stated procedure and the preceding plot, we can see that, unlike BOGP or SMAC, which constructs a predictive distribution over the objective function, TPE tries to utilize the information of the objective function to model the hyperparameter distributions. This way, we are not only focusing on the best-observed points during the trials â€“ we are focusing on the distribution of the best-observed points instead. You may be wondering why the Tree-structured term is within the TPE methodâ€™s name. This term refers to the conditional hyperparameters that we discussed in the previous section. This means that there are hyperparameters in the space that will only be utilized when a certain condition is met. We will see what a tree-structured or conditional hyperparameter space looks like in Chapter 8, Hyperparameter Tuning via Hyperopt, and Chapter 9, Hyperparameter Tuning via Optuna. One of the drawbacks that TPE has is that it may overlook the interdependencies among hyperparameters in a certain space since the Parzen estimators work univariately. However, this is not the case for BOGP or SMAC, since the surrogate model is constructed based on the configurations in the hyperparameter space. Thus, they can take into account the interdependencies among hyperparameters. Fortunately, there is an implementation of TPE that overcomes this drawback. The Optuna package provides the multivariate Tâ€ ([[@owenHyperparameterTuningPython2022]], p. 54)

**Why hyperparam tuning is necessary**
> â€œHyperparameter tuning leads to uncontrolled variance on a benchmark [Bouthillier et al., 2021], especially with a small budget of model evaluations. We design a benchmarking procedure that jointly samples the variance of hyperparameter tuning and explores increasingly high budgets of model evaluations. It relies on random searches for hyper-parameter tuning [Bergstra et al., 2013]. We use hyperparameter search spaces from the Hyperopt-Sklearn [Komer et al., 2014] when available, from the original paper when possible, and from Gorishniy et al. [2021] for MLP, Resnet and XGBoost (see A.3). We run a random search of â‰ˆ 400 iterations per dataset, on CPU for tree-based models and GPU for neural networks (more details in A.3). To study performance as a function of the number n of random search iterations, we compute the best hyperparameter combination on the validation set on these n iterations (for each model and dataset), and evaluate it on the test set. We do this 15 times while shuffling the random search order at each time. This gives us bootstrap-like estimates of the expected test score of the best (on the validation set) model after each number of random search iterations. In addition, we always start the random searches with the default hyperparameters of each model. In A.7, we show that using Bayesian optimization instead of random search does not seem to change our results.â€ ([[@grinsztajnWhyTreebasedModels2022]], 2022, p. 4)

**Visualizations**

- Visualize results https://github.com/LeoGrin/tabular-benchmark
![[comparsion-of-results.png]]
