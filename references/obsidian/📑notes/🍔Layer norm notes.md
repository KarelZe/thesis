
<mark style="background: #FF5582A6;">Each Transformer block can be expressed as: XA = LayerNorm(MultiheadAttention(X, X)) + X XB = LayerNorm(PositionFFN(XA)) + XA where X is the input of the Transformer block and XB is the output of the Transformer block. Note that the MultiheadAttention() function accepts two argument tensors, one for query and the other for key-values. If the first argument and second argument is the same input tensor, this is the MultiheadSelfAttention mechanism.” ([Tay et al., 2022, p. 4](zotero://select/library/items/SLWQVGHF)) ([pdf](zotero://open-pdf/library/items/PDDJFS9K?page=4&annotation=9YTWUXLT))([[@tayEfficientTransformersSurvey2022]])</mark>

“<mark style="background: #FFF3A3A6;">For Transformer, it is not easy to train stacked layers on neither the encoder-side nor the decoderside. Stacking all these sub-layers prevents the efficient information flow through the network, and probably leads to the failure of training. Residual connections and layer normalisation are adopted for a solution. Let F be a sub-layer in encoder or decoder, and θl be the parameters of the sub-layer.</mark>  ([[@wangLearningDeepTransformer2019]])

<mark style="background: #ADCCFFA6;">“2.2 On the Importance of Pre-Norm for Deep Residual Network The situation is quite different when we switch to deeper models. More specifically, we find that prenorm is more efficient for training than post-norm if the model goes deeper. This can be explained by seeing back-propagation which is the core process to obtain gradients for parameter update. Here we take a stack of L sub-layers as an example. Let E be the loss used to measure how many errors occur in system prediction, and xL be the output of the topmost sub-layer. For post-norm Transformer, given a sub-layer l, the differential of E with respect to xl can be computed by the chain rule, and we have ∂E ∂xl = ∂E ∂xL × L−1 ∏ k=l ∂LN(yk) ∂yk × L−1 ∏ k=l ( 1 + ∂F (xk; θk) ∂xk ) (5) where ∏L−1 k=l ∂ LN(yk ) ∂yk means the backward pass of the layer normalisation, and ∏L−1 k=l (1 + ∂F(xk;θk) ∂xk ) means the backward pass of the sub-layer with the residual connexion. Likewise, we have the gradient for pre-norm 4: ∂E ∂xl = ∂E ∂xL × ( 1+ L−1 ∑ k=l ∂F (LN(xk); θk) ∂xl ) (6) Obviously, Eq. (6) establishes a direct way to pass error gradient ∂E ∂xL from top to bottom. Its merit lies in that the number of product items on the right side does not depend on the depth of the stack. In contrast, Eq. (5) is inefficient for passing gradients back because the residual connexion is not 3We need to add an additional function of layer normalisation to the top layer to prevent the excessively increased value caused by the sum of unnormalized output. 4For a detailed derivation, we refer the reader to Appendix A. a bypass of the layer normalisation unit (see Figure 1(a)). Instead, gradients have to be passed through LN(·) of each sub-layer. It in turn introduces term ∏L−1 k=l ∂ LN(yk ) ∂yk into the right hand side of Eq. (5), and poses a higher risk of gradient vanishing or exploring if L goes larger. This was confirmed by our experiments in which we successfully trained a pre-norm Transformer system with a 20-layer encoder on the WMT English-German task, whereas the post-norm Transformer system failed to train for a deeper encoder (Section 5.1).” ([Wang et al., 2019, p. 1812](zotero://select/library/items/BJ43W6ZQ)) ([pdf](zotero://open-pdf/library/items/HVE8Q5EQ?page=3&annotation=AXKHQZJR)) [[@wangLearningDeepTransformer2019]]</mark>

<mark style="background: #FFB86CA6;">“Residual connections (He et al., 2016a) were first introduced to facilitate the training of deep convolutional networks, where the output of the `-th layer F` is summed with its input: x`+1 = x` + F`(x`). (1) The identity term x` is crucial to greatly extending the depth of such networks (He et al., 2016b). If one were to scale x` by a scalar λ`, then the contribution of x` to the final layer FL is (∏L−1 i=` λi)x`. For deep networks with dozens or even hundreds of layers L, the term ∏L−1 i=` λi becomes very large if λi > 1 or very small if  for enough i. When backpropagating from the last layer L back to `, these multiplicative terms can cause exploding or vanishing gradients, respectively. Therefore they fix λi = 1, keeping the total residual path an identity map.” (Nguyen and Salazar, 2019, p. 2) [[@nguyenTransformersTearsImproving2019]]</mark>

<mark style="background: #FFB86CA6;">layer norm is the same as batch norm except that it normalises the feature dimension ([[@zhangDiveDeepLearning2021]] p. 423)</mark>


<mark style="background: #D2B3FFA6;">“Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalise the activities of the neurons.” (Ba et al., 2016, p. 1)

“In this paper, we transpose batch normalisation into layer normalisation by computing the mean and variance used for normalisation from all of the summed inputs to the neurons in a layer on a single training case” (Ba et al., 2016, p. 1)

“This paper introduces layer normalisation, a simple normalisation method to improve the training speed for various neural network models. Unlike batch normalisation, the proposed method directly estimates the normalisation statistics from the summed inputs to the neurons within a hidden layer so the normalisation does not introduce any new dependencies between training cases. We show that layer normalisation works well for RNNs and improves both the training time and the generalisation performance of several existing RNN models.” (Ba et al., 2016, p. 2)
</mark>

<mark style="background: #ABF7F7A6;">The layer normalisation computes the average and standard deviation of the output activations of a given sublayer and normalises them accordingly. This guarantees that the input yt of the following sublayer is well conditioned, i.e., that yT t 1 = 0 and yT t yt = √d.” ([Sukhbaatar et al., 2019, p. 3](zotero://select/library/items/A7XG93GC)) ([pdf](zotero://open-pdf/library/items/I76F65M4?page=3&annotation=AZJKFERG))</mark>

<mark style="background: #D2B3FFA6;">“the impact of the layer normalisation positions [32, 33]. There are currently two major layer normalisation positions in Transformers: Pre-Layer Normalisation (Pre-LN) and Post-Layer Normalisation (Post-LN). Pre-LN applies the layer normalisation to an input for each sub-layer, and Post-LN places the layer normalisation after each residual connexion. The original Transformer [28] employs PostLN. However, recent studies often suggest using Pre-LN [32, 2, 5] because the training in Post-LN with deep Transformers (e.g., ten or more layers) often becomes unstable, resulting in useless models. Figure 1 shows an actual example; loss curves of training 18L-18L Transformer encoder-decoders on a widely used WMT English-to-German machine translation dataset. Here, XL-Y L represents the number of layers in encoder and decoder, where X and Y correspond to encoder and decoder, respectively. These figures clearly show that 18L-18L Post-LN Transformer encoder-decoder fails to train the model. However, in contrast, Liu et al. [13] reported that Post-LN consistently achieved better performance than Pre-LN in the machine translation task when they used 6L-6L (relatively shallow) Transformers.” ([Takase et al., 2022, p. 2](zotero://select/library/items/9867KZ4B)) ([pdf](zotero://open-pdf/library/items/JBR6QM9N?page=2&annotation=IGTZ6SLR)) ([[@takaseLayerNormalizationsResidual2022]])</mark>

<mark style="background: #BBFABBA6;">“Residual connexion and layer normalisation Besides the two sub-layers described above, the residual connexion and layer normalisation are also key components to the Transformer. For any vector v, the layer normalisation is computed as LayerNorm(v) = γ v−μ σ + β, in which μ, σ are the mean and standard deviation of the elements in v, i.e., μ = 1 d ∑d k=1 vk and σ2 = 1 d ∑d k=1(vk − μ)2. Scale γ and bias vector β are parameters” (Xiong et al., 2020, p. 3)
</mark>

<mark style="background: #CACFD9A6;">Update residual stream, refine inputs from previous layers? See [[@elhage2021mathematical]]</mark>

<mark style="background: #ADCCFFA6;">“First, we apply layer normalisation before the selfattention and feedforward blocks instead of after. This small change has been unanimously adopted by all current Transformer implementations because it leads to more effective training (Baevski and Auli, 2019; Xiong et al., 2020).” ([Narang et al., 2021, p. 4](zotero://select/library/items/LMH6LKL2)) ([pdf](zotero://open-pdf/library/items/EA82UUN6?page=4&annotation=K6BH3JER))</mark>

<mark style="background: #BBFABBA6;">“Inspired by He et al. (2016b), we apply LAYERNORM immediately before each sublayer (PRENORM): x\`+1 = x\` + F\`(LAYERNORM(x\`)). (3) This is cited as a stabiliser for Transformer training (Chen et al., 2018; Wang et al., 2019) and is already implemented in popular toolkits (Vaswani et al., 2018; Ott et al., 2019; Hieber et al., 2018), though not necessarily used by their default recipes. Wang et al. (2019) make a similar argument to motivate the success of PRENORM in training very deep Transformers. Note that one must append an additional normalisation after both encoder and decoder so their outputs are appropriately scaled.” ([Nguyen and Salazar, 2019, p. 2](zotero://select/library/items/EDLX35I6)) ([pdf](zotero://open-pdf/library/items/2PAADYR7?page=2&annotation=HGX5V3N6))</mark>

<mark style="background: #FFF3A3A6;">“Our analysis starts from the observation: the original Transformer (referred to as Post-LN) is less robust than its Pre-LN variant2 (Baevski and Auli, 2019; Xiong et al., 2019; Nguyen and Salazar, 2019).  We recognise that gradient vanishing issue is not the direct reason causing such difference, since fixing this issue alone cannot stabilise PostLN training. It implies that, besides unbalanced gradients, there exist other factors influencing model training greatly 
” (Liu et al., 2020, p. 1) [[@liuUnderstandingDifficultyTraining2020]]</mark>

<mark style="background: #ABF7F7A6;">“Different orders of the sub-layers, residual connexion and layer normalisation in a Transformer layer lead to variants of Transformer architectures. One of the original and most popularly used architecture for the Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2018) follows “selfattention (FFN) sub-layer → residual connexion → layer normalisation”, which we call the Transformer with PostLayer normalisation (Post-LN Transformer), as illustrated in Figure 1.” ([Xiong et al., 2020, p. 3](zotero://select/library/items/JKKHGAAC)) ([pdf](zotero://open-pdf/library/items/2E5NZTRP?page=3&annotation=NNLEUS79)) [[@xiongLayerNormalizationTransformer2020]]</mark>

<mark style="background: #D2B3FFA6;">“We conjecture this has caused past convergence failures (Popel and Bojar, 2018; Shazeer and Stern, 2018), with LAYERNORMs in the residual path acting similarly to λi 6= 1; furthermore, warmup was needed to let LAYERNORM safely adjust scale during early parts of training.” (Nguyen and Salazar, 2019, p. 2)</mark>

<mark style="background: #ABF7F7A6;">Example for brittle training of Transformers with Adam [[@shazeerAdafactorAdaptiveLearning2018]]</mark>

<mark style="background: #ADCCFFA6;">“As in Figure 7, at initialisation, a Pre-LN layer has roughly the same dependency on its residual branch and any previous layer, whereas a Post-LN layer has a stronger dependency on its residual branch (more discussions are elaborated in Section 4.1). We find that strong dependencies of Post-LN amplify fluctuations brought by parameter changes and destabilise the training (as in Theorem 2 and Figure 4). Besides, the loose reliance on residual branches in Pre-LN generally limits the algorithm’s potential and often produces inferior models.” (Liu et al., 2020, p. 2)</mark>

<mark style="background: #FF5582A6;">
“As mentioned earlier, the Transformer architecture makes use of layer normalisation and skip connections. The former normalises each input in the batch to have zero mean and unity variance. Skip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor.” ([Tunstall, 2022, p. 71](zotero://select/library/items/HYPN9IJ9)) ([pdf](zotero://open-pdf/library/items/TVF29AAM?page=95&annotation=TF8RNHLQ))
</mark>

<mark style="background: #FF5582A6;">“Post layer normalisation This is the arrangement used in the Transformer paper; it places layer normalisation in between the skip connections. This arrangement is tricky to train from scratch as the gradients can diverge. For this reason, you will often see a concept known as learning rate warm-up, where the learning rate is gradually increased from a small value to some maximum value during training.” ([Tunstall, 2022, p. 71](zotero://select/library/items/HYPN9IJ9)) ([pdf](zotero://open-pdf/library/items/TVF29AAM?page=95&annotation=F4AN27WP))
</mark>

<mark style="background: #FF5582A6;">“Pre layer normalisation This is the most common arrangement found in the literature; it places layer normalisation within the span of the skip connections. This tends to be much more stable during training, and it does not usually require any learning rate warm-up.” ([Tunstall, 2022, p. 71](zotero://select/library/items/HYPN9IJ9)) ([pdf](zotero://open-pdf/library/items/TVF29AAM?page=95&annotation=4RWZA2EP))</mark>

<mark style="background: #D2B3FFA6;">“To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimisation and bring more hyperparameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalisation matters. Specifically, we prove with mean field theory that at initialisation, for the original-designed Post-LN Transformer, which places the layer normalisation between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalisation is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialisation. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.” ([Xiong et al., 2020, p. 1](zotero://select/library/items/JKKHGAAC)) ([pdf](zotero://open-pdf/library/items/2E5NZTRP?page=1&annotation=WV34HEM3)) [[@xiongLayerNormalizationTransformer2020]]
</mark>