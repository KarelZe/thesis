Previous studies on trade classification with machine learning made arbitrary selections of methods, failing to account for the latest advancements in the field (cp. cref-[[üë™Related Work]]).  In this thesis, we perform a succinct discussion to select a set of supervised classifiers based on empirical evidence. To guide our discussion, we establish the following requirements a classifier must simulatenously meet:
-  *performance:* The approach must deliver state-of-the-art performance in tabular classification tasks. Trades are typically provided as tabular datasets, consisting of rows representing instances and columns representing features. The classifier must be suited for probabilistic classification on tabular data.
-  *scalability:* The approach must be able to scale to datasets with $>$ 10 Mio. samples. Due to the high trading activity and long data history, datasets may contain millions of samples, so classifiers must be able to handle large quantities of trades.
- *extensibility:* The approach must be extendable to train on partially-labelled trades. Most definitions of the trade initiator apply only to a subset of trades (e.g., certain order types), but excluded trades can still provide be valuable in training a classifier. The classifier must support training on unlabelled and labelled trades.

Trade classification, as we framed it, fits into supervised learning on tabular data, which is comprehensively covered by the research community with several studies reviewing and benchmarking newly proposed approaches against established machine learning methods.

**Shallow Tree-Based Ensembles**
Traditionally, tree-based ensembles, in particular gls-gbrt, have dominated modelling on tabular data with regard to predictive performance ([[@grinsztajnWhyTreebasedModels2022]]24--25) and ([[@kadraWelltunedSimpleNets2021]]7) and ([[@borisovDeepNeuralNetworks2022]]14). At its core, tree-based ensembles combine the estimates of individual decision trees into an ensemble to obtain a more accurate prediction. For gls-gbrt ([[@friedmanGreedyFunctionApproximation2001]]9) the ensemble is constructed by sequentially adding small-sized trees into the ensemble that improve upon the error of the previous trees.  Closely related to gradient-boosted trees are random forests of ([[@breimanRandomForests2001]]6). Random forests fuse decision trees with the bagging principle by growing multiple deep decision trees on random subsets of data and aggregate the individual estimates. 

([[@grinsztajnWhyTreebasedModels2022]]7-9) trace back the strong performance of tree-based ensembles in tabular classification tasks to being a non-rotationally-invariant learner and tabular data being non-invariant to rotation. By intuition, rows and columns in a tabular datasets may be arranged in an arbitrary order, but each features carries a distinct meaning, which implies that feature values cannot be simply rotated without affecting the the overall meaning. Thus, tabular data is non-invariant by rotation. So are tree-based ensembles, as they attend to each feature separately. This property also strengthens the model's ability to uninformative features ([[@grinsztajnWhyTreebasedModels2022]]8-9).

([[@ronenMachineLearningTrade2022]]13--14) have unparalleled success in classifying trades through random forests. Due to the framing as a *probabilistic* classification task, random forests are not optimal. This is because decision trees yield poorly calibrated probability estimates, which carry into the ensemble ([[@tanhaSemisupervisedSelftrainingDecision2017]]356--360). Gradient boosting is unaffected by this problem, scales to large data set due to the availability of highly optimised implementations that approximate the construction of ensemble members, and is extensible to learn on unlabelled and labelled instances simultaneously. The state-of-the art performance in tabular classification tasks, together with its ability to scale and extend, makes it a perfect candidate for trade classification.

**Deep Neural Networks**
Neural networks have emerged as powerful models for tabular data with several publications claiming to surpass gradient-boosted trees in terms of performance. For brevity, we focus on two lines of research, regularized networks and attention-based networks, which have attracted significant interest in the field. A comprehensive overview of tabular deep learning can be found in ([[@borisovDeepNeuralNetworks2022]]1--22).

*Regularized Networks*
Among the simplest neural networks are gls-mlp, which consists of multiple linear layers with non-linear activation functions in between. ([[@kadraWelltunedSimpleNets2021]]9--10) among others, advocate for the use of vanilla gls-mlp with an extensive mix of regularization techniques, such as dropout ([[@srivastavaDropoutSimpleWay]]) or residual connections ([[@heDeepResidualLearning2015]]), and report performance improvements over complex tabular-specific architectures or gradient-boosted trees. Regularization is expected to enhance generalization performance, but the benefit is non-exclusive to gls-mlp. Conversely, when regularization is equally applied to tabular-specific architectures, the effect reverses and multiple works including ([[@gorishniyRevisitingDeepLearning2021]]7) and ([[@grinsztajnWhyTreebasedModels2022]]5) report that regularized gls-mlp actually trail the performance of specialized tabular-specific architectures. To meet our performance criterion, we focus on specialized architectures, particularly attention-based networks, while still emphasizing the importance of careful regularization and optimization.

*Attention-Based Networks*

Another emerging strand of research focuses on neural networks with an attention mechanism. Attention, intuitively, allows to gather information from the immediate context and learn relationships between features or between features and instances.  It has been incorporated into various architectures, including the tree-like TabNet ([[@arikTabnetAttentiveInterpretable2020]]), and several Transformer-based architectures including TabTransformer ([[@huangTabTransformerTabularData2020]]2--3), Self-Attention and Intersample Attention Transformer (SAINT) ([[@somepalliSaintImprovedNeural2021]]4--5), Non-Parametric Transformer ([[@kossenSelfAttentionDatapointsGoing2021]]3--4), and FT-Transformer ([[@gorishniyRevisitingDeepLearning2021]]4--5). 

TabNet ([[@arikTabnetAttentiveInterpretable2020]]3--5), fuses the concept of decision trees with attention. Similar to growing a decision tree, several sub-networks are used to process the input in a sequential, hierarchical fashion. Sequential attention, a variant of attention, is used to decide which features to select in each step. The output of TabNet is the aggregate of all sub-networks. Its poor performance in independent comparisons e.g., ([[@kadraWelltunedSimpleNets2021]]7) and ([[@gorishniyRevisitingDeepLearning2021]]7), raises doubts about its usefulness. 

gls-SAINT uses a specialized attention mechanism, the intersample attention, to perform attention over both columns and rows ([[@somepalliSaintImprovedNeural2021]]4--5). Applied to our setting, the model would contextualize information from the trade itself, but also from neighbouring trades, which would be an unfair advantage over classical trade classification rules. Similarly, the Non-Parametric Transformer of ([[@kossenSelfAttentionDatapointsGoing2021]]3--4) uses the entire data set as a context, which rules out the application in our work. 

Differently, TabTransformer ([[@huangTabTransformerTabularData2020]]2--3) performs attention per sample on categorical features-only. All numerical features are processed in a separate stream, which breaks correlations between categorical and numerical features ([[@somepalliSaintImprovedNeural2021]]2). Most importantly though, most features in trade datasets are numerical. As such, trade classification would hardly profit from the Transformer architecture, causing the model to collapse to a vanilla gls-MLP. A more comprehensive approach is provided by ([[@gorishniyRevisitingDeepLearning2021]]4--5) in the form FT-Transformer, which is a Transformer-based architecture, that processes both numerical inputs and categorical input in the Transformer blocks featuring an attention mechanism. Since it achieved state-of-the-art performance in several empirical studies, like ([[@grinsztajnWhyTreebasedModels2022]]5), and is non-rotational invariant, we further consider FT-Transformer in our empirical study. Being based on the Transformer architecture, FT-Transformer naturally scales to large amounts of data and can utilize unlabelled data through self-training procedures. 

The findings ([[@ronenMachineLearningTrade2022]]50) do not support the use of neural networks in trade classification. But due to the lack of details regarding the model architecture, regularization techniques, and training insights, it is necessary to reevaluate these findings in the context of trade classification.

To summarize, our study considers gradient boosting and the FT-Transformer, each trained on labelled or partially labelled trades. This comparison is particularly appealing, as it enables a multi-faceted comparison of wide tree-based ensembles versus deep neural networks, as well as supervised versus semi-supervised methods.

#gbm #transformer #supervised-learning #deep-learning 

**Notes:**
[[üç™Selection of semisupervised Approaches notes]]
