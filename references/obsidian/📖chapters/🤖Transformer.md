
## TOC
- Check own explanations againstğŸ§¨: https://www.youtube.com/watch?v=8zAP2qWAsKg&t=2410s -> Multi-head attention allows the model to learn different representatins w/o expanding the vector's (embeddings) dimension


![[ğŸ—¼Overview Transformer]]

Embedding components:
![[ğŸ›ŒToken Embedding]]

![[ğŸ§µPositional Embedding]]

[[ğŸ…°ï¸Attention]]
(To be done)


![[ğŸ±Position-wise FFN]]
![[ğŸ”—Residual connections]]
![[ğŸ”Layer Norm]]

ğŸ“Š**Transformers For Tabular Data**

![[ğŸ’¤Embeddings For Tabular Data]]


![[ğŸ¤–TabTransformer]]
![[ğŸ¤–FTTransformer]]

**Notes:**
[[ğŸ¤–Transformer notes]]
