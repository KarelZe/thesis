
## TOC
- Check own explanations against🧨: https://www.youtube.com/watch?v=8zAP2qWAsKg&t=2410s -> Multi-head attention allows the model to learn different representatins w/o expanding the vector's (embeddings) dimension


![[🗼Overview Transformer]]

Embedding components:
![[🛌Token Embedding]]

![[🧵Positional Embedding]]

[[🅰️Attention]]
(To be done)


![[🎱Position-wise FFN]]
![[🔗Residual connections]]
![[🍔Layer Norm]]

📊**Transformers For Tabular Data**

![[💤Embeddings For Tabular Data]]


![[🤖TabTransformer]]
![[🤖FTTransformer]]

**Notes:**
[[🤖Transformer notes]]
