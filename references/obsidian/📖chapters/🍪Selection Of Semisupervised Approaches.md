## General
Our goal is to extend gradient-boosted trees and Transformers for the semi-supervised setting to make use of the abundant, unlabelled trade data. We are aimed to make minimally intrusive changes to maintain a fair comparison with the supervised counterparts.

## Self-Training + Gradient-Boosting

The success of supervised gradient-boosting, led to the development of gradient-boosting for the semi-supervised setting. An early work of ([[@dalche-bucSemisupervisedMarginBoost2001]]3--4) explores replacing supervised weak learners, i.e., regression trees, with semi-supervised weak learners, i.e., mixture models, and minimises a loss function over labelled and unlabelled instances. Another line of research, including ([[@bennettExploitingUnlabeledData2002]]290--291) or ([[@mallapragadaSemiBoostBoostingSemiSupervised2009]]2003--2004) retain supervised weak learners to generate pseudo labels of unlabelled instances per iteration. True labelled and pseudo-labelled data is then used in training weak learners of subsequent iterations. Approaches differ with regard to the selection criterion of the pseudo-labelled instances. Both lines of work, however, require changes to the boosting procedure or the base learners. 

An alternative is to pair gradient-boosting with self-training. Self-training is a wrapper algorithm around a supervised classifier, that incorporates its most-confident predictions of unlabelled instances into the training procedure ([[@yarowskyUnsupervisedWordSense1995]]190). Other than before, pseudo-labels are generated only from the fully-fledged ensemble and the ensemble is grown multiple times. Being a model-agnostic wrapper, it does not change the classifier and ensures maximum comparability. Its prevalent adoption in literature makes it a compelling option for the application in semi-supervised trade classification.

Both lines of work, however, require changes to the boosting procedure or the base learners. An alternative is to to pair gradient-boosting with self-training. Self-training is a wrapper algorithm around a supervised classifier, that incorporates its most-confident predictions of unlabelled instances into the training procedure ([[@yarowskyUnsupervisedWordSense1995]]190). Other than before ... drives computational cost / as a caveat... Being a model-agnostic wrapper, it does not change the classifier and ensures maximum comparability. Its prevalent adoption in literature makes it a compelling option for the application in semi-supervised trade classification.

## Pre-Training + Transformer
Whilst Transformers could be combined with self-training, a more promising approach is to pre-train Transformers on unlabelled data, and then fine-tune the network on the remaining labelled instances. Various studies report unanimously performance improvements from pre-training tabular Transformers, including ([[@somepalliSAINTImprovedNeural2021]]8), ([[@arikTabNetAttentiveInterpretable2020]]7), and ([[@huangTabTransformerTabularData2020]]7). 

Until now we assumed the parameters e.g., weights and biases, of the Transformer to be initialized randomly. The joint goal of pre-training objectives is to initialize a neural network with weights that already capture expressive representations of the input and thereby improve generalization performance over a random initialization when fine-tuning on a specific task ([[@erhanWhyDoesUnsupervised]]12). The training is now decomposed into two stages: in the first stage the model is trained w.r.t. the pre-training objective to obtain the parameter estimates on unlabelled instances, and in the second stage the Transformer is initialized with the parameters and then finetuned on the labelled dataset. Particularly beneficial, general embeddings can be learned during pre-training, even if the true label, i.e., the trade initiator, is unknown.

Pre-training objectives for tabular data differ vastly in their methodology and are often direct adaptions from other domains and include pre-training through gls-mlm ([[@devlinBERTPretrainingDeep2019]]4174), gls-rtd ([[@radfordImprovingLanguageUnderstanding]]1--3), or contrastive learning ([[@chenSimpleFrameworkContrastive2020]]2). As such, ([[@huangTabTransformerTabularData2020]]7) adapt gls-mlm, whereby features are randomly masked and the objective is to reconstruct the original input. Pre-training by gls-rtd aims to identify randomly replaced features and recover a binary mask used for replacement ([[@huangTabTransformerTabularData2020]]7). ([[@bahriSCARFSelfSupervisedContrastive2022]]3) and ([[@yoonVIMEExtendingSuccess2020]]4--5) reconstruct both the binary feature mask and the original input simultaneously. ([[@somepalliSAINTImprovedNeural2021]]3) alter the methodology of ([[@yoonVIMEExtendingSuccess2020]]4--5) through a contrastive loss function. 

With a multitude of methods, tested on differing datasets and neural architectures, a fair comparison between pre-training methods is tedious.  However, ([[@rubachevRevisitingPretrainingObjectives2022]]4) provide guidance for selecting objectives in their work. Among the pre-training objectives that they convincingly benchmarked, the gls-rtd objective performed among the best. The gls-rtd objective is easy to optimize, unsupervised, and leaves the model architecture unaltered, which makes gls-rtd a compelling choice for pre-training on unlabelled data.

The next chapter covers self-training in detail.

**Notes:**
[[üç™Selection of semisupervised Approaches notes]]
