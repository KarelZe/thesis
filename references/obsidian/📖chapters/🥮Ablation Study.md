3.3 ABLATIONS AND TRICKS Weâ€™ve tried various forms of regularisation to see what can induce networks to generalise better on our datasets. Here we present the data efficiency curves on a particular dataset S5 for a variety of interventions: full-batch gradient descent, stochastic gradient descent, large or small learning rates, 3 residual dropout Srivastava et al. (2014), weight decay Loshchilov & Hutter (2017) and gradient noise Neelakantan et al. (2015). The results are shown in Figure 2 (left). We find that adding weight decay has a very large effect on data efficiency, more than halving the amount of samples needed compared to most other interventions. We found that weight decay towards the initialisation of the network is also effective, but not quite as effective as weight decay towards the origin. This makes us believe that the prior, that approximately zero weights are suitable for small algorithmic tasks, explains part, but not all of the superior performance of weight decay. Adding some noise to the optimisation process (e.g. gradient noise from using minibatches, Gaussian noise applied to weights before or after computing the gradients) is beneficial for generalisation, consistent with the idea that such noise might induce the optimisation to find flatter minima that generalise better. We found that learning rate had to be tuned in a relatively narrow window for the generalisation to happen (within 1 order of magnitude).

(https://arxiv.org/pdf/2201.02177.pdf)

perform ablation study (https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)) when making important changes to the architecture. This has been done in [[@gorishniyRevisitingDeepLearning2021]].

- [[@melisStateArtEvaluation2017]] investigate hyperparam tuning by plotting validation losses against the hyperparams. 
- ![[validation-loss-vs-hyperparam.png]]
- [[@melisStateArtEvaluation2017]] also they try out different seeds. Follow their recommendations.
