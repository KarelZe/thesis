
Decision trees can be used in classification and regression. Counterintuitive to our initial problem framing of trade classification as a probabilistic classification task ([[ðŸªSelection Of Supervised Approaches]]), the focus is on regression trees only, as it is the prevailing prediction model used within the gradient boosting algorithm ([[@friedmanAdditiveLogisticRegression2000]]9). The ensemble method later adapts to classification.

A decision tree splits the feature space $\mathbb{R}^p$ into several disjoint regions $R$ through a sequence of recursive splits. For a binary decision tree, a single split leads to two new sub-regions, whose shape is determined by the features considered for splitting and the preceding splits. Trees are grown in depth until a minimum threshold for the number of samples within a node or some other stopping criterion applies ([[@breimanClassificationRegressionTrees2017]]42). 

A region corresponds to a terminal node in the tree. For each terminal node of the tree or unsplit region, the predicted response value is constant for the entire region and shared by all its samples  ([[@breimanClassificationRegressionTrees2017]] 229). For a tree with $M$ regions $R_1, R_2,\ldots, R_M$,  and some numerical input $x$ the tree can be modelled as: $$f(x)=\sum_{m=1}^{M} c_{m} \mathbb{I}\left(x \in R_{m}\right),$$where $\mathbb{I}$ is the indicator function for region conformance and $c_m$  the region's constant ([[@hastietrevorElementsStatisticalLearning2009]]326). In the regression case, $c_m$ is the mean of all target variables $y_i$ in the specific region. Since all samples of a region share a common response value, the tree estimates resemble a histogram that approximates the true regression surface.

So far, it remains open how the best split can be found. The best split is where the deviation between the prediction and the true response variable diminishes. Over the entire tree, this error can be captured in the SSE given by $$E(M)=\frac{1}{N} \sum_{m \in M} \sum_{x_{i} \in R_m}\left(y_{i}-c_{m}\right)^{2},$$which is subsequently minimized ([[@breimanClassificationRegressionTrees2017]] 231). As documented in ([[@hastietrevorElementsStatisticalLearning2009]]326) we start with the entire dataset and scan through all combinations of features and possible split values. For a split by the feature $j$ and value $s$, the child nodes are given by a pair of half-planes:
$$
R_1(j, s)=\left\{X \mid X_j \leq s\right\} \text { and } R_2(j, s)=\left\{X \mid X_j>s\right\}.
$$
Thereby, the feature $j$ and value $s$ are selected in a way, that the combined error in the child nodes is minimized:
$$
\min _{j, s}\left[\min _{c_1} \sum_{x_i \in R_1(j, s)}\left(y_i-c_1\right)^2+\min _{c_2} \sum_{x_i \in R_2(j, s)}\left(y_i-c_2\right)^2\right].
$$
The procedure is repeated on the so-created child nodes. Note that, splits are performed greedily to keep computations tractable. This entails, that only the reduction in SSE of the current node is considered, and not the improvement from any subsequent splits in the child nodes. Computational costs may still be high, when there are many split candidates, due to a large feature count or possible split values. Common approximations are to split on quantized features (see papers in) ([[@keLightGBMHighlyEfficient2017]]) and ([[@shiQuantizedTrainingGradient2022]]) or on a random feature subset.

Trivially, growing deeper trees leads to an improvement in the gls-SSE. Considering the extreme, where each sample has its own region, the tree would achieve a perfect fit in-sample but perform poorly on out-of-sample data. To reduce the sensitivity of the tree to changes in the training data, hence *variance*, size complexity pruning procedures are employed. Likewise, if the decision tree is too simplistic, a high bias contributes to the model's overall expected error. Both extremes are to be avoided.

Ensemble methods, such as *bagging* ([[@breimanBaggingPredictors1996]]) and *boosting* ([[@schapireStrengthWeakLearnability1990]]) decrease the expected error of the decision tree by combining multiple trees in a single model. Both approaches differ in the error term being minimized, which is reflected in the training procedure and the complexity of the ensemble members. More specifically, bagging aims at decreasing the variance, whereas boosting addresses the bias and variance ([[@schapireBoostingMarginNew1998]]1672) and ([[@breimanRandomForests2001]]29). Next, we derive gls-gbm, a variant of boosting introduced by ([[@friedmanGreedyFunctionApproximation2001]]), and apply it to probabilistic classification.