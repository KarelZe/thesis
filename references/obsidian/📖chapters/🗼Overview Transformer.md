The subsequent chapters provide an introduction to classifiers based on the Transformer architecture.

The *Transformer* is a neural network architecture of ([[@vaswaniAttentionAllYou2017]]6{,}002--6{,}006) proposed for sequence-to-sequence modelling. Its original application is in machine translation, whereby sentences in the source language are translated into sentences in the target language. More precisely, the sentence is first decomposed into individual tokens and mapped into a sequence of embeddings, which are rich vector representations of the raw input. The Transformer then processes the embeddings to generate the output sequence. As the network operates on embeddings, rather than words, the architecture is not constrained to process textual data. It has been successfully adapted to other representations including image data (cp.[[@dosovitskiyImageWorth16x162021]]3) and tabular data (cp.[[@gorishniyRevisitingDeepLearning2021]]18{,}932). The latter is important for our work, as derived in Section [[üç™Selection Of Supervised Approaches]]. At times we fall back to the Transformer for machine translations, to develop a deeper intuition for the architecture.

Following the classical architecture for machine translation of ([[@sutskeverSequenceSequenceLearning2014]]3), the network features two main components: the *encoder* and the *decoder*. A sequence of tokens is first mapped to a sequence of embeddings and augmented with positional information. The encoder receives the embeddings and creates an enriched representation from it by encoding the context in which the input appears i.e., the surrounding words. The output of the encoder is then fed to the decoder. The decoder takes the embedded target sequence along with parts of the encoded representation of the input, to autoregressively generate the output sequence, i.e., the translation in the target language token by token ([[@vaswaniAttentionAllYou2017]]3). The complete architecture is depicted in Figure [[ü§ñTransformer#^2cf7ee]]. It will serve as a guide through the subsequent sub-chapters.
![[classical_transformer_architecture.png]]
(own drawing after [[@daiTransformerXLAttentiveLanguage2019]] (p. 3) Alternatives https://github.com/negrinho/sane_tikz. ) ^2cf7ee

The encoder consists of $L=6$ stacked Transformer blocks ([[@vaswaniAttentionAllYou2017]]6). Each block itself is composed of two sub-layers: a multi-head self-attention layer, followed by a position-wise, feed-forward network. Both components serve a distinct purpose in the Transformer. The self-attention mechanism encodes the context in which the input appears onto the embeddings, whereas the feed-forward network serves as a long-term memory persisting information outside the immediate context. In the multi-head self-attention mechanism of the encoder, inputs can learn from any token of the input sequence, even if they appear causally before the other input. Each of the sub-layers is surrounded by skip connections ([[@heDeepResidualLearning2015]]2) and followed by layer normalisation ([[@baLayerNormalization2016]]4) to facilitate and stabilise training. Stacking multiple Transformer blocks allows the model to learn hierarchical features from the inputs and targets. Applied to language processing, the first layers in the stack extract coarse-grained syntactic features and subsequent layers learn fine-grained semantic features ([[@jawaharWhatDoesBERT2019]]3651) and ([[@tenneyBERTRediscoversClassical2019]]4596)). For tabular data this translates to frequent feature combinations or infrequent feature interactions.

Aside from the multi-headed self-attention and feed-forward sub-layer, the decoder contains a third sub-layer for multi-headed self-attention on the output of the encoder, known as cross-attention. Also, the multi-headed self-attention mechanism in the decoder differs from the one in the encoder. Specifically, future parts of the output sequence are causally masked to prevent the model from learning on subsequent tokens during training which would contradict the autoregressive sequence generation ([[@vaswaniAttentionAllYou2017]]3) and ([[@narangTransformerModificationsTransfer2021]]15). The output of the decoder is finally passed through a linear layer with a softmax activation function to unembed the output and retrieve the probabilities of the next token ([[@vaswaniAttentionAllYou2017]]5). Since the output sequence is generated autoregressively,  the most probable token is fed back as an input to the decoder to provide context for the following tokens until the remaining sequence is generated.

For its original application, machine translation, both the encoder and decoder are used. Yet, the modular design allows adapting Transformers to a wider range of use cases, some of which only require the encoder or decoder. ([[@raffelExploringLimitsTransfer2020]] 16--17)  differentiate these modes: encoder-only architecture, which encodes the input to obtain an enriched representation, decoder-only architectures to generate new tokens and encoder-decoder models for sequence-to-sequence modelling autoregressively. As our focus is on the probabilistic classification of tabular data, the goal is to learn an enriched representation of the input for classifying the label, here $y$, rather than generating new samples. As such, encoder-only Transformers suffice. This observation also guides the structure in the next chapters, which is limited to embeddings and the inner workings of the encoder.


