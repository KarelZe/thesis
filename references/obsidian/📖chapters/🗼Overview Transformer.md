The subsequent chapters provide an introduction to classifiers based on the Transformer architecture.

The *Transformer* is a neural network architecture of ([[@vaswaniAttentionAllYou2017]]6{,}002--6{,}006) proposed for sequence-to-sequence modelling. Its original application is in machine translation, whereby sentences in the source language are translated into sentences in the target language. More precisely, the sentence is first decomposed into individual tokens and mapped into a sequence of embeddings, which are rich vector representations of the raw input. The Transformer then processes the embeddings to generate the output sequence. As the network operates on embeddings, rather than words, the architecture is not constrained to process textual data. It has been successfully adapted to other representations including image data (cp.[[@dosovitskiyImageWorth16x162021]]3) and tabular data (cp.[[@gorishniyRevisitingDeepLearning2021]]18{,}932). The latter is important for our work, as derived in Section [[üç™Selection Of Supervised Approaches]]. At times we fall back to the Transformer for machine translations, to develop a deeper intuition for the architecture.

Following the classical architecture for machine translation of ([[@sutskeverSequenceSequenceLearning2014]]3), the network features two main components: the *encoder* and the *decoder*. A sequence of tokens is first mapped to a sequence of embeddings and augmented with positional information. The encoder receives the embeddings and creates an enriched representation from it by encoding the context in which the input appears i.e., the surrounding words. The output of the encoder is then fed to the decoder. The decoder takes the embedded target sequence along with parts of the encoded representation of the input, to autoregressively generate the output sequence, i.e., the translation in the target language token by token ([[@vaswaniAttentionAllYou2017]]3). The complete architecture is depicted in Figure [[ü§ñTransformer#^2cf7ee]]. It will serve as a guide through the subsequent sub-chapters.
![[classical_transformer_architecture.png]]
(own drawing after [[@daiTransformerXLAttentiveLanguage2019]] (p. 3) Alternatives https://github.com/negrinho/sane_tikz. The right half of the image depicts the encoder and decoder stack. Both components are preceded by embedding units, which we cover in [[üõåToken Embedding]] and [[üßµPositional Embedding]]. The left half depicts the multi-headed self-attention and the self-attention which we cover in [[üÖ∞Ô∏èAttention]]. Rotate by 45 degree and put it on its own page. Come back if needed.) ^2cf7ee

The encoder (*left*) consists of $L=6$ stacked Transformer blocks ([[@vaswaniAttentionAllYou2017]]; p. 6). Stacking multiple Transformer blocks allows the model to learn hierarchical features from the inputs and targets. As such the first layers in the stack extract coarse-grained syntactic features and subsequent layers learn fine-grained semantic features ( [[@jawaharWhatDoesBERT2019]] (p. 3651); [[@tenneyBERTRediscoversClassical2019]]) (p. 4,596)). Each block itself is composed of two sub-layers: a multi-head self-attention layer, followed by a position-wise, feed-forward network. In the multi-head self-attention mechanism of the encoder, inputs can attend to any other token of the input sequence. Each of these sub-layers is surrounded by skip connections ([[@heDeepResidualLearning2015]]) and followed by layer normalization ([[@baLayerNormalization2016]]) to stabilize training. 

Aside from the multi-headed self-attention and feed-forward sub-layer, the decoder (*right*) contains a third sub-layer for multi-headed self-attention on the output of the encoder, known as *cross attention*. Also, the multi-headed self-attention mechanism in the decoder differs from the one in the encoder. Specifically, future parts of the output sequence are causally masked to prevent the model from attending to subsequent positions during training. ([[@vaswaniAttentionAllYou2017]], p. 3) ([[@narangTransformerModificationsTransfer2021]], p. 15). The output of the decoder is finally passed through a linear layer with a softmax activation function to unembed the output and retrieve the probabilities for the next token ([[@vaswaniAttentionAllYou2017]]) (p. 5). Since the output sequence is generated autoregressively, or token by token, the most probable token is fed back as input to the decoder to provide context for the following token until the remaining sequence is generated.

For its original application, machine translation, both the encoder and decoder are used. Yet, the modular design allows adapting Transformers to a wider range of use cases, some of which only require the encoder or decoder. [[@raffelExploringLimitsTransfer2020]] (p. 16 f.) differentiate these modes: 
1. **encoder-only:** use of only the encoder stage to learn a rich representation of the input e. g., in sentiment classification,
2. **decoder-only:** Use of the decoder stage to generate a new token e. g., in auto-completion of sequences, 
3. **encoder-decoder:** Use of both the encoder and decoder stages e. g., in translation.

As our focus is on the probabilistic classification of tabular data, the goal is to learn an enriched representation of the input for classifying the label rather than generating new samples. As such, *encoder-only* Transformers suffice. 

This chapter can only provide a high level overview of the Transformer. A thorough explanation of all architectural components is given in the subsequent chapters.


