Graphically, our results show that particular attention heads in the Transformer specialise in patterns akin to classical trade classification rules. We are excited to explore this aspect systematically and potentially reverse engineer novel classification rules from attention heads that are yet unknown. 
This way, we can transfer the superior classification accuracy of Transformer to regimes where labelled data for training is abundant or the high computational cost of training are not feasible.

**Notes:**
[[ðŸ“‘notes/ðŸ“ºOutlook]]