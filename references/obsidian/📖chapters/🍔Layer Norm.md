 ([[@vaswaniAttentionAllYou2017]]3) extensively draw on *layer normalisation* after the multi-headed attention and feed-forward sub-layers. Layer normalisation is used for normalising the activations of the sub-layer and to stabilise and accelerate the training of the network ([[@baLayerNormalization2016]]2). For the transformer, the normalisation statistics are calculated separately for every instance, which guarantees scalability across different batch sizes. For a vector $\boldsymbol{e}\in \mathbb{R}^{d_e}$ the normalised output is given by: 
$$
\tag{4}
\widehat{\boldsymbol{e}}=\frac{e-m}{\sqrt{v}} \odot \boldsymbol{\gamma}+\boldsymbol{\beta},
$$
calculated with the statistics $m = \sum_{i=1}^{d_{e}} \boldsymbol{e}[i] / d_{e}$ and $v = \sum_{i=1}^{d_{e}}(\boldsymbol{e}[i]-m)^2 / d_{e}$. Typically, the scale $\gamma$ and bias $\beta$ are set to preserve a zero mean and unit variance.

![[layer-norm.png]]
(Own work inspired by [[@wangLearningDeepTransformer2019]])

Until now it remains unclear, how the layer normalisation intertwines with the sub-layers and the residual connections. Transformers are distinguished by the order in which layer normalisation is added into the pre-norm and post-norm Transformer. Post-norm Transformers add layer normalisation to the sub-layer *after* adding the input from the residual connections. The arrangement is depicted in (Cref). In contrast for the pre-norm Transformer, the normalisation is applied *before* the self-attention and feed-forward sub-layers and inside the residual connections. Pre-norm requires one additional normalisation layer to pass only well-conditioned outputs from the Transformer block to the successive layers ([[@xiongLayerNormalizationTransformer2020]]5). The setup is depicted in (Cref).

([[@vaswaniAttentionAllYou2017]]3) employ post-layer normalisation, but recently, recent research has shown a shift towards pre-norm setups. ([[@narangTransformerModificationsTransfer2021]]4). Parts of this wide-spread adaption lie in faster training and omitting of the need for costly learning rate warm-up stages, whereby the learning rate is initially decreased to keep the gradients balanced ([[@xiongLayerNormalizationTransformer2020]]2) and ([[@liuUnderstandingDifficultyTraining2020]]8). Also, post-norm Transformers have been found brittle to train and prone to convergence failures with its root cause in vanishing gradients, exploding gradients, and an overall higher dependency on the residual stream ([[@liuUnderstandingDifficultyTraining2020]]8) and  [[@wangLearningDeepTransformer2019]]1812). Pre-norm Transformers, although they may sacrifice some performance, introduce a certain robustness to the training process. We come back to this observation in Cref ([[ðŸ¤–TabTransformer]]) and [[ðŸ’¡Training of models (supervised)]]. 
