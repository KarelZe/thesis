 ([[@vaswaniAttentionAllYou2017]]3) extensively draw on *layer normalization* after the multi-headed attention and feed-forward sub-layers. Layer normalization is used for normalizing the activations of the sub-layer and to stabilize and accelerate the training of the network ([[@baLayerNormalization2016]]2). For the transformer, the normalization statistics are calculated separately for every instance, which guarantees scalability across different batch sizes. For a vector $\boldsymbol{e}\in \mathbb{R}^{d_e}$ the normalized output is given by: 
$$
\tag{4}
\widehat{\boldsymbol{e}}=\frac{e-m}{\sqrt{v}} \odot \boldsymbol{\gamma}+\boldsymbol{\beta},
$$
calculated with the statistics $m = \sum_{i=1}^{d_{e}} \boldsymbol{e}[i] / d_{e}$ and $v = \sum_{i=1}^{d_{e}}(\boldsymbol{e}[i]-m)^2 / d_{e}$. Typically, the scale $\gamma$ and bias $\beta$ are set to preserve a zero mean and unit variance.

![[layer-norm.png]]
(Own work inspired by [[@wangLearningDeepTransformer2019]])

Until now it remains unclear, how the layer normalization intertwines with the sub-layers and the residual connections. Transformers are distinguished by the order in which layer normalization is added into the pre-norm and post-norm Transformer. Post-norm Transformers add layer normalization to the sub-layer *after* adding the input from the residual connections. The arrangement is depicted in (Cref). In contrast for the pre-norm Transformer, the normalization is applied *before* the self-attention and feed-forward sub-layers and inside the residual connections. Pre-norm requires one additional normalization layer to pass only well-conditioned outputs from the Transformer block to the successive layers ([[@xiongLayerNormalizationTransformer2020]]5). The setup is depicted in (Cref).

([[@vaswaniAttentionAllYou2017]]3) employ post-layer normalization, but recently, recent research has shown a shift towards pre-norm setups. ([[@narangTransformerModificationsTransfer2021]]4). Parts of this wide-spread adaption lie in faster training and omitting of the need for costly learning rate warm-up stages, whereby the learning rate is initially decreased to keep the gradients balanced ([[@xiongLayerNormalizationTransformer2020]]2) and ([[@liuUnderstandingDifficultyTraining2020]]8). Also, post-norm Transformers have been found brittle to train and prone to convergence failures with its root cause in vanishing gradients, exploding gradients, and an overall higher dependency on the residual stream ([[@liuUnderstandingDifficultyTraining2020]]8) and  [[@wangLearningDeepTransformer2019]]1812). Pre-norm Transformers, although they may sacrifice some performance, introduce a certain robustness to the training process. We come back to this observation in Cref ([[ðŸ¤–TabTransformer]]) and [[ðŸ’¡Training of models (supervised)]]. 
