Recall from earlier chapters, that the encoder stacks multiple Transformer blocks, each of which consists of several sub-layers, resulting in a deep network.  While depth is required to learn hierarchical representations, the training of such a network is complicated. As neural networks are commonly trained using back-propagation, which relies on the gradient of the error to be propagated through the network starting at the last layer, vanishing or exploding gradients pose a major difficulty in training deep neural nets ([[@heDeepResidualLearning2015]]1). Without countermeasures, stacking multiple layers in the encoder and decoder of the Transformers impedes the gradient information to flow efficiently through the network and hampers the training behaviour ([[@wangLearningDeepTransformer2019]]1811).  

As a remedy, ([[@vaswaniAttentionAllYou2017]]3) add residual connections ([[@heDeepResidualLearning2015]]1--2) around each sub-layer. As shown in Equation cref, the encoded token sequence $X \in \mathbb{R}^{d_e \times \ell_x}$ consists of the sub-layers output added element-wisely to its input:
$$
\boldsymbol{X} = \boldsymbol{X} + \operatorname{sub\_layer}\left(\boldsymbol{X}\right).
$$
Intuitively, the residual connexion provides an alternative path for information to flow through the network, since some information can bypass the sub-layer and hence reach deeper layers within the stack. Also, exploding or vanishing gradients are mitigated, as gradients can bypass the sub-layer, eventually contributing towards an easier optimisation ([[@liuRethinkingSkipConnection2020]]3591).  Residual connections moreover help to preserve the positional embeddings (see chapter [[ðŸ§µPositional Embedding]]), as the layer's inputs are maintained in the identity mapping.

**Notes:**
[[ðŸ”—residual connections notes]]