## Discussion

**Why pre-training?**
Next, we look into extending Transformers for the semi-supervised setting. Whilst Transformers could be combined with self-training, a more practical approach is to pre-train Transformers on unlabelled data, and then fine-tune the network on the remaining labelled instances. Various studies report (√ºbereinstimmend) performance improvements from pre-training attention-based architectures, including ([[@somepalliSAINTImprovedNeural2021]]8), ([[@arikTabNetAttentiveInterpretable2020]]7), ([[@huangTabTransformerTabularData2020]]), ([[@bahriSCARFSelfSupervisedContrastive2022]]) or ([[@yoonVIMEExtendingSuccess2020]]). An exception are ([[@levinTransferLearningDeep2022]]7), who find no improvements from pre-training the FT-Transformer.

Gemeinsam ist allen pre-training objectives, dass sie bessere parameter over random initialization. Pre-training accelerates convergence and improves model performance through a better weight initialization over a random initialization.  Pre-training objectives for tabular data, however, differ greatly in their methodology and are often a direct adaption from pre-training objectives in computer vision or natural language processing. One can differentiate two lines of research: .
(1. tbd, 2. tbd) 

‚ÄúContrastive learning, in which models are pre-trained to be invariant to reordering, cropping, or other label-preserving ‚Äúviews‚Äù of the data [5, 12, 15, 32, 43], is a powerful tool in the vision and language domains that has never (to our knowledge) been applied to tabular data.‚Äù ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=HQX7N9D5))

‚ÄúExisting self-supervised objectives for tabular data include denoising [43], a variation of which was used by VIME [49], masking, and replaced token detection as used by TabTransformer [18]. We find that, while these methods are effective, superior results are achieved by contrastive learning.‚Äù ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=LXSCK4TP))

‚ÄúIn recent years, the self-supervised learning has successfully been used to learn meaningful representations of the data in natural language processing [34, 41, 11, 28, 10, 21, 9]. A similar success has been achieved in image and audio domains [7, 15, 37, 5, 17, 13, 8]. This progress is mainly enabled by taking advantage of spatial, semantic, or temporal structure in the data through data augmentation [7], pretext task generation [11] and using inductive biases through architectural choices (e.g. CNN for images). However, these methods can be less effective in the lack of such structures and biases in the tabular data commonly used in many fields such as healthcare, advertisement, finance, and law. And some augmentation methods such as cropping, rotation, color transformation etc. are domain specific, and not suitable for tabular setting. The difficulty in designing similarly effective methods tailored for tabular data is one of the reasons why self-supervised learning is under-studied in this domain [46].‚Äù ([Ucar et al., 2021, p. 1](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=1&annotation=KZDPXQEV))
 ([[@ucarSubTabSubsettingFeatures2021]])

‚ÄúIn this paper, we propose novel self- and semi-supervised learning frameworks for tabular data. For self-supervised learning, we introduce a novel pretext task, mask vector estimation in addition to feature vector estimation.‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=IVMX9FSR))

‚ÄúFurthermore, we observe that when the number of unlabeled data is small as shown in Table 4, TabTransformerRTD performs better than TabTransformer-MLM, thanks to its easier pre-training task (a binary classification) than that of MLM (a multi-class classification). This is consistent with the finding of the ELECTRA paper (Clark et al. 2020).‚Äù ([Huang et al., 2020, p. 7](zotero://select/library/items/MH4GW34I)) ([pdf](zotero://open-pdf/library/items/QYWHEUYE?page=7&annotation=HU3JVTCV)) ([[@huangTabTransformerTabularData2020]])



**Why a full overview is hard**
With a multitude of methods, tested on differing datasets and neural architectures, a fair comparison between methods is complicated /erschwert / impedes. 
Hilfestellung leistet die arbeit von Rubachev et al. We base our selection on ([[@rubachevRevisitingPretrainingObjectives2022]]), who convincingly benchmark several pre-training objectives for tabular data. Among the best-performing approaches is the gls-*masked language modelling* objective proposed by ([[@devlinBERTPretrainingDeep2019]] 4174). (additional advantages go here (easy to optimize, leave architecture unaltered, not target aware))

## Pre-training

gls-mlm is a pre-training task originating from l

**How masked language modelling works**
Originally proposed for pre-training a language model, masked language modelling gls-(mlm) randomly masks 15 sunitx-percentage of the tokens, i. e., word, in the input sequence and replaces them with a $\mathtt{[MASK]}$ token. The $\mathtt{[MASK]}$ token, is a specialized token within the vocabulary  (cp. [[üí§Embeddings For Tabular Data]]). The model learns to predict the masked token through the tokens in the bidirectional context. Like before, the cross-entropy loss is used to predict the most probable token ([[@devlinBERTPretrainingDeep2019]] 4174). 

A caveat of *mlm* is, that the mask token only appears during pre-training, but not in fine-tuning, creating a mismatch between the pre-training and fine-tuning task. (Devlin offer some alternatives) (framed as multi-classification task)

‚ÄúMasked Language Modeling (MLM) was first proposed for language models by Devlin et al. [20] as a powerful unsupervised learning strategy. MLM involves training a model to predict tokens in text masked at random so that its learned representations contain information useful for reconstructing these masked tokens. In the tabular domain, instead of masking tokens, a random subset of features is masked for each sample, and the masked values are predicted in a multi-target classification manner [34]. In our experiments, we mask one randomly selected feature for each sample, asking the network to learn the structure of the data and form representations from n ‚àí 1 features that are useful in producing the value in the n-th feature. For more detail, see Appendix A.‚Äù ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=DC87W4J4))

‚ÄúThe multi-attention sub-layer can now see the whole sequence, run the self-attention process, and predict the masked token. The input tokens were masked in a tricky way to force the model to train longer but produce better results with three methods: ‚Ä¢ Surprise the model by not masking a single token on 10% of the dataset; for example: "The cat sat on it [because] it was a nice rug." ‚Ä¢ Surprise the model by replacing the token with a random token on 10% of the dataset; for example: "The cat sat on it [often] it was a nice rug." ‚Ä¢ Replace a token by a [MASK] token on 80% of the dataset; for example: "The cat sat on it [MASK] it was a nice rug." The authors' bold approach avoids overfitting and forces the model to train efficiently.‚Äù (Rothman, 2021, p. 48)

3.3.1 Masked Language Modeling Unlike language modeling (LM), which aims at predicting the next word given the sequence of previous words, masked language modeling is the task of predicting a percentage of input tokens which are randomly masked. The MLM training objective was chosen over the traditional LM objective because of the bidirectionality of BERT (i.e., BERT uses both left and right context in the sequence to predict the target word). For such models, standard conditional language modeling cannot be used as training objective, as the bidirectional conditioning would allow each word to indirectly ‚Äúsee itself‚Äù, and the model could trivially predict the target word in a multi-layered context. Hence, BERT is trained using masked language modeling, also referred as a Cloze task in the literature (Taylor, 1953). As shown in Figure 3.11, the prediction is given by the final hidden vectors corresponding to the masked tokens, that are fed into an output softmax over the vocabulary, as in a standard LM. 24 Figure 3.11: Illustration of the masked language modeling (MLM) training objective (Jay Alammar, 2019). Formally, given an input sequence x = [x1, x2, ..., xN ] of N tokens, MLM first selects a random set of k positions (integers between 1 and N) to mask out m = [m1, ..., mk]. The tokens in the selected positions are then replaced with a [MASK] token, resulting in the masked input sequence x masked. BERT eventually learns to predict the original identities of the k masked-out tokens by computing an output word distribution yÀÜ (h) (h = 1, ..., k) for each one of them. More precisely, given the h-th masked word xmh from sequence x, the MLM loss function is the cross-entropy between the predicted probability distribution yÀÜ (h) , and the true next word distribution y (h) , which is simply the one-hot vector for xmh . Therefore, we have L (h) MLM(Œ∏) = CE(y (h) , yÀÜ (h) ) = X w‚ààV ‚àíy (h) w log ÀÜy (h) w = ‚àí log ÀÜy (h) xmh . (3.13) The overall loss of the sequence is simply the average loss for all the k masked-out tokens in x masked , LMLM(Œ∏) = 1 k X k h=1 L (h) MLM(Œ∏) = 1 k X k h=1 ‚àí log ÀÜy (h) xmh = 1 k X i‚ààm ‚àí log p  xi |x masked . (3.14) The masking procedure works as follows: BERT selects 15% of all WordPiece tokens in each training sequence at random. If the i-th token is chosen, it is replaced with: 1. the [MASK] token 80% of the time; 2. a random token 10% of the time; 3. the unchanged i-th token 10% of the time. 25 The selected words are not always replaced with the [MASK] token because it would then create a mismatch between pre-training and fine-tuning, since the masked token would never be seen before fine-tuning. 3.3.2 Next Sentence Prediction Next sentence prediction (NSP) is a binary classification task in which the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original corpus. This training objective helps understand the relationship between pairs of sentences, which is not directly captured by language modeling but still very important for many downstream tasks such as question-answering (QA) and natural language inference (NLI). This prediction task can can be easily generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and the other 50% of the time it is a random sentence from the corpus (labeled as NotNext). In this case, the final hidden vector corresponding to the [CLS] token is fed into an output softmax over the two possible predictions, as shown in Figure 3.12. Figure 3.12: Illustration of the next sentence prediction (NSP) training objective (Jay Alammar, 2019). (https://matheo.uliege.be/bitstream/2268.2/9060/7/Antoine_Louis_Thesis.pdf)


**How it transfers to my problem**
Applied to trade classification on tabular datasets, masked language modelling transfers to randomly masking elements in $\mathbf{x}_{i}$.  The classification task is trivialized for derived features or features with a high dependence on other features in the context. In a broader sense, gls-mlm can help learning expressive representations of the input, even if the true label is unavailable. Given previous research, we expect pre-training to improve the performance of the FT-Transformer and match the one of the gls-gbm.

**Notes:**
[[ü§ñPretraining notes]]


