## Discussion

## Pre-training
gls-mlm is a pre-training task proposed for the use in language models by  ([[@devlinBERTPretrainingDeep2019]]4174). The core idea of gls-mlm is to randomly mask sunitx-percentage-15 of the tokens, i. e., words, in the input sequence. Eventually, the model learns to predict the original token of the now masked token through the tokens in the bidirectional context. Separately for each token, the final hidden state of the masked token is fed through an softmax activation to obtain the predicted probability distribution for the original token and the cross entropy loss is used to compare against the true distribution. For masking, the authors introduce an additional $\mathtt{[MASK]}$ token (cp. [[ðŸ’¤Embeddings For Tabular Data]]), that extends the vocabulary ([[@devlinBERTPretrainingDeep2019]]4174). A caveat of gls-mlm is, that the mask token only appears during pre-training, but not in fine-tuning, causing a mismatch between the pre-training and fine-tuning task. The issues is mitigated by a sophisticated replacement strategy of the masked token: only 80 % of the time the token is replaced with the $\mathtt{[MASK]}$ token, in 10 % it remains unchanged, and replaced with a random token ([[@devlinBERTPretrainingDeep2019]] 4174). The random or no replacement avoids overfitting.  

Applied to tabular datasets, gls-mlm transfers to randomly masking features or elements in $\mathbf{x}_{i}$ instead of sequences.  Previous adaptions for tabular data, e.g., ([[@huangTabTransformerTabularData2020]]3) or ([[@levinTransferLearningDeep2022]]8), simplify the replacement strategy to substitution with the $\mathtt{[MASK]}$ token only, and tries to reconstruct the original features.  Other than in the language case, reconstruction of masked features requires distinct classification heads per feature and adequate loss functions, which significantly increases the memory footprint and computational demand ([[@levinTransferLearningDeep2022]]16). A lightweight alternative is presented in ([[@rubachevRevisitingPretrainingObjectives2022]]4). Here, features are masked and replaced. The objective is now to predict the binary mask $\mathbf{m}_{i}\in \{0,1\}^{M}$ vector corresponding to $\mathbf{x}_{i}$, indicating which features, or entries in $\mathbf{x}_{i}$, have been corrupted.

In summary, gls-mlm can help to learn expressive representations of the input, even if the true label is unavailable. Given previous research, we expect pre-training to improve the performance of the FT-Transformer and match or exceed the one of the gls-gbm.

**Notes:**
[[ðŸ¤–Pretraining notes]]


