## Discussion

Next, we look into extending FT-Transformer for the semi-supervised setting. Whilst Transformers could be combined with self-training, a more promising approach is to pre-train Transformers on unlabelled data, and then fine-tune the network on the remaining labelled instances. Various studies report unanimously performance improvements from pre-training attention-based architectures, including ([[@somepalliSAINTImprovedNeural2021]]8), ([[@arikTabNetAttentiveInterpretable2020]]7), and ([[@huangTabTransformerTabularData2020]]7). An exception are ([[@levinTransferLearningDeep2022]]7), who find no improvements from pre-training the FT-Transformer.

Until now we assumed the parameters e.g., weights and biases, of the Transformer to be initialized randomly. The joint goal of pre-training objectives is to initialize a neural network with weights that capture expressive representations of the input and thereby improve convergence and model performance over a random initialization when fine-tuning on a specific task. Pre-training objectives for tabular data differ vastly in their methodology and are often a direct adaption from other domains and include pre-training through masking, token replacement, or constrastive learning. As such, ([[@huangTabTransformerTabularData2020]]7) use gls-mlm, whereby features are randomly masked and the objective is to reconstruct the original input. Pre-training by gls-rtd aims to recover a binary mask, used for random feature replacement ([[@huangTabTransformerTabularData2020]]7). ([[@bahriSCARFSelfSupervisedContrastive2022]]3) and ([[@yoonVIMEExtendingSuccess2020]]4--5) reconstruct both the binary feature mask and to recover the original input. ([[@somepalliSAINTImprovedNeural2021]]3) alter the methodology of ([[@yoonVIMEExtendingSuccess2020]]4--5) through a contrastive loss function.  

With a multitude of methods, tested on differing datasets and neural architectures, a fair comparison between pre-training methods is complicated.  However, [[@rubachevRevisitingPretrainingObjectives2022]]2--3) provide guidance for selecting objectives in their work. Among the pre-training objectives that they convincingly benchmarked, the gls-mlm objective proposed by ([[@devlinBERTPretrainingDeep2019]] 4174) was found to be one of the best-performing approaches. The gls-mlm objective is easy to optimize and does not require modifications to the model architecture, which is an important property when comparing the supervised and semi-supervised variant. This makes gls-mlm a compelling choice for pre-training on unlabelled data.

## Pre-training
gls-mlm is a pre-training task proposed for the use in language models by  ([[@devlinBERTPretrainingDeep2019]]4174). The core idea of gls-mlm is to randomly mask sunitx-percentage-15 of the tokens, i. e., words, in the input sequence. Eventually, the model learns to predict the original token of the now masked token through the tokens in the bidirectional context. Separately for each token, the final hidden state of the masked token is fed through an softmax activation to obtain the predicted probability distribution for the original token and the cross entropy loss is used to compare against the true distribution. For masking, the authors introduce an additional $\mathtt{[MASK]}$ token (cp. [[ðŸ’¤Embeddings For Tabular Data]]), that extends the vocabulary ([[@devlinBERTPretrainingDeep2019]]4174). A caveat of gls-mlm is, that the mask token only appears during pre-training, but not in fine-tuning, causing a mismatch between the pre-training and fine-tuning task. The issues is mitigated by a sophisticated replacement strategy of the masked token: only 80 % of the time the token is replaced with the $\mathtt{[MASK]}$ token, in 10 % it remains unchanged, and replaced with a random token ([[@devlinBERTPretrainingDeep2019]] 4174). The random or no replacement avoids overfitting.  

Applied to tabular datasets, gls-mlm transfers to randomly masking features or elements in $\mathbf{x}_{i}$ instead of sequences.  Previous adaptions for tabular data, e.g., ([[@huangTabTransformerTabularData2020]]3) or ([[@levinTransferLearningDeep2022]]8), simplify the replacement strategy to substitution with the $\mathtt{[MASK]}$ token only, and tries to reconstruct the original features.  Other than in the language case, reconstruction of masked features requires distinct classification heads per feature and adequate loss functions, which significantly increases the memory footprint and computational demand ([[@levinTransferLearningDeep2022]]16). A lightweight alternative is presented in ([[@rubachevRevisitingPretrainingObjectives2022]]4). Here, features are masked and replaced <mark style="background: #BBFABBA6;">(at random? How does this work? Look up in source code).</mark> The objective is now to predict the binary mask $\mathbf{m}_{i}\in \{0,1\}^{m}$ vector corresponding to $\mathbf{x}_{i}$, indicating which features, or entries in $\mathbf{x}_{i}$, have been corrupted.

In summary, gls-mlm can help to learn expressive representations of the input, even if the true label is unavailable. Given previous research, we expect pre-training to improve the performance of the FT-Transformer and match or exceed the one of the gls-gbm.

**Notes:**
[[ðŸ¤–Pretraining notes]]


