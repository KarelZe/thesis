
The classical Transformer of [[@vaswaniAttentionAllYou2017]] (p. 3) features several *layer normalization* layers. Layer normalization is used for normalizing the activations of the sub-layer to stabilize and accelerate the training of the network ([[@baLayerNormalization2016]])(p. 2). In Transformers the normalization statistics are calculated separately for <mark style="background: #ADCCFFA6;">every instance</mark>, which guarantees scalability across different batch sizes. For a vector $\boldsymbol{e}\in \mathbb{R}^{d_e}$ the normalized output is given by: $\widehat{\boldsymbol{e}}=\frac{e-m}{\sqrt{v}} \odot \boldsymbol{\gamma}+\boldsymbol{\beta}$, calculated with the statistics $\boldsymbol{m} = \sum_{i=1}^{d_{\mathrm{e}}} e[i] / d_{\mathrm{e}}$ and $v = \sum_{i=1}^{d_{\mathrm{e}}}(e[i]-\boldsymbol{m})^2 / d_{\mathrm{e}}$. Typically, the scale $\gamma$ and bias $\beta$ are set to keep  a zero mean and unit-variance.

![Layer-normalization](layer-norm-first-last.png) 
(Similar to [[@xiongLayerNormalizationTransformer2020]]; <mark style="background: #ABF7F7A6;">somehow draw in final norm layer.</mark>)
^62bc77

[[@vaswaniAttentionAllYou2017]] (p. 3) employ *layer normalization* around each sub-layer after adding residual connections ([[üîóResidual connections]]). Derived from the order of the layers, this architecture is called *post layer normalization*. In contrast to *post layer normalization*, for *pre-normalization*, the normalization is applied before the self-attention and feed-forward sub-layers and inside the residual connections. *Pre-norm* requires one additional normalization layer to pass only well-conditioned outputs from the transformer block to the successive layers [[@xiongLayerNormalizationTransformer2020]] (p. 5). Both architectures are depicted in Figure ([[#^62bc77]]). Many modern Transformer feature a pre- normalization setup, as observed by [[@narangTransformerModificationsTransfer2021]]. Parts of it's success, lie in a faster training, omitting the need for costly learning rate warm-up stages, whereby the learning rate is initially decreased to keep the gradients balanced ([[@xiongLayerNormalizationTransformer2020]] (p. 2); [[@liuVarianceAdaptiveLearning2021]]; [[@liuUnderstandingDifficultyTraining2020]]). Also, *post-norm* transformers are particularly brittle to train with several documented convergence failures with its root cause in vanishing gradients, exploding gradients, and a higher dependency on the residual stream ([[@liuUnderstandingDifficultyTraining2020]] (p. 8), [[@shazeerAdafactorAdaptiveLearning2018]],  [[@wangLearningDeepTransformer2019]]). Their pre-norm counterpart increases the robustness in training, sometimes at the cost performance, as documented in [[@liuUnderstandingDifficultyTraining2020]].  We come back to this observation in Chapter [[üí°Training of models (supervised)]]. 

**Notes:**
[[üçîLayer norm notes]]