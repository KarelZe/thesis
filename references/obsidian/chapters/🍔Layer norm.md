 [[@vaswaniAttentionAllYou2017]] (p. 3) extensively draw on *layer normalization* after the multi-headed attention and feed-forward sub-layers. Layer normalization is used for normalizing the activations of the sub-layer and to stabilize and accelerate the training of the network ([[@baLayerNormalization2016]])(p. 2). For the transformer, the normalization statistics are calculated separately for every instance, which guarantees scalability across different batch sizes. For a vector $\boldsymbol{e}\in \mathbb{R}^{d_e}$ the normalized output is given by: 
$$
\tag{4}
\widehat{\boldsymbol{e}}=\frac{e-m}{\sqrt{v}} \odot \boldsymbol{\gamma}+\boldsymbol{\beta},
$$
calculated with the statistics $\boldsymbol{m} = \sum_{i=1}^{d_{\mathrm{e}}} e[i] / d_{\mathrm{e}}$ and $v = \sum_{i=1}^{d_{\mathrm{e}}}(e[i]-\boldsymbol{m})^2 / d_{\mathrm{e}}$. Typically, the scale $\gamma$ and bias $\beta$ are set for a zero mean and unit variance.

![Layer-normalization](layer-norm-first-last.png) 
(Similar to [[@xiongLayerNormalizationTransformer2020]]; picture from https://github.com/dvgodoy/PyTorchStepByStep/blob/master/Chapter10.ipynb <mark style="background: #ABF7F7A6;">somehow draw in final norm layer.</mark>)
^62bc77

[[@vaswaniAttentionAllYou2017]] (p. 3) employ *layer normalization* around each sub-layer after adding residual connections ([[ðŸ”—Residual connections]]). Derived from the order of the layers, this specific arrangement is called *post-layer normalization*. In contrast to post-layer normalization, for pre-normalization, the normalization is applied before the self-attention and feed-forward sub-layers and inside the residual connections. *Pre-norm* requires one additional normalization layer to pass only well-conditioned outputs from the transformer block to the successive layers [[@xiongLayerNormalizationTransformer2020]] (p. 5). Both architectures are depicted in Figure ([[#^62bc77]]). Many recent transformers feature a pre-normalization setup, as observed by [[@narangTransformerModificationsTransfer2021]]. Parts of its success, lie in faster training, omitting the need for costly learning rate warm-up stages, whereby the learning rate is initially decreased to keep the gradients balanced ([[@xiongLayerNormalizationTransformer2020]] (p. 2); [[@liuVarianceAdaptiveLearning2021]]; [[@liuUnderstandingDifficultyTraining2020]]). 

Also, *post-norm* transformers are particularly brittle to train with several documented convergence failures with its root cause in vanishing gradients, exploding gradients, and a higher dependency on the residual stream ([[@liuUnderstandingDifficultyTraining2020]] (p. 8), [[@shazeerAdafactorAdaptiveLearning2018]],  [[@wangLearningDeepTransformer2019]]). Their pre-norm counterpart increases the robustness in training, sometimes at the cost performance, as documented in [[@liuUnderstandingDifficultyTraining2020]].  We come back to this observation in Chapter [[ðŸ’¡Training of models (supervised)]]. 
