
The *Transformer* is a neural network architecture proposed by [[@vaswaniAttentionAllYou2017]] (p. 2 f.) for sequence-to-sequence modelling, such as machine translation. Since its introduction it has become ubiquitous in natural language processing ([[@lampleLargeMemoryLayers2019]], p. 3; ...). Its success for language representations has also led to adaptions for image representations ([[@parmarImageTransformer2018]], [[@carionEndtoEndObjectDetection2020]]) (found in [[@tayEfficientTransformersSurvey2022]]), as well as tabular data representations ([[@huangTabTransformerTabularData2020]], [[@somepalliSAINTImprovedNeural2021]], [[@gorishniyRevisitingDeepLearning2021]]).
![[classical_transformer_architecture.png]]
(own drawing after [[@daiTransformerXLAttentiveLanguage2019]] (p. 3) Alternatives https://github.com/negrinho/sane_tikz) ^2cf7ee

The transformer follows an encoder-decoder architecture. A sequence of inputs e. g., a sentence in the source language, is first mapped to a sequence of embeddings and enriched with positional information. The encoder receives the input and creates a rich representation from it by encoding the context in which the input appears. The output of the encoder is then fed to the decoder. The decoder takes the embedded target sequence along with parts of the encoded representation of the output, to autoregressively generate the output sequence e. g., a sentence in the target language ([[@vaswaniAttentionAllYou2017]]; p. 3). The complete architecture is depicted in Figure [[ðŸ¤–Transformer#^2cf7ee]].

The encoder *(left)* consists of $L$ stacked transformer blocks, whereby $L$ is typically set to $6$ ([[@vaswaniAttentionAllYou2017]]; p. 6). Stacking multiple transformer blocks allows the model to extract hierarchical features. As such the first layers in the stack extract coarse-grained features and subsequent layers learn fine-grained features <mark style="background: #BBFABBA6;">(citation ?)</mark>. Each block is composed of two sub-layers: a multi-head self-attention layer, followed by a position-wise, feed-forward network. In the encoder, inputs can attend to any other part of the input sequence. Each of this sub-layer is surrounded by skip connections ([[@heDeepResidualLearning2015]]), whereby the input of the sub-layer is added to the sub-layer output. Finally, layer normalization ([[@baLayerNormalization2016]]) is applied. 

Aside from the multi-headed self-attention and feed-forward sub-layer, the decoder *(right)* features a third sub-layer for multi-headed self-attention on the output of the encoder, known as *cross attention*. The multi-headed self-attention mechanism in the decoder differs from the one in the encoder. Specifically, future parts of the output sequence are causally masked to prevent the model from attending to subsequent positions during training. ([[@vaswaniAttentionAllYou2017]], p. 3) ([[@narangTransformerModificationsTransfer2021]], p. 15). The output of the decoder is finally passed through a linear layer with a softmax activation function to unembed the output and retrieve the probabilities for the next token ([[@vaswaniAttentionAllYou2017]]) (p. 5). Since the output sequence is generated autoregressively, the most probable token is fed back as input to the decoder to provide context for the following token until the remaining sequence is generated.
For its original application, machine translation, both the encoder and decoder are used, where the input sequence in the source language is first mapped to a rich, intermediate representation and subsequently the output sequence is generated. Yet, the modular design allows adapting transformers to a wide range of use cases, some of which only require the encoder or decoder. [[@raffelExploringLimitsTransfer2020]] (p. 16 f.) differentiate these modes: 
1. **encoder-only:** use of only the encoder stage e. g., in sentiment classification,
2. **decoder-only:** Use of the decoder stage e. g., in auto-completion of sequences, 
3. **encoder-decoder:** Use of both the encoder and decoder stages e. g., in translation.

As our focus is on the probabilistic classification of tabular data, the goal is to learn an enriched representation of the features, which can be used for classifying the label. As such, only *encoder-only* transformers suffice. 

This chapter can only provide a high level of the transformer. A thorough explanation of all architectural components follows in the subsequent chapters. Thereafter, we explain how the transformer of [[@vaswaniAttentionAllYou2017]] can be adapted to the tabular domain. With the [[ðŸ¤–TabTransformer]] ([[@huangTabTransformerTabularData2020]], p. 1 f.) and [[ðŸ¤–FTTransformer]] ([[@gorishniyRevisitingDeepLearning2021]] p. 1) we introduce two concrete architectures. Throughout this work we adhere to a notation suggested in [[@phuongFormalAlgorithmsTransformers2022]] (p. 1 f) to maintain consistency.