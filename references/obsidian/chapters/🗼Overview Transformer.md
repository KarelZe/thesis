
The *Transformer* is a neural network architecture proposed by [[@vaswaniAttentionAllYou2017]] (p. 2 f.) for sequence-to-sequence modelling, such as machine translation. Since its introduction it has become ubiquitous in natural language processing ([[@lampleLargeMemoryLayers2019]], p. 3; ...). Its success for language representations has also led to adaptions for image representations ([[@parmarImageTransformer2018]], [[@carionEndtoEndObjectDetection2020]]) (found in [[@tayEfficientTransformersSurvey2022]]), as well as tabular data representations ([[@huangTabTransformerTabularData2020]], [[@somepalliSAINTImprovedNeural2021]], [[@gorishniyRevisitingDeepLearning2021]]).

<mark style="background: #D2B3FFA6;">(shortest description A transformer¬†starts with a token embedding, followed by a series of ‚Äúresidual blocks‚Äù, and finally a token unembedding. Each residual block consists of an attention layer, followed by an MLP layer. Both the attention and MLP layers each ‚Äúread‚Äù their input from the residual stream (by performing a linear projection), and then ‚Äúwrite‚Äù their result to the residual stream by adding a linear projection back in.¬†Each attention layer consists of multiple heads, which operate in parallel. from https://transformer-circuits.pub/2021/framework/index.html Think about it!)</mark>

The *classical* Transformer follows an encoder-decoder architecture. A sequence of inputs e. g., a sentence in the source language, is first mapped to a sequence of embeddings and enriched with positional information. The encoder receives the input and creates a rich representation from it by encoding the context in which the input appears. The output of the encoder is then fed to the decoder. The decoder takes the embedded target sequence along with parts of the encoded representation of the output to generate the output sequence e. g., a sentence in the target language in an auto-regressive fashion. <mark style="background: #FFB8EBA6;">(citation -> Vaswani )</mark> The architecture is depicted in Figure [[ü§ñTransformer#^2cf7ee]].

The encoder consists of $L$ stacked Transformer blocks. In the classical implementation $L$ is set to $6$ ([[@vaswaniAttentionAllYou2017]]; p. 6) . Each block consists of two sub-layers: a multi-head self-attention layer, followed by a position-wise, feed-forward network. In the encoder inputs can attend to any token within the sequence. Each of these sub-layer are connected by skip connections ([[@heDeepResidualLearning2015]]), whereby the input of the sub-layer is added to the sub-layer's output. Finally layer normalization ([[@baLayerNormalization2016]]) is applied. <mark style="background: #FFF3A3A6;">(Shortly, address why do we stack many layers at all? What is learned in lower layers? https://arxiv.org/abs/1311.2901)</mark>

Aside from the multi-headed self-attention and feed-forward sub-layer the decoder features a third sub-layer for multi-headed self-attention on the output of the encoder, known as *cross attention*. The multi-headed self-attention mechanism in the decoder differs from the one in the encoder. Specifically, future parts of the output sequence are causally masked to prevent the model from attending to subsequent positions during training. ([[@vaswaniAttentionAllYou2017]], p. 3) ([[@narangTransformerModificationsTransfer2021]], p. 15).

The output of the decoder is finally passed through a linear layer with a softmax activation function to unembed the output and retrieve the probabilities for the next token ([[@vaswaniAttentionAllYou2017]]) (p. 5). Since the output sequence is generated token by token, with the most probable token being fed back as input to the decoder to provide context for the following token until the remaining sequence is generated.

This chapter can only provides a high level of the Transformer. Subsequent chapters cover multi-headed self-attention, token and positional embeddings in detail. 

## Transformer modes
For it's original application, machine translation, both the encoder and decoder are used, where the input sequence in the source language is first mapped to a rich, intermediate representation and subsequently the output sequence is generated. Yet, the modular design, allows to adapt Transformers to a much wider range of use cases, some of which only require the encoder or decoder. [[@raffelExploringLimitsTransfer2020]] (p. 16 f.) differentiate these modes: 
1. **encoder-only:** use of only the encoder stage e. g. in sentiment classification. Since inputs are not masked, hence *fully-visible masking*, inputs can attend to any other input within the sequence. <mark style="background: #ABF7F7A6;">(What about padding masking in the encoder? https://stats.stackexchange.com/a/446571/351242)</mark>
2. **decoder-only:** Use of the decoder stage e. g., in auto-completion to auto-regressively generate a sequence. Use of *causal masking*, so that inputs only depend on all previously generated outputs.
3. **encoder-decoder:** Use of both the encoder and decoder stages e. g., in translation. Combines *Fully-visible masking* in the encoder, with *causal masking* in the decoder.
![[different-mask-patterns.png]]

As our focus is on probabilistic classification of tabular data, the goal is to learn an enriched representation of the features, which can be used for classifying the label. As such, only *encoder-only* Transformers suffice. Also, *fully-visible masking* is appropriate, as inputs are free to attend to all previous and subsequent columns or rows within the dataset.

In the subsequent sections we introduce the classical Transformer of [[@vaswaniAttentionAllYou2017]] more thoroughly. Our focus on the central building blocks, attention, multi-head self-attention, and cross-attention (see Chapter [[üÖ∞Ô∏èAttention]]) as well as feed-forward networks (chapter [[üé±Position-wise FFN]]). In the subsequent chapters we show, that the self-attention mechanism and embeddings are generic enough to be transferred to the tabular domain. With the [[ü§ñTabTransformer]] ([[@huangTabTransformerTabularData2020]], p. 1 f.) and [[ü§ñFTTransformer]] ([[@gorishniyRevisitingDeepLearning2021]] p. 1) we introduce two promising alternatives. For consistency we adhere to a notation suggested in [[@phuongFormalAlgorithmsTransformers2022]] (p. 1 f) throughout the work.