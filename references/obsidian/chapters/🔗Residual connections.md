Before studying two concrete transformer models, we revisit *residual connections* and *layer norm* in the Transformer block and discuss alternatives for their arrangement. What seems like a pedantic detail, is vital for the training process and convergence of the network.

Recall from our introduction on Transformers (see Chapter [[ü§ñTransformer]]), that both the encoder and decoder stack multiple Transformer blocks, each of which consists off several sub-layers, resulting in a deep networks. As neural networks are commonly trained using back-propagation, which relies on the gradient for the error (with respect to the parameters) to be propagated through the network starting at the last layer, vanishing or exploding gradient pose a major difficulty in training deep neural networks (see e. g. [[@heDeepResidualLearning2015]]).

Thus, stacking multiple layers in the encoder or decoder prevents the gradient information to flow less efficiently through the network, and may affect the overall training performance ([[@wangLearningDeepTransformer2019]] ;  p. 1,811).  As a remedy, [[@vaswaniAttentionAllYou2017]] add residual connections around each sub-layer:

<mark style="background: #FFF3A3A6;">(Formula, similar to Wang et al)
for a solution. Let $\mathcal{F}$ be a sub-layer in encoder or decoder, and $\theta_l$ be the parameters of the sub-layer. A residual unit is defined to be (He et al., 2016b):
$$
\begin{aligned}
x_{l+1} & =f\left(y_l\right) \\
y_l & =x_l+\mathcal{F}\left(x_l ; \theta_l\right)
\end{aligned}
$$
where $x_l$ and $x_{l+1}$ are the input and output of the $l$-th sub-layer, and $y_l$ is the intermediate output followed by the post-processing function $f(\cdot)$. In this way, $x_l$ is explicitly exposed to $y_l$ (see Eq. (2)).</mark>

![[residual-connection.png]]
(from [[@heDeepResidualLearning2015]])

Intuitively, the residual connection provides an alternative path for information to flow through the network, since some information can bypass the sub-layer and is added to its output. Also, exploding or vanishing gradients are mitigated, as gradients can bypass the sub-layer, ultimately resulting in an easier optimization ([[@liuRethinkingSkipConnection2020]]).  Residual connections also help to preserve the positional embeddings ([[üßµPositional Embedding]]) as, the layer's input are maintained in the identity mapping.<mark style="background: #FFB8EBA6;"> (may come back and read this https://transformer-circuits.pub/2021/framework/index.html)</mark>

## Notes


<mark style="background: #CACFD9A6;">For a residual block $x+f(x)$, its shortcut output refers to $x$, its residual branch output refers to $f(x)$, and the dependency on its residual branch refers to $\frac{\operatorname{Var}[f(x)]}{\operatorname{Var}[x+f(x)]}$.(From [[@liuUnderstandingDifficultyTraining2020]])
</mark>


<mark style="background: #ADCCFFA6;">‚ÄúAs in Figure 7, at initialization, a Pre-LN layer has roughly the same dependency on its residual branch and any previous layer, whereas a Post-LN layer has a stronger dependency on its residual branch (more discussions are elaborated in Section 4.1). We find that strong dependencies of Post-LN amplify fluctuations brought by parameter changes and destabilize the training (as in Theorem 2 and Figure 4). Besides, the loose reliance on residual branches in Pre-LN generally limits the algorithm‚Äôs potential and often produces inferior models.‚Äù (Liu et al., 2020, p. 2)</mark>


<mark style="background: #FFF3A3A6;">‚ÄúTo summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme.‚Äù (Raffel et al., 2020, p. 5)</mark>

<mark style="background: #FFB86CA6;">‚ÄúResidual connections (He et al., 2016a) were first introduced to facilitate the training of deep convolutional networks, where the output of the `-th layer F` is summed with its input: x`+1 = x` + F`(x`). (1) The identity term x` is crucial to greatly extending the depth of such networks (He et al., 2016b). If one were to scale x` by a scalar Œª`, then the contribution of x` to the final layer FL is (‚àèL‚àí1 i=` Œªi)x`. For deep networks with dozens or even hundreds of layers L, the term ‚àèL‚àí1 i=` Œªi becomes very large if Œªi > 1 or very small if  for enough i. When backpropagating from the last layer L back to `, these multiplicative terms can cause exploding or vanishing gradients, respectively. Therefore they fix Œªi = 1, keeping the total residual path an identity map.‚Äù (Nguyen and Salazar, 2019, p. 2) [[@nguyenTransformersTearsImproving2019]]</mark>

<mark style="background: #ADCCFFA6;">‚ÄúInspired by He et al. (2016b), we apply LAYERNORM immediately before each sublayer (PRENORM): x`+1 = x` + F`(LAYERNORM(x`)). (3) This is cited as a stabilizer for Transformer training (Chen et al., 2018; Wang et al., 2019) and is already implemented in popular toolkits (Vaswani et al., 2018; Ott et al., 2019; Hieber et al., 2018), though not necessarily used by their default recipes. Wang et al. (2019) make a similar argument to motivate the success of PRENORM in training very deep Transformers. Note that one must append an additional normalization after both encoder and decoder so their outputs are appropriately scaled.‚Äù (Nguyen and Salazar, 2019, p. 2)</mark> [[@nguyenTransformersTearsImproving2019]]

<mark style="background: #FFB86CA6;">Skip Connection bypasses the gradient exploding or vanishing problem and tries to solve the model optimization problem from the perspective of information transfer. It enables the delivery and integration of information by adding an identity mapping from the input of the neural network to the output, which may ease the optimization and allow the error signal to pass through the non-linearities.</mark> [[@liuRethinkingSkipConnection2020]]

<mark style="background: #D2B3FFA6;">‚ÄúWe conjecture this has caused past convergence failures (Popel and Bojar, 2018; Shazeer and Stern, 2018), with LAYERNORMs in the residual path acting similarly to Œªi 6= 1; furthermore, warmup was needed to let LAYERNORM safely adjust scale during early parts of training.‚Äù (Nguyen and Salazar, 2019, p. 2)</mark>

<mark style="background: #BBFABBA6;">One of the main features of the high level architecture of a transformer is that each layer adds its results into what we call the ‚Äúresidual stream.‚Äù¬†2¬†The residual stream is simply the sum of the output of all the previous layers and the original embedding. We generally think of the residual stream as a communication channel, since it doesn't do any processing itself and all layers communicate through it.</mark>

![](data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAAKkAAAChCAYAAAC8hp9CAAAMbWlDQ1BJQ0MgUHJvZmlsZQAASImV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%0AAv8HIEyZRrOwQKgAAAAASUVORK5CYII=)

<mark style="background: #FFB8EBA6;">The residual stream has a deeply linear structure.¬†3¬†Every layer performs an arbitrary linear transformation to "read in" information from the residual stream at the start,¬†4¬†and performs another arbitrary linear transformation before adding to "write" its output back into the residual stream. This linear, additive structure of the residual stream has a lot of important implications. One basic consequence is that the residual stream doesn't have a¬†["privileged basis"](https://transformer-circuits.pub/2021/framework/index.html#def-privileged-basis); we could rotate it by rotating all the matrices interacting with it, without changing model behavior. ([[@elhage2021mathematical]])
</mark>
‚Äú<mark style="background: #FFF3A3A6;">For Transformer, it is not easy to train stacked layers on neither the encoder-side nor the decoderside. Stacking all these sub-layers prevents the efficient information flow through the network, and probably leads to the failure of training. Residual connections and layer normalization are adopted for a solution. Let F be a sub-layer in encoder or decoder, and Œ∏l be the parameters of the sub-layer.</mark>  ([[@wangLearningDeepTransformer2019]])


<mark style="background: #ADCCFFA6;">‚Äú2.2 On the Importance of Pre-Norm for Deep Residual Network The situation is quite different when we switch to deeper models. More specifically, we find that prenorm is more efficient for training than post-norm if the model goes deeper. This can be explained by seeing back-propagation which is the core process to obtain gradients for parameter update. Here we take a stack of L sub-layers as an example. Let E be the loss used to measure how many errors occur in system prediction, and xL be the output of the topmost sub-layer. For post-norm Transformer, given a sub-layer l, the differential of E with respect to xl can be computed by the chain rule, and we have ‚àÇE ‚àÇxl = ‚àÇE ‚àÇxL √ó L‚àí1 ‚àè k=l ‚àÇLN(yk) ‚àÇyk √ó L‚àí1 ‚àè k=l ( 1 + ‚àÇF (xk; Œ∏k) ‚àÇxk ) (5) where ‚àèL‚àí1 k=l ‚àÇ LN(yk ) ‚àÇyk means the backward pass of the layer normalization, and ‚àèL‚àí1 k=l (1 + ‚àÇF(xk;Œ∏k) ‚àÇxk ) means the backward pass of the sub-layer with the residual connection. Likewise, we have the gradient for pre-norm 4: ‚àÇE ‚àÇxl = ‚àÇE ‚àÇxL √ó ( 1+ L‚àí1 ‚àë k=l ‚àÇF (LN(xk); Œ∏k) ‚àÇxl ) (6) Obviously, Eq. (6) establishes a direct way to pass error gradient ‚àÇE ‚àÇxL from top to bottom. Its merit lies in that the number of product items on the right side does not depend on the depth of the stack. In contrast, Eq. (5) is inefficient for passing gradients back because the residual connection is not 3We need to add an additional function of layer normalization to the top layer to prevent the excessively increased value caused by the sum of unnormalized output. 4For a detailed derivation, we refer the reader to Appendix A. a bypass of the layer normalization unit (see Figure 1(a)). Instead, gradients have to be passed through LN(¬∑) of each sub-layer. It in turn introduces term ‚àèL‚àí1 k=l ‚àÇ LN(yk ) ‚àÇyk into the right hand side of Eq. (5), and poses a higher risk of gradient vanishing or exploring if L goes larger. This was confirmed by our experiments in which we successfully trained a pre-norm Transformer system with a 20-layer encoder on the WMT English-German task, whereas the post-norm Transformer system failed to train for a deeper encoder (Section 5.1).‚Äù ([Wang et al., 2019, p. 1812](zotero://select/library/items/BJ43W6ZQ)) ([pdf](zotero://open-pdf/library/items/HVE8Q5EQ?page=3&annotation=AXKHQZJR)) [[@wangLearningDeepTransformer2019]]</mark>