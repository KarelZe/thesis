
1. Why do we want to use pre-training? motivate with paper visiting pre-training objectives  [[@rubachevRevisitingPretrainingObjectives2022]]
2. Explain why self or semi-supervised training is hard(er) for tabular data
4. Overview for tabular data can be found in [[@rubachevRevisitingPretrainingObjectives2022]]. Come up with a selection, that makes sense e. g., does not require to alter the model.
5. Why do we use masked language modelling? -> No change in architecture, conceptually simple, successful in previous works
6. Explain how pre-training works using BERT: See [[@zhangDiveDeepLearning2021]] for BERT implementation and explanation 
7. Why do we not consider meta data, column positions etc. like in Tabi, Tabert etc.?

**To read:**
- good motivation and classification of pre-training objectives: [[@dongTablePretrainingSurvey2022]]
- [[@darabiContrastiveMixupSelf2021]]
- [[@badaroTransformersTabularData]]

## Notes
<mark style="background: #FFF3A3A6;">Difference of self-supervised learning and transfer learning: Recently, self-supervised learning (SSL) [2,3], as an unsupervised pretraining approach, has achieved promisingsuccess and outperforms transfer learning in a wide range of applications [2]. Similar to TL, SSLalso solves predictive tasks. But the output labels in SSL are constructed from the input data, ratherthan annotated by human as in TL. The auxiliary predictive tasks in SSL could be predicting whethertwo augmented data examples originate from the same original data example [3], inpainting maskedregions in images [4], etc. Since SSL does not leverage labels provided by human, it does not havethe risk of being biased to labels in a source task. On the other hand, the potential pitfall of not usinghuman-annotated labels is that the learned representations by SSL may not be as discriminative asthose in TL. (https://www.techrxiv.org/articles/preprint/Transfer_Learning_or_Self-supervised_Learning_A_Tale_of_Two_Pretraining_Paradigms/12502298) </mark> (Do not cite but keep in mind when writing.)

<mark style="background: #ABF7F7A6;">“Datasets like these present huge opportunities for self- and semi-supervised learning algorithms, which can leverage the unlabeled data to further improve the performance of a predictive model.” ([Yoon et al., 2020, p. 1](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=1&annotation=5AB6GP26))</mark>

<mark style="background: #ADCCFFA6;">“Self-supervised learning & contrastive learning. SSL uses unlabeled data with pretext tasks to learn useful representations and most of them are in CV and NLP [20, 17, 15, 16, 58, 23, 59, 60, 61, 62, 63]. Recent SSL tabular models can be classified into reconstruction and contrastive based methods: TabNet [30] and VIME [2] try to recover the corrupted inputs with auto-encoding loss; SCARF [11] takes a SimCLR-like [64] contrastive loss between the sample and its corrupted version; SubTab [9] takes a combination of both. Nevertheless, all fail to learn transferable models across tables such that cannot benefit from pretraining with scale. Contrastive learning can also be applied to supervised learning by leveraging class labels to build positive samples [26]. Our work extends it to to the tabular domain, which we prove works better than vanilla supervised pretraining. The vertical partition sampling also enjoys high query speed from large databases which are often column-oriented [25]. Another line of research takes table pretraining table semantic parsing [65, 66, 67, 68, 69] or table-to-text generation [70, 71]. But these methods either encode the whole table instead of each row or do not demonstrate to benefit tabular prediction yet.” ([Wang and Sun, p. 9](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=9&annotation=CXBL5HGM))</mark>

<mark style="background: #D2B3FFA6;">“Contrastive learning, in which models are pre-trained to be invariant to reordering, cropping, or other label-preserving “views” of the data [5, 12, 15, 32, 43], is a powerful tool in the vision and language domains that has never (to our knowledge) been applied to tabular data.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=HQX7N9D5))

“Existing self-supervised objectives for tabular data include denoising [43], a variation of which was used by VIME [49], masking, and replaced token detection as used by TabTransformer [18]. We find that, while these methods are effective, superior results are achieved by contrastive learning.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=LXSCK4TP))

“It is difficult to craft invariance transforms for tabular data. The authors of VIME [49] use mixup in the non-embedded space as a data augmentation method, but this is limited to continuous data. We instead use CutMix [50] to augment samples in the input space and we use mixup [51] in the embedding space. These two augmentations combined yield a challenging and effective self-supervision task” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=XZYZV3VZ))</mark>

<mark style="background: #FF5582A6;">“Self-supervised learning: Unsupervised representation learning improves supervised learning especially in small data regime (Raina et al. 2007). Recent work for text (Devlin et al. 2018) and image (Trinh, Luong, and Le 2019) data has shown significant advances – driven by the judicious choice of the unsupervised learning objective (masked input prediction) and attention-based deep learning” ([Arik and Pfister, 2020, p. 3](zotero://select/library/items/EH5DCRUW)) ([pdf](zotero://open-pdf/library/items/TPDKX93V?page=3&annotation=B6TI27FJ))</mark>

<mark style="background: #BBFABBA6;">“Image and tabular data are very different. The spatial correlations between pixels in images or the sequential correlations between words in text data are well-known and consistent across different datasets. By contrast, the correlation structure among features in tabular data is unknown and varies across different datasets. In other words, there is no “common” correlation structure in tabular data (unlike in image and text data). This makes the self- and semi-supervised learning in tabular data more challenging. Note that promising methods for image domain do not guarantee the favorable results on tabular domain (vice versa). Also, most augmentations and pretext tasks used in image data are not applicable to tabular data; because they directly utilize the spatial relationship of the image for augmentation (e.g., rotation) and pretext tasks (e.g., jigsaw puzzle and colorization).” ([Yoon et al., 2020, p. 9](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=9&annotation=7PWW4SF4))</mark>

<mark style="background: #CACFD9A6;">“Unfortunately, existing self- and semi-supervised learning algorithms are not effective for tabular data1 because they heavily rely on the spatial or semantic structure of image or language data.” ([Yoon et al., 2020, p. 1](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=1&annotation=IUVCZAIA))</mark> (from [[@yoonVIMEExtendingSuccess2020]])

<mark style="background: #BBFABBA6;">“Standard semi-supervised learning methods also suffer from the same problem, since the regularizers they use for the predictive model are based on some prior knowledge of these data structures” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=ZLC8Z4AD)) 
“The notion of rotation simply does not exist in tabular data.” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=KTJJNLDZ))</mark> [[@yoonVIMEExtendingSuccess2020]]

<mark style="background: #D2B3FFA6;">“Standard semi-supervised learning methods also suffer from the same problem, since the regularizers they use for the predictive model are based on some prior knowledge of these data structures” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=ZLC8Z4AD))</mark>

<mark style="background: #ADCCFFA6;">“A number of self-supervised learning techniques have been proposed in computer vision (Zhai et al., 2019; Tung et al., 2017; Jing & Tian, 2020). One framework involves learning features based on generated images through various methods, including using a GAN (Goodfellow et al., 2014; Donahue et al., 2016; Radford et al., 2015; Chen et al., 2018), predicting pixels (Larsson et al., 2016), predicting colorizations (Zhang et al., 2016; Larsson et al., 2017), ensuring local and global consistency (Iizuka et al., 2017), and learning synthetic artifacts (Jenni & Favaro, 2018). Most related to our approach are contrastive learning ones (Tian et al., 2019; Hassani & Khasahmadi, 2020; Oord et al., 2018; Henaff, 2020; Li et al., 2016; He et al., 2020; Bojanowski & Joulin, 2017; Wang & Gupta, 2015; Gidaris et al., 2018). In particular, our framework is similar to SimCLR (Chen et al., 2020), which involves generating views of a single image via image-based corruptions like random cropping, color distortion and blurring; however, we generate views that are applicable to tabular data. Self-supervised learning has had an especially large impact in language modeling (Qiu et al., 2020). One popular approach is masked language modeling, wherein the model is trained to predict input tokens that have been intentionally masked out (Devlin et al., 2018; Raffel et al., 2019; Song et al., 2019) as well as enhancements to this approach (Liu et al., 2019; Dong et al., 2019; Bao et al., 2020; Lample & Conneau, 2019; Joshi et al., 2020) and variations involving permuting the tokens (Yang et al., 2019; Song et al., 2020). Denoising autoencoders have been used by training them to reconstruct the input from a corrupted version (produced by, for example, token masking, deletion, and infilling) (Lewis et al., 2019; Wang et al., 2019; Freitag & Roy, 2018). Contrastive approaches include randomly replacing words and distinguishing between real and fake phrases (Collobert et al., 2011; Mnih & Kavukcuoglu, 2013), random token replacement (Mikolov et al., 2013; Clark et al., 2020), and adjacent sentences (Joshi et al., 2020; Lan et al., 2019; de Vries et al., 2019).” ([Bahri et al., 2022, p. 3](zotero://select/library/items/JZ2AZEJD)) ([pdf](zotero://open-pdf/library/items/TL73PSVV?page=3&annotation=94UU9FV6))</mark> (proabably the most in-depth overview from [[@bahriSCARFSelfSupervisedContrastive2022]])

<mark style="background: #D2B3FFA6;">“In recent years, the self-supervised learning has successfully been used to learn meaningful representations of the data in natural language processing [34, 41, 11, 28, 10, 21, 9]. A similar success has been achieved in image and audio domains [7, 15, 37, 5, 17, 13, 8]. This progress is mainly enabled by taking advantage of spatial, semantic, or temporal structure in the data through data augmentation [7], pretext task generation [11] and using inductive biases through architectural choices (e.g. CNN for images). However, these methods can be less effective in the lack of such structures and biases in the tabular data commonly used in many fields such as healthcare, advertisement, finance, and law. And some augmentation methods such as cropping, rotation, color transformation etc. are domain specific, and not suitable for tabular setting. The difficulty in designing similarly effective methods tailored for tabular data is one of the reasons why self-supervised learning is under-studied in this domain [46].” ([Ucar et al., 2021, p. 1](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=1&annotation=KZDPXQEV))
</mark> ([[@ucarSubTabSubsettingFeatures2021]])

<mark style="background: #ADCCFFA6;">“Self-supervised learning aims to learn informative representations from unlabeled data. In this subsection, we focus on self-supervised learning with various self-supervised/pretext tasks for a pretext model to solve. These tasks are set to be challenging but highly relevant to the downstream tasks that we attempt to solve. Ideally, the pretext model will extract some useful information from the raw data in the process of solving the pretext tasks. Then the extracted information can be utilized by the predictive model f in the downstream tasks.” ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=29PSENNY))</mark> ([[@yoonVIMEExtendingSuccess2020]])

<mark style="background: #FFB86CA6;">“Recently, Yao et al. (2020) adapted the contrastive framework to large-scale recommendation systems in a way similar to our approach. The key difference is in the way the methods generate multiple views. Yao et al. (2020) proposes masking random features in a correlated manner and applying a dropout for categorical features, while our approach involves randomizing random features based on the features’ respective empirical marginal distribution (in an uncorrelated way).” ([Bahri et al., 2022, p. 3](zotero://select/library/items/JZ2AZEJD)) ([pdf](zotero://open-pdf/library/items/TL73PSVV?page=3&annotation=8MD5CLIW))</mark> [[@bahriSCARFSelfSupervisedContrastive2022]]

<mark style="background: #BBFABBA6;">“Lastly, also similar to our work is VIME (Yoon et al., 2020), which proposes the same corruption technique for tabular data that we do. They pre-train an encoder network on unlabeled data by attaching “mask estimator” and “feature estimator” heads on top of the encoder state and teaching the model to recover both the binary mask that was used for corruption as well as the original uncorrupted input, given the corrupted input. The pre-trained encoder network is subsequently used for semisupervised learning via attachment of a task-specific head and minimization of the supervised loss as well as an auto-encoder reconstruction loss. VIME was shown to achieve state-of-art results on genomics and clinical datasets. The key differences with our work is that we pre-train using a contrastive loss, which we show to be more effective than the denoising auto-encoder loss that partly constitutes VIME. Furthermore, after pre-training we fine-tune all model weights, including the encoder (unlike VIME, which only fine-tunes the task head), and we do so using task supervision only” ([Bahri et al., 2022, p. 3](zotero://select/library/items/JZ2AZEJD)) ([pdf](zotero://open-pdf/library/items/TL73PSVV?page=3&annotation=ZGG37Q2J))</mark> [[@bahriSCARFSelfSupervisedContrastive2022]]

<mark style="background: #FFB8EBA6;">“In this paper, we propose novel self- and semi-supervised learning frameworks for tabular data. For self-supervised learning, we introduce a novel pretext task, mask vector estimation in addition to feature vector estimation.” ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=IVMX9FSR))
</mark> [[@yoonVIMEExtendingSuccess2020]]

<mark style="background: #ADCCFFA6;">“Recently, SSL has been adopted in the tabular domain for semi-supervised learning (saint, tabtransformer etc.). Contrastive pre-training on auxilary unlabelled data (saint) and MLM-like approaches (tabtransformer) have been shown to provide gains over training from scratch for transformer tabular architectures in cases of limited labelled data.” ([Levin et al., 2022, p. 3](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/TRH7QFZ2?page=3&annotation=CYRD7A54))</mark> [[@levinTransferLearningDeep2022]]

<mark style="background: #ABF7F7A6;">“n this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables.” ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=PP423JV9)) “These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019).” ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=JRUQ8NT6)) </mark>[[@yinTaBERTPretrainingJoint2020]]

<mark style="background: #D2B3FFA6;">In domains where established SSL methods are increasingly dominant, such as computer vision, self-supervised learners are known to extract more transferable features than models trained on labelled data [30, 31]. In this section, we compare supervised pre-training with unsupervised pre-training and find that the opposite is true in the tabular domain. We use the Masked Language Model (MLM) pre-training recently adapted to tabular data [34] and the tabular version of contrastive learning [64]. Since both methods were proposed for tabular transformer architectures, we conduct the experiments with the FT-Transformer model. The inferior performance of self-supervised pre-training might be a consequence of the fact that SSL is significantly less explored and tuned in the tabular domain than in vision or NLP.” ([Levin et al., 2022, p. 7](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=7&annotation=YAULJZUP))</mark>

<mark style="background: #FF5582A6;">“he most common approach in tabular data is to corrupt data through adding noise [43]. An autoencoder maps corrupted examples of data to a latent space, from which it maps back to uncorrupted data. Through this process, it learns a representation robust to the noise in the input. This approach may not be as effective since it treats all features equally as if features are equally informative. However, perturbing uninformative features may not result in the intended goal of the corruption. A recent work takes advantage of self-supervised learning in tabular data setting by introducing a pretext task [46], in which a de-noising autoencoder with a classifier attached to representation layer is trained on  corrupted data. The classifier’s task is to predict the location of corrupted features. However, this framework still relies on noisy data in the input. Additionally, training a classifier on an imbalanced binary mask for a high-dimensional data may not be ideal to learn meaningful representations.” ([Ucar et al., 2021, p. 2](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=2&annotation=3753MUZQ))</mark>


<mark style="background: #FFB86CA6;">“Masked Language Modeling (MLM) was first proposed for language models by Devlin et al. [20] as a powerful unsupervised learning strategy. MLM involves training a model to predict tokens in text masked at random so that its learned representations contain information useful for reconstructing these masked tokens. In the tabular domain, instead of masking tokens, a random subset of features is masked for each sample, and the masked values are predicted in a multi-target classification manner [34]. In our experiments, we mask one randomly selected feature for each sample, asking the network to learn the structure of the data and form representations from n − 1 features that are useful in producing the value in the n-th feature. For more detail, see Appendix A.” ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=DC87W4J4))
</mark>

<mark style="background: #FFF3A3A6;">“Contrastive pre-training uses data augmentations to generate positive pairs, or two different augmented views of a given example, and the loss function encourages a feature extractor to map positive pairs to similar features. Meanwhile, the network is also trained to map negative pairs, or augmented views of different base examples, far apart in feature space. We use the implementation of contrastive learning from Somepalli et al. [64]. In particular, we generate positive pairs by applying two data augmentations: CutMix [80] in the input space and Mixup [81] in the embedding space. For more details, see Appendix A” ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=J9CNVZ4T))</mark>

<mark style="background: #FF5582A6;">“Contrastive learning, in which models are pre-trained to be invariant to reordering, cropping, or other label-preserving “views” of the data [5, 12, 15, 32, 43], is a powerful tool in the vision and language domains that has never (to our knowledge) been applied to tabular data.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=HQX7N9D5))</mark> [[@somepalliSAINTImprovedNeural2021]]

<mark style="background: #FFF3A3A6;">“In this work, we turn the problem of learning representation from a single-view of the data into the one learnt from its multiple views by dividing the features into subsets, akin to cropping in image domain or feature bagging in ensemble learning, to generate different views of the data. Each subset can be considered a different view. We show that reconstructing data from the subset of its features forces the encoder to learn better representation than the ones learned through the existing methods such as adding noise.” ([Ucar et al., 2021, p. 2](zotero://select/library/items/F9DTPDH5)) ([pdf](zotero://open-pdf/library/items/MLWKHKKR?page=2&annotation=YC5S6D4S))</mark> [[@ucarSubTabSubsettingFeatures2021]] (Some AE like approach inspired by VIME. Requires change in loss etc.)

<mark style="background: #ABF7F7A6;">“Self-Supervised Learning Self-supervision via a ‘pretext task’ on unlabeled data coupled with finetuning on labeled data is widely used for improving model performance in language and computer vision. Some of the tasks previously used for self-supervision on tabular data include masking, denoising, and replaced token detection. Masking (or Masked Language Modeling(MLM)) is when individual features are masked and the model’s objective is to impute their value [1, 18, 32]. Denoising injects various types of noise into the data, and the objective there is to recover the original values [43, 49]. Replaced token detection (RTD) inserts random values into a given feature vector and seeks to detect the location of these replacements [18, 20]. Contrastive pre-training, where the distance between two views of the same point is minimized while maximizing the distance between two different points [5, 12, 15], is another pretext task that applies to tabular data. In this paper, to the best of our knowledge, we are the first to adopt contrastive learning for tabular data. We couple this strategy with denoising to perform pre-training on a plethora of datasets with varied volumes of labeled data, and we show that our method outperforms traditional boosting methods.” ([Somepalli et al., 2021, p. 3](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=3&annotation=NYSIVQZX)) [[@somepalliSAINTImprovedNeural2021]]</mark>

<mark style="background: #ADCCFFA6;">“Existing self-supervised objectives for tabular data include denoising [43], a variation of which was used by VIME [49], masking, and replaced token detection as used by TabTransformer [18]. We find that, while these methods are effective, superior results are achieved by contrastive learning.” ([Somepalli et al., 2021, p. 5](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=5&annotation=LXSCK4TP))</mark>

<mark style="background: #FFB86CA6;">“In Figure 3, we compare supervised pre-training with contrastive and MLM pre-training strategies and show that supervised pre-training always attains the best average rank. Contrastive pre-training produces better results than training from scratch on the downstream data when using a linear head, but it is still inferior to supervised pre-training. Tabular MLM pretraining also falls behind the supervised strategy and performs comparably to training from scratch in the lower data regimes but leads to a weaker downstream model in the higher data regimes.” ([Levin et al., 2022, p. 8](zotero://select/library/items/GNKZPFYK)) ([pdf](zotero://open-pdf/library/items/QCVUFCDQ?page=8&annotation=WVB866BS))</mark>

<mark style="background: #FFF3A3A6;">“Semi-supervised setting We perform 3 sets of experiments with 50, 200, and 500 labeled data points (in each case the rest are unlabeled). See Table 3 for numerical results. In all cases, the pre-trained SAINT model (with both self and intersample attention) performs the best. Interestingly, we note that when all the training data samples are labeled, pre-training does not contribute appreciably, hence the results with and without pre-training are fairly close.” ([Somepalli et al., 2021, p. 8](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=8&annotation=ZJHH2RKT))</mark>

<mark style="background: #D2B3FFA6;">“Finally, for the first time for tabular data, we show significant performance improvements by using unsupervised pre-training to predict masked features (see Fig. 2” ([Arik and Pfister, 2020, p. 1](zotero://select/library/items/EH5DCRUW)) ([pdf](zotero://open-pdf/library/items/TPDKX93V?page=1&annotation=X5WBP7CA))</mark> [[@arikTabNetAttentiveInterpretable2020]]

<mark style="background: #BBFABBA6;">“We explore two different types of pre-training procedures, the masked language modeling (MLM) (Devlin et al. 2019) and the replaced token detection (RTD) (Clark et al. 2020).” ([Huang et al., 2020, p. 3](zotero://select/library/items/MH4GW34I)) ([pdf](zotero://open-pdf/library/items/QYWHEUYE?page=3&annotation=7SXSI5CA))</mark> [[@huangTabTransformerTabularData2020]]

<mark style="background: #FF5582A6;">“Note, the pre-training is only applied in semi-supervised scenario. We do not find much benefit in using it when the entire data is labeled. Its benefit is evident when there is a large number of unlabeled examples and a few labeled examples.” ([Huang et al., 2020, p. 4](zotero://select/library/items/MH4GW34I)) ([pdf](zotero://open-pdf/library/items/QYWHEUYE?page=4&annotation=VN52BT3H))</mark>

<mark style="background: #FFB86CA6;">“Furthermore, we observe that when the number of unlabeled data is small as shown in Table 4, TabTransformerRTD performs better than TabTransformer-MLM, thanks to its easier pre-training task (a binary classification) than that of MLM (a multi-class classification). This is consistent with the finding of the ELECTRA paper (Clark et al. 2020).” ([Huang et al., 2020, p. 7](zotero://select/library/items/MH4GW34I)) ([pdf](zotero://open-pdf/library/items/QYWHEUYE?page=7&annotation=HU3JVTCV))</mark> ([[@huangTabTransformerTabularData2020]])

<mark style="background: #ADCCFFA6;">“Tabular self-supervised learning: We propose a decoder architecture to reconstruct tabular features from the TabNet encoded representations. The decoder is composed of feature transformers, followed by FC layers at each decision step. The outputs are summed to obtain the reconstructed features.” ([Arik and Pfister, 2020, p. 5](zotero://select/library/items/EH5DCRUW)) ([pdf](zotero://open-pdf/library/items/TPDKX93V?page=5&annotation=3QTDAR2A))
</mark>

Limitations of all these approaches? <mark style="background: #D2B3FFA6;">“Recent works enhance tabular ML modeling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=3CDUAZDS))</mark> [[@wangTransTabLearningTransferable]]
