[[@vaswaniAttentionAllYou2017]]
groshiny paper?



- nice visualization / explanation of self-attention. https://peltarion.com/blog/data-science/self-attention-video

- intuition behind multi-head and self-attention e. g. cosine similarity, key and querying mechanism: https://www.youtube.com/watch?v=mMa2PmYJlCo&list=PL86uXYUJ7999zE8u2-97i4KG_2Zpufkfb
- visualization of attention using a dedicated programming language https://github.com/srush/raspy
- Course on nlp / transformers: https://phontron.com/class/anlp2022/schedule.html
- 
