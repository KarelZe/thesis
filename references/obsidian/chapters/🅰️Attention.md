[[🅰️attention notes]]

## Agenda
- What attention is?
	- Where was it introduced?
	- Explain attention in on *catchy* sentence.
	- Why it is key to the transformer?
- What is self-attention?
	- How is it different from the standard attention?
	- Why was it proposed in the first place?
- What is multi-headed attention?
	- Why do we decompose?
	- How do different heads attend to different characteristics?
	- What is the importance of different heads?
	- Why can some heads be pruned?
- What is cross-attention?
	- Why is it needed to mask attention in the decoder?
	- What is the appropriate masking strategy?


