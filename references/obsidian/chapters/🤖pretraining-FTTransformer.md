- Variant of the classical transformer, but for tabular data. Published in [[@gorishniyRevisitingDeepLearning2021]]
- Firstly, Feature Tokenizer transforms features to embeddings. The embeddings are then processed by the Transformer module and the final representation of the (CLS) token is used for prediction.
- Very likely interpretable... 
- Work out differences between the three:

- NeurIPS paper and presentation: https://slideslive.com/38968794/revisiting-deep-learning-models-for-tabular-data?ref=recommended
- 

![[comparison.png]]