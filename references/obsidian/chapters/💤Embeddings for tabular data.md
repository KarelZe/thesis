#transformer #embeddings #numerical #continous #categorical #tabular

- Look into:
	- [[@gorishniyRevisitingDeepLearning2021]] 
	- [[@guoEmbeddingLearningFramework2021]] 
	- [[@wangTransTabLearningTransferable]]
	- On embeddings in general see [[@hancockSurveyCategoricalData2020]]. They refer to a learned embedding matrix as "automatic embeddings".

## Notes
Embeddings are a memory efficient way of representing one-hot encoded features that can also capture additional relationships between factors (or levels) of that variable. (https://medium.com/@michi.jeremias/embeddings-in-tabular-data-990202daa59f). See also their explanation on the relation between one-hot-encoding and embeddings.

“The investigation in (Grinsztajn et al., 2022) pointed out three inherent characteristics of tabular data that impeded known neural networks from top-tier performances, including irregular patterns of the target function, the negative effects of uninformative features, and the nonrotationally-invariant features. Based on this, we furthermore identify two points that highly promote the capabilities of neural networks on tabular data. (i) <mark style="background: #BBFABBA6;">An appropriate feature embedding approach. Though it was demonstrated (Rahaman et al., 2019; Grinsztajn et al., 2022) that neural networks are likely to predict overly smooth solutions on tabular data, a deep learning model was also observed to be capable of memorizing random labels (Zhang et al., 2021). Since the target function patterns are irregular and spurious correlations between the targets and features exist, an appropriate feature embedding network should well fit the irregular patterns while maintaining generalizability.</mark> (ii) A careful feature interaction approach. Since features of tabular data are non-rotationally-variant and a considerable portion of them are uninformative, it harms the generalization when a model incorporates needless feature interactions.” ([Chen et al., 2023, p. 1](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=1&annotation=49WB9QDJ)) (Do not cite paper, as it seems to be rather low-quality otherwise; cite Gristajin instead)

<mark style="background: #FF5582A6;">“Apart from model designs, various data representation approaches, such as feature embedding (Gorishniy et al., 2022), discretization of continuous features (Guo et al., 2021; Wang et al., 2020), and rule search approaches (Wang et al., 2021), were proposed against the irregular target patterns (Tancik et al., 2020; Grinsztajn et al., 2022).” ([Chen et al., 2023, p. 2](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=2&annotation=65MMYTUW))</mark>

“To answer all those questions, we propose to relax fixed table structures by introducing a Transferable Tabular Transformer (TransTab) for tables. The goal of TransTab is to convert each sample (a row in the table) to a <mark style="background: #FF5582A6;">generalizable embedding vector</mark>, and then apply stacked transformers for feature encoding.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=IRSLFH22)). “The other insight is to introduce supervised and self-supervised pretraining to improve model performance. We compare TransTab with multiple baseline methods on diverse benchmark datasets and five oncology clinical trial datasets. Overall, TransTab ranks 1.00, 1.00, 1.78 out of 12 methods in supervised learning, feature incremental learning, and transfer learning scenarios, respectively; and the proposed <mark style="background: #FF5582A6;">pretraining leads to 2.3% AUC lift on average over the supervised learning.</mark>” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=E6CC35EC)) [[@wangLearningDeepTransformer2019]]

“<mark style="background: #FFB86CA6;">Some previous approaches either designed feature embedding approaches (Gorishniy et al., 2022) to alleviate overly smooth solutions inspired by (Tancik et al., 2020) </mark>or employed regularization (Katzir et al., 2020) and shallow models (Cheng et al., 2016) to promote the model generalization, while some neural networks were equipped with sophisticated feature interaction approaches (Yan et al., 2023; Chen et al., 2022; Gorishniy et al., 2021) for better selectively feature interactions.” ([Chen et al., 2023, p. 1](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=1&annotation=X3Z257LE)) (pretty intuitive)

<mark style="background: #FFF3A3A6;">“Recent works enhance tabular ML modeling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=3CDUAZDS))</mark>

