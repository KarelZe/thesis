#transformer #embeddings #numerical #continous #categorical #tabular

- Look into:
	- [[@gorishniyRevisitingDeepLearning2021]] 
	- [[@guoEmbeddingLearningFramework2021]] 
	- [[@wangTransTabLearningTransferable]]
	- On embeddings in general see [[@hancockSurveyCategoricalData2020]]. They refer to a learned embedding matrix as "automatic embeddings".

## Notes
Embeddings are a memory efficient way of representing one-hot encoded features that can also capture additional relationships between factors (or levels) of that variable. (https://medium.com/@michi.jeremias/embeddings-in-tabular-data-990202daa59f). See also their explanation on the relation between one-hot-encoding and embeddings.

“The investigation in (Grinsztajn et al., 2022) pointed out three inherent characteristics of tabular data that impeded known neural networks from top-tier performances, including irregular patterns of the target function, the negative effects of uninformative features, and the nonrotationally-invariant features. Based on this, we furthermore identify two points that highly promote the capabilities of neural networks on tabular data. (i) <mark style="background: #BBFABBA6;">An appropriate feature embedding approach. Though it was demonstrated (Rahaman et al., 2019; Grinsztajn et al., 2022) that neural networks are likely to predict overly smooth solutions on tabular data, a deep learning model was also observed to be capable of memorizing random labels (Zhang et al., 2021). Since the target function patterns are irregular and spurious correlations between the targets and features exist, an appropriate feature embedding network should well fit the irregular patterns while maintaining generalizability.</mark> (ii) A careful feature interaction approach. Since features of tabular data are non-rotationally-variant and a considerable portion of them are uninformative, it harms the generalization when a model incorporates needless feature interactions.” ([Chen et al., 2023, p. 1](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=1&annotation=49WB9QDJ)) (Do not cite paper, as it seems to be rather low-quality otherwise; cite Gristajin instead)

<mark style="background: #FF5582A6;">“Apart from model designs, various data representation approaches, such as feature embedding (Gorishniy et al., 2022), discretization of continuous features (Guo et al., 2021; Wang et al., 2020), and rule search approaches (Wang et al., 2021), were proposed against the irregular target patterns (Tancik et al., 2020; Grinsztajn et al., 2022).” ([Chen et al., 2023, p. 2](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=2&annotation=65MMYTUW))</mark>

“To answer all those questions, we propose to relax fixed table structures by introducing a Transferable Tabular Transformer (TransTab) for tables. The goal of TransTab is to convert each sample (a row in the table) to a <mark style="background: #FF5582A6;">generalizable embedding vector</mark>, and then apply stacked transformers for feature encoding.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=IRSLFH22)). “The other insight is to introduce supervised and self-supervised pretraining to improve model performance. We compare TransTab with multiple baseline methods on diverse benchmark datasets and five oncology clinical trial datasets. Overall, TransTab ranks 1.00, 1.00, 1.78 out of 12 methods in supervised learning, feature incremental learning, and transfer learning scenarios, respectively; and the proposed <mark style="background: #FF5582A6;">pretraining leads to 2.3% AUC lift on average over the supervised learning.</mark>” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=E6CC35EC)) [[@wangLearningDeepTransformer2019]]

“<mark style="background: #FFB86CA6;">Some previous approaches either designed feature embedding approaches (Gorishniy et al., 2022) to alleviate overly smooth solutions inspired by (Tancik et al., 2020) </mark>or employed regularization (Katzir et al., 2020) and shallow models (Cheng et al., 2016) to promote the model generalization, while some neural networks were equipped with sophisticated feature interaction approaches (Yan et al., 2023; Chen et al., 2022; Gorishniy et al., 2021) for better selectively feature interactions.” ([Chen et al., 2023, p. 1](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=1&annotation=X3Z257LE)) (pretty intuitive)

<mark style="background: #FFF3A3A6;">“Recent works enhance tabular ML modeling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=3CDUAZDS))</mark>

Reason why embeddings could work. <mark style="background: #ABF7F7A6;">“Why are MLPs much more hindered by uninformative features, compared to other models? One answer is that this learner is rotationally invariant in the sense of Ng [2004]: the learning procedure which learns an MLP on a training set and evaluate it on a testing set is unchanged when applying a rotation (unitary matrix) to the features on both the training and testing set. On the contrary, tree-based models are not rotationally invariant, as they attend to each feature separately, and neither are FT Transformers, because of the initial FT Tokenizer, which implements a pointwise operation theoretical link between this concept and uninformative features is provided by Ng [2004], which shows that any rotationallly invariant learning procedure has a worst-case sample complexity that grows at least linearly in the number of irrelevant features. Intuitively, to remove uninformative features, a rotationaly invariant algorithm has to first find the original orientation of the features, and then select the least informative ones: the information contained in the orientation of the data is lost.” ([Grinsztajn et al., 2022, p. 8](zotero://select/library/items/G3KP2Z9W)) ([pdf](zotero://open-pdf/library/items/A3KU4A43?page=8&annotation=W6LGGVAC))

“Our findings shed light on the results of Somepalli et al. [2021] and Gorishniy et al. [2022], which add an embedding layer, even for numerical features, before MLP or Transformer models this layer breaks rotation invariance. The fact that very different types of embedding seem to improve performance suggests that the sheer presence of an embedding which breaks the invariance is a key part of these improvements. We note that a promising avenue for further research would be to find other ways to break rotation invariance which might be less computationally costly than embeddings.” ([Grinsztajn et al., 2022, p. 9](zotero://select/library/items/G3KP2Z9W)) ([pdf](zotero://open-pdf/library/items/A3KU4A43?page=9&annotation=3DIWPNHH))</mark>

