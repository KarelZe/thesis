In the chapter [[ü§ñTransformer]] we have shown that processing [[üõåToken Embedding]]s and contextualizing them, is the core idea behind transformers. Yet, [[üõåToken Embedding]]s  are tailored towards data in text representation. With all tokens coming from the same vocabulary, a homogeneous embedding procedure suffices. As this work is concerned about trade classification on tabular datasets containing both numerical and categorical features, the aforementioned concept is not directly applicable and must be evolved to a generic feature embedding. We do this separately for categorical and numerical features.

Tabular data is flexible with regard to the columns, their data type, ~~their distribution~~, and their semantics. While features maintain a shared meaning across rows, or samples, no universal semantics can be assumed across columns. For instance, every sample in a trade data set may contain the previous trade price, yet the the meaning of the trade price is different from other columns, urging the need for heterogeneous embeddings. 

**Numerical embedding** üî¢
Columns may be categorical or numerical. Transformer-like architectures handle numerical features by mapping the scalar value to a high-dimensional embedding vector and process sequences thereoff [[@gorishniyEmbeddingsNumericalFeatures2022]]. In the simplest case, a learned linear projection is utilized to obtain the embedding. Linear embeddings of numerical features were previously explored in [[@kossenSelfAttentionDatapointsGoing2021]], [[@somepalliSAINTImprovedNeural2021]], [[@chengWideDeepLearning2016]], or [[@gorishniyRevisitingDeepLearning2021]]. More sophisticated approaches rely on parametric embeddings, like the *piece-wise linear encoding* or the *periodic encoding* of [[@gorishniyEmbeddingsNumericalFeatures2022]]. Both enforce a non-linear mapping. [[@gorishniyEmbeddingsNumericalFeatures2022]] show that these can alleviate model's performance, but at an additional computational cost. Alternatively, numerical features can be processed as a scalar in non-transformer-based networks and therefore independent from other features. We explore this idea as part of our discussion on [[ü§ñTabTransformer]]. Despite this simpler alternative, numerical embeddings are desirable, as a recent line of research, e. g.,  [[@gorishniyEmbeddingsNumericalFeatures2022]] and [[@somepalliSAINTImprovedNeural2021]] suggests, that numerical embedding can significantly improve performance and robustness to missing values or noise of transformers. Exemplary, [[@somepalliSAINTImprovedNeural2021]] report an increase *AUC* (ROC) from 89.38 % to 91.72 % merely through embedding numerical features. Their work however offers no theoretical explanation. [[@grinsztajnWhyTreebasedModels2022]] (p. 8f.) fill this void. The authors find, that the mere use of embeddings breaks rotation invariance. *Rotational invariance* in the spirit of [[@ngFeatureSelectionVs2004]] refers to the model's dependency,   <mark style="background: #ABF7F7A6;">identify the rotational variance (...) are more. Would be interesting to know, but not sure how to integrate</mark>  

**Categorical embeddings** üóÉÔ∏è
Recall from chapter [[ü•†Selection of approaches (supervised)]] that categorical data is data, that is divided into groups. Categories can be ordered (*ordinal*) or arranged arbitrarily (*nominal*). In the context of trade classification, the option type is categorical and takes values $\{\text{'C'},\text{'P'}\}$ for calls and puts. Similar to a token, a category, e. g., $\text{'P'}$ in the previous example, must be represented as a multi-dimensional vector to be handled by the transformer. Even when processed in other types of neural networks, categories need to be converted to real-valued inputs first, in order to optimize parameters with gradient descent.

A classical strategy is to apply one-hot-encoding to categorical features, whereby each category is mapped to a sparse vector, which can then be processed by a neural network. <mark style="background: #D2B3FFA6;">standard basis ($\mathbf{e}_{1}$ etc. / orthogonal and equidistant -> found these properties in [[@cerdaEncodingHighcardinalityString2022]]) </mark> While this approach is conceptually simple and frequently employed in neural network architectures, it has several drawbacks like resulting in sparse vectors, where the cardinality of feature directly affects the one-hot vector. For instance, applying one-hot-encoding to the categorical underlyings $\texttt{GOOGL}$ (Alphabet Inc.), $\texttt{MSFT}$ (Microsoft Inc.), and $\texttt{K}$ (Kellogg Company) would result in sparse vectors equidistant in terms of cosine distance. Naturally, one would expect a greater similarity between the first two underlyings due to overlapping field of operations. 

For transformer-based architectures learned, categorical embeddings are common, which are a direct adaption of token embeddings ([[@wangTransTabLearningTransferable]], [[@gorishniyRevisitingDeepLearning2021]], [[@huangTabTransformerTabularData2020]], [[@somepalliSAINTImprovedNeural2021]]). A category is mapped to an embedding vector using a learned, embedding matrix, as in Equation [[üõåToken Embedding#^4bee48]]. These embeddings can potentially capture intrinsic properties of categorical variables by arranging similar items closer in the embedding space. For high cardinal variables, learned embeddings also have the advantage of being memory efficient, as the length of the embedding vector is untied from the cardinality of the variable [[@guoEntityEmbeddingsCategorical2016]] (p. 1). Despite these advantages, learned, categorical embeddings still lack a sound theoretical foundation and remain an open research problem [[@hancockSurveyCategoricalData2020]] (p. 28). In a similar vain, [[@borisovDeepNeuralNetworks2022]] (p. 2) note, that handling high-dimensional categoricals has not been resolved by existing approaches. Being dependent on few samples, high cardinality is equally problematic for learned embeddings. We acknowledge this issue in later chapters.

**Positional embeddings üßµ:**
In chapter [[üßµPositional Embedding]] we reckoned that using only the token embeddings would lose the ordering of the sequence and applied a positional encoding to overcome this issue. Contrary to sequences, columns in a tabular datasets may be arranged in an arbitrary order, hence no positional embedding is required, unless the feature embedding function is shared across features ([[@huangTabTransformerTabularData2020]]; p. 3; [[@somepalliSAINTImprovedNeural2021]]; p. 15). For shared embeddings the problem faced, is similar to the one found with [[üõåToken Embedding]], as the model wouldn't be able to relate the embeddings to specific features.  

Like in chapter [[üõåToken Embedding]] the dimension of the embedding $e_{d}$ affects the expressiveness of the network and is a tunable hyerparameter. One major drawback of learned embeddings is, that they contribute to the parameter count of the model through the embedding matrix or increased layer capacity of subsequent layers. ~~Feature embeddings are flexible way to to represent tabular data. Though typically, models assume a static table structure which restricts the transferability across tables [[@wangTransTabLearningTransferable]]~~. To this end, embeddings are non-exclusive to transformer-based architectures, and can be used in other deep learning-based approaches, and even classical machine learning models, like [[üêàgradient-boosting]]. Covering these combinations is outside the scope of this work. We refer the reader to [[@gorishniyEmbeddingsNumericalFeatures2022]] for an in-depth comparison. Next, our focus is on two concrete examples of transformers for tabular data.

## Notes
[[üí§Embeddings for tabular data notes]]

