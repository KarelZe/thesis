
The *Transformer* is a neural network architecture of [[@vaswaniAttentionAllYou2017]] (p. 2 f.) originally proposed for sequence-to-sequence modelling. Its original application is in machine translation, whereby arbitrary sentences in the source language are translated into sentences in the target language. More generally, a sequence of tokens (words) is transformed into an output sequence. 

Following [[@sutskeverSequenceSequenceLearning2014]] (p. 3) the network features two main components: the *encoder* and the *decoder*. A sequence of tokens e. g., the sentence in the source language, is first mapped to a sequence of embeddings and augmented with positional information. The embedding is a rich vector representation of the raw input. The encoder receives the embeddings and creates a enriched representation from it by encoding the context in which the input appears. The output of the encoder is then fed to the decoder. The decoder takes the embedded target sequence along with parts of the encoded representation of the input, to autoregressively generate the output sequence, i. e., the translation in the target language token by token ([[@vaswaniAttentionAllYou2017]]; p. 3). The complete architecture is depicted in Figure [[ü§ñTransformer#^2cf7ee]].
![[classical_transformer_architecture.png]]
(own drawing after [[@daiTransformerXLAttentiveLanguage2019]] (p. 3) Alternatives https://github.com/negrinho/sane_tikz. The right half of the image depicts the encoder and decoder stack. Both components are preceded by embedding units, which we cover in [[üõåToken Embedding]] and [[üßµPositional Embedding]]. The left half depicts the multi-headed self-attention and the self-attention which we cover in [[üÖ∞Ô∏èAttention]]) ^2cf7ee

The encoder (*left*) consists of $L$ stacked transformer blocks, whereby $L$ is typically set to $6$ ([[@vaswaniAttentionAllYou2017]]; p. 6). Stacking multiple transformer blocks allows the model to extract hierarchical features from the inputs and targets. As such the first layers in the stack extracts coarse-grained syntactic features and subsequent layers learn fine-grained semantic features ( [[@jawaharWhatDoesBERT2019]] (p. 3651); [[@tenneyBERTRediscoversClassical2019]]) (p. 4,596)). Each block itself is composed of two sub-layers: a multi-head self-attention layer, followed by a position-wise, feed-forward network. In the encoder, inputs can attend to any other part of the input sequence. Each of these sub-layers is surrounded by skip connections ([[@heDeepResidualLearning2015]]), whereby the input of the sub-layer is added to the sub-layer output. Finally, layer normalization ([[@baLayerNormalization2016]]) is applied to stabilize training. 

Aside from the multi-headed self-attention and feed-forward sub-layer, the decoder (*right*) contains a third sub-layer for multi-headed self-attention on the output of the encoder, known as *cross attention*. Also, the multi-headed self-attention mechanism in the decoder differs from the one in the encoder. Specifically, future parts of the output sequence are causally masked to prevent the model from attending to subsequent positions during training. ([[@vaswaniAttentionAllYou2017]], p. 3) ([[@narangTransformerModificationsTransfer2021]], p. 15). The output of the decoder is finally passed through a linear layer with a softmax activation function to unembed the output and retrieve the probabilities for the next token ([[@vaswaniAttentionAllYou2017]]) (p. 5). Since the output sequence is generated autoregressively, the most probable token is fed back as input to the decoder to provide context for the following token until the remaining sequence is generated.

For its original application, machine translation, both the encoder and decoder are used. Yet, the modular design allows adapting transformers to a wide range of use cases, some of which only require the encoder or decoder. [[@raffelExploringLimitsTransfer2020]] (p. 16 f.) differentiate these modes: 
1. **encoder-only:** use of only the encoder stage e. g., in sentiment classification,
2. **decoder-only:** Use of the decoder stage e. g., in auto-completion of sequences, 
3. **encoder-decoder:** Use of both the encoder and decoder stages e. g., in translation.

As our focus is on the probabilistic classification of tabular data, the goal is to learn an enriched representation of the input, which can be used for classifying the label. As such, *encoder-only* transformers suffice. 

This chapter can only provide a high level overview of the transformer. A thorough explanation of all architectural components is given in the subsequent chapters. Thereafter, we transfer the transformer of [[@vaswaniAttentionAllYou2017]] (p. 1) to the tabular domain. With the [[ü§ñTabTransformer]] ([[@huangTabTransformerTabularData2020]], p. 1 f.) and [[ü§ñFTTransformer]] ([[@gorishniyRevisitingDeepLearning2021]] p. 1) we present two concrete adaptations. Throughout this work we adhere to a notation suggested in [[@phuongFormalAlgorithmsTransformers2022]] (p. 1 f) to maintain consistency.


