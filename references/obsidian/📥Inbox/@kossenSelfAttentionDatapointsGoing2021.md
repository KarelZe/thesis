*title:* Self-attention between datapoints: going beyond individual input-output pairs in deep learning
*authors:* Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, Yarin Gal
*year:* 2021
*tags:* #numeric-embeddings #embeddings #self-attention #transformer
*status:* #📦 
*related:*
*code:*
*review:*

## Notes 📍

## Annotations 📖
“We stack these and apply an identical linear embedding to each of n datapoints, obtaining an input representation H(0) ∈ Rn×d×e (Fig. 2b).” ([Kossen et al., 2021, p. 3](zotero://select/library/items/WCUZUJHD)) ([pdf](zotero://open-pdf/library/items/DPPA9MFF?page=3&annotation=VKPT3UA2))