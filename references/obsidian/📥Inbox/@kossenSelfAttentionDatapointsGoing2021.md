*title:* Self-attention between datapoints: going beyond individual input-output pairs in deep learning
*authors:* Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, Yarin Gal
*year:* 2021
*tags:* #numeric-embeddings #embeddings #self-attention #transformer
*status:* #ğŸ“¦ 
*related:*
*code:*
*review:*

## Notes ğŸ“

## Annotations ğŸ“–
â€œWe stack these and apply an identical linear embedding to each of n datapoints, obtaining an input representation H(0) âˆˆ RnÃ—dÃ—e (Fig. 2b).â€ ([Kossen et al., 2021, p. 3](zotero://select/library/items/WCUZUJHD)) ([pdf](zotero://open-pdf/library/items/DPPA9MFF?page=3&annotation=VKPT3UA2))