*title:* Designing Machine Learning Systems
*authors:* Chip Huyen
*year:* 
*tags:* #feature-enginering #target-leakage #evaluation #robustness #pre-training 
*status:* #üì¶ 
*related:* [[@banachewiczKaggleBookData2022]]
*code:*
*review:*

## Notes üìç

## Annotations üìñ

‚ÄúNonprobability sampling is when the selection of data isn‚Äôt based on any probability criteria. Here are some of the criteria for nonprobability sampling:‚Äù ([Huyen, p. 83](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=103&annotation=T3X54EM2))

‚ÄúThe samples selected by nonprobability criteria are not representative of the realworld data and therefore are riddled with selection biases.‚Äù ([Huyen, p. 83](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=103&annotation=JRDUAQGG))

‚ÄúNonprobability sampling can be a quick and easy way to gather your initial data to get your project off the ground. However, for reliable models, you might want to use probability-based sampling, which we will cover next.‚Äù ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=AWZPM7YD))

‚ÄúSimple Random Sampling In the simplest form of random sampling, you give all samples in the population equal probabilities of being selected.4 For example, you randomly select 10% of the population, giving all members of this population an equal 10% chance of being selected.‚Äù ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=WAV65NAQ))

‚ÄúThe advantage of this method is that it‚Äôs easy to implement. The drawback is that rare categories of data might not appear in your selection.‚Äù ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=ZF42UQVI))

‚ÄúTo avoid the drawback of simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately.‚Äù ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=IZGKJRQB))

‚ÄúThis method allows you to leverage domain expertise. For example, if you know that a certain subpopulation of data, such as more recent data, is more valuable to your model and want it to have a higher chance of being selected, you can give it a higher weight‚Äù ([Huyen, p. 85](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=105&annotation=ZRILC2ZN))

‚ÄúImportance Sampling Importance sampling is one of the most important sampling methods, not just in ML. It allows us to sample from a distribution when we only have access to another distribution‚Äù ([Huyen, p. 87](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=107&annotation=IIWWB4YX))

‚ÄúWeak supervision Leverages (often noisy) heuristics to generate labels No, but a small number of labels are recommended to guide the development of heuristics Semisupervision Leverages structural assumptions to generate labels Yes, a small number of initial labels as seeds to generate more labels Transfer learning Leverages models pretrained on another task for your new task No for zero-shot learning Yes for fine-tuning, though the number of ground truths required is often much smaller than what would be needed if you train the model from scratch‚Äù ([Huyen, p. 94](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=114&annotation=79C2ZXFA))

‚ÄúIf weak supervision leverages heuristics to obtain noisy labels, semi-supervision leverages structural assumptions to generate new labels based on a small set of initial labels. Unlike weak supervision, semi-supervision requires an initial set of labels‚Äù ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=XV9C9DB9))

‚ÄúFor a comprehensive review, I recommend ‚ÄúSemi-Supervised Learning Literature Survey‚Äù (Xiaojin Zhu, 2008) and ‚ÄúA Survey on Semi-Supervised Learning‚Äù (Engelen and Hoos, 2018)‚Äù ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=YMPAQ9IX))

‚ÄúA classic semi-supervision method is self-training. You start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples. Assuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set. This goes on until you‚Äôre happy with your model performance‚Äù ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=I7J7ZKVS))

‚ÄúAnother semi-supervision method assumes that data samples that share similar characteristics share the same labels.‚Äù ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=4M4T6I5T))

‚ÄúIn some cases, semi-supervision approaches have reached the performance of purely supervised learning, even when a substantial portion of the labels in a given dataset has been discarded.17‚Äù ([Huyen, p. 99](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=119&annotation=G8GCXEWJ))

‚ÄúSemi-supervision is the most useful when the number of training labels is limited. One thing to consider when doing semi-supervision with limited data is how much of this limited data should be used to evaluate multiple candidate models and select the best one. If you use a small amount, the best performing model on this small evaluation set might be the one that overfits the most to this set. On the other hand, if you use a large amount of data for evaluation, the performance boost gained by selecting the best model based on this evaluation set might be less than the boost gained by adding the evaluation set to the limited training set.‚Äù ([Huyen, p. 99](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=119&annotation=7PSQMDAB))

‚ÄúActive learning is a method for improving the efficiency of data labels. The hope here is that ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.‚Äù ([Huyen, p. 101](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=121&annotation=9UFU8KFH))

‚ÄúThe first reason is that class imbalance often means there‚Äôs insufficient signal for your model to learn to detect the minority classes. In the case where there is a small number of instances in the minority class, the problem becomes a few-shot learning problem where your model only gets to see the minority class a few times before having to make a decision on it.‚Äù ([Huyen, p. 103](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=123&annotation=CQ2EXCBJ))

‚ÄúThe second reason is that class imbalance makes it easier for your model to get stuck in a nonoptimal solution by exploiting a simple heuristic instead of learning anything useful about the underlying pattern of the data.‚Äù ([Huyen, p. 103](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=123&annotation=Z3BZ96BT))

‚ÄúThe third reason is that class imbalance leads to asymmetric costs of error‚Äîthe cost of a wrong prediction on a sample of the rare class might be much higher than a wrong prediction on a sample of the majority class.‚Äù ([Huyen, p. 104](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=124&annotation=LGJPLXII))

‚ÄúData-level methods: Resampling Data-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn. A common family of techniques is resampling. Resampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes. The simplest way to undersample is to randomly remove instances from the majority class, whereas the simplest way to oversample is to randomly make copies of the minority class until you have a ratio that you‚Äôre happy with.‚Äù ([Huyen, p. 109](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=129&annotation=JQRGYMFD))

‚ÄúA popular method of oversampling low-dimensional data is SMOTE (synthetic minority oversampling technique).39‚Äù ([Huyen, p. 109](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=129&annotation=RA5UX6V5))

‚ÄúUndersampling runs the risk of losing important data from removing data. Oversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.‚Äù ([Huyen, p. 110](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=130&annotation=8W6DBK6G))

‚ÄúAlgorithm-level methods If data-level methods mitigate the challenge of class imbalance by altering the distribution of your training data, algorithm-level methods keep the training data distribution intact but alter the algorithm to make it more robust to class imbalance.‚Äù ([Huyen, p. 110](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=130&annotation=NX8QDGEU))

‚ÄúCost-sensitive learning. Back in 2001, based on the insight that misclassification of different classes incurs different costs, Elkan proposed cost-sensitive learning in which the individual loss function is modified to take into account this varying cost‚Äù ([Huyen, p. 111](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=131&annotation=XYXJDIT5))

‚ÄúClass-balanced loss. What might happen with a model trained on an imbalanced dataset is that it‚Äôll bias toward majority classes and make wrong predictions on minority classes.‚Äù ([Huyen, p. 112](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=132&annotation=U9MECFM4))

‚ÄúMissing not at random (MNAR) This is when the reason a value is missing is because of the true value itself. In this example, we might notice that some respondents didn‚Äôt disclose their income. Upon investigation it may turn out that the income of respondents who failed to report tends to be higher than that of those who did disclose. The income values are missing for reasons related to the values themselves. Missing at random (MAR) This is when the reason a value is missing is not due to the value itself, but due to another observed variable. In this example, we might notice that age values are often missing for respondents of the gender ‚ÄúA,‚Äù which might be because the people of gender A in this survey don‚Äôt like disclosing their age. Missing completely at random (MCAR) This is when there‚Äôs no pattern in when the value is missing. In this example, we might think that the missing values for the column ‚ÄúJob‚Äù might be completely random, not because of the job itself and not because of any other variable. People just forget to fill in that value sometimes for no particular reason. However, this type of missing is very rare. There are usually reasons why certain values are missing, and you should investigate.‚Äù ([Huyen, p. 124](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=144&annotation=B8VPY8QV))

‚ÄúEven though deletion is tempting because it‚Äôs easy to do, deleting data can lead to losing important information and introduce biases into your model. If you don‚Äôt want to delete missing values, you will have to impute them, which means ‚Äúfill them with certain values.‚Äù‚Äù ([Huyen, p. 125](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=145&annotation=PTK2ARZU))

‚ÄúBefore inputting features into models, it‚Äôs important to scale them to be similar ranges. This process is called feature scaling. This is one of the simplest things you can do that often results in a performance boost for your model.‚Äù ([Huyen, p. 126](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=146&annotation=BP67NQT2))

‚ÄúWhile this technique can yield performance gain in many cases, it doesn‚Äôt work for all cases, and you should be wary of the analysis performed on log-transformed data instead of the original data.5‚Äù ([Huyen, p. 127](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=147&annotation=4JSYC9AG))

‚ÄúOne is that it‚Äôs a common source of data leakage (this will be covered in greater detail in the section ‚ÄúData Leakage‚Äù on page 135).‚Äù ([Huyen, p. 128](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=148&annotation=5NC6VRAP))

‚ÄúAnother is that it often requires global statistics‚Äîyou have to look at the entire or a subset of training data to calculate its min, max, or mean‚Äù ([Huyen, p. 128](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=148&annotation=GH9HTQX2))

‚ÄúThe downside is that this categorization introduces discontinuities at the category boundaries‚Äî$34,999 is now treated as completely different from $35,000, which is treated the same as $100,000.‚Äù ([Huyen, p. 129](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=149&annotation=882RUYWS))

‚ÄúOne solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft.7 The gist of this trick is that you use a hash function to generate a hashed value of each category. The hashed value will become the index of that category.‚Äù ([Huyen, p. 130](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=150&annotation=XDE65LJ4))

‚ÄúBecause you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many 130 | Chapter 5: Feature Engineerin‚Äù ([Huyen, p. 130](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=150&annotation=NNRIJQFT))

‚Äú8 Lucas Bernardi, ‚ÄúDon‚Äôt Be Tricked by the Hashing Trick,‚Äù Booking.com, January 10, 2018, https://oreil.ly/VZmaY. categories there will be. For example, if you choose a hash space of 18 bits, which corresponds to 218 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143‚Äù ([Huyen, p. 131](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=151&annotation=TQJ7ZVQB))

‚ÄúFixed positional embedding is a special case of what is known as Fourier features. If positions in positional embeddings are discrete, Fourier features can also be continuous. Consider the task involving representations of 3D objects, such as a teapot. Each position on the surface of the teapot is represented by a three-dimensional coordinate, which is continuous. When positions are continuous, it‚Äôd be very hard to build an embedding matrix with continuous column indices, but fixed position embeddings using sine and cosine functions still work.‚Äù ([Huyen, p. 135](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=155&annotation=L8NUYQ6Y))

‚ÄúThe following is the generalized format for the embedding vector at coordinate v, also called the Fourier features of coordinate v. Fourier features have been shown to improve models‚Äô performance for tasks that take in coordinates (or positions) as inputs. If interested, you might want to read more about it in ‚ÄúFourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains‚Äù (Tancik et al. 2020). Œ≥ v = a1 cos 2œÄb1Tv , a1 sin 2œÄb1Tv , ..., am cos 2œÄbmTv , am sin 2œÄbmTv T‚Äù ([Huyen, p. 135](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=155&annotation=7EZDLF82))

‚ÄúTo prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible. For example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test splits as shown in Figure 5-7.‚Äù ([Huyen, p. 137](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=157&annotation=HKRCB9G6))

‚ÄúTo avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits. Some even suggest that we split our data before any exploratory data analysis and data processing, so that we don‚Äôt accidentally gain information about the test split.‚Äù ([Huyen, p. 138](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=158&annotation=2DP53DAQ))

‚ÄúLeakage might occur if the mean or median is calculated using entire data instead of just the train split‚Äù ([Huyen, p. 138](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=158&annotation=CMC7WJ74))

‚ÄúPoor handling of data duplication before splitting If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits.‚Äù ([Huyen, p. 139](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=159&annotation=B2S6D86D))

‚ÄúTo avoid this, always check for duplicates before splitting and also after splitting just to make sure. If you oversample your data, do it after splitting‚Äù ([Huyen, p. 139](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=159&annotation=A5SUSMR3))

‚ÄúGroup leakage A group of examples have strongly correlated labels but are divided into different splits.‚Äù ([Huyen, p. 139](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=159&annotation=WKTMP5WF))

‚ÄúThe example earlier about how information on whether a CT scan shows signs of lung cancer is leaked via the scan machine is an example of this type of leakage. Detecting this type of data leakage requires a deep understanding of the way data is collected.‚Äù ([Huyen, p. 140](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=160&annotation=5VCQJP2S))

‚ÄúMeasure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has unusually high correlation, investigate how this feature is generated and whether the correlation makes sense.‚Äù ([Huyen, p. 140](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=160&annotation=B6RDU5LR))

‚ÄúDo ablation studies to measure how important a feature or a set of features is to your model. If removing a feature causes the model‚Äôs performance to deteriorate significantly, investigate why that feature is so important.‚Äù ([Huyen, p. 141](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=161&annotation=PVAULN8H))

‚ÄúSHAP is great because it not only measures a feature‚Äôs importance to an entire model, it also measures each feature‚Äôs contribution to a model‚Äôs specific prediction.‚Äù ([Huyen, p. 142](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=162&annotation=MU6BYYGJ))

‚ÄúNot only good for choosing the right features, feature importance techniques are also great for interpretability as they help you understand how your models work under the hood.‚Äù ([Huyen, p. 144](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=164&annotation=A6CYBUPT))

‚ÄúSplit data by time into train/valid/test splits instead of doing it randomly. ‚Ä¢ ‚Ä¢ If you oversample your data, do it after splitting. ‚Ä¢ Scale and normalize your data after splitting to avoid data leakage. ‚Ä¢ ‚Ä¢ Use statistics from only the train split, instead of the entire data, to scale your ‚Ä¢ features and handle missing values.‚Äù ([Huyen, p. 146](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=166&annotation=MC8QYV44))

‚ÄúUnderstand how your data is generated, collected, and processed. Involve ‚Ä¢ domain experts if possible. ‚Ä¢ Keep track of your data‚Äôs lineage. ‚Ä¢ ‚Ä¢ Understand feature importance to your model. ‚Ä¢ ‚Ä¢ Use features that generalize well. ‚Ä¢ Remove no longer useful features from your models.‚Äù ([Huyen, p. 147](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=167&annotation=VI653Q4A))

‚ÄúA simple way to estimate how your model‚Äôs performance might change with more data is to use learning curves. A learning curve of a model is a plot of its performance‚Äîe.g., training loss, training accuracy, validation accuracy‚Äîagainst the number of training samples it uses, as shown in Figure 6-1.‚Äù ([Huyen, p. 153](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=173&annotation=PSKKER3M))

‚ÄúWith different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper ‚ÄúOn the State of the Art of Evaluation in Neural Language Models‚Äù that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space‚Äîthe performance of each set evaluated on a validation set.‚Äù ([Huyen, p. 173](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=193&annotation=243LBHBX))

‚ÄúSimple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect?‚Äù ([Huyen, p. 180](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=200&annotation=46RJB7FE))

‚Äúero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.‚Äù ([Huyen, p. 180](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=200&annotation=I2PC9PSH))

‚ÄúIn academic settings, when evaluating ML models, people tend to fixate on their performance metrics. However, in production, we also want our models to be robust, fair, calibrated, and overall make sense. We‚Äôll introduce some evaluation methods that help with measuring these characteristics of a model.‚Äù ([Huyen, p. 181](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=201&annotation=JUG2RLW3))

‚Äúlice-based evaluation Slicing means to separate your data into subsets and look at your model‚Äôs performance on each subset separately. A common mistake that I‚Äôve seen in many companies is that they are focused too much on coarse-grained metrics like overall F1 or accuracy on the entire data and not enough on sliced-based metrics. This can lead to two problems.‚Äù ([Huyen, p. 185](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=205&annotation=DGUPRLCA))

‚ÄúA fascinating and seemingly counterintuitive reason why slice-based evaluation is crucial is Simpson‚Äôs paradox, a phenomenon in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately.‚Äù ([Huyen, p. 186](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=206&annotation=ES99LNKR))

‚ÄúHeuristics-based Slice your data using domain knowledge you have of the data and the task at hand. For example, when working with web traffic, you might want to slice your data along dimensions like mobile versus desktop, browser type, and locations. Mobile users might behave very differently from desktop users. Similarly, internet users in different geographic locations might have different expectations on what a website should look like.47 Error analysis Manually go through misclassified examples and find patterns among them. We discovered our model‚Äôs problem with mobile users when we saw that most of the misclassified examples were from mobile users. Slice finder There has been research to systemize the process of finding slices, including Chung et al.‚Äôs ‚ÄúSlice Finder: Automated Data Slicing for Model Validation‚Äù in 2019 and covered in Sumyea Helal‚Äôs ‚ÄúSubgroup Discovery Algorithms: A Survey and Empirical Evaluation‚Äù (2016). The process generally starts with generating slice candidates with algorithms such as beam search, clustering, or decision, then pruning out clearly bad candidates for slices, and then ranking the candidates that are left.‚Äù ([Huyen, p. 188](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=208&annotation=732MU3UM))