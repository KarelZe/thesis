*title:* Designing Machine Learning Systems
*authors:* Chip Huyen
*year:* 
*tags:* #feature-enginering #target-leakage #evaluation #robustness #pre-training 
*status:* #ğŸ“¦ 
*related:* [[@banachewiczKaggleBookData2022]]
*code:*
*review:*

## Notes ğŸ“

## Annotations ğŸ“–

â€œNonprobability sampling is when the selection of data isnâ€™t based on any probability criteria. Here are some of the criteria for nonprobability sampling:â€ ([Huyen, p. 83](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=103&annotation=T3X54EM2))

â€œThe samples selected by nonprobability criteria are not representative of the realworld data and therefore are riddled with selection biases.â€ ([Huyen, p. 83](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=103&annotation=JRDUAQGG))

â€œNonprobability sampling can be a quick and easy way to gather your initial data to get your project off the ground. However, for reliable models, you might want to use probability-based sampling, which we will cover next.â€ ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=AWZPM7YD))

â€œSimple Random Sampling In the simplest form of random sampling, you give all samples in the population equal probabilities of being selected.4 For example, you randomly select 10% of the population, giving all members of this population an equal 10% chance of being selected.â€ ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=WAV65NAQ))

â€œThe advantage of this method is that itâ€™s easy to implement. The drawback is that rare categories of data might not appear in your selection.â€ ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=ZF42UQVI))

â€œTo avoid the drawback of simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately.â€ ([Huyen, p. 84](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=104&annotation=IZGKJRQB))

â€œThis method allows you to leverage domain expertise. For example, if you know that a certain subpopulation of data, such as more recent data, is more valuable to your model and want it to have a higher chance of being selected, you can give it a higher weightâ€ ([Huyen, p. 85](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=105&annotation=ZRILC2ZN))

â€œImportance Sampling Importance sampling is one of the most important sampling methods, not just in ML. It allows us to sample from a distribution when we only have access to another distributionâ€ ([Huyen, p. 87](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=107&annotation=IIWWB4YX))

â€œWeak supervision Leverages (often noisy) heuristics to generate labels No, but a small number of labels are recommended to guide the development of heuristics Semisupervision Leverages structural assumptions to generate labels Yes, a small number of initial labels as seeds to generate more labels Transfer learning Leverages models pretrained on another task for your new task No for zero-shot learning Yes for fine-tuning, though the number of ground truths required is often much smaller than what would be needed if you train the model from scratchâ€ ([Huyen, p. 94](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=114&annotation=79C2ZXFA))

â€œIf weak supervision leverages heuristics to obtain noisy labels, semi-supervision leverages structural assumptions to generate new labels based on a small set of initial labels. Unlike weak supervision, semi-supervision requires an initial set of labelsâ€ ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=XV9C9DB9))

â€œFor a comprehensive review, I recommend â€œSemi-Supervised Learning Literature Surveyâ€ (Xiaojin Zhu, 2008) and â€œA Survey on Semi-Supervised Learningâ€ (Engelen and Hoos, 2018)â€ ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=YMPAQ9IX))

â€œA classic semi-supervision method is self-training. You start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples. Assuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set. This goes on until youâ€™re happy with your model performanceâ€ ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=I7J7ZKVS))

â€œAnother semi-supervision method assumes that data samples that share similar characteristics share the same labels.â€ ([Huyen, p. 98](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=118&annotation=4M4T6I5T))

â€œIn some cases, semi-supervision approaches have reached the performance of purely supervised learning, even when a substantial portion of the labels in a given dataset has been discarded.17â€ ([Huyen, p. 99](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=119&annotation=G8GCXEWJ))

â€œSemi-supervision is the most useful when the number of training labels is limited. One thing to consider when doing semi-supervision with limited data is how much of this limited data should be used to evaluate multiple candidate models and select the best one. If you use a small amount, the best performing model on this small evaluation set might be the one that overfits the most to this set. On the other hand, if you use a large amount of data for evaluation, the performance boost gained by selecting the best model based on this evaluation set might be less than the boost gained by adding the evaluation set to the limited training set.â€ ([Huyen, p. 99](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=119&annotation=7PSQMDAB))

â€œActive learning is a method for improving the efficiency of data labels. The hope here is that ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.â€ ([Huyen, p. 101](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=121&annotation=9UFU8KFH))

â€œThe first reason is that class imbalance often means thereâ€™s insufficient signal for your model to learn to detect the minority classes. In the case where there is a small number of instances in the minority class, the problem becomes a few-shot learning problem where your model only gets to see the minority class a few times before having to make a decision on it.â€ ([Huyen, p. 103](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=123&annotation=CQ2EXCBJ))

â€œThe second reason is that class imbalance makes it easier for your model to get stuck in a nonoptimal solution by exploiting a simple heuristic instead of learning anything useful about the underlying pattern of the data.â€ ([Huyen, p. 103](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=123&annotation=Z3BZ96BT))

â€œThe third reason is that class imbalance leads to asymmetric costs of errorâ€”the cost of a wrong prediction on a sample of the rare class might be much higher than a wrong prediction on a sample of the majority class.â€ ([Huyen, p. 104](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=124&annotation=LGJPLXII))

â€œData-level methods: Resampling Data-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn. A common family of techniques is resampling. Resampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes. The simplest way to undersample is to randomly remove instances from the majority class, whereas the simplest way to oversample is to randomly make copies of the minority class until you have a ratio that youâ€™re happy with.â€ ([Huyen, p. 109](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=129&annotation=JQRGYMFD))

â€œA popular method of oversampling low-dimensional data is SMOTE (synthetic minority oversampling technique).39â€ ([Huyen, p. 109](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=129&annotation=RA5UX6V5))

â€œUndersampling runs the risk of losing important data from removing data. Oversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.â€ ([Huyen, p. 110](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=130&annotation=8W6DBK6G))

â€œAlgorithm-level methods If data-level methods mitigate the challenge of class imbalance by altering the distribution of your training data, algorithm-level methods keep the training data distribution intact but alter the algorithm to make it more robust to class imbalance.â€ ([Huyen, p. 110](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=130&annotation=NX8QDGEU))

â€œCost-sensitive learning. Back in 2001, based on the insight that misclassification of different classes incurs different costs, Elkan proposed cost-sensitive learning in which the individual loss function is modified to take into account this varying costâ€ ([Huyen, p. 111](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=131&annotation=XYXJDIT5))

â€œClass-balanced loss. What might happen with a model trained on an imbalanced dataset is that itâ€™ll bias toward majority classes and make wrong predictions on minority classes.â€ ([Huyen, p. 112](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=132&annotation=U9MECFM4))

â€œMissing not at random (MNAR) This is when the reason a value is missing is because of the true value itself. In this example, we might notice that some respondents didnâ€™t disclose their income. Upon investigation it may turn out that the income of respondents who failed to report tends to be higher than that of those who did disclose. The income values are missing for reasons related to the values themselves. Missing at random (MAR) This is when the reason a value is missing is not due to the value itself, but due to another observed variable. In this example, we might notice that age values are often missing for respondents of the gender â€œA,â€ which might be because the people of gender A in this survey donâ€™t like disclosing their age. Missing completely at random (MCAR) This is when thereâ€™s no pattern in when the value is missing. In this example, we might think that the missing values for the column â€œJobâ€ might be completely random, not because of the job itself and not because of any other variable. People just forget to fill in that value sometimes for no particular reason. However, this type of missing is very rare. There are usually reasons why certain values are missing, and you should investigate.â€ ([Huyen, p. 124](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=144&annotation=B8VPY8QV))

â€œEven though deletion is tempting because itâ€™s easy to do, deleting data can lead to losing important information and introduce biases into your model. If you donâ€™t want to delete missing values, you will have to impute them, which means â€œfill them with certain values.â€â€ ([Huyen, p. 125](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=145&annotation=PTK2ARZU))

â€œBefore inputting features into models, itâ€™s important to scale them to be similar ranges. This process is called feature scaling. This is one of the simplest things you can do that often results in a performance boost for your model.â€ ([Huyen, p. 126](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=146&annotation=BP67NQT2))

â€œWhile this technique can yield performance gain in many cases, it doesnâ€™t work for all cases, and you should be wary of the analysis performed on log-transformed data instead of the original data.5â€ ([Huyen, p. 127](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=147&annotation=4JSYC9AG))

â€œOne is that itâ€™s a common source of data leakage (this will be covered in greater detail in the section â€œData Leakageâ€ on page 135).â€ ([Huyen, p. 128](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=148&annotation=5NC6VRAP))

â€œAnother is that it often requires global statisticsâ€”you have to look at the entire or a subset of training data to calculate its min, max, or meanâ€ ([Huyen, p. 128](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=148&annotation=GH9HTQX2))

â€œThe downside is that this categorization introduces discontinuities at the category boundariesâ€”$34,999 is now treated as completely different from $35,000, which is treated the same as $100,000.â€ ([Huyen, p. 129](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=149&annotation=882RUYWS))

â€œOne solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft.7 The gist of this trick is that you use a hash function to generate a hashed value of each category. The hashed value will become the index of that category.â€ ([Huyen, p. 130](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=150&annotation=XDE65LJ4))

â€œBecause you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many 130 | Chapter 5: Feature Engineerinâ€ ([Huyen, p. 130](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=150&annotation=NNRIJQFT))

â€œ8 Lucas Bernardi, â€œDonâ€™t Be Tricked by the Hashing Trick,â€ Booking.com, January 10, 2018, https://oreil.ly/VZmaY. categories there will be. For example, if you choose a hash space of 18 bits, which corresponds to 218 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143â€ ([Huyen, p. 131](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=151&annotation=TQJ7ZVQB))

â€œFixed positional embedding is a special case of what is known as Fourier features. If positions in positional embeddings are discrete, Fourier features can also be continuous. Consider the task involving representations of 3D objects, such as a teapot. Each position on the surface of the teapot is represented by a three-dimensional coordinate, which is continuous. When positions are continuous, itâ€™d be very hard to build an embedding matrix with continuous column indices, but fixed position embeddings using sine and cosine functions still work.â€ ([Huyen, p. 135](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=155&annotation=L8NUYQ6Y))

â€œThe following is the generalized format for the embedding vector at coordinate v, also called the Fourier features of coordinate v. Fourier features have been shown to improve modelsâ€™ performance for tasks that take in coordinates (or positions) as inputs. If interested, you might want to read more about it in â€œFourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domainsâ€ (Tancik et al. 2020). Î³ v = a1 cos 2Ï€b1Tv , a1 sin 2Ï€b1Tv , ..., am cos 2Ï€bmTv , am sin 2Ï€bmTv Tâ€ ([Huyen, p. 135](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=155&annotation=7EZDLF82))

â€œTo prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible. For example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test splits as shown in Figure 5-7.â€ ([Huyen, p. 137](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=157&annotation=HKRCB9G6))

â€œTo avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits. Some even suggest that we split our data before any exploratory data analysis and data processing, so that we donâ€™t accidentally gain information about the test split.â€ ([Huyen, p. 138](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=158&annotation=2DP53DAQ))

â€œLeakage might occur if the mean or median is calculated using entire data instead of just the train splitâ€ ([Huyen, p. 138](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=158&annotation=CMC7WJ74))

â€œPoor handling of data duplication before splitting If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits.â€ ([Huyen, p. 139](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=159&annotation=B2S6D86D))

â€œTo avoid this, always check for duplicates before splitting and also after splitting just to make sure. If you oversample your data, do it after splittingâ€ ([Huyen, p. 139](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=159&annotation=A5SUSMR3))

â€œGroup leakage A group of examples have strongly correlated labels but are divided into different splits.â€ ([Huyen, p. 139](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=159&annotation=WKTMP5WF))

â€œThe example earlier about how information on whether a CT scan shows signs of lung cancer is leaked via the scan machine is an example of this type of leakage. Detecting this type of data leakage requires a deep understanding of the way data is collected.â€ ([Huyen, p. 140](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=160&annotation=5VCQJP2S))

â€œMeasure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has unusually high correlation, investigate how this feature is generated and whether the correlation makes sense.â€ ([Huyen, p. 140](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=160&annotation=B6RDU5LR))

â€œDo ablation studies to measure how important a feature or a set of features is to your model. If removing a feature causes the modelâ€™s performance to deteriorate significantly, investigate why that feature is so important.â€ ([Huyen, p. 141](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=161&annotation=PVAULN8H))

â€œSHAP is great because it not only measures a featureâ€™s importance to an entire model, it also measures each featureâ€™s contribution to a modelâ€™s specific prediction.â€ ([Huyen, p. 142](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=162&annotation=MU6BYYGJ))

â€œNot only good for choosing the right features, feature importance techniques are also great for interpretability as they help you understand how your models work under the hood.â€ ([Huyen, p. 144](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=164&annotation=A6CYBUPT))

â€œSplit data by time into train/valid/test splits instead of doing it randomly. â€¢ â€¢ If you oversample your data, do it after splitting. â€¢ Scale and normalize your data after splitting to avoid data leakage. â€¢ â€¢ Use statistics from only the train split, instead of the entire data, to scale your â€¢ features and handle missing values.â€ ([Huyen, p. 146](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=166&annotation=MC8QYV44))

â€œUnderstand how your data is generated, collected, and processed. Involve â€¢ domain experts if possible. â€¢ Keep track of your dataâ€™s lineage. â€¢ â€¢ Understand feature importance to your model. â€¢ â€¢ Use features that generalize well. â€¢ Remove no longer useful features from your models.â€ ([Huyen, p. 147](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=167&annotation=VI653Q4A))

â€œA simple way to estimate how your modelâ€™s performance might change with more data is to use learning curves. A learning curve of a model is a plot of its performanceâ€”e.g., training loss, training accuracy, validation accuracyâ€”against the number of training samples it uses, as shown in Figure 6-1.â€ ([Huyen, p. 153](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=173&annotation=PSKKER3M))

â€œWith different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper â€œOn the State of the Art of Evaluation in Neural Language Modelsâ€ that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search spaceâ€”the performance of each set evaluated on a validation set.â€ ([Huyen, p. 173](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=193&annotation=243LBHBX))

â€œSimple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect?â€ ([Huyen, p. 180](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=200&annotation=46RJB7FE))

â€œero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.â€ ([Huyen, p. 180](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=200&annotation=I2PC9PSH))

â€œIn academic settings, when evaluating ML models, people tend to fixate on their performance metrics. However, in production, we also want our models to be robust, fair, calibrated, and overall make sense. Weâ€™ll introduce some evaluation methods that help with measuring these characteristics of a model.â€ ([Huyen, p. 181](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=201&annotation=JUG2RLW3))

â€œlice-based evaluation Slicing means to separate your data into subsets and look at your modelâ€™s performance on each subset separately. A common mistake that Iâ€™ve seen in many companies is that they are focused too much on coarse-grained metrics like overall F1 or accuracy on the entire data and not enough on sliced-based metrics. This can lead to two problems.â€ ([Huyen, p. 185](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=205&annotation=DGUPRLCA))

â€œA fascinating and seemingly counterintuitive reason why slice-based evaluation is crucial is Simpsonâ€™s paradox, a phenomenon in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately.â€ ([Huyen, p. 186](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=206&annotation=ES99LNKR))

â€œHeuristics-based Slice your data using domain knowledge you have of the data and the task at hand. For example, when working with web traffic, you might want to slice your data along dimensions like mobile versus desktop, browser type, and locations. Mobile users might behave very differently from desktop users. Similarly, internet users in different geographic locations might have different expectations on what a website should look like.47 Error analysis Manually go through misclassified examples and find patterns among them. We discovered our modelâ€™s problem with mobile users when we saw that most of the misclassified examples were from mobile users. Slice finder There has been research to systemize the process of finding slices, including Chung et al.â€™s â€œSlice Finder: Automated Data Slicing for Model Validationâ€ in 2019 and covered in Sumyea Helalâ€™s â€œSubgroup Discovery Algorithms: A Survey and Empirical Evaluationâ€ (2016). The process generally starts with generating slice candidates with algorithms such as beam search, clustering, or decision, then pruning out clearly bad candidates for slices, and then ranking the candidates that are left.â€ ([Huyen, p. 188](zotero://select/library/items/IANYQ9ET)) ([pdf](zotero://open-pdf/library/items/WWK72R8B?page=208&annotation=732MU3UM))