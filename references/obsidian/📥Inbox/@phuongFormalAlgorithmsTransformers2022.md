*title:* Formal Algorithms for Transformers
*authors:* Mary Phuong, Marcus Hutter
*year:* 2021
*tags:* #transformer #deep-learning #attention #encoder #embeddings #multiheaded-attention #sgd #adam #tokenization 
*status:* #ðŸ“¦ 
*related:*
- [[@vaswaniAttentionAllYou2017]]
- [[@huangTabTransformerTabularData2020]]
- [[@huangTabTransformerTabularData2020]]
- [[@devlinBERTPretrainingDeep2019]]
# Notes 

### Positional Encoding:
The [[@vaswaniAttentionAllYou2017]] paper uses
$$
\begin{aligned}
W_p[2 i-1, t] & =\sin \left(t / \ell_{\max }^{2 i / d_{\mathrm{e}}}\right), \\
W_p[2 i, t] & =\cos \left(t / \ell_{\max }^{2 i / d_{\mathrm{e}}}\right) .
\end{aligned}
$$
for $0<i \leq d_{\mathrm{e}} / 2$.
The positional embedding of a token is usually added to the token embedding to form a token's initial embedding. For the $t$-th token of a sequence $x$, the embedding is
$$
e=W_e[:, x[t]]+W_p[:, t] .
$$

# Annotations

â€œIt covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.â€ ([Phuong and Hutter, 2022, p. 1](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=1&annotation=77QFWC4I))

â€œIt aims to be a self-contained, complete, precise and compact overview of transformer architectures and formal algorithms (but not results).â€ ([Phuong and Hutter, 2022, p. 1](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=1&annotation=8C5HP6PY))

â€œn practise, the distribution estimate is often decomposed via the chain rule as Ë† ð‘ƒÂ¹ð’™Âº = Ë† ð‘ƒðœ½ Â¹ð‘¥ Â»1Â¼Âº Ë† ð‘ƒðœ½Â¹ð‘¥ Â»2Â¼ j ð‘¥ Â»1Â¼Âº Ë† ð‘ƒðœ½ Â¹ð‘¥ Â»Â¼ j ð’™ Â»1 : 1Â¼Âº, where ðœ½ consists of all neural network parameters to be learnt. The goal is to learn a distribution over a single token ð‘¥ Â»ð‘¡Â¼ given its preceding tokens ð‘¥ Â»1 : ð‘¡ 1Â¼ as context.â€ ([Phuong and Hutter, 2022, p. 3](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=3&annotation=7H4SEVMT))

â€œGiven a vocabulary ð‘‰ and a set of classes Â»ð‘CÂ¼, let Â¹ð’™ð‘› ð‘ð‘›Âº 2 ð‘‰ Â»ð‘CÂ¼ for ð‘› 2 Â»ð‘dataÂ¼ be an i.i.d. dataset of sequence-class pairs sampled from ð‘ƒÂ¹ð’™ ð‘Âº. The goal in classification is to learn an estimate of the conditional distribution ð‘ƒÂ¹ð‘jð’™Âº.â€ ([Phuong and Hutter, 2022, p. 4](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=4&annotation=46X5JAU9))

â€œhere are in fact many ways to do subword tokenization. One of the simplest and most successful ones is Byte Pair Encoding [Gag94, SHB16] used in GPT-2 [RWC Ì§19].â€ ([Phuong and Hutter, 2022, p. 4](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=4&annotation=57PTDTJ3))

â€œToken embedding. The token embedding learns to represent each vocabulary element as a vector in â„ð‘‘e; see Algorithm 1â€ ([Phuong and Hutter, 2022, p. 4](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=4&annotation=UC6DXFP7))

â€œThe positional embedding learns to represent a tokenâ€™s position in a sequence as a vector in â„ð‘‘e. For example, the position of the first token in a sequence is represented by a (learnt) vector ð‘¾ð’‘ Â»: 1Â¼, the position of the second token is represented by another (learnt) vector ð‘¾ð’‘ Â»: 2Â¼, etc. The purpose of the positional embedding is to allow a Transformer to make sense of word ordering; in its absence the representation would be permutation invariant and the model would perceive sequences as â€œbags of wordsâ€ instead.â€ ([Phuong and Hutter, 2022, p. 5](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=5&annotation=Q2WE7H5S))

â€œThe positional embedding of a token is usually added to the token embedding to form a tokenâ€™s initial embedding. For the ð‘¡-th token of a sequence ð’™, the embedding is ð’† = ð‘¾ð’† Â»: ð‘¥ Â»ð‘¡Â¼Â¼ Ì§ ð‘¾ð’‘ Â»: ð‘¡Â¼â€ ([Phuong and Hutter, 2022, p. 5](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=5&annotation=DKMMC563))

â€œAttention. Attention is the main architectural component of transformers. It enables a neural Algorithm 2: Positional embedding. Input: 2 Â»maxÂ¼, position of a token in the sequence. Output: ð’†ð’‘ 2 â„ð‘‘e, the vector representation of the position. Parameters: ð‘¾ð’‘ 2 â„ð‘‘emax , the positional embedding matrix. 1 return ð’†ð’‘ = ð‘¾ð’‘ Â»: Â¼ network to make use of contextual information (e.g. preceding text or the surrounding text) for predicting the current token.â€ ([Phuong and Hutter, 2022, p. 5](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=5&annotation=B84PMF5K))

â€œOn a high level, attention works as follows: the token currently being predicted is mapped to a query vector ð’’ 2 â„ð‘‘attn, and the tokens in the context are mapped to key vectors ð’Œð‘¡ 2 â„ð‘‘attn and value vectors ð’—ð‘¡ 2 â„ð‘‘value . The inner products ð’’áµ€ð’Œð‘¡ are interpreted as the degree to which token ð‘¡ 2 ð‘‰ is important for predicting the current token ð‘ž â€“ they are used to derive a distribution over the context tokens, which is then used to combine the value vectors. An intuitive explanation how this achieves attention can be found at The precise algorithm is given in Algorithm 3â€ ([Phuong and Hutter, 2022, p. 5](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=5&annotation=7SA5TCXB))

â€œhe attention algorithm presented so far (Algorithm 4) describes the operation of a single attention head. In practise, transformers run multiple attention heads (with separate learnable parameters) in parallel and combine their outputs; this is called multi-head attention; see Algorithm 5â€ ([Phuong and Hutter, 2022, p. 6](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=6&annotation=GIP6PKMQ))

â€œLayer normalisation explicitly controls the mean and variance of individual neural network activations; the pseudocode is given in Algorithm 6â€ ([Phuong and Hutter, 2022, p. 7](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=7&annotation=8B7ERQGB))

â€œIt uses the GELU nonlinearity instead of ReLU: GELUÂ¹ð‘¥Âº = ð‘¥ â„™ð‘‹N Â¹01Âº Â»ð‘‹ ð‘¥Â¼ (5) (When called with vector or matrix arguments, GELU is applied element-wise.)â€ ([Phuong and Hutter, 2022, p. 8](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=8&annotation=A4VRNZJQ))

â€œEDTraining() Algorithm 11 shows how to train a sequence-to-sequence transformer (the original Transformer .â€ ([Phuong and Hutter, 2022, p. 8](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=8&annotation=AZPM2ABG))

â€œGradient descent. The described training Algorithms 11 to 13 use Stochastic Gradient Descentâ€ ([Phuong and Hutter, 2022, p. 8](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=8&annotation=4CQKYJTC))

â€œFormal Algorithms for Transformers (SGD) ðœ½ ðœ½ ðœ‚ rlossÂ¹ðœ½Âº to minimise the log loss (aka cross entropy) as the update rule. Computation of the gradient is done via automatic differentiation tools; see  In practise, vanilla SGD is usually replaced by some more refined variation such as RMSProp or AdaGrad or others . Adam  is used most often these days.â€ ([Phuong and Hutter, 2022, p. 9](zotero://select/library/items/DYN5Q8UB)) ([pdf](zotero://open-pdf/library/items/9X32MT2H?page=9&annotation=NHTHE4DY))