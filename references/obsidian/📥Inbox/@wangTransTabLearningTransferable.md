*title:* TransTab: Learning Transferable Tabular Transformers Across Tables
*authors:* Zifeng Wang, Jimeng Sun
*year:* 
*tags:* 
*status:* #transformer #embeddings #self-learning #semi-supervised 
*related:*
*code:*
*review:*

## Notes üìç

## Annotations üìñ

<mark style="background: #FFF3A3A6;">‚ÄúRecent works enhance tabular ML modelling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.‚Äù ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=3CDUAZDS))</mark>

‚ÄúColumns are mapped to unique indexes then models take the cell values for training and inference. The premise of this modelling formulation is to keep the same column structure in all the tables. But tables often have divergent protocols where the nomenclatures of columns and cells differ. By contrast, our proposed work contextualises the columns and cells. For example, previous methods represent a cell valued man under the column gender by 0 referring to the codebook {man : 0, woman : 1}. Our model converts the tabular input into a sequence input (e.g., gender is man), which can be modelled with downstream sequence models. We argue such featurizing protocol is generalizable across tables, thus enabling models to apply to different tables.‚Äù ([Wang and Sun, p. 2](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=2&annotation=ESQU7HS5))

<mark style="background: #FFB86CA6;">‚ÄúWe build the input processor (1) to accept variable-column tables (2) to retain knowledge across tabular datasets. The idea is to convert tabular data (cells in columns (categorical, binary, and continuous)) into a sequence of semantically encoded tokens. We utilise the following observation to create the sequence: the column description (e.g., column name) decides the meaning of cells in that column. For example, if a cell in column smoking history has value 1, it indicates the individual has a smoking history. Similarly, cell value 60 in column weight indicates 60 kg in weight instead of 60 years old. Motivated by the discussion, we‚Äù ([Wang and Sun, p. 3](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=3&annotation=SHVARP6R))</mark>

<mark style="background: #ABF7F7A6;">‚Äúpropose to include column names into the tabular modelling. As a result, TransTab treats any tabular data as the composition of three elements: text (for categorical & textual cells and column names), continuous values (for numerical cells), and boolean values (for binary cells) .‚Äù ([Wang and Sun, p. 4](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=4&annotation=2L6GMYKW))</mark>

‚ÄúCategorical/Textual feature. A category or textual feature contains a sequence of text tokens. For the categorical feature cat, we concatenate the column name with the feature value xc, which forms as a sequence of tokens. This sentence is then tokenized and matched to the token embedding matrix to generate the feature embedding Ec ‚àà Rnc√ód where d is the embedding dimension and nc is the number of tokens. Binary feature. The binary feature bin is usually an assertive description and its value xb ‚àà {0, 1}. If xb = 1, then bin is tokenized and encoded to the embeddings Eb ‚àà Rnb√ód; if not, it will not be processed to the subsequent steps. This design significantly reduces the computational and memory cost when the inputs have high-dimensional and sparse one-hot features. Numerical feature. We do not concatenate column names and values for numerical feature because the tokenization-embedding paradigm was notoriously known to be bad at discriminating numbers [21]. Instead, we process them separately. num is encoded as same as cat and bin to get Eu,col ‚àà Rnu√ód. We then multiply the numerical features with the column embedding to yield the numerical embedding as Eu = xu √ó Eu,col2, which we identify gets an edge on more complicated numerical embedding techniques empirically.‚Äù ([Wang and Sun, p. 4](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=4&annotation=34SD7E4A))

<mark style="background: #ABF7F7A6;">‚ÄúSelf-supervised learning & contrastive learning. SSL uses unlabeled data with pretext tasks to learn useful representations and most of them are in CV and NLP [20, 17, 15, 16, 58, 23, 59, 60, 61, 62, 63]. Recent SSL tabular models can be classified into reconstruction and contrastive based methods: TabNet [30] and VIME [2] try to recover the corrupted inputs with auto-encoding loss; SCARF [11] takes a SimCLR-like [64] contrastive loss between the sample and its corrupted version; SubTab [9] takes a combination of both. Nevertheless, all fail to learn transferable models across tables such that cannot benefit from pretraining with scale. Contrastive learning can also be applied to supervised learning by leveraging class labels to build positive samples [26]. Our work extends it to to the tabular domain, which we prove works better than vanilla supervised pretraining. The vertical partition sampling also enjoys high query speed from large databases which are often column-oriented [25]. Another line of research takes table pretraining table semantic parsing [65, 66, 67, 68, 69] or table-to-text generation [70, 71]. But these methods either encode the whole table instead of each row or do not demonstrate to benefit tabular prediction yet.‚Äù ([Wang and Sun, p. 9](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=9&annotation=CXBL5HGM))</mark>