*title:* Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding
*authors:* Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
*year:* 2018
*tags:* #semi-supervised #deep-learning #semi-supervised #self-learning 
*status:* #üì¶ 
*related:*
- [[@clarkElectraPretrainingText2020]]
- [[@yoonVIMEExtendingSuccess2020]]
- [[@huangTabTransformerTabularData2020]]
# Notes 
- The masked language model randomly masks some of the tokens from the input sequence and the objective is to predict the original vocabulary id of the mask based only on its context.
- The model is first pre-trained on unlabelled data on the Wikipedia and BooksCorpus. This model is then fine-tuned on labelled data.
- The distinctive feature of BERT is that there is only a minimal difference between the pre-trained architecture and the final downstream architecture.
- In masked language modelling, simply some percentage of the input takens are masked at random and then those masked tokens are predicted. The final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary. To avoid mismatches between pre-training and finet-uning through (mask) tokens, which are not present during fine-tuning they do not always replace "masked" words with a (mask) token. They have different variants: replace the $i$-th token with (mask) 80 % of the times, a random token 10 % of the time, and the unchanged $i$-th token 10 % of the time.
- MLM is sometimes called **stochastic masking** e. g. in [[@kossenSelfAttentionDatapointsGoing2021]]

![[bert-architecture.png]]
# Annotations#
The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ‚àà RH , and the final hidden vector for the ith input token as Ti ‚àà RH .

‚ÄúWe argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.‚Äù ([Devlin et al., 2019, p. 4171](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=1&annotation=XRLRAYCB))

‚ÄúThe masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the maske‚Äù ([Devlin et al., 2019, p. 4171](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=1&annotation=DUYZPXAL))

‚Äú4172 word based only on its context.‚Äù ([Devlin et al., 2019, p. 4172](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=2&annotation=TTWTRB35))

‚ÄúWe show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.‚Äù ([Devlin et al., 2019, p. 4172](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=2&annotation=DYF7MRXG))

‚ÄúDuring pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.‚Äù ([Devlin et al., 2019, p. 4173](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=3&annotation=3PWNNFTN))

‚ÄúA distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.‚Äù ([Devlin et al., 2019, p. 4173](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=3&annotation=MEXCVUI3))

‚ÄúTask #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly ‚Äúsee itself‚Äù, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a ‚ÄúTransformer encoder‚Äù while the left-context-only version is referred to as a ‚ÄúTransformer decoder‚Äù since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a ‚Äúmasked LM‚Äù (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input. Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the MASK token does not appear during fine-tuning. To mitigate this, we do not always replace ‚Äúmasked‚Äù words with the actual MASK token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the MASK token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.‚Äù ([Devlin et al., 2019, p. 4174](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=4&annotation=Q8F9TXRS))

‚ÄúPre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers.‚Äù ([Devlin et al., 2019, p. 4175](zotero://select/library/items/PA8LRNM4)) ([pdf](zotero://open-pdf/library/items/WVJNKICQ?page=5&annotation=JI8W5C6W))