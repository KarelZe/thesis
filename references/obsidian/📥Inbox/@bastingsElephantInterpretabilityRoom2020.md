*title:* The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?
*authors:* Jasmijn Bastings, Katja Filippova
*year:* 2020
*tags:* 
*status:* #ğŸ“¥
*related:*
*code:*
*review:*

## Notes ğŸ“

## Annotations ğŸ“–

â€œWhile many papers published on the topic of explainable AI have been criticised for not defining explanations (Lipton, 2018; Miller, 2019), the first key studies which spawned interest in attention as explanation (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) do say that they are interested in whether attention weights faithfully represent the responsibility each input token has on a model prediction. That is, the narrow definition of explanation implied there is that it points at the most important input tokens for a prediction (arg max), accurately summarizing the reasoning process of the model (Jacovi and Goldberg, 2020b).â€ (Bastings and Filippova, 2020, p. 149)