*title:* TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data
*authors:* Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel
*year:* 2020
*tags:* 
*status:* #📥
*related:*
*code:*
*review:*

## Notes 📍

## Annotations 📖

“n this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables.” ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=PP423JV9))

“These models allow us to capture the syntax and semantics of text via representations learnt in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019).” ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=JRUQ8NT6))

“These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables.” ([Yin et al., 2020, p. 8414](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=2&annotation=BU5LX6AU))