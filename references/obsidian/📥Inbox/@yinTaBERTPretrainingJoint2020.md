*title:* TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data
*authors:* Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel
*year:* 2020
*tags:* 
*status:* #ğŸ“¥
*related:*
*code:*
*review:*

## Notes ğŸ“

## Annotations ğŸ“–

â€œn this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables.â€ ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=PP423JV9))

â€œThese models allow us to capture the syntax and semantics of text via representations learnt in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019).â€ ([Yin et al., 2020, p. 8413](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=1&annotation=JRUQ8NT6))

â€œThese challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables.â€ ([Yin et al., 2020, p. 8414](zotero://select/library/items/3I9DXIYX)) ([pdf](zotero://open-pdf/library/items/3C6UVTC6?page=2&annotation=BU5LX6AU))