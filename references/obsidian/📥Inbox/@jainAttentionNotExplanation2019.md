*title:* Attention is not Explanation
*authors:* Sarthak Jain, Byron C. Wallace
*year:* 2019
*tags:* 
*status:* #📥
*related:*
*code:*
*review:*

## Notes 📍

## Annotations 📖

“We have provided evidence that correlation between intuitive feature importance measures (including gradient and feature erasure approaches) and learned attention weights is weak for recurrent encoders (Section 4.1). We also established that counterfactual attention distributions — which would tell a different story about why a model made the prediction that it did — often have modest effects on model output (Section 4.2).” (Jain and Wallace, 2019, p. 10)