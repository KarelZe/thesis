*title:* Training Tips for the Transformer Model
*authors:* Martin Popel, Ond≈ôej Bojar
*year:* 2018
*tags:* #transformer #lr-warmup #gpu #multi-gpu
*status:* #üì¶ 
*related:*
*code:*
*review:*

## Notes üìç
- Interesting for my work as it does the analysis for the vanilla transformer

## Annotations üìñ

‚ÄúThe situation in NMT is further complicated by the fact that the training of NMT systems is usually non-deterministic,4 and (esp. with the most recent models) hardly ever converges or starts overfitting5 on reasonably big datasets. This leads to learning curves that never fully flatten let alone start decreasing (see Section 4.2). The common practice of machine learning to evaluate the model on a final test set when it started overfitting (or a bit sooner) is thus not applicable in practice.‚Äù ([Popel and Bojar, 2018, p. 45](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=3&annotation=YDKPA2DP))

‚ÄúMost probably, the training was run until no further improvements were clearly apparent on the development test set, and the model was evaluated at that point. Such an approximate stopping criterion is rather risky: it is conceivable that different setups were stopped at different stages of training and their comparison is not fair‚Äù ([Popel and Bojar, 2018, p. 45](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=3&annotation=M4VIHAG4))

‚ÄúWhen we tried the standard technique of early stopping, when N subsequent evaluations on the development test set do not give improvements larger than a given delta, we saw a big variance in the training time and final BLEU, even for experiments with the same hyper-parameters and just a different random seed. Moreover to get the best results, we would have had to use a very large N and a very small delta‚Äù ([Popel and Bojar, 2018, p. 45](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=3&annotation=BZFIKK7K))

‚ÄúBy overfitting we mean here that the translation quality (test-set BLEU) begins to worsen, while the training loss keeps improving‚Äù ([Popel and Bojar, 2018, p. 45](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=3&annotation=9BWTHNJZ))

‚ÄúBatch Size is the number of training examples used by one GPU in one training step. In sequence-to-sequence models, batch size is usually specified as the number of sentence pairs. However, the parameter batch_size in T2T translation specifies the approximate number of tokens (subwords) in one batch.6 This allows to use a higher number of short sentences in one batch or a smaller number of long sentences. Effective Batch Size is the number of training examples consumed in one training step. When training on multiple GPUs, the parameter batch_size is interpreted per GPU. That is, with batch_size=1500 and 8 GPUs, the system actually digests 12k subwords of each language in one step.‚Äù ([Popel and Bojar, 2018, p. 46](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=4&annotation=W2V7DY2S))

‚ÄúIn this section, we focus however only on the computation speed and training throughput. Both are affected by three important factors: batch size, number of used GPUs and model size. The speed is usually almost constant for a given experiment‚Äù ([Popel and Bojar, 2018, p. 50](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=8&annotation=85VMX8US))

‚ÄúThe training throughput grows sub-linearly with increasing batch size, so based on these experiments only, there is just a small advantage when setting the batch size to the maximum value.‚Äù ([Popel and Bojar, 2018, p. 51](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=9&annotation=J4BAZIXJ))

‚Äúable 3 uses the BIG model and batch_size=1500, while varying the number of GPUs. The overhead in GPU synchronization is apparent from the decreasing computation speed. Nevertheless, the training throughput still grows with more GPUs, so e.g. with 6 GPUs we process 3.2 times more training data per hour relative to a single GPU (while without any overhead we would hypothetically expect 6 times more data).‚Äù ([Popel and Bojar, 2018, p. 51](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=9&annotation=3WTNFFSE))

‚ÄúTraining on the bigger dataset gives slightly worse results in the first eight hours of training (not shown in the graph) but clearly better results after two days of training, reaching over 26.5 BLEU after eight days.‚Äù ([Popel and Bojar, 2018, p. 52](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=10&annotation=BP9ZWDG3))

‚ÄúThis means about 10 epochs in the smaller dataset were needed for reaching the convergence and this is also the moment when the bigger‚Äù ([Popel and Bojar, 2018, p. 52](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=10&annotation=D8RD7JG8))

‚ÄúFor comparing different datasets (e.g. smaller and cleaner vs. bigger and noisier), we need to train long enough because results after first hours (or days if training on a single GPU) may be misleading. ‚Ä¢ For large training data (as CzEng 1.7 which has over half a gigaword), BLEU improves even after one week of training on eight GPUs (or after 20 days of training on two GPUs in another experiment). ‚Ä¢ We cannot easily interpolate one dataset results to another dataset. While the smaller training data (with CzEng 1.0) converged after 2 days, the main training data (with CzEng 1.7), which is 2.5 times bigger, continues improving even after 2.52 days.‚Äù ([Popel and Bojar, 2018, p. 53](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=11&annotation=E2UGZ67A))

‚ÄúFor example, to get over BLEU of 18 with batch_size=3000, we need 7 hours (260M examples), and with batch_size=1500, we need about 3 days (2260M examples) i.e. 10 times longer (9 time more examples). From Table 2a we know that bigger batches have slower computation speed, so when re-plotting Figure 5 with steps instead of time on the x-axis, the difference between the curves would be even bigger. From Table 2b we know that bigger batches have slightly higher training throughput, so when re-plotting with number of examples processed on the x-axis, the difference will be smaller, but still visible.‚Äù ([Popel and Bojar, 2018, p. 57](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=15&annotation=UYXSVFD9))

‚ÄúBatch size should be set as high as possible while keeping a reserve for not hitting the out-of-memory errors. It is advisable to establish the largest possible batch size before starting the main and long training.‚Äù ([Popel and Bojar, 2018, p. 59](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=17&annotation=2B8DLI6K))

‚ÄúThe default learning rate in T2T translation models is 0.20. Figure 7 shows that varying the value within range 0.05‚Äì0.25 makes almost no difference. Setting the learning rate too low (0.01) results in notably slower convergence. Setting the learning rate too high (0.30, not shown in the figure) results in diverged training, which means in this case that the learning curve starts growing as usual, but at one moment drops down almost to zero and stays there forever. A common solution to prevent diverged training is to decrease the learning_rate parameter or increase learning_rate_warmup_steps or introduce gradient clipping.‚Äù ([Popel and Bojar, 2018, p. 59](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=17&annotation=4H63KPLB))

‚ÄúFigure 8 shows the effect of different warmup steps with a fixed learning rate (the default 0.20). Setting warmup steps too low (12k) results in diverged training. Setting them too high (48k, green curve) results in a slightly slower convergence at first, but matching the baseline after a few hours of training.‚Äù ([Popel and Bojar, 2018, p. 60](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=18&annotation=VQE8AIRJ))

‚ÄúIn case of diverged training, try gradient clipping and/or more warmup steps.‚Äù ([Popel and Bojar, 2018, p. 60](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=18&annotation=SKAKHS3U))

‚ÄúIf that does not help (or if the warmup steps are too high relative to the expected total training steps), try decreasing the learning rate.‚Äù ([Popel and Bojar, 2018, p. 61](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=19&annotation=PQ5XWYLH))

‚ÄúFor the fastest BLEU convergence use as many GPUs as available (in our experiments up to 8). ‚Ä¢ This holds even when there are more experiments to be done. For example, it is better to run one 8-GPUs experiment after another, rather than running two 4-GPUs experiments in parallel or eight single-GPU experiments in parallel.‚Äù ([Popel and Bojar, 2018, p. 62](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=20&annotation=YPZ7VUER))

‚ÄúThere is a growing number of papers on scaling deep learning to multiple machines with synchronous SGD (or its variants) by increasing the effective batch size. We will focus mostly on the question how to adapt the learning rate schedule, when scaling from one GPU (or any device, in general) to k GPUs. Krizhevsky (2014) says ‚ÄúTheory suggests that when multiplying the batch size by k, one should multiply the learning rate by pk to keep the variance in the gradient expectation constant‚Äù, without actually explaining which theory suggests so. However, in the experimental part he reports that what worked the best, was a linear scaling heuristics, i.e. multiplying the learning rate by k, again without any explanation nor details on the difference between pk scaling and k scaling.‚Äù ([Popel and Bojar, 2018, p. 63](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=21&annotation=4DFMEUV6))

‚ÄúWe can see that large-batch training is still an open research question. Most of the papers cited above have experimental support only from the image recognition tasks (usually ImageNet) and convolutional networks (e.g. ResNet), so it is not clear whether their suggestions can be applied also on sequence-to-sequence tasks (NMT) with self-attentional networks (Transformer). There are several other differences as well: Modern convolutional networks are usually trained with batch normalization 29 To close the gap between small-batch training and large-batch training, Hoffer et al. (2017) introduce (in addition to pk scaling) so-called ghost batch normalization and adapted training regime, which means decaying the learning rate after a given number of steps instead of epochs. 63 Unauthenticated Download Date | 11/13/18 8:36 A‚Äù ([Popel and Bojar, 2018, p. 63](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=21&annotation=R6YXE6V7))

‚ÄúPBML 110 APRIL 2018 (Ioffe and Szegedy, 2015), which seems to be important for the scaling, while Transformer uses layer normalization (Lei Ba et al., 2016).30 Also, Transformer uses Adam together with an inverse-square-root learning-rate decay, while most ImageNet papers use SGD with momentum and piecewise-constant learning-rate decay.‚Äù ([Popel and Bojar, 2018, p. 64](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=22&annotation=UJ46ZCJ4))

‚ÄúJastrzebski et al. (2017) shows that ‚Äúthe invariance under simultaneous rescaling of learning rate and batch size breaks down if the learning rate gets too large or the batch size gets too small‚Äù. A similar observation was reported e.g. by Bottou et al. (2016). Thus our initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for stable training in our experiments even when we scale from a single GPU to 8 GPUs. Considering this initial hypothesis, we were surprised that we were able to achieve so good Time Till Score with 8 GPUs (more than 8 times smaller relative to a single GPU, as reported in Table 6).‚Äù ([Popel and Bojar, 2018, p. 64](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=22&annotation=TAG6NJXZ))

‚ÄúIn deed, many researchers (e.g. Goyal et al., 2017) suggest to use a warmup when scaling to more GPUs in order to prevent divergence. Transformer uses learning rate warmup by default even for single-GPU training (cf. Section 4.6), but it makes sense to use more warmup training examples in multi-GPU setting.‚Äù ([Popel and Bojar, 2018, p. 65](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=23&annotation=QFUSWIJB))

‚ÄúTips on Learning Rate and Warmup Steps on Multiple GPUs ‚Ä¢ Keep the learning_rate parameter at its optimal value found in single-GPU experiments. ‚Ä¢ You can try decreasing the warmup steps, but less than linearly and you should not expect to improve the final BLEU this way.‚Äù ([Popel and Bojar, 2018, p. 65](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=23&annotation=6IHF39MY))

‚ÄúFor example, Smith et al. (2017) suggest to increase the effective batch size (and number of GPUs) during training, instead of decaying the learning rate.‚Äù ([Popel and Bojar, 2018, p. 66](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=24&annotation=6T7FUD8R))

‚ÄúAmong other practical observations, we‚Äôve seen that for the Transformer model, larger batch sizes lead not only to faster training but more importantly better translation quality. Given at least a day and a 11GB GPU for training, the larger setup (BIG) should be always preferred. The Transformer model and its implementation in Tensor2Tensor is also best fit for ‚Äúintense training‚Äù: using as many GPUs as possible and running experiments one after another should be preferred over running several single-GPU experiments concurrently.‚Äù ([Popel and Bojar, 2018, p. 68](zotero://select/library/items/7NCFQLPX)) ([pdf](zotero://open-pdf/library/items/2MRGXVHC?page=26&annotation=J6ALIZZQ))