*title:* What Does BERT Learn about the Structure of Language?
*authors:* Ganesh Jawahar, BenoÃ®t Sagot, DjamÃ© Seddah
*year:* 2019
*tags:* 
*status:* #ğŸ“¥
*related:*
*code:*
*review:*

## Notes ğŸ“

<<<<<<< HEAD
## Annotations ğŸ“–

â€œWe also show that BERTâ€™s intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top.â€ ([Jawahar et al., 2019, p. 3651](zotero://select/library/items/WRNPXDVJ)) ([pdf](zotero://open-pdf/library/items/CEY6IR5F?page=1&annotation=BV7KMVHX))

â€œBERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.â€ ([Jawahar et al., 2019, p. 3651](zotero://select/library/items/WRNPXDVJ)) ([pdf](zotero://open-pdf/library/items/CEY6IR5F?page=1&annotation=FQKFQWKR))

â€œWe have shown that phrasal representations learned by BERT reflect phraselevel information and that BERT composes a hierarchy of linguistic signals ranging from surface to semantic features. We have also shown that BERT requires deeper layers to model long-range dependency information. Finally, we have shown that BERTâ€™s internal representations reflect a compositional modelling that shares parallels with traditional syntactic analysis. It would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.â€ ([Jawahar et al., 2019, p. 3655](zotero://select/library/items/WRNPXDVJ)) ([pdf](zotero://open-pdf/library/items/CEY6IR5F?page=5&annotation=8Z282GVT))
=======
## Annotations ğŸ“–
>>>>>>> origin/main
