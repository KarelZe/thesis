*title:* Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks
*authors:* Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang, Eunho Yang
*year:* 2019
*tags:* #imputation #missing-value #neural_network 
*status:* #üì•
*related:*
- [[@josseConsistencySupervisedLearning2020]]
- [[@perez-lebelBenchmarkingMissingvaluesApproaches2022]] (use native support of model (not possible here) + missing indicator improves model. They also consider MCAR)
- [[@hintonImprovingNeuralNetworks2012]] (there might be a link to dropout)
*code:* None
*review:*
- accepted. https://openreview.net/forum?id=BylsKkHYvH

## Notes üìç
- Missing data is common in many data sets. The popular approach of zero imputation that replaces missing values with zeros hampers the performance in training neural networks. Authors investigate the adverse effects of zero imputation both theoretically and empirically.
- The most simplest and natural way, to impute missing data in neural networks is zero imputation. The choice of using zero imputation is natural, as weights associated with the input are not updated. (Similar to dropout (?)) However, the performance of the neural net degraded in various studies.
- Authors show that zero imputation leds to a situation where the output (expected value of output layer) of a multi-layer neural network is largely dependent on the rate of missingness (sparsity) in the input even if inputs themselves are otherwise similar. They coin the term *variable sparsity problem* for this problem.
- They consider the missing completely at random (MCAR) case.
- They include a short discussion for techniques to handle missing values. Their mentioned pros and cons could be interesting.
- They include three theoretical proofs (identity function as activation, affine function with nonzero bias, non-linear, but non-decreasing and convex activation) that all show that all show that the expected value of of the output layer depends on the missingness rates for otherwise similar data.

## Annotations üìñ

‚ÄúAmong many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks‚Äù ([Yi et al., 2020, p. 1](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=1&annotation=SER6UR6P))

‚ÄúWhile various imputing techniques, from imputing using global statistics such as mean, to individually imputing by learning auxiliary models such as GAN, can be applied with their own pros and cons, the most simple and natural way to do this is zero imputation, where we simply treat a missing feature as zero. In neural networks, at first glance, zero imputation can be thought of as a reasonable solution since it simply drops missing input nodes by preventing the weights associated with them from being updated‚Äù ([Yi et al., 2020, p. 1](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=1&annotation=UK9QFAVZ))

‚ÄúSome what surprisingly, however, many previous studies have reported that this intuitive approach has an adverse effect on model performances (Hazan et al., 2015; Luo et al., 2018; ÃÅ Smieja et al., 2018), and none of them has investigated the reasons of such performance degradations.‚Äù ([Yi et al., 2020, p. 1](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=1&annotation=QBZSJ3KB))

‚ÄúIn this work, we find that zero imputation causes the output of a neural network to largely vary with respect to the number of missing entries in the input.‚Äù ([Yi et al., 2020, p. 1](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=1&annotation=I4FJ5T94))

‚ÄúTo best of our knowledge, we are the first in exploring the adverse effect of zero imputation, both theoretically and empirically.‚Äù ([Yi et al., 2020, p. 2](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=2&annotation=3IMMDI3M))

‚ÄúWe formally define the Variable Sparsity Problem (VSP) as follows: a phenomenon in which the expected value of the output layer of a neural network (over the weight and input distributions) depends on the sparsity (the number of zero values) of the input data (Figure 2a).‚Äù ([Yi et al., 2020, p. 3](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=3&annotation=U4LVVFGM))

‚ÄúAssumption 1. (i) Every coordinate of input vector, h0 l , is generated by the element-wise multiplication of two random variables ÃÉ h0 l and ml where ml is binary mask indicating missing value and ÃÉ h0 l is a (possibly unobserved) feature value. Here, missing mask ml is MCAR (missing completely at random), with no dependency with other mask variables or their values ÃÉ h0. All ml follow some identical distribution with mean Œºm. (ii) The elements of matrix W i are mutually independent and follow the identical distribution with mean Œºiw. Similarly, bi and ÃÉ h0 consist of i.i.d. coordinates with mean Œºi b and Œºx, respectively. (iii) Œºiw is not zero uniformly over all i.‚Äù ([Yi et al., 2020, p. 3](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=3&annotation=NUCQCRBQ))

‚ÄúHere, missing mask ml is MCAR (missing completely at random), with no dependency with other mask variables or their values ÃÉ h0.‚Äù ([Yi et al., 2020, p. 3](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=3&annotation=HZUN2Y3J))

‚Äú(Case 1) For simplicity, let us first consider networks without the non-linearity nor the bias term. Theorem 1 shows that the average value of the output layer E is directly proportional to the expectation of the mask vector Œºm: Theorem 1. Suppose that activation œÉ is an identity function and that bi l is uniformly fixed as zero under Assumption 1. Then, we have EhL l  = ‚àèL i=1 ni‚àí1ŒºiwŒºxŒºm. (Case 2) When the activation function is affine but now with a possibly nonzero bias, E is influenced by Œºm in the following way: Theorem 2. Suppose that activation œÉ is an affine function under Assumption 1. Suppose further that fi(x) is defined as œÉ(ni‚àí1Œºiwx + Œºi b). Then, E= fL ‚ó¶ ¬∑ ¬∑ ¬∑ ‚ó¶ f1(ŒºxŒºm). (Case 3) Finally, when the activation function is non-linear but non-decreasing and convex, we can show that Eis lower-bounded by some quantity involving Œºm: Theorem 3. Suppose that œÉ is a non-decreasing convex function under Assumption 1. Suppose further that fi(x) is defined as œÉ(ni‚àí1Œºiwx + Œºi b) and Œºiw > 0. Then, E‚â• fL ‚ó¶ ¬∑ ¬∑ ¬∑ ‚ó¶ f1(ŒºxŒºm)‚Äù ([Yi et al., 2020, p. 4](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=4&annotation=QJ5MMDP8))

‚ÄúIf the expected value of the output layer (or the lower bound of it) depends on the level of sparsity/missingness as in Theorem 1-3, even similar data instances may have different output values depending on their sparsity levels, which would hinder fair and correct inference of the model.‚Äù ([Yi et al., 2020, p. 4](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=4&annotation=WS94ACJP))

‚ÄúMissing handling techniques Missing imputation can be understood as a technique to increase the generalization performance by injecting plausible noise into data. Noise injection using global statistics like mean or median values is the simplest way to do this (Lipton et al., 2016; ÃÅ Smieja et al., 2018). However, it could lead to highly incorrect estimation since they do not take into consideration the characteristics of each data instance (Tresp et al., 1994; Che et al., 2018). To overcome this limitation, researchers have proposed various ways to model individualized noise using autoencoders (Pathak et al., 2016; Gondara & Wang, 2018), or GANs (Yoon et al., 2018; Li et al., 2019). However, those model based imputation techniques have not properly worked for high dimensional datasets with the large number of features and/or extremely high missing rates (Yoon et al., 2018) because excessive noise can ruin the training of neural networks rather increasing generalization performance.‚Äù ([Yi et al., 2020, p. 9](zotero://select/library/items/NXXDX9GE)) ([pdf](zotero://open-pdf/library/items/WY8W2JH7?page=9&annotation=S6KDC83A))