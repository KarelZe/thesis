*title:* Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning
*authors:* Sebastian Raschka
*year:* 2020
*tags:* #train-test-split #bias #variance
*status:* #üì¶ 
*related:*
*code:*
*review:*

## Notes üìç

## Annotations üìñ

‚ÄúTypically, machine learning involves a lot of experimentation, though ‚Äì for example, the tuning of the internal knobs of a learning algorithm, the so-called hyperparameters. Running a learning algorithm over a training dataset with different hyperparameter settings will result in different models. Since we are typically interested in selecting the best-performing model from this set, we need to find a way to estimate their respective performances in order to rank them against each other.‚Äù ([Raschka, 2020, p. 4](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=4&annotation=4NRFHQ5A))

‚ÄúWe want to estimate the generalisation performance, the predictive performance of our model on future (unseen) data.‚Äù ([Raschka, 2020, p. 4](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=4&annotation=3MAAAIG4))
‚ÄúWe want to increase the predictive performance by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space.‚Äù ([Raschka, 2020, p. 4](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=4&annotation=8HBEJAAH))

‚Äúi.i.d. We assume that the training examples are i.i.d (independent and identically distributed), which means that all examples have been drawn from the same probability distribution and are statistically independent from each other. A scenario where training examples are not independent would be working with temporal data or time-series data.‚Äù ([Raschka, 2020, p. 5](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=5&annotation=4IDA9UEZ))

‚ÄúThe holdout method is inarguably the simplest model evaluation technique; it can be summarised as follows. First, we take a labelled dataset and split it into two parts: A training and a test set. Then, we fit a model to the training data and predict the labels of the test set. The fraction of correct predictions, which can be computed by comparing the predicted labels to the ground truth labels of the test set, constitutes our estimate of the model‚Äôs prediction accuracy. Here, it is important to note that we do not want to train and evaluate a model on the same training dataset (this is called resubstitution validation or resubstitution evaluation), since it would typically introduce a very optimistic bias due to overfitting. In other words, we cannot tell whether the model simply memorised the training data, or whether it generalises well to new, unseen data. (On a side note, we can estimate this so-called optimism bias as the difference between the training and test accuracy.)‚Äù ([Raschka, 2020, p. 7](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=7&annotation=5VZ59UHE))

‚ÄúTypically, the splitting of a dataset into training and test sets is a simple process of random subsampling. We assume that all data points have been drawn from the same probability distribution (with respect to each class). And we randomly choose 2/3 of these samples for the training set and 1/3 of the samples for the test set. Note that there are two problems with this approach, which we will discuss in the next sections.‚Äù ([Raschka, 2020, p. 7](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=7&annotation=5WGELATP))

‚ÄúWhen we randomly divide a labelled dataset into training and test sets, we violate the assumption of statistical independence‚Äù ([Raschka, 2020, p. 7](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=7&annotation=4JUNWP72))

‚ÄúStep 1. First, we randomly divide our available data into two subsets: a training and a test set. Setting test data aside is a work-around for dealing with the imperfections of a non-ideal world, such as limited data and resources, and the inability to collect more data from the generating distribution. Here, the test set shall represent new, unseen data to the model; it is important that the test set is only used once to avoid introducing bias when we estimating the generalisation performance. Typically, we assign 2/3 to the training set and 1/3 of the data to the test set. Other common training/test splits are 60/40, 70/30, or 80/20 ‚Äì or even 90/10 if the dataset is relatively large. Step 2. After setting test examples aside, we pick a learning algorithm that we think could be appropriate for the given problem. As a quick reminder regarding the Hyperparameter Values depicted in Figure 2, hyperparameters are the parameters of our learning algorithm, or meta-parameters. And we have to specify these hyperparameter values manually ‚Äì the learning algorithm does not learn these from the training data in contrast to the actual model parameters. Since hyperparameters are not‚Äù ([Raschka, 2020, p. 8](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=8&annotation=DN24YHYJ))

‚ÄúLearning Algorithm Hyperparameter Values Model Prediction Test Labels Performance Model Learning Algorithm Hyperparameter Values Final Model 2 3 4 1 Test Labels Test Data Training Data Training Labels Data Labels Data Labels Training Data Training Labels Test Data Figure 2: Visual summary of the holdout validation method. learnt during model fitting, we need some sort of "extra procedure" or "external loop" to optimise these separately ‚Äì this holdout approach is ill-suited for the task. So, for now, we have to go with some fixed hyperparameter values ‚Äì we could use our intuition or the default parameters of an off-the-shelf algorithm if we are using an existing machine learning library. Step 3. After the learning algorithm fit a model in the previous step, the next question is: How "good" is the performance of the resulting model? This is where the independent test set comes into play. Since the learning algorithm has not "seen" this test set before, it should provide a relatively unbiased estimate of its performance on new, unseen data. Now, we take this test set and use the model to predict the class labels. Then, we take the predicted class labels and compare them to the "ground truth," the correct class labels, to estimate the models generalisation accuracy or error.‚Äù ([Raschka, 2020, p. 9](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=9&annotation=SN52MB9V))

‚Äúf a model has not reached its capacity, the performance estimate would be pessimistically biased. This assumes that the algorithm could learn a better model if it was given more data ‚Äì by splitting off a portion of the dataset for testing, we withhold valuable data for estimating the generalisation performance (for instance, the test dataset). To address this issue, one might fit the model to the whole dataset after estimating the generalisation performance (see Figure 2 step 4). However, using this approach, we cannot estimate its generalisation performance of the refit model, since we have now "burned" the test dataset. It is a dilemma that we cannot really avoid in real-world application, but we should be aware that our estimate of the generalisation performance may be pessimistically biased if only a portion of the dataset, the training dataset, is used for model fitting (this is especially affects models fit to relatively small datasets).‚Äù ([Raschka, 2020, p. 10](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=10&annotation=YIV3XXNW))

‚Äú1.7 Confidence Intervals via Normal Approximation Using the holdout method as described in Section 1.5, we computed a point estimate of the generalisation performance of a model. Certainly, a confidence interval around this estimate would not only be more informative and desirable in certain applications, but our point estimate could be quite sensitive to the particular training/test split (for instance, suffering from high variance). A simple approach for computing confidence intervals of the predictive accuracy or error of a model is via the so-called normal approximation. Here, we assume that the predictions follow a normal distribution, to compute the confidence interval on the mean on a single training-test split under the central limit theorem. The following text illustrates how this works. As discussed earlier, we compute the prediction accuracy on a dataset S (here: test set) of size n as follows: ACCS = 1 n n ‚àë i=1 Œ¥(L( ÀÜ yi, yi)), (10) where L(¬∑) is the 0-1 loss function (Equation 3), and n denotes the number of samples in the test dataset. Further, let ÀÜ yi be the predicted class label and yi be the ground truth class label of the ith test example, respectively. So, we could now consider each prediction as a Bernoulli trial, and the number of correct predictions X is following a binomial distribution X ‚àº (n, p) with n test examples, k trials, and the probability of success p, where n ‚àà N and p ‚àà [0, 1] : f (k; n, p) = Pr(X = k) = (n k ) pk(1 ‚àí p)n‚àík, (11) for k = 0, 1, 2, ..., n, where (n k ) = n! k!(n ‚àí k)! . (12) (Here, p is the probability of success, and consequently, (1 ‚àí p) is the probability of failure ‚Äì a wrong prediction.) Now, the expected number of successes is computed as Œº = np, or more concretely, if the model has a 50% success rate, we expect 20 out of 40 predictions to be correct. The estimate has a variance of œÉ2 = np(1 ‚àí p) = 10 (13) 1‚Äù ([Raschka, 2020, p. 10](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=10&annotation=VC7JH43F))

‚Äúand a standard deviation of œÉ = ‚àönp(1 ‚àí p) = 3.16. (14) Since we are interested in the average number of successes, not its absolute value, we compute the variance of the accuracy estimate as œÉ2 = 1 n ACCS(1 ‚àí ACCS), (15) and the respective standard deviation as œÉ= ‚àö1 n ACCS(1 ‚àí ACCS). (16) Under the normal approximation, we can then compute the confidence interval as ACCS ¬± z ‚àö1 n ACCS (1 ‚àí ACCS), (17) where Œ± is the error quantile and z is the 1 ‚àí Œ± 2 quantile of a standard normal distribution. For a typical confidence interval of 95%, (Œ± = 0.05), we have z = 1.96. In practise, however, I would rather recommend repeating the training-test split multiple times to compute the confidence interval on the mean estimate (for instance, averaging the individual runs). In any case, one interesting take-away for now is that having fewer samples in the test set increases the variance (see n in the denominator above) and thus widens the confidence interval. Confidence intervals and estimating uncertainty will be discussed in more detail in the next section, Section 2‚Äù ([Raschka, 2020, p. 11](zotero://select/library/items/TZ7CA5ML)) ([pdf](zotero://open-pdf/library/items/PYSTN2HI?page=11&annotation=ZNI6HNKL))