*title:* Learning Deep Transformer Models for Machine Translation
*authors:* Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, Lidia S. Chao
*year:* 2019
*tags:* 
*status:* #transformer #layernorm #residual-connections 
*related:*
- [[@xiongLayerNormalizationTransformer2020]]
- [[@liuRethinkingSkipConnection2020]]
*code:*
*review:*

## Notes 📍

## Annotations 📖

“For Transformer, it is not easy to train stacked layers on neither the encoder-side nor the decoderside. Stacking all these sub-layers prevents the efficient information flow through the network, and probably leads to the failure of training. Residual connections and layer normalisation are adopted for a solution. Let F be a sub-layer in encoder or decoder, and θl be the parameters of the sub-layer. A residual unit is defined to be (He et al., 2016b): xl+1 = f (yl) (1) yl = xl + F (xl; θl) (2) where xl and xl+1 are the input and output of the l-th sub-layer, and yl is the intermediate output followed by the post-processing function f (·). In this way, xl is explicitly exposed to yl (see Eq. (2)). Moreover, layer normalisation is adopted to reduce the variance of sub-layer output because hidden state dynamics occasionally causes a much longer training time for convergence. There are two ways to incorporate layer normalisation into the residual network. • Post-Norm. In early versions of Transformer (Vaswani et al., 2017), layer normalisation is placed after the element-wise residual addition (see Figure 1(a)), like this: xl+1 = LN(xl + F (xl; θl)) (3) where LN(·) is the layer normalisation function, whose parameter is dropped for simplicity. It can be seen as a post-processing step of the output (i.e., f (x) = LN(x)). • Pre-Norm. In recent implementations (Klein et al., 2017; Vaswani et al., 2018; Domhan, 2018), layer normalisation is applied to the input of every sub-layer (see Figure 1(b)): xl+1 = xl + F (LN(xl); θl) (4” ([Wang et al., 2019, p. 1811](zotero://select/library/items/BJ43W6ZQ)) ([pdf](zotero://open-pdf/library/items/HVE8Q5EQ?page=2&annotation=I7ZJMKU9))

“1812 Eq. (4) regards layer normalisation as a part of the sub-layer, and does nothing for postprocessing of the residual connexion (i.e., f (x) = x).3 Both of these methods are good choices for implementation of Transformer. In our experiments, they show comparable performance in BLEU for a system based on a 6-layer encoder (Section 5.1).” ([Wang et al., 2019, p. 1812](zotero://select/library/items/BJ43W6ZQ)) ([pdf](zotero://open-pdf/library/items/HVE8Q5EQ?page=3&annotation=MQ9ZMJU4))

“2.2 On the Importance of Pre-Norm for Deep Residual Network The situation is quite different when we switch to deeper models. More specifically, we find that prenorm is more efficient for training than post-norm if the model goes deeper. This can be explained by seeing back-propagation which is the core process to obtain gradients for parameter update. Here we take a stack of L sub-layers as an example. Let E be the loss used to measure how many errors occur in system prediction, and xL be the output of the topmost sub-layer. For post-norm Transformer, given a sub-layer l, the differential of E with respect to xl can be computed by the chain rule, and we have ∂E ∂xl = ∂E ∂xL × L−1 ∏ k=l ∂LN(yk) ∂yk × L−1 ∏ k=l ( 1 + ∂F (xk; θk) ∂xk ) (5) where ∏L−1 k=l ∂ LN(yk ) ∂yk means the backward pass of the layer normalisation, and ∏L−1 k=l (1 + ∂F(xk;θk) ∂xk ) means the backward pass of the sub-layer with the residual connexion. Likewise, we have the gradient for pre-norm 4: ∂E ∂xl = ∂E ∂xL × ( 1+ L−1 ∑ k=l ∂F (LN(xk); θk) ∂xl ) (6) Obviously, Eq. (6) establishes a direct way to pass error gradient ∂E ∂xL from top to bottom. Its merit lies in that the number of product items on the right side does not depend on the depth of the stack. In contrast, Eq. (5) is inefficient for passing gradients back because the residual connexion is not 3We need to add an additional function of layer normalisation to the top layer to prevent the excessively increased value caused by the sum of unnormalized output. 4For a detailed derivation, we refer the reader to Appendix A. a bypass of the layer normalisation unit (see Figure 1(a)). Instead, gradients have to be passed through LN(·) of each sub-layer. It in turn introduces term ∏L−1 k=l ∂ LN(yk ) ∂yk into the right hand side of Eq. (5), and poses a higher risk of gradient vanishing or exploring if L goes larger. This was confirmed by our experiments in which we successfully trained a pre-norm Transformer system with a 20-layer encoder on the WMT English-German task, whereas the post-norm Transformer system failed to train for a deeper encoder (Section 5.1).” ([Wang et al., 2019, p. 1812](zotero://select/library/items/BJ43W6ZQ)) ([pdf](zotero://open-pdf/library/items/HVE8Q5EQ?page=3&annotation=AXKHQZJR))