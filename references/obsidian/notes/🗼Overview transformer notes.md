<mark style="background: #FFB8EBA6;">“In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector” ([Sutskever et al., 2014, p. 1](zotero://select/library/items/53V4NR45)) ([pdf](zotero://open-pdf/library/items/GIFI2QHW?page=1&annotation=9R6ENRET))</mark>

<mark style="background: #FFB86CA6;">“First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously [18].” ([Sutskever et al., 2014, p. 3](zotero://select/library/items/53V4NR45)) ([pdf](zotero://open-pdf/library/items/GIFI2QHW?page=3&annotation=XDBIHRA3))</mark>

<mark style="background: #FFB8EBA6;">“The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M for the “decoder” LSTM). The complete training details are given below:” ([Sutskever et al., 2014, p. 5](zotero://select/library/items/53V4NR45)) ([pdf](zotero://open-pdf/library/items/GIFI2QHW?page=5&annotation=A6KRMP78))</mark>

<mark style="background: #CACFD9A6;">Sequence to sequence model was first proposed by Sutskever et al. [151] as a generic solution for sequence to sequence mapping. The main motivation was the fact that one sequence does not necessarily map to another sequence on element to element basis. An example is e.g. language translation where relevant words in the input sentence might occur at different places in the translated sentence. The lengths of the sentences might differ too. For these reasons, sequence to sequence models consist of two stages: encoder and decoder. Encoder processes input and encodes its representation into a fixed length hidden state (sometimes also called context vector ).</mark>
(https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/3018228/no.ntnu:inspera:112405386:20639743.pdf?sequence=1)