
![[Pasted image 20230126132519.png]]
(found here: https://images.ctfassets.net/dkgr2j75jrom/A6Xf1MfISZhiQWuyGFDpV/708d5fd82c14d8db6eb0d4af1c27d525/PillarPage-Qual-Quan-3.svg)

- Explain how embeddings can be implemented. (querying matrix with one-hot-vector)

Embeddings are a memory efficient way of representing one-hot encoded features that can also capture additional relationships between factors (or levels) of that variable. (https://medium.com/@michi.jeremias/embeddings-in-tabular-data-990202daa59f). See also their explanation on the relation between one-hot-encoding and embeddings.

<mark style="background: #ABF7F7A6;">“Encoding the Data In language models, all tokens are embedded using the same procedure. However, in the tabular domain, different features can come from distinct distributions, necessitating a heterogeneous embedding approach. Note that tabular data can contain multiple categorical features which may use the same set of tokens. Unless it is known that common tokens possess identical relationships within multiple columns, it is important to embed these columns independently. Unlike the embedding of TabTransformer[18], which uses attention to embed only categorical features, we propose also projecting continuous features into a d−dimensional space before passing their embedding through the transformer encoder.” ([Somepalli et al., 2021, p. 4](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=4&annotation=Z6DBZJTE))
</mark>


<mark style="background: #FFF3A3A6;">
“The most common variable types in structured data are continuous variables and discrete variables. Continuous variables such as temperature, price, weight can be represented by real numbers. Discrete variables such as age, color, bus line number can be represented by integers. Often the integers are just used for convenience to label the different states and have no information in themselves. For example if we use 1, 2, 3 to represent red, blue and yellow, one can not assume that ”blue is bigger than red” or ”the average of red and yellow are blue” or anything that introduces additional information based on the properties of integers. These integers are called nominal numbers. Other times there is an intrinsic ordering in the integer index such as age or month of the year. These integers are called cardinal number or ordinal numbers. Note that the meaning or order may not be more useful for the problem than only considering the” ([Guo and Berkhahn, 2016, p. 3](zotero://select/library/items/5CUI2BTM)) ([pdf](zotero://open-pdf/library/items/TMTTKAZP?page=3&annotation=BW76G7AJ))</mark>

<mark style="background: #FFB86CA6;">“Therefore, naively applying neural networks on structured data with integer representation for category variables does not work well. A common way to circumvent this problem is to use onehot encoding, but it has two shortcomings: First when we have many high cardinality features one-hot encoding often results in an unrealistic amount of computational resource requirement. Second, it treats different values of categorical variables completely independent of each other and often ignores the informative relations between them.” ([Guo and Berkhahn, 2016, p. 1](zotero://select/library/items/5CUI2BTM)) ([pdf](zotero://open-pdf/library/items/TMTTKAZP?page=1&annotation=XLJYFFSJ))</mark>

<mark style="background: #ADCCFFA6;">“We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables.” ([Guo and Berkhahn, 2016, p. 1](zotero://select/library/items/5CUI2BTM)) ([pdf](zotero://open-pdf/library/items/TMTTKAZP?page=1&annotation=CYY7CN4R))</mark>

<mark style="background: #ABF7F7A6;">“With entity embedding we want to put similar values of a categorical variable closer to each other in the embedding space. If we use a real number to define similarity of the values then entity embedding is closely related to the embedding of finite metric space problem in topology” ([Guo and Berkhahn, 2016, p. 4](zotero://select/library/items/5CUI2BTM)) ([pdf](zotero://open-pdf/library/items/TMTTKAZP?page=4&annotation=EEMP9PII))</mark>

“The investigation in (Grinsztajn et al., 2022) pointed out three inherent characteristics of tabular data that impeded known neural networks from top-tier performances, including irregular patterns of the target function, the negative effects of uninformative features, and the nonrotationally-invariant features. Based on this, we furthermore identify two points that highly promote the capabilities of neural networks on tabular data. (i) <mark style="background: #BBFABBA6;">An appropriate feature embedding approach. Though it was demonstrated (Rahaman et al., 2019; Grinsztajn et al., 2022) that neural networks are likely to predict overly smooth solutions on tabular data, a deep learning model was also observed to be capable of memorizing random labels (Zhang et al., 2021). Since the target function patterns are irregular and spurious correlations between the targets and features exist, an appropriate feature embedding network should well fit the irregular patterns while maintaining generalizability.</mark> (ii) A careful feature interaction approach. Since features of tabular data are non-rotationally-variant and a considerable portion of them are uninformative, it harms the generalization when a model incorporates needless feature interactions.” ([Chen et al., 2023, p. 1](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=1&annotation=49WB9QDJ)) (Do not cite paper, as it seems to be rather low-quality otherwise; cite Gristajin instead)

<mark style="background: #ABF7F7A6;">“The deep component is a feed-forward neural network, as shown in Figure 1 (right). For categorical features, the original inputs are feature strings (e.g., “language=en”). Each of these sparse, high-dimensional categorical features are first converted into a low-dimensional and dense real-valued vector, often referred to as an embedding vector. The dimensionality of the embeddings are usually on the order of O(10) to O(100). The embedding vectors are initialized randomly and then the values are trained to minimize the final loss function during model training. These low-dimensional dense embedding vectors are then fed into the hidden layers of a neural network in the forward pass.” ([Cheng et al., 2016, p. 2](zotero://select/library/items/EZUKWCIB)) ([pdf](zotero://open-pdf/library/items/QGHKZ7UB?page=2&annotation=KS6UM9FZ))</mark>

<mark style="background: #D2B3FFA6;">“Encoding the Data In language models, all tokens are embedded using the same procedure. However, in the tabular domain, different features can come from distinct distributions, necessitating a heterogeneous embedding approach. Note that tabular data can contain multiple categorical features which may use the same set of tokens. Unless it is known that common tokens possess identical relationships within multiple columns, it is important to embed these columns independently. Unlike the embedding of TabTransformer[18], which uses attention to embed only categorical features, we propose also projecting continuous features into a d−dimensional space before passing their embedding through the transformer encoder.” ([Somepalli et al., 2021, p. 4](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=4&annotation=Z6DBZJTE))
</mark>

<mark style="background: #D2B3FFA6;">“Effect of embedding continuous features To understand the effect of learning embeddings for continuous data, we perform a simple experiment with TabTransformer. We modify TabTransformer by embedding continuous features into d dimensions using a single layer ReLU MLP, just as they use on categorical features, and we pass the embedded features through the transformer block. We keep the entire architecture and all training hyper-parameters the same for both TabTransformer and its modified version. The average AUROC of the original TabTransformer is 89.38. Just by embedding the continuous features, the performance jumps to 91.72. This experiment shows that embedding the continuous data is important and can boost the performance of the model significantly.” ([Somepalli et al., 2021, p. 8](zotero://select/library/items/PCV7XCHY)) ([pdf](zotero://open-pdf/library/items/N8H76CQW?page=8&annotation=JK3TUUXE)) -> better performance. -> leave open why it works. [[@grinsztajnWhyTreebasedModels2022]]
</mark>

<mark style="background: #FF5582A6;">“Apart from model designs, various data representation approaches, such as feature embedding (Gorishniy et al., 2022), discretization of continuous features (Guo et al., 2021; Wang et al., 2020), and rule search approaches (Wang et al., 2021), were proposed against the irregular target patterns (Tancik et al., 2020; Grinsztajn et al., 2022).” ([Chen et al., 2023, p. 2](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=2&annotation=65MMYTUW))</mark>

“To answer all those questions, we propose to relax fixed table structures by introducing a Transferable Tabular Transformer (TransTab) for tables. The goal of TransTab is to convert each sample (a row in the table) to a <mark style="background: #FF5582A6;">generalizable embedding vector</mark>, and then apply stacked transformers for feature encoding.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=IRSLFH22)). “The other insight is to introduce supervised and self-supervised pretraining to improve model performance. We compare TransTab with multiple baseline methods on diverse benchmark datasets and five oncology clinical trial datasets. Overall, TransTab ranks 1.00, 1.00, 1.78 out of 12 methods in supervised learning, feature incremental learning, and transfer learning scenarios, respectively; and the proposed <mark style="background: #FF5582A6;">pretraining leads to 2.3% AUC lift on average over the supervised learning.</mark>” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=E6CC35EC)) [[@wangLearningDeepTransformer2019]]

“<mark style="background: #FFB86CA6;">Some previous approaches either designed feature embedding approaches (Gorishniy et al., 2022) to alleviate overly smooth solutions inspired by (Tancik et al., 2020) </mark>or employed regularization (Katzir et al., 2020) and shallow models (Cheng et al., 2016) to promote the model generalization, while some neural networks were equipped with sophisticated feature interaction approaches (Yan et al., 2023; Chen et al., 2022; Gorishniy et al., 2021) for better selectively feature interactions.” ([Chen et al., 2023, p. 1](zotero://select/library/items/UKRXZCJB)) ([pdf](zotero://open-pdf/library/items/Q8RGMXPL?page=1&annotation=X3Z257LE)) (pretty intuitive)

<mark style="background: #FFF3A3A6;">“Recent works enhance tabular ML modeling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.” ([Wang and Sun, p. 1](zotero://select/library/items/38EXIFQ9)) ([pdf](zotero://open-pdf/library/items/C9P6BQ9N?page=1&annotation=3CDUAZDS))</mark>

Reason why embeddings could work. <mark style="background: #ABF7F7A6;">“Why are MLPs much more hindered by uninformative features, compared to other models? One answer is that this learner is rotationally invariant in the sense of Ng [2004]: the learning procedure which learns an MLP on a training set and evaluate it on a testing set is unchanged when applying a rotation (unitary matrix) to the features on both the training and testing set. On the contrary, tree-based models are not rotationally invariant, as they attend to each feature separately, and neither are FT Transformers, because of the initial FT Tokenizer, which implements a pointwise operation theoretical link between this concept and uninformative features is provided by Ng [2004], which shows that any rotationallly invariant learning procedure has a worst-case sample complexity that grows at least linearly in the number of irrelevant features. Intuitively, to remove uninformative features, a rotationaly invariant algorithm has to first find the original orientation of the features, and then select the least informative ones: the information contained in the orientation of the data is lost.” ([Grinsztajn et al., 2022, p. 8](zotero://select/library/items/G3KP2Z9W)) ([pdf](zotero://open-pdf/library/items/A3KU4A43?page=8&annotation=W6LGGVAC))

“Our findings shed light on the results of Somepalli et al. [2021] and Gorishniy et al. [2022], which add an embedding layer, even for numerical features, before MLP or Transformer models this layer breaks rotation invariance. The fact that very different types of embedding seem to improve performance suggests that the sheer presence of an embedding which breaks the invariance is a key part of these improvements. We note that a promising avenue for further research would be to find other ways to break rotation invariance which might be less computationally costly than embeddings.” ([Grinsztajn et al., 2022, p. 9](zotero://select/library/items/G3KP2Z9W)) ([pdf](zotero://open-pdf/library/items/A3KU4A43?page=9&annotation=3DIWPNHH))</mark>

<mark style="background: #D2B3FFA6;">“4.3 Embedding Layer Since the feature representations of the categorical features are very sparse and high-dimensional, a common way is to represent them into low-dimensional spaces (e.g., word embeddings). Specifically, we represent each categorical feature with a low-dimensional vector, i.e., ei = Vixi, (2) where Vi is an embedding matrix for field i, and xi is an one-hot vector. Often times categorical features can be multi-valued, i.e., xi is a multi-hot vector. Take movie watching prediction as an example, there could be a feature field Genre which describes the types of a movie and it may be multi-valued (e.g., Drama and Romance for movie “Titanic”). To be compatible with multi-valued inputs, we further modify the Equation 2 and represent the multi-valued feature field as the average of corresponding feature embedding vectors: ei = 1 q Vixi, (3) where q is the number of values that a sample has for i-th field and xi is the multi-hot vector representation for this field. To allow the interaction between categorical and numerical features, we also represent the numerical features in the same lowdimensional feature space. Specifically, we represent the numerical feature as em = vmxm, (4) where vm is an embedding vector for field m, and xm is a scalar value. By doing this, the output of the embedding layer would be a concatenation of multiple embedding vectors, as presented in Figure 2.” ([Song et al., 2019, p. 4](zotero://select/library/items/2PWVWL5T)) ([pdf](zotero://open-pdf/library/items/HBV6667L?page=4&annotation=UVTMKE9G))
</mark>

“Transformer-like architectures have a specific way to handle numerical features of the data. Namely, they map scalar values of numerical features to high-dimensional embedding vectors, which are then mixed by the self-attention modules.” ([Gorishniy et al., 2022, p. 1](zotero://select/library/items/V9AJAB5T)) ([pdf](zotero://open-pdf/library/items/YMZCLKEQ?page=1&annotation=NINXYJZY))

<mark style="background: #FFF3A3A6;">“As another important finding, we demonstrate that the step of embedding the numerical features is universally beneficial for different deep architectures, not only for arXiv:2203.05556v2 [cs.LG] 15 Mar 202” ([Gorishniy et al., 2022, p. 1](zotero://select/library/items/V9AJAB5T)) ([pdf](zotero://open-pdf/library/items/YMZCLKEQ?page=1&annotation=7CKCYKGR))</mark>

<mark style="background: #D2B3FFA6;">“To sum up, our contributions are as follows: 1. We show that embedding schemes for numerical features are an underexplored research question in tabular DL. Namely, we show that more expressive embedding schemes can provide substantial performance improvements over prior models. 2. We demonstrate that the profit from embedding numerical features is not specific for Transformer-like architectures, and proper embedding schemes benefit traditional models as well. 3. On a number of public benchmarks, we achieve the new state-of-the-art of tabular DL.” ([Gorishniy et al., 2022, p. 2](zotero://select/library/items/V9AJAB5T)) ([pdf](zotero://open-pdf/library/items/YMZCLKEQ?page=2&annotation=UI5Y5QHM))</mark>

<mark style="background: #BBFABBA6;">“the existing architectures (Gorishniy et al., 2021; Somepalli et al., 2021; Kossen et al., 2021; Song et al., 2019; Guo et al., 2021) construct embeddings for numerical features using quite restrictive parametric mappings, e.g., linear functions, which can lead to suboptimal performance.” ([Gorishniy et al., 2022, p. 1](zotero://select/library/items/V9AJAB5T)) ([pdf](zotero://open-pdf/library/items/YMZCLKEQ?page=1&annotation=MFFC6S4Z))
</mark>

<mark style="background: #D2B3FFA6;">“We stack these and apply an identical linear embedding to each of n datapoints, obtaining an input representation H(0) ∈ Rn×d×e (Fig. 2b).” ([Kossen et al., 2021, p. 3](zotero://select/library/items/WCUZUJHD)) ([pdf](zotero://open-pdf/library/items/DPPA9MFF?page=3&annotation=VKPT3UA2))</mark>


We are unable to find a seminal work that explicitly documents a theoretical justification for when encoding categorical variables with embedding layers can improve a model’s performance. Therefore, we see an opportunity for future work to provide this theoretical justification. ([[@hancockSurveyCategoricalData2020]])

<mark style="background: #D2B3FFA6;">“Guo and Berkhahn employed an automatic technique to win third place in a 2015 Kaggle competition. The technique is known as entity embedding. Entity embedding is a method to embed categorical variables in real-valued vector spaces [3]. Guo and Berkhan’s success gives us an indication that neural networks are perhaps capable of operating on all sorts of qualitative data, if we can map that data to vectors we can use as input to neural networks effectively. In [3] Guo and Berkhahn do not reveal a new technique. In fact, we find that in the source code for their Kaggle competition entry [35] Guo and Berkhahn use the Keras library Embedding Layer function.” (Hancock and Khoshgoftaar, 2020, p. 27) (? Not sure if they deserve all the credit?) </mark>

<mark style="background: #FF5582A6;">Unlike unstructured data found in nature, structured data with categorical features may not have continuity at all and even if it has it may not be so obvious. The continuous nature of neural networks limits their applicability to categorical variables. Therefore, naively applying neural networks on structured data with integer representation for category variables does not work well. A common way to circumvent this problem is to use onehot encoding, but it has two shortcomings: First when we have many high cardinality features one-hot encoding often results in an unrealistic amount of computational resource requirement. Second, it treats different values of categorical variables completely independent of each other and often ignores the informative relations between them. (Guo Berkhahn)</mark>