
title: SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training
authors: Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, Tom Goldstein
year: 2021
*tags:* #semi-supervised #self-learning #attention #transformer #row-attention
*status:* #ðŸ“¦ 
*related:*
- [[@vaswaniAttentionAllYou2017]]
- [[@huangTabTransformerTabularData2020]]
- [[@arikTabNetAttentiveInterpretable2020]]
- [[@grinsztajnWhyTreebasedModels2022]]
*review:*
- paper was rejected. Comparsion not very rigorous. https://openreview.net/forum?id=nL2lDlsrZU

## Notes 
- Authors propose SAINT architecture that performs attention both over rows and columns and includes an enhanced embedding method.

## Annotations
