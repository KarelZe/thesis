
title: VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain
authors: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela Schaar
year: 2019
*tags:* #deep-learning #gradient_boosting  #semi-supervised #self-supervised
*status:* #üì¶ 
*related:* 
*code:*¬†[https://github.com/jsyoon0823/VIME](https://github.com/jsyoon0823/VIME)

## Notes Sebastian Raschka
-   VIME (Value Imputation and Mask Estimation) includes self- and semi-supervised learning frameworks for tabular data.
-   The authors provide good ablation studies showing that the semi-supervised learning variant of VIME is better than the supervised-only and self-supervised-only variants. The best VIME variant uses both self- and semi-supervised learning and outperforms XGBoost on all datasets.
-   The comparison is based on only five datasets.

## Annotations
‚ÄúDatasets like these present huge opportunities for self- and semi-supervised learning algorithms, which can leverage the unlabeled data to further improve the performance of a predictive model.‚Äù ([Yoon et al., 2020, p. 1](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=1&annotation=5AB6GP26))

‚ÄúUnfortunately, existing self- and semi-supervised learning algorithms are not effective for tabular data1 because they heavily rely on the spatial or semantic structure of image or language data.‚Äù ([Yoon et al., 2020, p. 1](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=1&annotation=IUVCZAIA))

‚ÄúTabular data is a database that is structured in a tabular form. It arranges data elements in vertical columns (features) and horizontal rows (samples)‚Äù ([Yoon et al., 2020, p. 1](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=1&annotation=8MAKL2B9))

‚ÄúStandard semi-supervised learning methods also suffer from the same problem, since the regularizers they use for the predictive model are based on some prior knowledge of these data structures‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=ZLC8Z4AD))

‚ÄúThe notion of rotation simply does not exist in tabular data.‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=KTJJNLDZ))

‚ÄúEven in a setting where all variables are continuous, there is no guarantee that the data manifold is convex and as such taking convex combinations will either generate out-of-distribution samples (therefore degrading model performances) or be restricted to generating samples that are very close to real samples‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=T2Q5TQ9X))

‚ÄúIn this paper, we propose novel self- and semi-supervised learning frameworks for tabular data. For self-supervised learning, we introduce a novel pretext task, mask vector estimation in addition to feature vector estimation.‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=IVMX9FSR))

‚ÄúSelf-supervised learning (Self-SL) frameworks are representation learning methods using unlabeled data. It can be categorized into two types: using pretext task(s) and contrastive learning. Most existing works with pretext tasks are appropriate only for images or natural language:‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=C2T7DTDK))

‚ÄúMost existing works with contrastive learning are also applicable only for image or natural languages due to their data augmentation scheme, and temporal and spatial relationships for defining the similarity‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=8Y8DW8M3))

‚ÄúThere is some existing work on self-supervised learning which can be applied to tabular data. In Denoising auto-encoder [21], the pretext task is to recover the original sample from a corrupted sample. In Context Encoder [22], the pretext task is to reconstruct the original sample from both the corrupted sample and the mask vector. The pretext task for self-supervised learning in TabNet [23] and TaBERT [24] is also recovering corrupted tabular data.‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=RKAIPGEV))

‚Äún this paper, we propose a new pretext task: to recover the mask vector, in addition to the original sample with a novel corrupted sample generation scheme. Also, we propose a novel tabular data augmentation scheme that can be combined with various contrastive learning frameworks to extend the self-supervised learning to tabular domains.‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=VBU77GCJ))

‚Äúemi-supervised learning (Semi-SL) frameworks can be categorized into two types: entropy minimization and consistency regularization. Entropy minimization encourages a classifier to output low entropy predictions on unlabeled data. For instance, [25] constructs hard labels from high-confidence predictions on unlabeled data, and train the network using these pseudo-labels together with labeled data in a supervised way. Consistency regularization encourages some sort of consistency between a sample and some stochastically altered version of itself. Œ†-model [26] uses an L2 loss to encourage consistency between predictions. Mean teacher [27] uses an L2 loss to encourage consistency between the intermediate representations. Virtual Adversarial Training (VAT) [28] encourages prediction consistency by minimizing the maximum difference in predictions between a sample and multiple‚Äù ([Yoon et al., 2020, p. 2](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=2&annotation=64BA76V4))

‚Äúaugmented versions. MixMatch [29] and ReMixMatch [30] combine entropy minimization with consistency regularization in one unified framework with MixUp [10] as the data augmentation method. There is a series of interesting works on graph-based semi-supervised learning [31, 32, 33] which consider a special case of network data where samples are connected by a given edge, i.e. a citation network where an article is connected with its citations. Here, we introduce a novel data augmentation method for general tabular data which can be combined with various semi-supervised learning frameworks to train a predictive model in a semi-supervised way.‚Äù ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=ZV5UDFX9))

‚ÄúWhen only limited labeled samples from pX,Y are available, a predictive model f : X ‚Üí Y solely trained by supervised learning is likely to overfit the training samples since the empirical supervised loss ‚àëNl i=1 l(f (xi), yi ) we minimize deviates significantly from the expected supervised loss E(x,y)‚àºpX,Y [l(f (x), y)], where l(¬∑, ¬∑) is some standard supervised loss function (e.g. cross-entropy).‚Äù ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=659VZW8U))

‚ÄúSelf-supervised learning aims to learn informative representations from unlabeled data.‚Äù ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=PF4EFRKT))

‚ÄúIn this subsection, we focus on self-supervised learning with various self-supervised/pretext tasks for a pretext model to solve. These tasks are set to be challenging but highly relevant to the downstream tasks that we attempt to solve. Ideally, the pretext model will extract some useful information from the raw data in the process of solving the pretext tasks. Then the extracted information can be utilized by the predictive model f in the downstream tasks.‚Äù ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=29PSENNY))

‚ÄúWe define the pretext predictive model as h : Z ‚Üí Ys, which is trained jointly with the encoder function e by minimizing the expected self-supervised loss function lss as follows, min e,h E(xs,ys)‚àºpXs,Ys [ lss (ys, (h ‚ó¶ e)(xs)) ] (1) where pXs,Ys is a pretext distribution that generates pseudo-labeled samples (xs, ys) for training the encoder e and pretext predictive model h.‚Äù ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=CBFTWD54))

‚ÄúSemi-supervised learning optimizes the predictive model f by minimizing the supervised loss function jointly with some unsupervised loss function defined over the output space Y. Formally, semi-supervised learning is formulated as an optimization problem as follows, min f E(x,y)‚àºpXY [ l (y, f (x)) ] + Œ≤ ¬∑ Ex‚àºpX ,x‚Ä≤‚àº ÃÉ pX (x‚Ä≤|x) [ lu (f (x), f (x‚Ä≤)) ] (2) where lu : Y √ó Y ‚Üí R is an unsupervised loss function, and a hyperparameter Œ≤ ‚â• 0 is introduced to control the trade-off between the supervised and unsupervised losses. x‚Ä≤ is a perturbed version of x assumed to be drawn from a conditional distribution ÃÉ pX (x‚Ä≤|x).‚Äù ([Yoon et al., 2020, p. 3](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=3&annotation=6MR29JZ5))

‚ÄúImage and tabular data are very different. The spatial correlations between pixels in images or the sequential correlations between words in text data are well-known and consistent across different datasets. By contrast, the correlation structure among features in tabular data is unknown and varies across different datasets. In other words, there is no ‚Äúcommon‚Äù correlation structure in tabular data (unlike in image and text data). This makes the self- and semi-supervised learning in tabular data more challenging. Note that promising methods for image domain do not guarantee the favorable results on tabular domain (vice versa). Also, most augmentations and pretext tasks used in image data are not applicable to tabular data; because they directly utilize the spatial relationship of the image for augmentation (e.g., rotation) and pretext tasks (e.g., jigsaw puzzle and colorization).‚Äù ([Yoon et al., 2020, p. 9](zotero://select/library/items/XSYUS7JZ)) ([pdf](zotero://open-pdf/library/items/78GQQ36U?page=9&annotation=7PWW4SF4))