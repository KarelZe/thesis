*title:* Electra: Pre-Training Text Encoders as Discriminators Rather Than Generators
*authors:* Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning
*year:* 2019
*tags:* #semi-supervised #self-learning #deep-learning #transformer 
*status:* #üì¶ 
*related:*
- [[@devlinBERTPretrainingDeep2019]]
- [[@yoonVIMEExtendingSuccess2020]]
- [[@vaswaniAttentionAllYou2017]]
*code:*
- https://github.com/google-research/electra
## Notes 
- More efficient than BERT.
- Uses a GAN like network consisting of a generator and descriminiator to destinguish real input tokens from generated syhetic replacements. Instead of masking their method replaces some input with tokens from the proposal distribution. This corrcuption procedure solves the mismatchin BERT where the network during pretraining lerns the (MASK) tokens, which are not present during finetuning (see [[@devlinBERTPretrainingDeep2019]])
- ![[electra-architecture.png]]

## Annotations
‚ÄúMasked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with (MASK) and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out‚Äù ([Clark et al., 2020, p. 1](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=1&annotation=WPHUJVIM))

‚ÄúAs an alternative, we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. Instead of masking, our method corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language model. This corruption procedure solves a mismatch in BERT (although not in XLNet) where the network sees artificial (MASK) tokens during pre-training but not when being fine-tuned on downstream tasks. We then pre-train the network as a discriminator that predicts for every token whether it is an original or a replacement.‚Äù ([Clark et al., 2020, p. 1](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=1&annotation=VLJFDP45))

‚ÄúIn contrast, MLM trains the network as a generator that predicts the original identities of the corrupted tokens.‚Äù ([Clark et al., 2020, p. 1](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=1&annotation=FGKMDU8N))

‚ÄúA key advantage of our discriminative task is that the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient.‚Äù ([Clark et al., 2020, p. 1](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=1&annotation=X54RQXJK))

‚ÄúWe call our approach ELECTRA1 for ‚ÄúEfficiently Learning an Encoder that Classifies Token Replacements Accurately.‚Äù ([Clark et al., 2020, p. 2](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=2&annotation=JIZE2J9K))

‚ÄúAn overview of replaced token detection. The generator can be any model that produces an output distribution over tokens, but we usually use a small masked language model that is trained jointly with the discriminator. Although the models are structured like in a GAN, we train the generator with maximum likelihood rather than adversarially due to the difficulty of applying GANs to text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.‚Äù ([Clark et al., 2020, p. 3](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=3&annotation=W9ACIAUZ))

‚ÄúOur approach trains two neural networks, a generator G and a discriminator D. Each one primarily consists of an encoder (e.g., a Transformer network) that maps a sequence on input tokens x = (x1, ..., xn) into a sequence of contextualized vector representations h(x) = (h1, ..., hn). For a given position t, (in our case only positions where xt = (MASK)), the generator outputs a probability for generating a particular token xt with a softmax layer: pG(xt|x) = exp (e(xt)T hG(x)t )/ ‚àë x‚Ä≤ exp (e(x‚Ä≤)T hG(x)t ) where e denotes token embeddings. For a given position t, the discriminator predicts whether the token xt is ‚Äúreal,‚Äù i.e., that it comes from the data rather than the generator distribution, with a sigmoid output layer: D(x, t) = sigmoid(wT hD(x)t) The generator is trained to perform masked language modeling (MLM). Given an input x = (x1, x2, ..., xn), MLM first select a random set of positions (integers between 1 and n) to mask out m = (m1, ..., mk).3 The tokens in the selected positions are replaced with a (MASK) token: we denote this as xmasked = REPLACE(x, m, (MASK). The generator then learns to predict the original identities of the masked-out tokens. The discriminator is trained to distinguish tokens in the data from tokens that have been replaced by generator samples. More specifically, we create a corrupted example xcorrupt by replacing the masked-out tokens with generator samples and train the discriminator to predict which tokens in xcorrupt match the original input x. Formally, model inputs are constructed according to mi ‚àº unif{1, n} for i = 1 to k xmasked = REPLACE(x, m, (MASK)) ÀÜ xi ‚àº pG(xi|xmasked) for i ‚àà m xcorrupt = REPLACE(x, m, ÀÜ x) and the loss functions are LMLM(x, Œ∏G) = E ( ‚àë i‚ààm ‚àí log pG(xi|xmasked) ) LDisc(x, Œ∏D) = E (n ‚àë t=1 ‚àí1(xcorrupt t = xt) log D(xcorrupt, t) ‚àí 1(xcorrupt t 6= xt) log(1 ‚àí D(xcorrupt, t)) ) Although similar to the training objective of a GAN, there are several key differences. First, if the generator happens to generate the correct token, that token is considered ‚Äúreal‚Äù instead of ‚Äúfake‚Äù; we found this formulation to moderately improve results on downstream tasks. More importantly, the generator is trained with maximum likelihood rather than being trained adversarially to fool the discriminator. Adversarially training the generator is challenging because it is impossible to backpropagate through sampling from the generator. Although we experimented circumventing this issue 3Typically k = d0.15ne, i.e., 15% of the tokens are masked out.‚Äù ([Clark et al., 2020, p. 3](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=3&annotation=YXUTVF8Z))

‚ÄúPublished as a conference paper at ICLR 2020 by using reinforcement learning to train the generator (see Appendix F), this performed worse than maximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input, as is typical with a GAN. We minimize the combined loss min Œ∏G ,Œ∏D ‚àë x‚ààX LMLM(x, Œ∏G) + ŒªLDisc(x, Œ∏D) over a large corpus X of raw text. We approximate the expectations in the losses with a single sample. We don‚Äôt back-propagate the discriminator loss through the generator (indeed, we can‚Äôt because of the sampling step). After pre-training, we throw out the generator and fine-tune the discriminator on downstream tasks.‚Äù ([Clark et al., 2020, p. 4](zotero://select/library/items/GNZMQFS7)) ([pdf](zotero://open-pdf/library/items/PND3FKSH?page=4&annotation=R4IYWK8M))