{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853e9-a97e-4a0b-b1af-1bd0ede09c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f8e26-ad62-4519-a1a2-cd5c942f07c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9361c4b-0819-45a0-b9a4-5089914cd280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n",
    "# fs = gcsfs.GCSFileSystem(project=\"thesis\")\n",
    "# fs_prefix = \"gs://\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a63a56-6aff-4459-9765-1242403443b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_supervised_log_standardized:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7b797-6256-45c0-9bec-d4faf98d9daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.fttransformer import FeatureTokenizer, FTTransformer, Transformer\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical, features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd424255-737f-4590-93ee-e9e6dcfc3258",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2106.11959.pdf\n",
    "\n",
    "Layer count 3\n",
    "Feature embedding size 192\n",
    "Head count 8\n",
    "Activation & FFN size factor (ReGLU,\n",
    "4/3)\n",
    "Attention dropout 0.2\n",
    "FFN dropout 0.1\n",
    "Residual dropout 0.0\n",
    "Initialization Kaiming (He et al., 2015a)\n",
    "Parameter count 929K The value is given for 100 numerical features\n",
    "Optimizer AdamW\n",
    "Learning rate 1e−4\n",
    "Weight decay 1e−5 0.0 for Feature Tokenizer, LayerNorm and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf75ce-b0b4-4198-9be7-f0da4e699f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ache\n",
    "\n",
    "frac = 0.05\n",
    "\n",
    "\n",
    "# sample\n",
    "X_train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\")\n",
    "X_train = X_train.tail(int(len(X_train)*frac))# .sample(frac=frac)# .sample(frac=frac, random_state=42).sort_index()\n",
    "y_train = X_train[\"buy_sell\"]\n",
    "X_train = X_train[features_classical_size]\n",
    "\n",
    "X_val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\").sample(frac=frac)# .sample(frac=frac, random_state=42).sort_index()\n",
    "y_val = X_val[\"buy_sell\"]\n",
    "X_val = X_val[features_classical_size]\n",
    "\n",
    "# eps = 0.1\n",
    "\n",
    "# y_train[np.where(y_train == 0)] = eps\n",
    "# y_train[np.where(y_train == 1)] = 1.0 - eps\n",
    "\n",
    "# y_val[np.where(y_val == 0)] = eps\n",
    "# y_val[np.where(y_val == 1)] = 1.0 - eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61405d57-ae2c-49af-9dce-396f15314e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\")\n",
    "y_test = X_test[\"buy_sell\"]\n",
    "X_test = X_test[features_classical_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0708b-7838-401c-96ee-819ea7ea29ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7ebfd-fd80-43a5-8295-405f07408e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.quantile(q=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0cc35-55b4-4f0f-8c07-906edc6bf260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.clip(lower=X_train.quantile(q=0.01), upper=X_train.quantile(q=0.99), axis=1, inplace=True)\n",
    "X_val.clip(lower=X_val.quantile(q=0.01), upper=X_val.quantile(q=0.99), axis=1, inplace=True)\n",
    "X_test.clip(lower=X_test.quantile(q=0.01), upper=X_test.quantile(q=0.99), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0a9c0-2f42-43d0-b86c-f316aa622b3b",
   "metadata": {},
   "source": [
    "## Test Ground Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff47c4a-ba58-4966-aea7-3538ff32e5f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\")\n",
    "df_test = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee10ea-a416-434d-9bf9-1690c394f90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.reset_index()\n",
    "df_test.columns = df.columns\n",
    "df_tot = pd.concat([df, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6281a8-7a8d-4e9f-8acd-46cb930a636d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f03d4a-7cbd-4d92-b811-cfe5035d1325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df_tot[' <=50K'].str.replace(\".\",\"\")\n",
    "# < = 0 and > = 1\n",
    "y = y.str.contains(\">\") +0\n",
    "#y = np.where(y, -1,1)\n",
    "X = df_tot.drop(columns=[' <=50K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae40a53-e299-40c5-9788-301434b26278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X.iloc[:26048]\n",
    "X_val = X.iloc[26048:32561]\n",
    "X_test = X.iloc[32561:]\n",
    "y_train = y.iloc[:26048]\n",
    "y_val = y.iloc[26048:32561]\n",
    "y_test = y.iloc[32561:]\n",
    "\n",
    "# X_train, X_test, y_train, y_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fe6f2-8c51-4519-91c7-7260c99c05bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = list(X_train.select_dtypes(include=['object']).columns)\n",
    "continuous_features = list(X_train.select_dtypes(include=['float',\"int64\"]).columns)\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('le', OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))])\n",
    "# adapted from groshiny\n",
    "continuous_transformer = Pipeline(steps=[(\"ce\", QuantileTransformer(\n",
    "            output_distribution='normal',\n",
    "            n_quantiles=1000,\n",
    "            subsample=1e9,\n",
    "            random_state=42,\n",
    "))])\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', \n",
    "                                                 categorical_transformer, \n",
    "                                                 categorical_features), (\"cont\", continuous_transformer, continuous_features)])\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_val = pipeline.transform(X_val)\n",
    "X_test = pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00a06c-8166-4c4f-ae52-18ddf43d65ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get feature names and cat feature names\n",
    "feature_names = pipeline.get_feature_names_out()\n",
    "cat_feature_names = [col for col in feature_names if col.startswith(\"cat_\")]\n",
    "cont_feature_names = [col for col in feature_names if col.startswith(\"cont_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02797e8d-7ef7-4f3f-8309-64bd05192411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get cardinalities\n",
    "temp = pd.DataFrame(X_train, columns = feature_names)\n",
    "val_temp = temp[cat_feature_names].nunique() + 2\n",
    "cat_unique_counts = tuple(val_temp.values)\n",
    "\n",
    "cat_idx = [list(feature_names).index(x) for x in cat_feature_names if x in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a82bf5-2794-4d47-ad56-a94ad3eb65bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bee396-8eb2-4304-8126-78b95dd2a00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_val = pd.DataFrame(X_val, columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "X_train[cat_feature_names] = X_train[cat_feature_names].astype(int)\n",
    "X_val[cat_feature_names] = X_val[cat_feature_names].astype(int)\n",
    "X_test[cat_feature_names] = X_test[cat_feature_names].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12284e-0196-4d92-afc2-e263b4b9fc43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "clf = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    #learning_rate=0.1, \n",
    "    # loss_function='CrossEntropy',\n",
    "    task_type=\"GPU\",\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, \n",
    "        cat_features=cat_idx, \n",
    "        eval_set=(X_val, y_val), \n",
    ")\n",
    "\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288b449-15dd-4cc4-9616-2cd19451e6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/Yura52/rtdl/blob/main/examples/rtdl.ipynb#scrollTo=RtYkwZjE4mEx\n",
    "import rtdl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda')\n",
    "task_type = 'binclass'\n",
    "d_out = 1\n",
    "epochs = 1000\n",
    "\n",
    "clf = rtdl.FTTransformer.make_default(\n",
    "    n_num_features= len(cont_feature_names),\n",
    "    cat_cardinalities=list(cat_unique_counts),\n",
    "    last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "    d_out=d_out,\n",
    ")\n",
    "\n",
    "clf.to(device)\n",
    "optimizer = (\n",
    "    clf.make_default_optimizer()\n",
    "    if isinstance(clf, rtdl.FTTransformer)\n",
    "    else torch.optim.AdamW(clf.parameters(), lr=lr, weight_decay=weight_decay)\n",
    ")\n",
    "criterion = (\n",
    "    F.binary_cross_entropy_with_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae754f11-5ddf-42b1-92d7-9ba0d594d95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": 256,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    torch.Tensor(X_train[cat_feature_names].values).long(), torch.Tensor(X_train[cont_feature_names].values),None, torch.Tensor(y_train.values), **dl_params\n",
    ")\n",
    "\n",
    "val_loader = TabDataLoader(\n",
    "    torch.Tensor(X_val[cat_feature_names].values).long(), torch.Tensor(X_val[cont_feature_names].values),None, torch.Tensor(y_val.values), **dl_params\n",
    ")\n",
    "test_loader = TabDataLoader(\n",
    "    torch.Tensor(X_test[cat_feature_names].values).long(), torch.Tensor(X_test[cont_feature_names].values),None, torch.Tensor(y_test.values), **dl_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e552ddd-f088-44ac-987f-7814051ae379",
   "metadata": {},
   "source": [
    "## Carlifornia Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4fd731-5859-4e70-b12a-515bddbec51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from https://colab.research.google.com/github/Yura52/rtdl/blob/main/examples/rtdl.ipynb#scrollTo=3bzc8TEGEvmh\n",
    "# !!! NOTE !!! The dataset splits, preprocessing and other details are\n",
    "# significantly different from those used in the\n",
    "# paper \"Revisiting Deep Learning Models for Tabular Data\",\n",
    "# so the results will be different from the reported in the paper.\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "task_type = 'regression'\n",
    "device = \"cuda\"\n",
    "# dataset = sklearn.datasets.fetch_covtype()\n",
    "# task_type = 'multiclass'\n",
    "\n",
    "assert task_type in ['binclass', 'multiclass', 'regression']\n",
    "\n",
    "X_all = dataset['data'].astype('float32')\n",
    "y_all = dataset['target'].astype('float32' if task_type == 'regression' else 'int64')\n",
    "if task_type != 'regression':\n",
    "    y_all = sklearn.preprocessing.LabelEncoder().fit_transform(y_all).astype('int64')\n",
    "n_classes = int(max(y_all)) + 1 if task_type == 'multiclass' else None\n",
    "\n",
    "X = {}\n",
    "y = {}\n",
    "X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n",
    "    X_all, y_all, train_size=0.8\n",
    ")\n",
    "X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n",
    "    X['train'], y['train'], train_size=0.8\n",
    ")\n",
    "\n",
    "# not the best way to preprocess features, but enough for the demonstration\n",
    "preprocess = sklearn.preprocessing.StandardScaler().fit(X['train'])\n",
    "X = {\n",
    "    k: torch.tensor(preprocess.transform(v), device=device)\n",
    "    for k, v in X.items()\n",
    "}\n",
    "y = {k: torch.tensor(v, device=device) for k, v in y.items()}\n",
    "\n",
    "# !!! CRUCIAL for neural networks when solving regression problems !!!\n",
    "y_mean = y['train'].mean().item()\n",
    "y_std = y['train'].std().item()\n",
    "y = {k: (v - y_mean) / y_std for k, v in y.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e798428-c1f8-42d1-8f22-3b8fe854d17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_params = {\n",
    "    \"batch_size\": 256,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    None, torch.Tensor(X[\"train\"]),None, torch.Tensor(y[\"train\"]), **dl_params\n",
    ")\n",
    "\n",
    "val_loader = TabDataLoader(\n",
    "    None, torch.Tensor(X[\"val\"]),None, torch.Tensor(y[\"val\"]), **dl_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61894937-00aa-442d-a29b-ec55d4d4336a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rtdl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "d_out = 1\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "model = rtdl.FTTransformer.make_default(\n",
    "    n_num_features=X_all.shape[1],\n",
    "    cat_cardinalities=None,\n",
    "    last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "    d_out=d_out,\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "optimizer = (\n",
    "    model.make_default_optimizer()\n",
    "    if isinstance(model, rtdl.FTTransformer)\n",
    "    else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    ")\n",
    "criterion = (\n",
    "    F.binary_cross_entropy_with_logits\n",
    "    if task_type == 'binclass'\n",
    "    else F.cross_entropy\n",
    "    if task_type == 'multiclass'\n",
    "    else F.mse_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cabe5-50b4-42ca-997a-a685739ed097",
   "metadata": {},
   "source": [
    "## Run Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1535c-438d-4db7-aee6-b0f1bbf39d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frac=1\n",
    "\n",
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "epochs = 100\n",
    "\n",
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "\n",
    "\n",
    "# clipping_value = 5\n",
    "reduction = \"mean\"\n",
    "\n",
    "# other_kwargs = {\n",
    "#  \"clipping_value\": clipping_value,\n",
    "#  \"frac\": frac,\n",
    "# }\n",
    "\n",
    "# feature_tokenizer_kwargs = {\n",
    "#     \"num_continous\": len(continuous_features),\n",
    "#     \"cat_cardinalities\": list(cat_unique_counts),\n",
    "#     \"d_token\": d_token,\n",
    "# }\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "transformer_kwargs = {\n",
    "    \"d_token\": d_token,\n",
    "    \"n_blocks\": n_blocks,\n",
    "    \"attention_n_heads\": attention_heads,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": ffn_dropout,\n",
    "    # fix at 4/3, as activation (see search space B in\n",
    "    # https://arxiv.org/pdf/2106.11959v2.pdf)\n",
    "    # is static with ReGLU / GeGLU\n",
    "    \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "    \"attention_dropout\": attention_dropout,\n",
    "    \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.GELU, # nn.ReLU\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,  # fix at 1, due to binary classification\n",
    "}\n",
    "\n",
    "\n",
    "optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "module_params = {\n",
    "    \"transformer\": Transformer(**transformer_kwargs),  # type: ignore\n",
    "    \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),  # type: ignore # noqa: E501\n",
    "    \"cat_features\": cat_idx,\n",
    "    \"cat_cardinalities\": cat_unique_counts,\n",
    "}\n",
    "\n",
    "clf = FTTransformer(**module_params)\n",
    "# use multiple gpus, if available\n",
    "clf = nn.DataParallel(clf).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# wandb.log(other_kwargs)\n",
    "# wandb.log(transformer_kwargs)\n",
    "# wandb.log(optim_params)\n",
    "# wandb.log(feature_tokenizer_kwargs)\n",
    "# wandb.log(dl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c271935-f54d-4b6d-a51c-423c5d5d2818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "val_data = TabDataset(X_val, y_val)\n",
    "test_data = TabDataset(X_test, y_test)\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    training_data.x_cat,\n",
    "    training_data.x_cont,\n",
    "    training_data.weight,\n",
    "    training_data.y,\n",
    "    **dl_params\n",
    ")\n",
    "val_loader = TabDataLoader(\n",
    "    val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    ")\n",
    "\n",
    "test_loader = TabDataLoader(\n",
    "    test_data.x_cat, test_data.x_cont, test_data.weight, test_data.y, **dl_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574866f-25a0-4535-8462-58464b699a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c2ccf-8984-4ce1-ae41-f1dcb0ae3d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/latest/notebooks/course_UvA-DL/05-transformers-and-MH-attention.html?highlight=warmup\n",
    "# Needed for initializing the lr scheduler\n",
    "p = nn.Parameter(torch.empty(4, 4))\n",
    "\n",
    "# clf = FTTransformer(**module_params)\n",
    "# clf = model\n",
    "# use multiple gpus, if available\n",
    "# clf = nn.DataParallel(clf).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "max_iters = epochs * len(train_loader)\n",
    "# saw recommendation of 5 - 10 % of total training budget or 100 to 500 steps\n",
    "warmup = int(0.05 * max_iters)\n",
    "print(f\"warmup steps: {warmup}\")\n",
    "print(max_iters)\n",
    "\n",
    "scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=warmup, max_iters=max_iters)\n",
    "\n",
    "# Plotting\n",
    "epochs_plt = list(range(max_iters))\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(epochs_plt, [scheduler.get_lr_factor(e) for e in epochs_plt])\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99743844-ba1e-4a9c-8650-8ce14ca48385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    \n",
    "    # remove old files\n",
    "    for filename in glob.glob(f\"checkpoints/{run.id}*\"):\n",
    "        os.remove(filename) \n",
    "    \n",
    "    # create_dir\n",
    "    dir_checkpoints = \"checkpoints/\n",
    "    os.makedirs(dir_checkpoints, exist_ok = True) \n",
    "    \n",
    "    # save new file\n",
    "    print(\"saving new checkpoints.\")\n",
    "    torch.save(model.state_dict(), os.path.join(path,f\"{run.id}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c959b-29fa-4329-a1d5-c05bea812dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# Generate the optimizers\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "# compiled_clf = clf #torch.compile(clf)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "# see https://stackoverflow.com/a/53628783/5755604\n",
    "# no sigmoid required; numerically more stable\n",
    "# do not reduce, calculate mean after multiplication with weight\n",
    "\n",
    "step = 0\n",
    "best_accuracy = -1\n",
    "best_step = -1\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    batch = 0\n",
    "    \n",
    "    for x_cat, x_cont, weights, targets in train_loader:\n",
    "    \n",
    "        clf.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for rtd implementation\n",
    "        # logits = clf(x_cont,x_cat).flatten() #\n",
    "        # for my implementation\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            train_loss = criterion(logits, targets)\n",
    "\n",
    "        # train_loss.backward()\n",
    "        # optimizer.step()\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss_in_epoch_train += train_loss  # .item()\n",
    "        wandb.log({\"train_loss_step\": train_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "        batch += 1\n",
    "        step +=1\n",
    "\n",
    "    clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, weights, targets in val_loader:\n",
    "            # for rtd implementation\n",
    "            # logits = clf(x_cont,x_cat).flatten() #\n",
    "            # for my implementation\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            logits = logits.flatten()\n",
    "\n",
    "            val_loss = criterion(logits, targets)\n",
    "            \n",
    "            # get probabilities and round to nearest integer\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "            loss_in_epoch_val += val_loss  # val_loss #.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "            batch +=1      \n",
    "\n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "    \n",
    "    # correct samples / no samples\n",
    "    val_accuracy = correct / len(X_val)\n",
    "    if best_accuracy < val_accuracy:\n",
    "        checkpoint(clf, f\"checkpoints/{run.id}-{step}.ptx\")\n",
    "        best_accuracy = val_accuracy\n",
    "        best_step = step\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, 'epoch': epoch})\n",
    "    wandb.log({\"val_loss\": val_loss, 'epoch': epoch})\n",
    "    # wandb.log({\"val_accuracy\": val_accuracy, 'epoch': epoch})    \n",
    "    \n",
    "    print(f\"train:{train_loss} val:{val_loss}\")\n",
    "    print(f\"val accuracy:{val_accuracy}\")\n",
    "\n",
    "    # return early if val accuracy doesn't improve. Minus to minimize.\n",
    "    early_stopping(-val_accuracy)\n",
    "    if early_stopping.early_stop or math.isnan(train_loss) or math.isnan(val_loss):\n",
    "        print(\"meh... early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca46d5-7448-4e63-8d93-ff3773fe5753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp =  glob.glob(f\"checkpoints/{run.id}*\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca09c3b-6598-4a6f-be55-ce0cf7de61d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(cp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73fd9d-9260-4866-9cb5-0928dcece535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred, y_true = [], []\n",
    "\n",
    "for x_cat, x_cont, weights, targets in test_loader:\n",
    "    # logits = clf(x_cont,x_cat).flatten() #\n",
    "    # for my implementation\n",
    "    logits = clf(x_cat, x_cont).flatten()\n",
    "    logits = logits.flatten()\n",
    "\n",
    "\n",
    "    # map between zero and one, sigmoid is otherwise included in loss already\n",
    "    # https://stackoverflow.com/a/66910866/5755604\n",
    "    preds = torch.sigmoid(logits.squeeze())\n",
    "    y_pred.append(preds.detach().cpu().numpy())\n",
    "    y_true.append(targets.detach().cpu().numpy())  # type: ignore\n",
    "\n",
    "# round prediction to nearest int\n",
    "y_pred = np.rint(np.concatenate(y_pred))\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "acc = accuracy_score(y_pred, y_true)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "thesis2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
