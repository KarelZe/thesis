{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853e9-a97e-4a0b-b1af-1bd0ede09c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9361c4b-0819-45a0-b9a4-5089914cd280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n",
    "# fs = gcsfs.GCSFileSystem(project=\"thesis\")\n",
    "# fs_prefix = \"gs://\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a63a56-6aff-4459-9765-1242403443b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_supervised_log_standardized:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7b797-6256-45c0-9bec-d4faf98d9daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.fttransformer import FeatureTokenizer, FTTransformer, Transformer\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical, features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd424255-737f-4590-93ee-e9e6dcfc3258",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2106.11959.pdf\n",
    "\n",
    "Layer count 3\n",
    "Feature embedding size 192\n",
    "Head count 8\n",
    "Activation & FFN size factor (ReGLU,\n",
    "4/3)\n",
    "Attention dropout 0.2\n",
    "FFN dropout 0.1\n",
    "Residual dropout 0.0\n",
    "Initialization Kaiming (He et al., 2015a)\n",
    "Parameter count 929K The value is given for 100 numerical features\n",
    "Optimizer AdamW\n",
    "Learning rate 1e−4\n",
    "Weight decay 1e−5 0.0 for Feature Tokenizer, LayerNorm and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf75ce-b0b4-4198-9be7-f0da4e699f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ache\n",
    "\n",
    "frac = 1.0\n",
    "\n",
    "# sample\n",
    "X_train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\").sample(frac=frac)# .sample(frac=frac, random_state=42).sort_index()\n",
    "y_train = X_train[\"buy_sell\"]\n",
    "X_train = X_train[features_classical_size]\n",
    "\n",
    "X_val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\").sample(frac=frac)# .sample(frac=frac, random_state=42).sort_index()\n",
    "y_val = X_val[\"buy_sell\"]\n",
    "X_val = X_val[features_classical_size]\n",
    "\n",
    "# eps = 0.1\n",
    "\n",
    "# y_train[np.where(y_train == 0)] = eps\n",
    "# y_train[np.where(y_train == 1)] = 1.0 - eps\n",
    "\n",
    "# y_val[np.where(y_val == 0)] = eps\n",
    "# y_val[np.where(y_val == 1)] = 1.0 - eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74302042-fe5a-4d75-849e-e1f93f6e9d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61405d57-ae2c-49af-9dce-396f15314e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\")\n",
    "y_test = X_test[\"buy_sell\"]\n",
    "X_test = X_test[features_classical_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7d45b-7390-4b6f-8a3b-e6bef4e9deba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff3d25-37f3-4141-be7b-447ac067f689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0cc35-55b4-4f0f-8c07-906edc6bf260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.clip(lower=X_train.quantile(q=0.01), upper=X_train.quantile(q=0.99), axis=1, inplace=True)\n",
    "X_val.clip(lower=X_val.quantile(q=0.01), upper=X_val.quantile(q=0.99), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af21365-b9c9-49b3-b4b1-49a86796640b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70e02f-e033-4e4a-a1e8-43156306e86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8878c6-6340-40ad-819f-f4ec3b8d575b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "val_data = TabDataset(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1535c-438d-4db7-aee6-b0f1bbf39d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "epochs = 10 # 50\n",
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "\n",
    "clipping_value = 5\n",
    "reduction = \"mean\"\n",
    "\n",
    "other_kwargs = {\n",
    " \"clipping_value\": clipping_value,\n",
    " \"frac\": frac,\n",
    "}\n",
    "\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "transformer_kwargs = {\n",
    "    \"d_token\": d_token,\n",
    "    \"n_blocks\": n_blocks,\n",
    "    \"attention_n_heads\": attention_heads,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": ffn_dropout,\n",
    "    # fix at 4/3, as activation (see search space B in\n",
    "    # https://arxiv.org/pdf/2106.11959v2.pdf)\n",
    "    # is static with ReGLU / GeGLU\n",
    "    \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "    \"attention_dropout\": attention_dropout,\n",
    "    \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.GELU, # nn.ReLU\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,  # fix at 1, due to binary classification\n",
    "}\n",
    "\n",
    "\n",
    "# module_params = {\n",
    "#             \"transformer\": Transformer(**transformer_kwargs),  # type: ignore\n",
    "#             \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),  # type: ignore # noqa: E501\n",
    "#             \"cat_features\": self._cat_features,\n",
    "#             \"cat_cardinalities\": self._cat_cardinalities,\n",
    "#         }\n",
    "\n",
    "optim_params = {\"lr\": 3e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "module_params = {\n",
    "    \"transformer\": Transformer(**transformer_kwargs),  # type: ignore\n",
    "    \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),  # type: ignore # noqa: E501\n",
    "    \"cat_features\": None,\n",
    "    \"cat_cardinalities\": [],\n",
    "}\n",
    "\n",
    "wandb.log(other_kwargs)\n",
    "wandb.log(transformer_kwargs)\n",
    "wandb.log(optim_params)\n",
    "wandb.log(feature_tokenizer_kwargs)\n",
    "wandb.log(dl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338aad0-df27-45df-ac41-f605ed3722db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = TabDataLoader(\n",
    "    training_data.x_cat,\n",
    "    training_data.x_cont,\n",
    "    training_data.weight,\n",
    "    training_data.y,\n",
    "    **dl_params\n",
    ")\n",
    "val_loader = TabDataLoader(\n",
    "    val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    ")\n",
    "test_data = TabDataset(X_test, y_test)\n",
    "test_loader = TabDataLoader(\n",
    "    test_data.x_cat, test_data.x_cont, test_data.weight, test_data.y, **dl_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574866f-25a0-4535-8462-58464b699a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bf560-44e6-40f4-83f6-7cf1cb75fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37\n",
    "# https://github.com/huggingface/transformers/blob/v4.27.2/src/transformers/optimization.py#L220\n",
    "\n",
    "#   \"\"\"Creates an optimizer training op.\"\"\"\n",
    "#   global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "#   learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
    "\n",
    "#   # Implements linear decay of the learning rate.\n",
    "#   learning_rate = tf.train.polynomial_decay(\n",
    "#       learning_rate,\n",
    "#       global_step,\n",
    "#       num_train_steps,\n",
    "#       end_learning_rate=0.0,\n",
    "#       power=1.0,\n",
    "#       cycle=False)\n",
    "\n",
    "#   # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
    "#   # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "#   if num_warmup_steps:\n",
    "#     global_steps_int = tf.cast(global_step, tf.int32)\n",
    "#     warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "\n",
    "#     global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "#     warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "#     warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "#     warmup_learning_rate = init_lr * warmup_percent_done\n",
    "\n",
    "#     is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "#     learning_rate = (\n",
    "#         (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c2ccf-8984-4ce1-ae41-f1dcb0ae3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/latest/notebooks/course_UvA-DL/05-transformers-and-MH-attention.html?highlight=warmup\n",
    "# Needed for initializing the lr scheduler\n",
    "# p = nn.Parameter(torch.empty(4, 4))\n",
    "\n",
    "clf = FTTransformer(**module_params)\n",
    "\n",
    "# use multiple gpus, if available\n",
    "clf = nn.DataParallel(clf).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "max_iters = epochs * len(train_loader)\n",
    "# saw recommendation of 5 - 10 % of total training budget or 100 to 500 steps\n",
    "warmup = int(0.05 * max_iters)\n",
    "print(f\"warmup steps: {warmup}\")\n",
    "print(max_iters)\n",
    "\n",
    "scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=warmup, max_iters=max_iters)\n",
    "\n",
    "# Plotting\n",
    "epochs_plt = list(range(max_iters))\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(epochs_plt, [scheduler.get_lr_factor(e) for e in epochs_plt])\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c959b-29fa-4329-a1d5-c05bea812dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# Generate the optimizers\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "compiled_clf = torch.compile(clf)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "# see https://stackoverflow.com/a/53628783/5755604\n",
    "# no sigmoid required; numerically more stable\n",
    "# do not reduce, calculate mean after multiplication with weight\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    compiled_clf.train()\n",
    "\n",
    "    batch = 0\n",
    "    \n",
    "    for x_cat, x_cont, weights, targets in train_loader:\n",
    "\n",
    "        # print(x_cat)\n",
    "        # print(x_cont)\n",
    "        # print(weights)\n",
    "        # reset the gradients back to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute the model output and train loss\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = compiled_clf(x_cat, x_cont).flatten()\n",
    "            # print(logits)\n",
    "            train_loss = criterion(logits, targets)\n",
    "            # intermediate_loss = criterion(logits, targets)\n",
    "            # print(intermediate_loss)\n",
    "            # weight train loss with (decaying) weights\n",
    "            # train_loss = torch.mean(weights * intermediate_loss)\n",
    "            # compute accumulated gradients\n",
    "            \n",
    "            # https://pytorch.org/docs/stable/amp.html\n",
    "            # https://discuss.huggingface.co/t/why-is-grad-norm-clipping-done-during-training-by-default/1866\n",
    "            \n",
    "            # scaler.scale(train_loss).backward()\n",
    "            scaler.scale(train_loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(compiled_clf.parameters(), 5, error_if_nonfinite=True)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # apply lr scheduler per step (tot steps = no. batches * epochs)\n",
    "            scheduler.step()\n",
    "            \n",
    "            # scaler.unscale_(optimizer)\n",
    "            # nn.utils.clip_grad_norm_(compiled.parameters(), 5)\n",
    "            # scaler.scale(train_loss).backward()\n",
    "\n",
    "#             # perform parameter update based on current gradients\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "\n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            loss_in_epoch_train += train_loss  # .item()\n",
    "            wandb.log({\"train_loss_step\": train_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "            batch += 1\n",
    "\n",
    "    compiled_clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "    batch = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, weights, targets in val_loader:\n",
    "            logits = clf(x_cat, x_cont)\n",
    "            logits = logits.flatten()\n",
    "\n",
    "            # get probabilities and round to nearest integer\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "            # loss calculation.\n",
    "            # Criterion contains softmax already.\n",
    "            # Weight sample loss with (equal) weights\n",
    "            val_loss = criterion(logits, targets)\n",
    "            \n",
    "            # intermediate_loss = criterion(preds, targets)\n",
    "            # val_loss = torch.mean(weights * intermediate_loss)\n",
    "            \n",
    "            loss_in_epoch_val += val_loss  # val_loss #.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "            batch +=1\n",
    "            \n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "    \n",
    "    \n",
    "    # # update lr\n",
    "    # scheduler.step(val_loss)\n",
    "    \n",
    "    # correct samples / no samples\n",
    "    val_accuracy = correct / len(X_val)\n",
    "\n",
    "    wandb.log({\"train_loss\": train_loss, 'epoch': epoch})\n",
    "    wandb.log({\"val_loss\": val_loss, 'epoch': epoch})\n",
    "    wandb.log({\"val_accuracy\": val_accuracy, 'epoch': epoch})    \n",
    "    \n",
    "    print(f\"train:{train_loss} val:{val_loss} val acc: {val_accuracy}\")\n",
    "\n",
    "    # return early if val accuracy doesn't improve. Minus to minimize.\n",
    "    early_stopping(-val_accuracy)\n",
    "    if early_stopping.early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcd4ed-b7b3-4f08-875c-964b5c32b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1723a2f-2ce8-49c8-99d0-f2c1a0f027cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer count 3\n",
    "# Feature embedding size 192\n",
    "# Head count 8\n",
    "# Activation & FFN size factor (ReGLU,\n",
    "# 4/3)\n",
    "# Attention dropout 0.2\n",
    "# FFN dropout 0.1\n",
    "# Residual dropout 0.0\n",
    "# Initialization Kaiming (He et al., 2015a)\n",
    "# Parameter count 929K The value is given for 100 numerical features\n",
    "# Optimizer AdamW\n",
    "# Learning rate 1e−4\n",
    "# Weight decay 1e−5 0.0 for Feature Tokenizer, LayerNorm and biases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://wandb.ai/craiyon/report/reports/Recipe-Training-Large-Models--VmlldzozNjc4MzQz#your-model-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832522b0-9119-47e7-95eb-664e6f25aa81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params = clf.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8cfe0-c6fc-46dc-8606-d6deb1e9dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:nan val:0.5720667839050293 val acc: 0.7240031123338945\n",
    "# train:nan val:0.5636194348335266 val acc: 0.7252551656097276\n",
    "# train:nan val:0.5604064464569092 val acc: 0.7272873365643292\n",
    "# train:nan val:0.5546848773956299 val acc: 0.7298300930140309\n",
    "# train:nan val:0.5749767422676086 val acc: 0.7012596815451823"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f447b-d59b-481f-b073-d590dae40ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class label_smooth_loss(torch.nn.Module):\n",
    "#     def __init__(self, num_classes, smoothing=0.1):\n",
    "#         super(label_smooth_loss, self).__init__()\n",
    "#         eps = smoothing / num_classes\n",
    "#         self.negative = eps\n",
    "#         self.positive = (1 - smoothing) + eps\n",
    "    \n",
    "#     def forward(self, pred, target):\n",
    "#         pred = pred.log_softmax(dim=1)\n",
    "#         true_dist = torch.zeros_like(pred)\n",
    "#         true_dist.fill_(self.negative)\n",
    "#         true_dist.scatter_(1, target.data.unsqueeze(1), self.positive)\n",
    "#         return torch.sum(-true_dist * pred, dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97f40d-9d4e-4018-98f1-5fc87e8e9f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred, y_true = [], []\n",
    "\n",
    "for x_cat, x_cont, weights, targets in test_loader:\n",
    "    logits = clf(x_cat, x_cont)\n",
    "\n",
    "    # map between zero and one, sigmoid is otherwise included in loss already\n",
    "    # https://stackoverflow.com/a/66910866/5755604\n",
    "    preds = torch.sigmoid(logits.squeeze())\n",
    "    y_pred.append(preds.detach().cpu().numpy())\n",
    "    y_true.append(targets.detach().cpu().numpy())  # type: ignore\n",
    "\n",
    "print(len(y_pred))\n",
    "print(len(y_true))\n",
    "\n",
    "# round prediction to nearest int\n",
    "y_pred = np.rint(np.concatenate(y_pred))\n",
    "y_pred[y_pred == 0] = -1\n",
    "y_true = np.concatenate(y_true)\n",
    "y_true[y_true == 0] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4e8c4-16f4-4a5f-9151-8d904cac165b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa74e5-fdb3-4e3b-a0f2-5fa998ddde06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef71391-1e86-4f17-ae6b-4fc51bba6c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_pred, y_true)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe09899-2aa5-42c2-8736-a514269ec8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtwo",
   "language": "python",
   "name": "torchtwo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
