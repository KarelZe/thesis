{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    Transformer,\n",
    ")\n",
    "\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical, features_classical_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_unsupervised_log_standardized_clipped:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ache\n",
    "frac = 0.05\n",
    "\n",
    "X_train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\")\n",
    "\n",
    "X_train = X_train.sample(frac=frac, random_state=42)\n",
    "y_train = X_train[\"buy_sell\"]\n",
    "X_train = X_train[features_classical_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 0\n",
    "training_data = TabDataset(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:ðŸ’¡\n",
    "- pass pre-training command to objective âœ…\n",
    "- filter unlabelled trades in fit âœ…\n",
    "- add pretraining method\n",
    "- save pre-trained model to checkpoint\n",
    "- make sure it works with finetuning\n",
    "- refactor Transformer head to TargetHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSHead(nn.Module):\n",
    "    \"\"\"\n",
    "    2 Layer MLP projection head\n",
    "    \"\"\"\n",
    "    # d_in is last dim of transformer output torch.Size([4, 17, 96]) -> 96 (ok)\n",
    "    # d_out -> last dim of output here [4,17,1] -> [4, 16] (ok)\n",
    "    def __init__(self, *, d_in: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(d_in, d_hidden)\n",
    "        self.out = nn.Linear(d_hidden, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        x = self.out(F.relu(self.first(x))).squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSHead(\n",
      "  (first): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "head = CLSHead(d_in=768, d_hidden=768)\n",
    "\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShufflePermutations(object):\n",
    "    \"\"\"\n",
    "    Generate permutations by shuffeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_num, X_cat):\n",
    "        self.X_num = X_num\n",
    "        self.X_cat = X_cat\n",
    "\n",
    "    def permute(self, X):\n",
    "        \"\"\"\n",
    "        generate random index\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            return None\n",
    "\n",
    "        # generate random index array\n",
    "        return torch.randint_like(X, X.shape[0], dtype=torch.long)\n",
    "\n",
    "    def gen_permutations(self):\n",
    "        # permute numerical and categorical by random index\n",
    "        X_num = self.X_num\n",
    "        X_cat = self.X_cat if self.X_cat is not None else None\n",
    "        return self.permute(X_num), self.permute(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 0, 3, 2, 1, 2, 2, 3],\n",
      "        [0, 2, 0, 3, 1, 0, 2, 2],\n",
      "        [2, 2, 1, 1, 0, 2, 1, 2],\n",
      "        [2, 0, 1, 3, 0, 2, 0, 2]])\n",
      "tensor([[2, 2, 3, 2, 2, 1, 0, 2],\n",
      "        [2, 2, 3, 0, 1, 2, 3, 0],\n",
      "        [3, 2, 1, 2, 0, 1, 2, 3],\n",
      "        [0, 0, 2, 2, 0, 2, 3, 2]])\n"
     ]
    }
   ],
   "source": [
    "d_num, d_cat = 8,8\n",
    "batch_size = 4\n",
    "\n",
    "X_num = torch.randn(batch_size, d_num)\n",
    "X_cat = torch.randint(0, 10, (batch_size, d_cat))\n",
    "\n",
    "perm_class = ShufflePermutations(X_num, X_cat)\n",
    "x_num_perm, x_cat_perm = perm_class.gen_permutations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 3, 2, 2, 1, 0, 2],\n",
       "        [2, 2, 3, 0, 1, 2, 3, 0],\n",
       "        [3, 2, 1, 2, 0, 1, 2, 3],\n",
       "        [0, 0, 2, 2, 0, 2, 3, 2]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_probability = 0.15\n",
    "\n",
    "def gen_masks(X, perm):\n",
    "    # generate binary masks\n",
    "    masks = torch.empty_like(X).bernoulli(p=corrupt_probability).bool()\n",
    "    new_masks = masks & (X != X[perm, torch.arange(X.shape[1], device=X.device)])\n",
    "    return new_masks\n",
    "\n",
    "# FIXME: probably generate for train and val set\n",
    "x_num_mask = gen_masks(X_num, x_num_perm)\n",
    "x_cat_mask = gen_masks(X_cat, x_cat_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0082, -2.0447, -1.8514, -0.3190,  0.7799,  2.2147, -0.2914, -0.4975],\n",
       "        [ 0.6028,  1.1816,  1.0263,  1.2949,  0.4654,  0.1010,  0.6004, -1.8047],\n",
       "        [-0.1933,  2.0371,  0.3731,  0.9525,  0.5256, -1.1464, -0.4436,  1.2238],\n",
       "        [-0.3117, -0.8931,  2.6008, -0.0808,  0.2362, -0.6265, -0.5757,  0.2028]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values at permuted places\n",
    "x_num_permuted = torch.gather(X_num, 0, x_num_perm)\n",
    "\n",
    "# replace at mask = True\n",
    "X_num[x_num_mask] = x_num_permuted[x_num_mask]\n",
    "\n",
    "if X_cat is not None:\n",
    "\n",
    "    # along the 0 axis get elements based on perm_cat\n",
    "    x_cat_permuted = torch.gather(X_cat, 0, x_cat_perm)\n",
    "    \n",
    "    # replace at mask\n",
    "    X_cat[x_cat_mask] = x_cat_permuted[x_cat_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_num_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0082, -2.0447, -1.8514, -0.3190,  0.7799,  2.2147, -0.2914,  0.2028],\n",
       "        [ 0.6028,  1.1816,  1.0263,  1.2949,  0.4654,  0.1010,  0.6004, -1.8047],\n",
       "        [-0.1933,  2.0371,  0.3731,  0.9525,  0.5256, -1.1464, -0.4436,  1.2238],\n",
       "        [-0.3117, -0.8931,  2.6008, -0.0808,  0.2362, -0.6265, -0.5757,  1.2238]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_token = 96\n",
    "\n",
    "params_feature_tokenizer = {\n",
    "            \"num_continous\": d_num,\n",
    "            \"cat_cardinalities\": [11] * d_cat,\n",
    "            \"d_token\": d_token,\n",
    "        }\n",
    "\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "\n",
    "params_transformer = {\n",
    "            \"d_token\": d_token,\n",
    "            \"n_blocks\": 3,\n",
    "            \"attention_n_heads\": 8,\n",
    "            \"attention_initialization\": \"kaiming\",\n",
    "            \"ffn_activation\": ReGLU,\n",
    "            \"attention_normalization\": nn.LayerNorm,\n",
    "            \"ffn_normalization\": nn.LayerNorm,\n",
    "            \"ffn_dropout\": 0.1,\n",
    "            \"ffn_d_hidden\": 96 * 2,\n",
    "            \"attention_dropout\": 0.1,\n",
    "            \"residual_dropout\": 0.1,\n",
    "            \"prenormalization\": True,\n",
    "            \"first_prenormalization\": False,\n",
    "            \"last_layer_query_idx\": None,\n",
    "            \"n_tokens\": None,\n",
    "            \"kv_compression_ratio\": None,\n",
    "            \"kv_compression_sharing\": None,\n",
    "            \"head_activation\": nn.ReLU,\n",
    "            \"head_normalization\": nn.LayerNorm,\n",
    "            \"d_out\": 1,\n",
    "        }\n",
    "\n",
    "transformer = Transformer(**params_transformer)\n",
    "\n",
    "target_head = transformer.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hidden = 32\n",
    "\n",
    "class PretrainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = CLSToken(d_token, \"uniform\")\n",
    "\n",
    "        self.feature_tokenizer = feature_tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # disable BERT-like classification head and replace with idenity mapping\n",
    "        self.transformer.head = nn.Identity()\n",
    "\n",
    "        # enable RTD-like head with one class per feature\n",
    "        self.head = CLSHead(\n",
    "                d_in=d_token,\n",
    "                d_hidden=d_hidden,\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "\n",
    "        # tokenize\n",
    "        x = self.feature_tokenizer(x_num, x_cat)\n",
    "        \n",
    "        # add cls token to input\n",
    "        x = self.cls_token(x)\n",
    "\n",
    "        # add backbone\n",
    "        h = self.transformer(x)\n",
    "\n",
    "        # add classification head\n",
    "        return self.head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "model = PretrainModel()\n",
    "predictions = model(X_num, X_cat)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to device\n",
    "device = \"cpu\"\n",
    "\n",
    "model = PretrainModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False, False,  True, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False,  True,\n",
      "         False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False,  True, False,  True,\n",
      "          True, False, False, False, False, False]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor(0.0781)\n"
     ]
    }
   ],
   "source": [
    "# cat masks if cat variables are present\n",
    "if X_cat != None:\n",
    "    masks = torch.cat([x_num_mask, x_cat_mask], dim=1)\n",
    "else:\n",
    "    masks = x_num_mask\n",
    "\n",
    "# logits to binary mask\n",
    "hard_predictions = torch.zeros_like(predictions, dtype=torch.long)\n",
    "hard_predictions[predictions > 0] = 1\n",
    "\n",
    "# calculate column-wise accuracy\n",
    "features_accuracy = (hard_predictions.bool() == masks).sum(0) / hard_predictions.shape[0]\n",
    "\n",
    "mean_acc = features_accuracy.mean()\n",
    "\n",
    "print(masks)\n",
    "print(hard_predictions)\n",
    "print(mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.head = target_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1340],\n",
       "        [-0.3609],\n",
       "        [-0.6000],\n",
       "        [-0.3740]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# swap pretraining head with classification head\n",
    "transformer.head = target_head\n",
    "clf = FTTransformer(feature_tokenizer, transformer)\n",
    "\n",
    "# TODO: save to checkpoint\n",
    "\n",
    "# classify\n",
    "clf(X_cat, X_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runfield ðŸ›«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSHead(nn.Module):\n",
    "    \"\"\"\n",
    "    2 Layer MLP projection head\n",
    "    \"\"\"\n",
    "    # d_in is last dim of transformer output torch.Size([4, 17, 96]) -> 96 (ok)\n",
    "    # d_out -> last dim of output here [4,17,1] -> [4, 16] (ok)\n",
    "    def __init__(self, *, d_in: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(d_in, d_hidden)\n",
    "        self.out = nn.Linear(d_hidden, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        x = self.out(F.relu(self.first(x))).squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "epochs = 100\n",
    "\n",
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "\n",
    "\n",
    "reduction = \"mean\"\n",
    "\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "transformer_kwargs = {\n",
    "    \"d_token\": d_token,\n",
    "    \"n_blocks\": n_blocks,\n",
    "    \"attention_n_heads\": attention_heads,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": ffn_dropout,\n",
    "    # fix at 4/3, as activation (see search space B in\n",
    "    # https://arxiv.org/pdf/2106.11959v2.pdf)\n",
    "    # is static with ReGLU / GeGLU\n",
    "    \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "    \"attention_dropout\": attention_dropout,\n",
    "    \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.GELU, # nn.ReLU\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,  # fix at 1, due to binary classification\n",
    "}\n",
    "\n",
    "head_kwargs = {\"d_in\": d_token, \"d_hidden\": 32}\n",
    "\n",
    "optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "module_params = {\n",
    "    \"transformer\": Transformer(**transformer_kwargs),  # type: ignore\n",
    "    \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),  # type: ignore # noqa: E501\n",
    "    \"cat_features\": None,\n",
    "    \"cat_cardinalities\": [],\n",
    "}\n",
    "\n",
    "clf = FTTransformer(**module_params)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# swap target head with classification head, and restore later\n",
    "target_head = clf.transformer.head\n",
    "clf_head = CLSHead(**head_kwargs)\n",
    "clf.transformer.head = clf_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "val_data = TabDataset(X_val, y_val)\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    training_data.x_cat,\n",
    "    training_data.x_cont,\n",
    "    training_data.weight,\n",
    "    training_data.y,\n",
    "    **dl_params\n",
    ")\n",
    "\n",
    "val_loader = TabDataLoader(\n",
    "    val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "max_iters = epochs * len(train_loader)\n",
    "# saw recommendation of 5 - 10 % of total training budget or 100 to 500 steps\n",
    "warmup = int(0.05 * max_iters)\n",
    "print(f\"warmup steps: {warmup}\")\n",
    "print(max_iters)\n",
    "\n",
    "scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=warmup, max_iters=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, filename):\n",
    "    \n",
    "    # remove old files\n",
    "    for filename in glob.glob(f\"checkpoints/{run.id}*\"):\n",
    "        os.remove(filename) \n",
    "    \n",
    "    # create_dir\n",
    "    dir_checkpoints = \"checkpoints/\"\n",
    "    os.makedirs(dir_checkpoints, exist_ok = True) \n",
    "    \n",
    "    # save new file\n",
    "    print(\"saving new checkpoints.\")\n",
    "    torch.save(model.state_dict(), os.path.join(dir_checkpoints,f\"{run.id}*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "# see https://stackoverflow.com/a/53628783/5755604\n",
    "# no sigmoid required; numerically more stable\n",
    "# do not reduce, calculate mean after multiplication with weight\n",
    "\n",
    "step = 0\n",
    "best_accuracy = -1\n",
    "best_step = -1\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    batch = 0\n",
    "    \n",
    "    for x_cat, x_cont, _, masks in train_loader:\n",
    "    \n",
    "        clf.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for my implementation\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            train_loss = criterion(logits, masks)\n",
    "\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss_in_epoch_train += train_loss  # .item()\n",
    "        wandb.log({\"train_loss_step\": train_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "        batch += 1\n",
    "        step +=1\n",
    "\n",
    "    clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, weights, targets in val_loader:\n",
    "\n",
    "            # for my implementation\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            logits = logits.flatten()\n",
    "\n",
    "            val_loss = criterion(logits, targets)\n",
    "            \n",
    "            # get probabilities and round to nearest integer\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "            loss_in_epoch_val += val_loss  # val_loss #.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "            batch +=1      \n",
    "\n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "    \n",
    "    # correct samples / no samples\n",
    "    val_accuracy = correct / len(X_val)\n",
    "    if best_accuracy < val_accuracy:\n",
    "        checkpoint(clf, f\"checkpoints/{run.id}-{step}.ptx\")\n",
    "        best_accuracy = val_accuracy\n",
    "        best_step = step\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, 'epoch': epoch})\n",
    "    wandb.log({\"val_loss\": val_loss, 'epoch': epoch}) \n",
    "    \n",
    "    print(f\"train:{train_loss} val:{val_loss}\")\n",
    "    print(f\"val accuracy:{val_accuracy}\")\n",
    "\n",
    "    # return early if val accuracy doesn't improve. Minus to minimize.\n",
    "    early_stopping(-val_accuracy)\n",
    "    if early_stopping.early_stop or math.isnan(train_loss) or math.isnan(val_loss):\n",
    "        print(\"meh... early stopping\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
