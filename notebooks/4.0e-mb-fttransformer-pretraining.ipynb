{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CategoricalFeatureTokenizer,\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    MultiheadAttention,\n",
    "    NumericalFeatureTokenizer,\n",
    "    Transformer,\n",
    ")\n",
    "\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical, features_classical_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSHead(nn.Module):\n",
    "    \"\"\"\n",
    "    2 Layer MLP projection head\n",
    "    \"\"\"\n",
    "    # d_in is last dim of transformer output torch.Size([4, 17, 96]) -> 96 (ok)\n",
    "    # d_out -> last dim of output here [4,17,1] -> [4, 16] (ok)\n",
    "    def __init__(self, *, d_in: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(d_in, d_hidden)\n",
    "        self.out = nn.Linear(d_hidden, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        x = self.out(F.relu(self.first(x))).squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = CLSHead(d_in=768, d_hidden=768)\n",
    "\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShufflePermutations(object):\n",
    "    \"\"\"\n",
    "    Generate permutations by shuffeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_num, X_cat):\n",
    "        self.X_num = X_num\n",
    "        self.X_cat = X_cat\n",
    "\n",
    "    def permute(self, X):\n",
    "        \"\"\"\n",
    "        generate random index\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            return None\n",
    "\n",
    "        idx = torch.randint_like(X, X.shape[0], dtype=torch.long)\n",
    "\n",
    "        print(idx)\n",
    "        # generate random index array\n",
    "        return idx\n",
    "\n",
    "    def gen_permutations(self):\n",
    "        # permute numerical and categorical by random index\n",
    "        X_num = self.X_num\n",
    "        X_cat = self.X_cat if self.X_cat is not None else None\n",
    "        return self.permute(X_num), self.permute(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_num, d_cat = 8,8\n",
    "batch_size = 4\n",
    "\n",
    "X_num = torch.randn(batch_size, d_num)\n",
    "X_cat = torch.randint(0, 10, (batch_size, d_cat))\n",
    "\n",
    "perm_class = ShufflePermutations(X_num, X_cat)\n",
    "x_num_perm, x_cat_perm = perm_class.gen_permutations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_probability = 0.15\n",
    "\n",
    "def gen_masks(X, perm):\n",
    "    # generate binary masks\n",
    "    masks = torch.empty_like(X).bernoulli(p=corrupt_probability).bool()\n",
    "    new_masks = masks & (X != X[perm, torch.arange(X.shape[1], device=X.device)])\n",
    "    return new_masks\n",
    "\n",
    "# FIXME: probably generate for train and val set\n",
    "x_num_mask = gen_masks(X_num, x_num_perm)\n",
    "x_cat_mask = gen_masks(X_cat, x_cat_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values at permuted places\n",
    "x_num_permuted = torch.gather(X_num, 0, x_num_perm)\n",
    "\n",
    "# replace at mask = True\n",
    "X_num[x_num_mask] = x_num_permuted[x_num_mask]\n",
    "\n",
    "if X_cat is not None:\n",
    "\n",
    "    # along the 0 axis get elements based on perm_cat\n",
    "    x_cat_permuted = torch.gather(X_cat, 0, x_cat_perm)\n",
    "    \n",
    "    # replace at mask\n",
    "    X_cat[x_cat_mask] = x_cat_permuted[x_cat_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_token = 96\n",
    "\n",
    "params_feature_tokenizer = {\n",
    "            \"num_continous\": d_num,\n",
    "            \"cat_cardinalities\": [11] * d_cat,\n",
    "            \"d_token\": d_token,\n",
    "        }\n",
    "\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "\n",
    "params_transformer = {\n",
    "            \"d_token\": d_token,\n",
    "            \"n_blocks\": 3,\n",
    "            \"attention_n_heads\": 8,\n",
    "            \"attention_initialization\": \"kaiming\",\n",
    "            \"ffn_activation\": ReGLU,\n",
    "            \"attention_normalization\": nn.LayerNorm,\n",
    "            \"ffn_normalization\": nn.LayerNorm,\n",
    "            \"ffn_dropout\": 0.1,\n",
    "            \"ffn_d_hidden\": 96 * 2,\n",
    "            \"attention_dropout\": 0.1,\n",
    "            \"residual_dropout\": 0.1,\n",
    "            \"prenormalization\": True,\n",
    "            \"first_prenormalization\": False,\n",
    "            \"last_layer_query_idx\": None,\n",
    "            \"n_tokens\": None,\n",
    "            \"kv_compression_ratio\": None,\n",
    "            \"kv_compression_sharing\": None,\n",
    "            \"head_activation\": nn.ReLU,\n",
    "            \"head_normalization\": nn.LayerNorm,\n",
    "            \"d_out\": 1,\n",
    "        }\n",
    "\n",
    "transformer = Transformer(**params_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hidden = 32\n",
    "\n",
    "class PretrainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = CLSToken(d_token, \"uniform\")\n",
    "\n",
    "        self.feature_tokenizer = feature_tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # disable BERT-like classification head and replace with idenity mapping\n",
    "        self.transformer.head = nn.Identity()\n",
    "\n",
    "        # enable RTD-like head with one class per feature\n",
    "        self.head = CLSHead(\n",
    "                d_in=d_token,\n",
    "                d_hidden=d_hidden,\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "\n",
    "        # tokenize\n",
    "        x = self.feature_tokenizer(x_num, x_cat)\n",
    "        \n",
    "        # add cls token to input\n",
    "        x = self.cls_token(x)\n",
    "\n",
    "        # add backbone\n",
    "        h = self.transformer(x)\n",
    "\n",
    "        # add classification head\n",
    "        return self.head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PretrainModel()\n",
    "predictions = model(X_num, X_cat)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to device\n",
    "device = \"cuda\"\n",
    "\n",
    "model = PretrainModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat masks if cat variables are present\n",
    "if X_cat != None:\n",
    "    masks = torch.cat([x_num_mask, x_cat_mask], dim=1)\n",
    "else:\n",
    "    masks = x_num_mask\n",
    "\n",
    "# logits to binary mask\n",
    "hard_predictions = torch.zeros_like(predictions, dtype=torch.long)\n",
    "hard_predictions[predictions > 0] = 1\n",
    "\n",
    "# calculate column-wise accuracy\n",
    "features_accuracy = (hard_predictions.bool() == masks).sum(0) / hard_predictions.shape[0]\n",
    "\n",
    "print(masks)\n",
    "print(hard_predictions)\n",
    "print(features_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
