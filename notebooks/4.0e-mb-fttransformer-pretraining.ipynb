{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CategoricalFeatureTokenizer,\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    MultiheadAttention,\n",
    "    NumericalFeatureTokenizer,\n",
    "    Transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSHead(nn.Module):\n",
    "    \"\"\"\n",
    "    2 Layer MLP projection head\n",
    "    \"\"\"\n",
    "    def __init__(self, *, d_in: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.first = nn.Linear(d_in, d_hidden)\n",
    "        self.out = nn.Linear(d_hidden, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        x = self.out(F.relu(self.first(x))).squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = CLSHead(d_in=768, d_hidden=768)\n",
    "\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShufflePermutations(object):\n",
    "    \"\"\"\n",
    "    Generate permutations by shuffeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_num, X_cat):\n",
    "        self.X_num = X_num\n",
    "        self.X_cat = X_cat\n",
    "\n",
    "    def permute(self, X):\n",
    "        \"\"\"\n",
    "        generate random index\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            return None\n",
    "\n",
    "        idx = torch.randint_like(X, X.shape[0], dtype=torch.long)\n",
    "\n",
    "        print(idx)\n",
    "        # generate random index array\n",
    "        return idx\n",
    "\n",
    "    def gen_permutations(self):\n",
    "        # permute numerical and categorical by random index\n",
    "        X_num = self.X_num\n",
    "        X_cat = self.X_cat if self.X_cat is not None else None\n",
    "        return self.permute(X_num), self.permute(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_num, d_cat = 8,8\n",
    "batch_size = 4\n",
    "\n",
    "X_num = torch.randn(batch_size, d_num)\n",
    "X_cat = torch.randint(0, 10, (batch_size, d_cat))\n",
    "\n",
    "perm_class = ShufflePermutations(X_num, X_cat)\n",
    "x_num_perm, x_cat_perm = perm_class.gen_permutations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_probability = 0.15\n",
    "\n",
    "def gen_masks(X, perm):\n",
    "    # generate masks\n",
    "    masks = torch.empty_like(X).bernoulli(p=corrupt_probability).bool()\n",
    "    new_masks = masks & (X != X[perm, torch.arange(X.shape[1], device=X.device)])\n",
    "    return new_masks\n",
    "\n",
    "# FIXME: probably generate for train and val set\n",
    "x_num_mask = gen_masks(X_num, x_num_perm)\n",
    "x_cat_mask = gen_masks(X_cat, x_cat_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature_tokenizer = {\n",
    "            \"num_continous\": 3,\n",
    "            \"cat_cardinalities\": None,\n",
    "            \"d_token\": 96,\n",
    "        }\n",
    "\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "\n",
    "params_transformer = {\n",
    "            \"d_token\": 96,\n",
    "            \"n_blocks\": 3,\n",
    "            \"attention_n_heads\": 8,\n",
    "            \"attention_initialization\": \"kaiming\",\n",
    "            \"ffn_activation\": ReGLU,\n",
    "            \"attention_normalization\": nn.LayerNorm,\n",
    "            \"ffn_normalization\": nn.LayerNorm,\n",
    "            \"ffn_dropout\": 0.1,\n",
    "            \"ffn_d_hidden\": 96 * 2,\n",
    "            \"attention_dropout\": 0.1,\n",
    "            \"residual_dropout\": 0.1,\n",
    "            \"prenormalization\": True,\n",
    "            \"first_prenormalization\": False,\n",
    "            \"last_layer_query_idx\": None,\n",
    "            \"n_tokens\": None,\n",
    "            \"kv_compression_ratio\": None,\n",
    "            \"kv_compression_sharing\": None,\n",
    "            \"head_activation\": nn.ReLU,\n",
    "            \"head_normalization\": nn.LayerNorm,\n",
    "            \"d_out\": 1,\n",
    "        }\n",
    "\n",
    "transformer = Transformer(**params_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_token = 16\n",
    "d_hidden = 32\n",
    "\n",
    "class PretrainModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # # Input modules\n",
    "        # d_cat_embedding = C.model.d_cat_embedding\n",
    "        # d_num_embedding = C.model.d_num_embedding\n",
    "\n",
    "        # self.category_sizes = D.get_category_sizes(\"train\")\n",
    "        # if self.category_sizes and (\n",
    "        #     C.model.kind == \"transformer\"\n",
    "        #     or C.model.d_cat_embedding == \"d_num_embedding\"\n",
    "        # ):\n",
    "        #     d_cat_embedding = C.model.d_num_embedding\n",
    "\n",
    "        # if d_num_embedding:\n",
    "        #     self.num_embeddings = lib.NumEmbeddings(\n",
    "        #         C.model.num_embedding_arch,\n",
    "        #         D.n_num_features,\n",
    "        #         d_num_embedding,\n",
    "        #         d_feature=bins_store.n_bins if bins_store else None,\n",
    "        #         periodic_embedding_options=C.model.positional_encoding,\n",
    "        #     )\n",
    "        #     d_in_num = D.n_num_features * C.model.d_num_embedding\n",
    "        # else:\n",
    "        #     self.num_embeddings = None\n",
    "        #     d_in_num = bins_store.n_bins if bins_store else D.n_num_features\n",
    "\n",
    "        # if d_cat_embedding:\n",
    "        #     self.cat_embeddings = rtdl.CategoricalFeatureTokenizer(\n",
    "        #         self.category_sizes, d_cat_embedding, True, \"uniform\"\n",
    "        #     )\n",
    "        #     d_in_cat = d_cat_embedding * D.n_cat_features\n",
    "        # else:\n",
    "        #     self.cat_embeddings = None\n",
    "        #     d_in_cat = sum(self.category_sizes)\n",
    "\n",
    "        # d_in = d_in_num + d_in_cat\n",
    "        # print(f\"Model: Built embeddings flattened input dim: {d_in}\")\n",
    "\n",
    "        # # Backbones\n",
    "        # self.cls_token = None\n",
    "        # if C.model.kind == \"transformer\":\n",
    "        #     # load configuration\n",
    "        #     baseline_config = rtdl.FTTransformer.get_baseline_transformer_subconfig()\n",
    "        #     C.model.config = baseline_config | C.model.config\n",
    "        #     C.model.config[\"d_token\"] = C.model.d_num_embedding\n",
    "        #     # set backbone and cls token\n",
    "        #     self.backbone = lib.Transformer(C.model.config)\n",
    "        #     self.cls_token = rtdl.CLSToken(self.backbone.d, \"uniform\")\n",
    "\n",
    "\n",
    "        self.feature_tokenizer = feature_tokenizer\n",
    "\n",
    "        d_in = d_num + d_cat\n",
    "        \n",
    "        self.cls_token = CLSToken(d_token, \"uniform\")\n",
    "\n",
    "        # change later\n",
    "        # self.backbone = nn.Identity()\n",
    "        self.backbone = transformer\n",
    "\n",
    "        self.head = CLSHead(\n",
    "                d_in=d_token,\n",
    "                d_hidden=d_hidden,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "\n",
    "        # # concat embeddings, if available\n",
    "        # if self.num_embeddings:\n",
    "        #     x_num = self.num_embeddings(x_num)\n",
    "        # if self.cat_embeddings is not None:\n",
    "        #     assert x_cat is not None\n",
    "        #     x_cat = self.cat_embeddings(x_cat)\n",
    "\n",
    "        # print(x_num.shape)\n",
    "        # print(x_cat.shape)\n",
    "\n",
    "\n",
    "        # x = torch.cat(\n",
    "        #     [\n",
    "        #         x_ for x_ in [x_num, x_cat]\n",
    "        #         if x_ is not None\n",
    "        #     ],\n",
    "        #     dim=1,\n",
    "        # )\n",
    "\n",
    "        # tokenize\n",
    "        x = self.feature_tokenizer(x_num, x_cat)\n",
    "        # add cls token to input\n",
    "        x = self.cls_token(x)\n",
    "\n",
    "        # add backbone\n",
    "        h = self.backbone(x)\n",
    "        \n",
    "        # add classification head\n",
    "        return self.head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PretrainModel()\n",
    "model(X_num, X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to device\n",
    "device = \"cuda\"\n",
    "\n",
    "model = PretrainModel().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
