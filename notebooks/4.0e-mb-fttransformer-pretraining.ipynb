{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    Transformer,\n",
    "    CLSHead,\n",
    ")\n",
    "\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical, features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping\n",
    "from otc.optim.scheduler import CosineWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_unsupervised_log_standardized_clipped:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ache\n",
    "frac = 1 #0.05\n",
    "\n",
    "X_train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\")\n",
    "X_train = X_train.sample(frac=frac, random_state=42)\n",
    "\n",
    "y_train = X_train[\"buy_sell\"] # here: y = 0\n",
    "X_train = X_train[features_classical_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "x_cont = training_data.x_cont\n",
    "x_cat = training_data.x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_perm(X):\n",
    "    \"\"\"\n",
    "    Generate index permutation.\n",
    "    \"\"\"\n",
    "    if X is None:\n",
    "        return None\n",
    "    return torch.randint_like(X, X.shape[0], dtype=torch.long)\n",
    "\n",
    "x_cont_perm = gen_perm(x_cont)\n",
    "x_cat_perm = gen_perm(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_masks(X, perm, corrupt_probability = 0.15):\n",
    "    \"\"\"\n",
    "    Generate binary mask for detection.\n",
    "    \"\"\"\n",
    "    masks = torch.empty_like(X).bernoulli(p=corrupt_probability).bool()\n",
    "    new_masks = masks & (X != X[perm, torch.arange(X.shape[1], device=X.device)])\n",
    "    return new_masks\n",
    "\n",
    "# generate masks for numeric and for categorical features (optional)\n",
    "x_cont_mask = gen_masks(training_data.x_cont, x_cont_perm)\n",
    "\n",
    "if training_data.x_cat is not None:\n",
    "    x_cat_mask = gen_masks(training_data.x_cat, x_cat_perm)\n",
    "else:\n",
    "    x_cat_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace at permutation\n",
    "x_cont_permuted = torch.gather(x_cont, 0, x_cont_perm)\n",
    "\n",
    "# replace at mask = True\n",
    "x_cont[x_cont_mask] = x_cont_permuted[x_cont_mask]\n",
    "\n",
    "if x_cat is not None:\n",
    "\n",
    "    # along the 0 axis get elements based on perm_cat\n",
    "    x_cat_permuted = torch.gather(x_cat, 0, x_cat_perm)\n",
    "    \n",
    "    # replace at mask\n",
    "    x_cat[x_cat_mask] = x_cat_permuted[x_cat_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge masks [mask_num, mask_cat]\n",
    "if x_cat != None:\n",
    "    masks = torch.cat([x_cont_mask, x_cat_mask], dim=1)\n",
    "else:\n",
    "    masks = x_cont_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up into train (first 80 %) and val (last 20 %)\n",
    "idx = int (len(x_cont) * 0.8)\n",
    "\n",
    "x_cont_train, x_cont_val = torch.split(x_cont, idx, dim=0)\n",
    "masks_train, masks_val = torch.split(masks, idx, dim=0)\n",
    "\n",
    "if x_cat is not None:\n",
    "    x_cat_train, x_cat_val = torch.split(x_cat, idx, dim=0)\n",
    "else:\n",
    "    x_cat_train, x_cat_val = None, None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runfield 🛫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "epochs = 100\n",
    "\n",
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "\n",
    "\n",
    "reduction = \"mean\"\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": False,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "transformer_kwargs = {\n",
    "    \"d_token\": d_token,\n",
    "    \"n_blocks\": n_blocks,\n",
    "    \"attention_n_heads\": attention_heads,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": ffn_dropout,\n",
    "    # fix at 4/3, as activation (see search space B in\n",
    "    # https://arxiv.org/pdf/2106.11959v2.pdf)\n",
    "    # is static with ReGLU / GeGLU\n",
    "    \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "    \"attention_dropout\": attention_dropout,\n",
    "    \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.GELU, # nn.ReLU\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,  # fix at 1, due to binary classification\n",
    "}\n",
    "\n",
    "head_kwargs = {\"d_in\": d_token, \"d_hidden\": 32}\n",
    "\n",
    "optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "module_params = {\n",
    "    \"transformer\": Transformer(**transformer_kwargs),  # type: ignore\n",
    "    \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),  # type: ignore # noqa: E501\n",
    "    \"cat_features\": None,\n",
    "    \"cat_cardinalities\": [],\n",
    "}\n",
    "\n",
    "clf = FTTransformer(**module_params)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# swap target head with classification head, and restore later\n",
    "target_head = clf.transformer.head\n",
    "clf_head = CLSHead(**head_kwargs)\n",
    "clf.transformer.head = clf_head\n",
    "\n",
    "clf.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TabDataLoader(\n",
    "    x_cat_train,\n",
    "    x_cont_train,\n",
    "    masks_train, \n",
    "    **dl_params\n",
    ")\n",
    "\n",
    "val_loader = TabDataLoader(\n",
    "    x_cat_val,\n",
    "    x_cont_val,\n",
    "    masks_val, \n",
    "    **dl_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "max_iters = epochs * len(train_loader)\n",
    "# saw recommendation of 5 - 10 % of total training budget or 100 to 500 steps\n",
    "warmup = int(0.05 * max_iters)\n",
    "print(f\"warmup steps: {warmup}\")\n",
    "print(max_iters)\n",
    "\n",
    "scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=warmup, max_iters=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model):\n",
    "    \n",
    "    # remove old files\n",
    "    for fn in glob.glob(f\"checkpoints/{run.id}*\"):\n",
    "        os.remove(fn) \n",
    "    \n",
    "    # create_dir\n",
    "    dir_checkpoints = \"checkpoints/\"\n",
    "    os.makedirs(dir_checkpoints, exist_ok = True) \n",
    "    \n",
    "    # save new file\n",
    "    print(\"saving new checkpoints.\")\n",
    "    torch.save(model.state_dict(), os.path.join(dir_checkpoints,f\"{run.id}*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "step = 0\n",
    "best_accuracy = -1\n",
    "best_step = -1\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    batch = 0\n",
    "    \n",
    "    for x_cat, x_cont, masks in train_loader:\n",
    "    \n",
    "        clf.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = clf(x_cat, x_cont)\n",
    "            train_loss = criterion(logits, masks.float())\n",
    "\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss_in_epoch_train += train_loss.item()\n",
    "        \n",
    "        wandb.log({\"train_loss_step\": train_loss.item(), \"epoch\": epoch, \"batch\": batch})\n",
    "\n",
    "        batch += 1\n",
    "        step +=1\n",
    "\n",
    "    clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, masks in val_loader:\n",
    "\n",
    "            # for my implementation\n",
    "            logits = clf(x_cat, x_cont)\n",
    "            val_loss = criterion(logits, masks.float())\n",
    "            \n",
    "\n",
    "            # hard_predictions = torch.zeros_like(logits, dtype=torch.long)\n",
    "            # hard_predictions[logits > 0] = 1\n",
    "            # correct += (hard_predictions.bool() == masks).sum()  / hard_predictions.shape[0]\n",
    "\n",
    "            loss_in_epoch_val += val_loss.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss.item(), \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "            batch +=1      \n",
    "\n",
    "    # correct / (rows * columns)\n",
    "    # val_accuracy = correct / (X_train.shape[0] * X_train.shape[1])        \n",
    "    \n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "    \n",
    "    print(f\"train loss: {train_loss}\")\n",
    "    print(f\"val loss: {val_loss}\")\n",
    "    \n",
    "    # correct samples / no samples\n",
    "    # val_accuracy = correct / len(X_val)\n",
    "    # if best_accuracy < val_accuracy:\n",
    "    #     checkpoint(clf, f\"checkpoints/{run.id}-{step}.ptx\")\n",
    "    #     best_accuracy = val_accuracy\n",
    "    #     best_step = step\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, 'epoch': epoch})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Finetuning🕰️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap heads and save model\n",
    "clf.transformer.head = target_head\n",
    "checkpoint(clf)\n",
    "\n",
    "# FIXME: Think about which weights to freeze and which to update\n",
    "# https://ai.stackexchange.com/questions/23884/why-arent-the-bert-layers-frozen-during-fine-tuning-tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
