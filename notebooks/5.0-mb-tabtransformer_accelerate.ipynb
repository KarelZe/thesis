{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vajs4UZ4V3-N",
    "outputId": "973dbdd5-5772-4262-ea0b-93d21ab5c350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /pfs/data5/home/kit/stud/uloak/.local/lib/python3.8/site-packages (0.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/2022-03-30/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gcsfs\n",
      "  Using cached gcsfs-2022.11.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting fsspec==2022.11.0\n",
      "  Using cached fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from gcsfs) (3.8.1)\n",
      "Requirement already satisfied: decorator>4.1.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from gcsfs) (5.1.1)\n",
      "Collecting google-cloud-storage\n",
      "  Using cached google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: requests in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from gcsfs) (2.27.1)\n",
      "Collecting google-auth-oauthlib\n",
      "  Using cached google_auth_oauthlib-0.7.1-py2.py3-none-any.whl (19 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Using cached google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.0.12)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Using cached google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Using cached google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests->gcsfs) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests->gcsfs) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests->gcsfs) (3.3)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Using cached protobuf-4.21.10-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.0)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to fsspec in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/2022-03-30/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fastparquet\n",
      "  Using cached fastparquet-2022.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting cramjam>=2.3\n",
      "  Using cached cramjam-2.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: fsspec in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from fastparquet) (2022.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from fastparquet) (1.22.3)\n",
      "Requirement already satisfied: packaging in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from fastparquet) (21.3)\n",
      "Collecting pandas>=1.5.0\n",
      "  Using cached pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from packaging->fastparquet) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to pandas in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/2022-03-30/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pynvml in /pfs/data5/home/kit/stud/uloak/.local/lib/python3.8/site-packages (11.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/2022-03-30/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu116\n",
      "Collecting torch[dynamo]\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu116/torch-1.14.0.dev20221207%2Bcu116-cp38-cp38-linux_x86_64.whl (1991.1 MB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting torchtriton==2.0.0+0d7e753227\n",
      "  Using cached https://download.pytorch.org/whl/nightly/torchtriton-2.0.0%2B0d7e753227-cp38-cp38-linux_x86_64.whl (18.7 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting cmake\n",
      "  Using cached cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.2-py3-none-any.whl (10 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[31mERROR: Will not install to the user site because it will lack sys.path precedence to Jinja2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/2022-03-30/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/get-started/pytorch-2.0/#faqs\n",
    "!python -m pip install einops\n",
    "!python -m pip install gcsfs\n",
    "!python -m pip install fastparquet\n",
    "!python -m pip install pynvml\n",
    "!python -m pip install --pre torch[dynamo] --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NRm1-RKDHY2C"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from typing import List, Optional,Tuple, Union, Callable\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn, optim, tensor, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qxmlWlnPHYyp"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TabDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for tabular data.\n",
    "    Args:\n",
    "        Dataset (Dataset): dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        cat_features: list[str] | None = None,\n",
    "        cat_unique_counts: tuple[int, ...] | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tabular data set holding data for the model.\n",
    "        Args:\n",
    "            x (pd.DataFrame): feature matrix.\n",
    "            y (pd.Series): target.\n",
    "            cat_features (Optional[List[str]], optional): List with categorical columns.\n",
    "            Defaults to None.\n",
    "            cat_unique_counts (Optional[Tuple[int]], optional): Number of categories per\n",
    "            categorical feature. Defaults to None.\n",
    "        \"\"\"\n",
    "        self._cat_unique_counts = () if not cat_unique_counts else cat_unique_counts\n",
    "\n",
    "        # calculate cat indices\n",
    "        features = x.columns.tolist()\n",
    "        cat_features = [] if not cat_features else cat_features\n",
    "        self._cat_idx = [features.index(i) for i in cat_features if i in features]\n",
    "\n",
    "        # calculate cont indices\n",
    "        cont_features = [f for f in features if f not in cat_features]\n",
    "        self._cont_idx = [features.index(i) for i in cont_features if i in features]\n",
    "\n",
    "        assert (\n",
    "            x.shape[0] == y.shape[0]\n",
    "        ), \"Length of feature matrix must match length of target.\"\n",
    "        assert len(cat_features) == len(\n",
    "            self._cat_unique_counts\n",
    "        ), \"For all categorical features the number of unique entries must be provided.\"\n",
    "\n",
    "        # adjust target to be either 0 or 1\n",
    "        self.y = torch.tensor(y.values).float()\n",
    "        self.y[self.y < 0] = 0\n",
    "\n",
    "        # cut into continous and categorical tensor\n",
    "        self.x_cat: torch.Tensor | None = None\n",
    "        if len(self._cat_idx) > 0:\n",
    "            self.x_cat = torch.tensor(x.iloc[:, self._cat_idx].values).int()\n",
    "        self.x_cont = torch.tensor(x.iloc[:, self._cont_idx].values).float()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Length of dataset.\n",
    "        Returns:\n",
    "            int: length\n",
    "        \"\"\"\n",
    "        return len(self.x_cont)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[torch.Tensor | None, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get sample for model.\n",
    "        Args:\n",
    "            idx (int): _description_\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor | None, torch.Tensor, torch.Tensor]:\n",
    "            x_cat (if present if present otherwise None), x_cont and y.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.x_cat[idx] if self.x_cat else None,\n",
    "            self.x_cont[idx],\n",
    "            self.y[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HUjGdNsxJiHF"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A fast dataloader-like object to load batches of tabular data sets.\n",
    "Adapted from here:\n",
    "https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class TabDataLoader:\n",
    "    \"\"\"\n",
    "    PyTorch Implementation of a dataloader for tabular data.\n",
    "    Due to a chunk-wise reading or several rows at once it is preferred\n",
    "    over the standard dataloader that reads row-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *tensors: torch.Tensor | None,\n",
    "        batch_size: int = 4096,\n",
    "        shuffle: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        TabDataLoader.\n",
    "        Tensors can be None e. g., if there is no categorical data.\n",
    "        Args:\n",
    "            batch_size (int, optional): size of batch. Defaults to 4096.\n",
    "            shuffle (bool, optional): shuffle data. Defaults to False.\n",
    "            device (str, optional): device where. Defaults to \"cpu\".\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        # check for tensors that are None\n",
    "        self.none_mask = tuple(t is None for t in tensors)\n",
    "        # filter if for not none tensors\n",
    "        self.tensors = tuple(t for t in tensors if t is not None)\n",
    "\n",
    "        # check if all tensors have same length\n",
    "        assert all(t.shape[0] == self.tensors[0].shape[0] for t in self.tensors)\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __iter__(self) -> TabDataLoader:\n",
    "        \"\"\"\n",
    "        Return itself.\n",
    "        Returns:\n",
    "            TabDataLoader: TabDataLoader\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            r = torch.randperm(self.dataset_len)\n",
    "            self.tensors = tuple(t[r] for t in self.tensors if t)\n",
    "        # reset counter on new iteration\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[torch.Tensor | None, ...]:\n",
    "        \"\"\"\n",
    "        Generate next batch with size of 'batch_size'.\n",
    "        Batches can be underful.\n",
    "        Raises:\n",
    "            StopIteration: stopping criterion.\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor | None, torch.Tensor, torch.Tensor]: (X_cat), X_cont, y\n",
    "        \"\"\"\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        mixed_batch: list[torch.Tensor | None] = [\n",
    "            t[self.i : self.i + self.batch_size].to(self.device) for t in self.tensors\n",
    "        ]\n",
    "        self.i += self.batch_size\n",
    "\n",
    "        # tensors + nones if input tensors contained none\n",
    "        for i, is_none in enumerate(self.none_mask):\n",
    "            if is_none:\n",
    "                mixed_batch.insert(i, None)\n",
    "\n",
    "        return tuple(mixed_batch)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get number of full and partial batches in data set.\n",
    "        Returns:\n",
    "            int: number of batches.\n",
    "        \"\"\"\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EeBiVlxWHW2z"
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of residual connections.\n",
    "    Args:\n",
    "        nn (nn.Module): module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fn: nn.Module):\n",
    "        \"\"\"\n",
    "        Residual connection.\n",
    "        Args:\n",
    "            fn (nn.Module): network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of residual connections.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of pre-normalization.\n",
    "    Args:\n",
    "        nn (nn.module): module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, fn: nn.Module):\n",
    "        \"\"\"\n",
    "        Pre-normalization.\n",
    "        Consists of layer for layer normalization followed by another network.\n",
    "        Args:\n",
    "            dim (int): Number of dimensions of normalized shape.\n",
    "            fn (nn.Module): network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of pre-normalization layers.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    r\"\"\"\n",
    "    Implementation of the GeGLU activation function.\n",
    "    Given by:\n",
    "    $\\operatorname{GeGLU}(x, W, V, b, c)=\\operatorname{GELU}(x W+b) \\otimes(x V+c)$\n",
    "    Proposed in https://arxiv.org/pdf/2002.05202v1.pdf.\n",
    "    Args:\n",
    "        nn (torch.Tensor): module\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of GeGlU activation.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of feed forward network.\n",
    "    Args:\n",
    "        nn (nn.module): module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, mult: int = 4, dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Feed forward network.\n",
    "        Network consists of input layer, GEGLU activation, dropout layer,\n",
    "        and output layer.\n",
    "        Args:\n",
    "            dim (int): dimension of input and output layer.\n",
    "            mult (int, optional): Scaling factor for output dimension of input layer or\n",
    "            input dimension of output layer. Defaults to 4.\n",
    "            dropout (float, optional): Degree of dropout. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of feed forward network.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation of attention.\n",
    "    Args:\n",
    "        nn (nn.Module): module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dim: int, heads: int = 8, dim_head: int = 16, dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Attention.\n",
    "        Args:\n",
    "            dim (int): Number of dimensions.\n",
    "            heads (int, optional): Number of attention heads. Defaults to 8.\n",
    "            dim_head (int, optional): Dimension of attention heads. Defaults to 16.\n",
    "            dropout (float, optional): Degree of dropout. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of attention module.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n",
    "        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\", h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer.\n",
    "    Based on paper:\n",
    "    https://arxiv.org/abs/1706.03762\n",
    "    Args:\n",
    "        nn (nn.Module): Module with transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens: int,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        dim_head: int,\n",
    "        attn_dropout: float,\n",
    "        ff_dropout: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Classical transformer.\n",
    "        Args:\n",
    "            num_tokens (int): Number of tokens i. e., unique classes + special tokens.\n",
    "            dim (int): Number of dimensions.\n",
    "            depth (int): Depth of encoder / decoder.\n",
    "            heads (int): Number of attention heads.\n",
    "            dim_head (int): Dimensions of attention heads.\n",
    "            attn_dropout (float): Degree of dropout in attention.\n",
    "            ff_dropout (float): Degree of dropout in feed-forward network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(num_tokens, dim)  # (Embed the categorical features.)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        Residual(\n",
    "                            PreNorm(\n",
    "                                dim,\n",
    "                                Attention(\n",
    "                                    dim,\n",
    "                                    heads=heads,\n",
    "                                    dim_head=dim_head,\n",
    "                                    dropout=attn_dropout,\n",
    "                                ),\n",
    "                            )\n",
    "                        ),\n",
    "                        Residual(PreNorm(dim, FeedForward(dim, dropout=ff_dropout))),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of transformer.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        x = self.embeds(x)\n",
    "\n",
    "        for attn, ff in self.layers:  # type: ignore\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch model of a vanilla multi-layer perceptron.\n",
    "    Args:\n",
    "        nn (nn.Module): module with implementation of MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims: List[int], act: Union[str, Callable[..., nn.Module]]):\n",
    "        \"\"\"\n",
    "        Multilayer perceptron.\n",
    "        Depth of network is given by `len(dims)`. Capacity is given by entries\n",
    "        of `dim`. Activation function is used after each linear layer. There is\n",
    "        no activation function for the final linear layer, as it is sometimes part\n",
    "        of the loss function already e. g., `nn.BCEWithLogitsLoss()`.\n",
    "        Args:\n",
    "            dims (List[int]): List with dimensions of layers.\n",
    "            act (Union[str, Callable[..., nn.Module]]): Activation function of each linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        layers = []\n",
    "        for dim_in, dim_out in dims_pairs:\n",
    "            linear = nn.Linear(dim_in, dim_out)\n",
    "            layers.append(linear)\n",
    "            layers.append(act)\n",
    "\n",
    "        # drop last layer, as a sigmoid layer is included from BCELogitLoss\n",
    "        del layers[-1]\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward propagate tensor through MLP.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: output tensor.\n",
    "        \"\"\"\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch model of TabTransformer.\n",
    "    Based on paper:\n",
    "    https://arxiv.org/abs/2012.06678\n",
    "    Args:\n",
    "        nn (nn.Module): Module with implementation of TabTransformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        categories: Union[List[int], Tuple[()]],\n",
    "        num_continuous: int,\n",
    "        dim: int = 32,\n",
    "        depth: int = 4,\n",
    "        heads: int = 8,\n",
    "        dim_head: int = 16,\n",
    "        dim_out: int = 1,\n",
    "        mlp_hidden_mults: Tuple[(int, int)] = (4, 2),\n",
    "        mlp_act: Union[str, Callable[..., nn.Module]] = nn.ReLU,\n",
    "        num_special_tokens: int = 2,\n",
    "        continuous_mean_std: Optional[torch.Tensor] = None,\n",
    "        attn_dropout: float = 0.0,\n",
    "        ff_dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        TabTransformer.\n",
    "        Originally introduced in https://arxiv.org/abs/2012.06678.\n",
    "        Args:\n",
    "            categories (Union[List[int],Tuple[()]]): List with number of categories\n",
    "            for each categorical feature. If no categorical variables are present,\n",
    "            use empty tuple. For categorical variables e. g., option type ('C' or 'P'),\n",
    "            the list would be `[1]`.\n",
    "            num_continuous (int): Number of continous features.\n",
    "            dim (int, optional): Dimensionality of transformer. Defaults to 32.\n",
    "            depth (int, optional): Depth of encoder / decoder of transformer.\n",
    "            Defaults to 4.\n",
    "            heads (int, optional): Number of attention heads. Defaults to 8.\n",
    "            dim_head (int, optional): Dimensionality of attention head. Defaults to 16.\n",
    "            dim_out (int, optional): Dimension of output layer of MLP. Set to one for\n",
    "            binary classification. Defaults to 1.\n",
    "            mlp_hidden_mults (Tuple[(int, int)], optional): multipliers for dimensions\n",
    "            of hidden layer in MLP. Defaults to (4, 2).\n",
    "            mlp_act (Union[str, Callable[..., nn.Module]], optional): Activation function used in MLP.\n",
    "            Defaults to nn.ReLU().\n",
    "            num_special_tokens (int, optional): Number of special tokens in transformer.\n",
    "            Defaults to 2.\n",
    "            continuous_mean_std (Optional[torch.Tensor]): List with mean and\n",
    "            std deviation of each continous feature. Shape eq. `[num_continous x 2]`.\n",
    "            Defaults to None.\n",
    "            attn_dropout (float, optional): Degree of attention dropout used in\n",
    "            transformer. Defaults to 0.0.\n",
    "            ff_dropout (float, optional): Dropout in feed forward net. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert all(\n",
    "            map(lambda n: n > 0, categories)\n",
    "        ), \"number of each category must be positive\"\n",
    "\n",
    "        # categories related calculations\n",
    "\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position\n",
    "        #  in the categories embedding table\n",
    "\n",
    "        categories_offset = F.pad(\n",
    "            torch.tensor(list(categories)), (1, 0), value=num_special_tokens\n",
    "        )  # Prepend num_special_tokens.\n",
    "        categories_offset = categories_offset.cumsum(dim=-1)[:-1]\n",
    "        self.register_buffer(\"categories_offset\", categories_offset)\n",
    "\n",
    "        # continuous\n",
    "\n",
    "        if continuous_mean_std is not None:\n",
    "            assert continuous_mean_std.shape == (num_continuous, 2,), (\n",
    "                f\"continuous_mean_std must have a shape of ({num_continuous}, 2)\"\n",
    "                f\"where the last dimension contains the mean and variance respectively\"\n",
    "            )\n",
    "        self.register_buffer(\"continuous_mean_std\", continuous_mean_std)\n",
    "\n",
    "        self.norm = nn.LayerNorm(num_continuous)\n",
    "        self.num_continuous = num_continuous\n",
    "\n",
    "        # transformer\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            num_tokens=total_tokens,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            attn_dropout=attn_dropout,\n",
    "            ff_dropout=ff_dropout,\n",
    "        )\n",
    "\n",
    "        # mlp to logits\n",
    "\n",
    "        input_size = (dim * self.num_categories) + num_continuous\n",
    "        j = input_size // 8\n",
    "\n",
    "        hidden_dimensions = list(map(lambda t: j * t, mlp_hidden_mults))\n",
    "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
    "\n",
    "        self.mlp = MLP(all_dimensions, act=mlp_act)\n",
    "\n",
    "    def forward(self, x_categ: torch.Tensor, x_cont: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of TabTransformer.\n",
    "        Args:\n",
    "            x_categ (torch.Tensor): tensor with categorical data.\n",
    "            x_cont (torch.Tensor): tensor with continous data.\n",
    "        Returns:\n",
    "            torch.Tensor: predictions with shape [batch, 1]\n",
    "        \"\"\"\n",
    "        # Adaptation to work without categorical data\n",
    "        if x_categ is not None:\n",
    "            assert x_categ.shape[-1] == self.num_categories, (\n",
    "                f\"you must pass in {self.num_categories} \"\n",
    "                f\"values for your categories input\"\n",
    "            )\n",
    "            x_categ += self.categories_offset\n",
    "            x = self.transformer(x_categ)\n",
    "            flat_categ = x.flatten(1)\n",
    "\n",
    "        assert x_cont.shape[1] == self.num_continuous, (\n",
    "            f\"you must pass in {self.num_continuous} \"\n",
    "            f\"values for your continuous input\"\n",
    "        )\n",
    "\n",
    "        if self.continuous_mean_std is not None:\n",
    "            mean, std = self.continuous_mean_std.unbind(dim=-1)\n",
    "            x_cont = (x_cont - mean) / std\n",
    "\n",
    "        normed_cont = self.norm(x_cont)\n",
    "\n",
    "        # Adaptation to work without categorical data\n",
    "        if x_categ is not None:\n",
    "            x = torch.cat((flat_categ, normed_cont), dim=-1)\n",
    "        else:\n",
    "            x = normed_cont\n",
    "\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dx-aWaOpCQwr"
   },
   "outputs": [],
   "source": [
    "# https://svn.blender.org/svnroot/bf-blender/trunk/blender/build_files/scons/tools/bcolors.py\n",
    "# https://stackoverflow.com/a/287944/5755604\n",
    "class colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "    def disable(self):\n",
    "        self.HEADER = ''\n",
    "        self.OKBLUE = ''\n",
    "        self.OKGREEN = ''\n",
    "        self.OKCYAN = ''\n",
    "        self.WARNING = ''\n",
    "        self.FAIL = ''\n",
    "        self.ENDC = ''\n",
    "        self.BOLD = ''\n",
    "        self.UNDERLINE = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-T5Ki40INJso"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/uloak/thesis/thesis/lib64/python3.8/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"TRADE_SIZE\",\n",
    "    \"TRADE_PRICE\",\n",
    "    \"BEST_ASK\",\n",
    "    \"BEST_BID\",\n",
    "    \"price_ex_lag\",\n",
    "    \"price_ex_lead\",\n",
    "    \"price_all_lag\",\n",
    "    \"price_all_lead\",\n",
    "    \"bid_ex\",\n",
    "    \"ask_ex\",\n",
    "    \"bid_size_ex\",\n",
    "    \"ask_size_ex\",\n",
    "    \"OPTION_TYPE\",\n",
    "    \"buy_sell\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "X = pd.read_parquet(\n",
    "    f\"gs://thesis-bucket-option-trade-classification/data/preprocessed/val_set_20.parquet\",\n",
    "    engine=\"fastparquet\", columns=columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "23PR7UObWvyG",
    "outputId": "96ca6a2d-f0a1-4b71-d83f-ef8aa190fc38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRADE_SIZE</th>\n",
       "      <th>TRADE_PRICE</th>\n",
       "      <th>BEST_ASK</th>\n",
       "      <th>BEST_BID</th>\n",
       "      <th>price_ex_lag</th>\n",
       "      <th>price_ex_lead</th>\n",
       "      <th>price_all_lag</th>\n",
       "      <th>price_all_lead</th>\n",
       "      <th>bid_ex</th>\n",
       "      <th>ask_ex</th>\n",
       "      <th>bid_size_ex</th>\n",
       "      <th>ask_size_ex</th>\n",
       "      <th>OPTION_TYPE</th>\n",
       "      <th>buy_sell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29510320</th>\n",
       "      <td>20</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.38</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29510321</th>\n",
       "      <td>20</td>\n",
       "      <td>6.27</td>\n",
       "      <td>6.31</td>\n",
       "      <td>5.85</td>\n",
       "      <td>10.29</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.69</td>\n",
       "      <td>6.32</td>\n",
       "      <td>5.85</td>\n",
       "      <td>6.31</td>\n",
       "      <td>115.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29510322</th>\n",
       "      <td>2</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.44</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29510323</th>\n",
       "      <td>20</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.70</td>\n",
       "      <td>99.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29510324</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TRADE_SIZE  TRADE_PRICE  BEST_ASK  BEST_BID  price_ex_lag  \\\n",
       "29510320          20         1.47      1.62      1.38          2.73   \n",
       "29510321          20         6.27      6.31      5.85         10.29   \n",
       "29510322           2         1.32      1.44      1.19          1.19   \n",
       "29510323          20         1.66      1.70      1.62          1.60   \n",
       "29510324           1         0.85      0.00      0.00          0.86   \n",
       "\n",
       "          price_ex_lead  price_all_lag  price_all_lead  bid_ex  ask_ex  \\\n",
       "29510320           1.12           1.62            1.60     NaN     NaN   \n",
       "29510321           5.92           7.69            6.32    5.85    6.31   \n",
       "29510322           1.02           1.25            1.30    1.19    1.44   \n",
       "29510323           1.62           1.60            1.62    1.62    1.70   \n",
       "29510324           0.65           0.86            0.50     NaN     NaN   \n",
       "\n",
       "          bid_size_ex  ask_size_ex OPTION_TYPE  buy_sell  \n",
       "29510320          NaN          NaN           P        -1  \n",
       "29510321        115.0         11.0           P         1  \n",
       "29510322         82.0         82.0           C         1  \n",
       "29510323         99.0        172.0           P         1  \n",
       "29510324          NaN          NaN           P         1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "haMEXGBAXQcD"
   },
   "outputs": [],
   "source": [
    "# select categorical e. g., option type and strings e. g., ticker\n",
    "cat_columns = X.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
    "\n",
    "# binarize categorical similar to Borisov et al.\n",
    "X[cat_columns] = X[cat_columns].apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "X.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RQWxsVKX0Sy5"
   },
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSwU1h3OHicX",
    "outputId": "b42704ce-7ff0-4271-83cb-fdfdfd7e703b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_21380449/ipykernel_3385721/2777296165.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_train.drop(columns=['buy_sell'], inplace=True)\n",
      "/scratch/slurm_tmpdir/job_21380449/ipykernel_3385721/2777296165.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_val.drop(columns=['buy_sell'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_available: cuda\n",
      "num of cores:80\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "x_train = X.head(100000)\n",
    "y_train = x_train['buy_sell']\n",
    "x_train.drop(columns=['buy_sell'], inplace=True)\n",
    "\n",
    "x_val = X.tail(50000)\n",
    "y_val = x_val['buy_sell']\n",
    "x_val.drop(columns=['buy_sell'], inplace=True)\n",
    "\n",
    "features = x_train.columns.tolist()\n",
    "cat_features = [\"OPTION_TYPE\"]\n",
    "\n",
    "_cat_unique = [2]\n",
    "if not _cat_unique:\n",
    "    _cat_unique = ()\n",
    "# assume columns are duplicate free, which is standard in pandas\n",
    "cont_features = [x for x in x_train.columns.tolist() if x not in cat_features]\n",
    "\n",
    "# print(cat_features)\n",
    "\n",
    "# static params\n",
    "epochs = 8\n",
    "\n",
    "\n",
    "#  use gpu if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"cuda_available: {device}\")\n",
    "print(f\"num of cores:{os.cpu_count()}\")\n",
    "\n",
    "\n",
    "\n",
    "# create training and val set\n",
    "training_data = TabDataset(x_train,y_train,cat_features,_cat_unique)\n",
    "val_data = TabDataset(x_val,y_val,cat_features,_cat_unique)\n",
    "\n",
    "dim: int = 64 # type: ignore\n",
    "\n",
    "depth: int = 3 \n",
    "heads: int = 8\n",
    "weight_decay: float = 1e-5\n",
    "lr = 4e-3\n",
    "dropout = 0.2\n",
    "batch_size: int = 8192\n",
    "\n",
    "# span as many workers as cores\n",
    "dl_kwargs = {'num_workers': os.cpu_count(), 'pin_memory': True, 'batch_size':batch_size, 'shuffle':False} if use_cuda else {'batch_size':batch_size, 'shuffle':False}\n",
    "\n",
    "\n",
    "train_loader = TabDataLoader(training_data.x_cat, training_data.x_cont, training_data.y, **dl_kwargs)\n",
    "val_loader = TabDataLoader(val_data.x_cat, val_data.x_cont, val_data.y, **dl_kwargs)\n",
    "\n",
    "\n",
    "_clf = TabTransformer(\n",
    "    categories=_cat_unique,\n",
    "    num_continuous=len(cont_features),  # number of continuous values\n",
    "    dim_out=1,\n",
    "    mlp_act=nn.ReLU(),  # sigmoid of last layer already included in loss.\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    attn_dropout=dropout,\n",
    "    ff_dropout=dropout,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    ").to(device)\n",
    "\n",
    "# # API NOT FINAL\n",
    "# # default: optimizes for large models, low compile-time\n",
    "# #          and no extra memory usage\n",
    "# torch.compile(model)\n",
    "\n",
    "# # reduce-overhead: optimizes to reduce the framework overhead\n",
    "# #                and uses some extra memory. Helps speed up small models\n",
    "# torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "# max-autotune: optimizes to produce the fastest model,\n",
    "#               but takes a very long time to compile\n",
    "# _clf = torch.compile(_clf, mode=\"max-autotune\")\n",
    "\n",
    "\n",
    "# prof = torch.profiler.profile(\n",
    "#         schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "#         on_trace_ready=torch.profiler.tensorboard_trace_handler('./drive/MyDrive/log/tabtransformer'),\n",
    "#         record_shapes=True,\n",
    "#         with_stack=True)\n",
    "# prof.start()\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Generate the optimizers\n",
    "optimizer = optim.AdamW(\n",
    "    _clf.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# see https://stackoverflow.com/a/53628783/5755604\n",
    "# no sigmoid required; numerically more stable\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def our()->None:\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "      # perform training\n",
    "      loss_in_epoch_train = 0\n",
    "\n",
    "      _clf.train()\n",
    "\n",
    "      for x_cat, x_cont, targets in train_loader:\n",
    "\n",
    "          x_cat = x_cat.to(device)\n",
    "          x_cont = x_cont.to(device)\n",
    "          targets = targets.to(device)\n",
    "          # print(x_cat.is_cuda)\n",
    "\n",
    "          # reset the gradients back to zero\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          outputs = _clf(x_cat, x_cont)\n",
    "          outputs = outputs.flatten()\n",
    "\n",
    "          with torch.cuda.amp.autocast():\n",
    "            train_loss = criterion(outputs, targets)\n",
    "\n",
    "          # compute accumulated gradients\n",
    "          scaler.scale(train_loss).backward()\n",
    "\n",
    "          # perform parameter update based on current gradients\n",
    "          scaler.step(optimizer)\n",
    "          scaler.update()\n",
    "\n",
    "          # add the mini-batch training loss to epoch loss\n",
    "          loss_in_epoch_train += train_loss.item()\n",
    "\n",
    "      #     prof.step()\n",
    "      # prof.stop()\n",
    "\n",
    "      _clf.eval()\n",
    "\n",
    "      loss_in_epoch_val = 0.0\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for x_cat, x_cont, targets in val_loader:\n",
    "          x_cat = x_cat.to(device)\n",
    "          x_cont = x_cont.to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          outputs = _clf(x_cat, x_cont)\n",
    "\n",
    "          outputs = outputs.flatten()\n",
    "\n",
    "          val_loss = criterion(outputs, targets)\n",
    "          loss_in_epoch_val += val_loss.item()\n",
    "\n",
    "      train_loss = loss_in_epoch_train / len(train_loader)\n",
    "      val_loss = loss_in_epoch_val / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Downloading torchdata-0.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from torchdata) (2.27.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from torchdata) (1.26.9)\n",
      "Collecting portalocker>=2.0.0\n",
      "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting torch==1.13.0\n",
      "  Using cached torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Requirement already satisfied: typing-extensions in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from torch==1.13.0->torchdata) (4.1.1)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: setuptools in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchdata) (61.2.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests->torchdata) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests->torchdata) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/base/2022-03-30/lib/python3.8/site-packages (from requests->torchdata) (3.3)\n",
      "Installing collected packages: wheel, portalocker, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchdata\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/home/kit/stud/uloak/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/kit/stud/uloak/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.6.0 torch-1.13.0 torchdata-0.5.0 wheel-0.38.4\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/bwhpc/common/jupyter/base/2022-03-30/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes.iter as pipes\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5], [6, 7, 8]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "dp = IterableWrapper(range(10))\n",
    "dp = dp.batch(batch_size=3, drop_last=True)\n",
    "list(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PinMemory' from 'torchdata.dataloader2.adapter' (/home/kit/stud/uloak/.local/lib/python3.8/site-packages/torchdata/dataloader2/adapter.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PinMemory\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PinMemory' from 'torchdata.dataloader2.adapter' (/home/kit/stud/uloak/.local/lib/python3.8/site-packages/torchdata/dataloader2/adapter.py)"
     ]
    }
   ],
   "source": [
    "from torchdata.dataloader2.adapter import PinMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "import torcharrow.dtypes as dt\n",
    "source_data = [(i,) for i in range(3)]\n",
    "source_dp = IterableWrapper(source_data)\n",
    "DTYPE = dt.Struct([dt.Field(\"Values\", dt.int32)])\n",
    "df_dp = source_dp.dataframe(dtype=DTYPE)\n",
    "list(df_dp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(tensor([20.0000,  1.4700,  1.6200,  1.3800,  2.7300,  1.1200,  1.6200,  1.6000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  0.0000]),\n",
       "   tensor(0.)),\n",
       "  (tensor([ 20.0000,   6.2700,   6.3100,   5.8500,  10.2900,   5.9200,   7.6900,\n",
       "             6.3200,   5.8500,   6.3100, 115.0000,  11.0000,   0.0000]),\n",
       "   tensor(1.)),\n",
       "  (tensor([ 2.0000,  1.3200,  1.4400,  1.1900,  1.1900,  1.0200,  1.2500,  1.3000,\n",
       "            1.1900,  1.4400, 82.0000, 82.0000,  1.0000]),\n",
       "   tensor(1.)),\n",
       "  (tensor([ 20.0000,   1.6600,   1.7000,   1.6200,   1.6000,   1.6200,   1.6000,\n",
       "             1.6200,   1.6200,   1.7000,  99.0000, 172.0000,   0.0000]),\n",
       "   tensor(1.))],\n",
       " [(tensor([ 1.0000,  0.8500,  0.0000,  0.0000,  0.8600,  0.6500,  0.8600,  0.5000,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  0.0000]),\n",
       "   tensor(1.)),\n",
       "  (tensor([  2.0000,   0.6900,   0.7900,   0.5600,  -1.0000,   0.6000,  -1.0000,\n",
       "             0.6000,   0.5600,   0.7900, 155.0000, 155.0000,   1.0000]),\n",
       "   tensor(1.)),\n",
       "  (tensor([ 3.0000,  0.1500,  0.1800,  0.1500, -1.0000,  0.1400,  0.2100,  0.1600,\n",
       "           -1.0000, -1.0000, -1.0000, -1.0000,  1.0000]),\n",
       "   tensor(0.)),\n",
       "  (tensor([ 8.0000,  0.8000,  0.8300,  0.6100,  0.9400,  0.6800,  0.8800,  0.7300,\n",
       "            0.6100,  0.8300, 42.0000, 42.0000,  1.0000]),\n",
       "   tensor(1.))]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapipe = (pipes.IterableWrapper(training_data).batch(batch_size=4))\n",
    "\n",
    "it = iter(datapipe)\n",
    "list(islice(it, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://pytorch.org/data/main/dataloader2.html (pin memory has not been implemented yet)\n",
    "* https://pytorch.org/data/main/generated/torchdata.datapipes.iter.DataFrameMaker.html (would likely not be able to handle 'None')\n",
    "* https://pytorch.org/data/main/generated/torchdata.datapipes.iter.ParquetDataFrameLoader.html#torchdata.datapipes.iter.ParquetDataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3mUGlf-boHh",
    "outputId": "e80448e1-ace7-46e9-e294-25fc4b784891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our: 1245.7 ms\n"
     ]
    }
   ],
   "source": [
    "# warm up\n",
    "our()\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "our()\n",
    "\n",
    "torch.cuda.synchronize()  # wait for all_reduce to complete\n",
    "end.record()\n",
    "\n",
    "torch.cuda.synchronize()  # need to wait once more for op to finish\n",
    "\n",
    "our_time = start.elapsed_time(end)\n",
    "\n",
    "print(f\"our: {our_time :>5.1f} ms\")  # milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBIA3T7ndo7J",
    "outputId": "dcb6c6ff-45a5-4c71-d545-cec9b5be15ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OPTION_TYPE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm_tmpdir/job_21380449/ipykernel_3385721/629278537.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_train.drop(columns=['buy_sell'], inplace=True)\n",
      "/scratch/slurm_tmpdir/job_21380449/ipykernel_3385721/629278537.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_val.drop(columns=['buy_sell'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = X.head(100000)\n",
    "y_train = x_train['buy_sell']\n",
    "x_train.drop(columns=['buy_sell'], inplace=True)\n",
    "\n",
    "x_val = X.tail(50000)\n",
    "y_val = x_val['buy_sell']\n",
    "x_val.drop(columns=['buy_sell'], inplace=True)\n",
    "\n",
    "features = x_train.columns.tolist()\n",
    "cat_features = [\"OPTION_TYPE\"]\n",
    "\n",
    "\n",
    "_cat_idx = [features.index(i) for i in cat_features if i in features]\n",
    "\n",
    "# assume columns are duplicate free, which is standard in pandas\n",
    "cont_features = [x for x in x_train.columns.tolist() if x not in cat_features]\n",
    "_cont_idx = [features.index(i) for i in cont_features if i in features]\n",
    "\n",
    "_cat_unique = [2]\n",
    "if not _cat_unique:\n",
    "    _cat_unique = ()\n",
    "# assume columns are duplicate free, which is standard in pandas\n",
    "cont_features = [x for x in x_train.columns.tolist() if x not in cat_features]\n",
    "\n",
    "print(cat_features)\n",
    "\n",
    "# static params\n",
    "epochs = 8\n",
    "\n",
    "# FIXME: fix embedding lookup for ROOT / Symbol.\n",
    "# convert to tensor\n",
    "x_train = tensor(x_train.values).float()\n",
    "# FIXME: Integrate at another part of the code e. g., pre-processing / data set.\n",
    "x_train = torch.nan_to_num(x_train, nan=0)\n",
    "\n",
    "y_train = tensor(y_train.values).float()\n",
    "# FIXME: set -1 to 0, due to rounding before output + binary classification\n",
    "y_train[y_train < 0] = 0\n",
    "\n",
    "x_val = tensor(x_val.values).float()\n",
    "x_val = torch.nan_to_num(x_val, nan=0)\n",
    "y_val = tensor(y_val.values).float()\n",
    "y_val[y_val < 0] = 0\n",
    "\n",
    "# create training and val set\n",
    "training_data = TensorDataset(x_train, y_train)\n",
    "val_data = TensorDataset(x_val, y_val)\n",
    "\n",
    "dim: int = 64 # type: ignore\n",
    "\n",
    "depth: int = 3 \n",
    "heads: int = 8\n",
    "weight_decay: float = 1e-5\n",
    "lr = 4e-3\n",
    "dropout = 0.2\n",
    "batch_size: int = 1024\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_data, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "_clf = TabTransformer(\n",
    "    categories=_cat_unique,\n",
    "    num_continuous=len(_cont_idx),  # number of continuous values\n",
    "    dim_out=1,\n",
    "    mlp_act=nn.ReLU(),  # sigmoid of last layer already included in loss.\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    attn_dropout=dropout,\n",
    "    ff_dropout=dropout,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    ").to(device)\n",
    "\n",
    "# Generate the optimizers\n",
    "optimizer = optim.AdamW(\n",
    "    _clf.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# see https://stackoverflow.com/a/53628783/5755604\n",
    "# no sigmoid required; numerically more stable\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def their()->None:\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "      # perform training\n",
    "      loss_in_epoch_train = 0\n",
    "\n",
    "      _clf.train()\n",
    "\n",
    "      for inputs, targets in train_loader:\n",
    "\n",
    "          # FIXME: refactor to custom data loader\n",
    "          x_cat = (\n",
    "              inputs[:, _cat_idx].int().to(device) if _cat_idx else None\n",
    "          )\n",
    "\n",
    "          x_cont = inputs[:, _cont_idx].to(device)\n",
    "          targets = targets.to(device)\n",
    "\n",
    "          # reset the gradients back to zero\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          outputs = _clf(x_cat, x_cont)\n",
    "          outputs = outputs.flatten()\n",
    "\n",
    "          train_loss = criterion(outputs, targets)\n",
    "\n",
    "          # compute accumulated gradients\n",
    "          train_loss.backward()\n",
    "\n",
    "          # perform parameter update based on current gradients\n",
    "          optimizer.step()\n",
    "\n",
    "          # add the mini-batch training loss to epoch loss\n",
    "          loss_in_epoch_train += train_loss.item()\n",
    "\n",
    "      _clf.eval()\n",
    "\n",
    "      loss_in_epoch_val = 0.0\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for inputs, targets in val_loader:\n",
    "\n",
    "              x_cat = (\n",
    "                  inputs[:, _cat_idx].int().to(device)\n",
    "                  if _cat_idx\n",
    "                  else None\n",
    "              )\n",
    "              x_cont = inputs[:, _cont_idx].to(device)\n",
    "              targets = targets.to(device)\n",
    "\n",
    "              outputs = _clf(x_cat, x_cont)\n",
    "\n",
    "              outputs = outputs.flatten()\n",
    "\n",
    "              val_loss = criterion(outputs, targets)\n",
    "              loss_in_epoch_val += val_loss.item()\n",
    "\n",
    "      train_loss = loss_in_epoch_train / len(train_loader)\n",
    "      val_loss = loss_in_epoch_val / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74CfZOHchR3C",
    "outputId": "2a2d8abf-9aa2-4aca-eff2-bfa71eb07ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our: 16162.4 ms\n",
      "speedup: 12.974862389816307\n"
     ]
    }
   ],
   "source": [
    "# warm up\n",
    "their()\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "their()\n",
    "\n",
    "torch.cuda.synchronize()  # wait for all_reduce to complete\n",
    "end.record()\n",
    "\n",
    "torch.cuda.synchronize()  # need to wait once more for op to finish\n",
    "\n",
    "their_time = start.elapsed_time(end)\n",
    "print(f\"our: {their_time :>5.1f} ms\")  # milliseconds\n",
    "print(f\"speedup: {their_time / our_time}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMagLePMs2PZPiLGh1CoNWC",
   "include_colab_link": true,
   "mount_file_id": "https://github.com/KarelZe/thesis/blob/transformer/notebooks/4.0d-mb-transformer-performance-sol.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
