{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44d4a0-223b-4117-ade7-5c5cfacfa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import torch\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.classical_classifier import ClassicalClassifier\n",
    "from otc.models.fttransformer import FTTransformer\n",
    "from otc.models.tabtransformer import TabTransformer\n",
    "from otc.models.transformer_classifier import TransformerClassifier\n",
    "\n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(\n",
    "    [[1, 2, 2, 5], [1, 2, 3, 4], [1, 3, 4, 5]],\n",
    "    columns=[\"TRADE_PRICE\", \"ask_ex\", \"bid_ex\", \"unrelated\"],\n",
    ")\n",
    "y_train = pd.Series([-1, 1, -1])\n",
    "X_test = pd.DataFrame(\n",
    "    [\n",
    "        [1, 1, 3, 9],\n",
    "        [1, 1, 3, 7],\n",
    "        [1, 1, 3, 4],\n",
    "        [3, 1, 3],\n",
    "        [1, 1, 1],\n",
    "        [3, 2, 4],\n",
    "        [1, np.nan, 1],\n",
    "        [3, np.nan, np.nan],\n",
    "    ],\n",
    "    columns=[\"TRADE_PRICE\", \"ask_ex\", \"bid_ex\", \"unrelated\"],\n",
    ")\n",
    "y_test = pd.Series([-1, -1, -1, 1, 1, -1, -1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical = ClassicalClassifier(layers=[(\"quote\", \"ex\")], random_state=45)\n",
    "catboost = CatBoostClassifier(logging_level=\"Silent\")\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": 8,\n",
    "    \"shuffle\": False,\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "module_params = {\n",
    "    \"depth\": 1,\n",
    "    \"heads\": 2,\n",
    "    \"dim\": 2,\n",
    "    \"dim_out\": 1,\n",
    "    \"mlp_act\": nn.ReLU,\n",
    "    \"mlp_hidden_mults\": (4, 2),\n",
    "    \"attn_dropout\": 0.5,\n",
    "    \"ff_dropout\": 0.5,\n",
    "    \"cat_features\": [],\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"num_continuous\": X_train.shape[1],\n",
    "}\n",
    "\n",
    "optim_params = {\"lr\": 0.1, \"weight_decay\": 1e-3}\n",
    "\n",
    "# transformer = TransformerClassifier(\n",
    "#     module=TabTransformer,  # type: ignore\n",
    "#     module_params=module_params,\n",
    "#     optim_params=optim_params,\n",
    "#     dl_params=dl_params,\n",
    "#     callbacks=CallbackContainer([]),\n",
    "# )\n",
    "# transformer.epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [classical, catboost]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # shap values with kernel explainer\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb42de",
   "metadata": {},
   "source": [
    "## Compare different calculation methods for CatBoost üêà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f2b8-dd9a-405c-be61-1df3f7adc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *shap.datasets.iris(), test_size=0.2, random_state=0\n",
    ")\n",
    "shap.initjs()\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, model.predict(X_test)))\n",
    "print(model.predict_proba(X_test))\n",
    "\n",
    "\n",
    "# shap values with kernel explainer\n",
    "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1311117-fe54-4665-8b79-dbe2b1bd185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap values with tree explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904df82-7bb3-4655-8dd8-bf46216666bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://catboost.ai/en/docs/concepts/shap-values\n",
    "shap_values = model.get_feature_importance(data=Pool(X_test, y_test), type=\"ShapValues\")\n",
    "# shape (observations, features + 1 * expected_value)shap_values = model.get_feature_importance(data=Pool(X_test, y_test), type=\"ShapValues\")\n",
    "shap.summary_plot(shap_values[:, 0, :-1], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9586d12-4dd8-405c-91f7-9e795ed6f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to random feature permutation\n",
    "# https://catboost.ai/en/docs/concepts/fstr#regular-feature-importance\n",
    "model.get_feature_importance(\n",
    "    data=Pool(X_test, y_test), type=\"FeatureImportance\", prettified=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a276069-0b78-44e7-8268-9ea43b1ba006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random feature permutation sklearn\n",
    "r = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=0)\n",
    "# results are average; obviously not normalized to one.\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(\n",
    "        f\"{X_train.columns[i]}\"\n",
    "        f\"{r.importances_mean[i]:.3f}\"\n",
    "        f\" +/- {r.importances_std[i]:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ba17c-50af-4f24-9cf5-b60a95057020",
   "metadata": {},
   "source": [
    "## Attention Maps for Transformers\n",
    "\n",
    "We calculate the average attention map from all transformer blocks, as done in the Gorishniy paper (see [here](https://github.com/Yura52/tabular-dl-revisiting-models/issues/2)). This is different from the Borisov paper (see [here](https://github.com/kathrinse/TabSurvey/blob/main/models/basemodel_torch.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dad110-fac4-4b88-920c-0f4d5690eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.tabtransformer import TabTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252711f-a31e-4ee6-91be-be4163e94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_cont = 5\n",
    "num_features_cat = 3\n",
    "num_unique_cat = tuple([2, 2, 2])\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(device)\n",
    "x_cont = torch.randn(batch_size, num_features_cont).float().to(device)\n",
    "expected_outputs = torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "\n",
    "model = TabTransformer(\n",
    "    cat_cardinalities=num_unique_cat,\n",
    "    num_continuous=num_features_cont,\n",
    "    dim_out=1,\n",
    "    mlp_act=nn.ReLU,\n",
    "    dim=32,\n",
    "    depth=2,\n",
    "    heads=6,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5018b6-3993-4cd2-a06e-efc37e62f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveAttentionMaps:\n",
    "    \"\"\"\n",
    "    Hook for attention maps.\n",
    "\n",
    "    Inspired by:\n",
    "    https://github.com/Yura52/tabular-dl-revisiting-models/issues/2#issuecomment-1068123629\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.attention_maps: List[torch.Tensor] = []\n",
    "\n",
    "    def __call__(self, _, __, output):\n",
    "        print(output[1][\"attention_probs\"].shape)\n",
    "        self.attention_maps.append(output[1][\"attention_probs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759907c7-07e5-4675-9cbc-5f9c0cee1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following hook will save all attention maps from all attention modules.\n",
    "hook = SaveAttentionMaps()\n",
    "for block in model.transformer.blocks:\n",
    "    block.attention.fn.fn.register_forward_hook(hook)\n",
    "\n",
    "# Apply the model to all objects.\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    model(x_cat.clone(), x_cont.clone())\n",
    "\n",
    "# Collect attention maps\n",
    "n_objects = len(x_cat)\n",
    "n_blocks = len(model.transformer.blocks)\n",
    "n_heads = model.transformer.blocks[0].attention.fn.fn.n_heads\n",
    "\n",
    "attention_maps = torch.cat(hook.attention_maps)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "attention_maps = attention_maps.reshape(\n",
    "    n_objects * n_blocks * n_heads, num_features_cat, num_features_cat\n",
    ")\n",
    "assert attention_maps.shape == (\n",
    "    n_objects * n_blocks * n_heads,\n",
    "    num_features_cat,\n",
    "    num_features_cat,\n",
    ")\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "average_attention_map = attention_maps.mean(0)\n",
    "feature_importance = average_attention_map[-1]\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_ranks = scipy.stats.rankdata(-feature_importance)\n",
    "feature_indices_sorted_by_importance = feature_importance.argsort()[::-1]\n",
    "\n",
    "print(feature_importance)\n",
    "print(feature_ranks)\n",
    "print(feature_indices_sorted_by_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005bccf-f32e-490f-bb59-8b8d658f3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=feature_importance, y=[\"f1\", \"f2\", \"f3\"])\n",
    "ax.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af3d57-8812-43dc-99bd-ab1a45cbc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CategoricalFeatureTokenizer,\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    MultiheadAttention,\n",
    "    NumericalFeatureTokenizer,\n",
    "    Transformer,\n",
    ")\n",
    "\n",
    "num_features_cont = 5\n",
    "num_features_cat = 1\n",
    "cat_cardinalities = [2]\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(device)\n",
    "x_cont = torch.randn(batch_size, num_features_cont).float().to(device)\n",
    "expected_outputs = torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "\n",
    "params_feature_tokenizer = {\n",
    "    \"num_continous\": num_features_cont,\n",
    "    \"cat_cardinalities\": cat_cardinalities,\n",
    "    \"d_token\": 96,\n",
    "}\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "params_transformer = {\n",
    "    \"d_token\": 96,\n",
    "    \"n_blocks\": 3,\n",
    "    \"attention_n_heads\": 8,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": 0.1,\n",
    "    \"ffn_d_hidden\": 96 * 2,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.ReLU,\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,\n",
    "}\n",
    "\n",
    "transformer = Transformer(**params_transformer)\n",
    "\n",
    "model = FTTransformer(feature_tokenizer, transformer).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2a775-30cc-4d4c-9edd-59c018fc0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and model.\n",
    "n_objects = len(x_cat)  # 12\n",
    "n_features = num_features_cont + num_features_cat\n",
    "\n",
    "# The following hook will save all attention maps from all attention modules.\n",
    "hook = SaveAttentionMaps()\n",
    "for block in model.transformer.blocks:\n",
    "    block.attention.register_forward_hook(hook)\n",
    "\n",
    "# Apply the model to all objects.\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    model(x_cat, x_cont)\n",
    "\n",
    "# Collect attention maps\n",
    "n_blocks = len(model.transformer.blocks)\n",
    "n_heads = model.transformer.blocks[0].attention.n_heads\n",
    "n_tokens = n_features + 1\n",
    "attention_maps = torch.cat(hook.attention_maps)\n",
    "assert attention_maps.shape == (n_objects * n_blocks * n_heads, n_tokens, n_tokens)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "average_attention_map = attention_maps.mean(0)\n",
    "average_cls_attention_map = average_attention_map[-1]  # consider only the [CLS] token\n",
    "feature_importance = average_cls_attention_map[:-1]  # drop the [CLS] token importance\n",
    "assert feature_importance.shape == (n_features,)\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_ranks = scipy.stats.rankdata(-feature_importance)\n",
    "feature_indices_sorted_by_importance = feature_importance.argsort()[::-1]\n",
    "\n",
    "print(feature_importance)\n",
    "print(feature_ranks)\n",
    "print(feature_indices_sorted_by_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb4596-2d71-45ad-a746-a4f2cda29efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=feature_importance, y=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\"])\n",
    "ax.set(xlim=(0, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8ea8b642289b706932f10b33ee389827410dbaef0ce2c5bf73615e8d3267d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
