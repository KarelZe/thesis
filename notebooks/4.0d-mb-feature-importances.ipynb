{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44d4a0-223b-4117-ade7-5c5cfacfa28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "# import shap\n",
    "import sklearn\n",
    "# import torch\n",
    "#from catboost import CatBoostClassifier, Pool\n",
    "# from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.classical_classifier import ClassicalClassifier\n",
    "# from otc.models.fttransformer import FTTransformer\n",
    "# from otc.models.tabtransformer import TabTransformer\n",
    "# from otc.models.transformer_classifier import TransformerClassifier\n",
    "\n",
    "\n",
    "from otc.features.build_features import (\n",
    "    features_categorical,\n",
    "    features_classical,\n",
    "    features_classical_size,\n",
    "    features_ml,\n",
    ")\n",
    "# shap.initjs()\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22901e37-f5e0-43fa-a489-8581b094f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# set globally here\n",
    "EXCHANGE = \"ise\"  \n",
    "STRATEGY = \"supervised\"  \n",
    "SUBSET = \"test\"  \n",
    "\n",
    "# Change mode depending on model!\n",
    "MODE = \"none\" # \"log_standardized\"\n",
    "# Change depending on model!\n",
    "FEATURES = features_classical_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583efa12-cbd0-4d1b-a155-9040ce7d10b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# key used for files and artefacts\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_{MODE}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f00103-842c-473b-80d7-05e57f6ee25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set project name. Required to access files and artefacts\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334b77d-6ec7-44cc-b57d-94977a69d438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see https://wandb.ai/fbv/thesis/runs/kwlaw02g/overview?workspace=user-karelze\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfd371-1b90-4133-8e7c-6b8a6afb4c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    *FEATURES,\n",
    "    \"buy_sell\",\n",
    "]\n",
    "data = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=columns)\n",
    "\n",
    "y_test = data[\"buy_sell\"]\n",
    "X_test = data.drop(columns=\"buy_sell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf137a6-6bcf-4be0-b442-85558638299f",
   "metadata": {},
   "source": [
    "## Define dependency structure for permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80fbbbe-2f4d-4c4c-85b6-58555994974f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features classical\n",
    "n1 = ['TRADE_PRICE', 'bid_ex', 'ask_ex', 'prox_ex']\n",
    "n2 = ['TRADE_PRICE', 'BEST_ASK', 'BEST_BID', 'prox_best']\n",
    "n3 = ['TRADE_PRICE', 'price_ex_lag', 'price_all_lag',  'chg_ex_lag' ,  'chg_all_lag' ]\n",
    "n4 = ['TRADE_PRICE', 'price_ex_lead', 'price_all_lead', 'chg_ex_lead', 'chg_all_lead']\n",
    "\n",
    "n12 = list(set(n1) | set(n2))\n",
    "n34 = list(set(n3) | set(n4))\n",
    "n_classical = list(set(n12) | set(n34))\n",
    "\n",
    "\n",
    "# features size\n",
    "n_size = ['TRADE_SIZE','ask_size_ex','bid_ask_size_ratio_ex',\n",
    "      'bid_size_ex','depth_ex','rel_ask_size_ex','rel_bid_size_ex']\n",
    "\n",
    "# ml features\n",
    "n6 = [\"issue_type\"]\n",
    "n7 = [\"option_type\"]\n",
    "n8 = [\"root\"]\n",
    "n9 = [\"day_vol\"]\n",
    "n10 = ['TRADE_PRICE', \"myn\", 'STRK_PRC', \"ttm\"]\n",
    "\n",
    "n_option = [*n6, *n7, *n8, *n9, *n10]\n",
    "\n",
    "if FEATURES == features_classical:\n",
    "    permutations = [n1, n2, n3, n4]\n",
    "if FEATURES == features_classical_size:\n",
    "    permutations = [n1, n2, n3, n4, n_classical, n_size]\n",
    "if FEATURES == features_ml:\n",
    "    permutations = [n1,n2,n3,n4,n_classical,n_size,n6,n7,n8,n9,n10,n_option]\n",
    "\n",
    "print(permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2e08c-a2e0-4880-ba21-6bcbc1bfea70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = ClassicalClassifier(layers=[(\"trade_size\", \"ex\"), (\"rev_lr\", \"best\")], \n",
    "                                  random_state=SEED, strategy=\"random\")\n",
    "# fit is only used to set sklearn attributes, no leakage\n",
    "clf.fit(X=X_test.head(5), y=y_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e1c8b-1e3d-4f60-bab7-f0fb9094fe57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_repeats = 2\n",
    "\n",
    "base_acc = clf.score(X_test, y_test)\n",
    "\n",
    "results = []\n",
    "for permutation in tqdm(permutations):\n",
    "    accuracies_iter = [base_acc]\n",
    "    indices_iter = [\"base\"]\n",
    "    # similar to Fisher et. al permute multiple times\n",
    "    for i in tqdm(range(n_repeats)):\n",
    "        \n",
    "        # generate random permutation\n",
    "        np.random.seed(i) \n",
    "        permuted_indices = np.random.permutation(len(X_test))  \n",
    "        X_test_perm = X_test.copy()\n",
    "        y_test_perm = y_test.copy()\n",
    "        \n",
    "        # permute relevant columns\n",
    "        X_test_perm[permutation] = X_test_perm[permutation].values[permuted_indices]      \n",
    "        perm_acc = clf.score(X_test_perm, y_test)\n",
    "        \n",
    "        # store raw scores to estimate uncertainties etc. Calculate change later.\n",
    "        accuracies_iter.append(perm_acc)\n",
    "        indices_iter.append(f\"iter-{i}\")\n",
    "        \n",
    "    results.append(pd.DataFrame(accuracies_iter, index = indices_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c1d04-36b3-43de-a83a-f596bf0ccd51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keys = [\"/\".join(p) for p in permutations]\n",
    "\n",
    "joint_results = pd.concat(results, axis=1, keys=keys)\n",
    "joint_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f2b8-dd9a-405c-be61-1df3f7adc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *shap.datasets.iris(), test_size=0.2, random_state=0\n",
    ")\n",
    "shap.initjs()\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, model.predict(X_test)))\n",
    "print(model.predict_proba(X_test))\n",
    "\n",
    "\n",
    "# shap values with kernel explainer\n",
    "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1311117-fe54-4665-8b79-dbe2b1bd185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap values with tree explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904df82-7bb3-4655-8dd8-bf46216666bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://catboost.ai/en/docs/concepts/shap-values\n",
    "shap_values = model.get_feature_importance(data=Pool(X_test, y_test), type=\"ShapValues\")\n",
    "# shape (observations, features + 1 * expected_value)shap_values = model.get_feature_importance(data=Pool(X_test, y_test), type=\"ShapValues\")\n",
    "shap.summary_plot(shap_values[:, 0, :-1], X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9586d12-4dd8-405c-91f7-9e795ed6f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to random feature permutation\n",
    "# https://catboost.ai/en/docs/concepts/fstr#regular-feature-importance\n",
    "model.get_feature_importance(\n",
    "    data=Pool(X_test, y_test), type=\"FeatureImportance\", prettified=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a276069-0b78-44e7-8268-9ea43b1ba006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random feature permutation sklearn\n",
    "r = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=0)\n",
    "# results are average; obviously not normalized to one.\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(\n",
    "        f\"{X_train.columns[i]}\"\n",
    "        f\"{r.importances_mean[i]:.3f}\"\n",
    "        f\" +/- {r.importances_std[i]:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ba17c-50af-4f24-9cf5-b60a95057020",
   "metadata": {},
   "source": [
    "## Attention Maps for Transformers\n",
    "\n",
    "We calculate the average attention map from all transformer blocks, as done in the Gorishniy paper (see [here](https://github.com/Yura52/tabular-dl-revisiting-models/issues/2)). This is different from the Borisov paper (see [here](https://github.com/kathrinse/TabSurvey/blob/main/models/basemodel_torch.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dad110-fac4-4b88-920c-0f4d5690eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.tabtransformer import TabTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252711f-a31e-4ee6-91be-be4163e94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_cont = 5\n",
    "num_features_cat = 3\n",
    "num_unique_cat = tuple([2, 2, 2])\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(device)\n",
    "x_cont = torch.randn(batch_size, num_features_cont).float().to(device)\n",
    "expected_outputs = torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "\n",
    "model = TabTransformer(\n",
    "    cat_cardinalities=num_unique_cat,\n",
    "    num_continuous=num_features_cont,\n",
    "    dim_out=1,\n",
    "    mlp_act=nn.ReLU,\n",
    "    dim=32,\n",
    "    depth=2,\n",
    "    heads=6,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5018b6-3993-4cd2-a06e-efc37e62f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveAttentionMaps:\n",
    "    \"\"\"\n",
    "    Hook for attention maps.\n",
    "\n",
    "    Inspired by:\n",
    "    https://github.com/Yura52/tabular-dl-revisiting-models/issues/2#issuecomment-1068123629\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.attention_maps: List[torch.Tensor] = []\n",
    "\n",
    "    def __call__(self, _, __, output):\n",
    "        print(output[1][\"attention_probs\"].shape)\n",
    "        self.attention_maps.append(output[1][\"attention_probs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759907c7-07e5-4675-9cbc-5f9c0cee1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following hook will save all attention maps from all attention modules.\n",
    "hook = SaveAttentionMaps()\n",
    "for block in model.transformer.blocks:\n",
    "    block.attention.fn.fn.register_forward_hook(hook)\n",
    "\n",
    "# Apply the model to all objects.\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    model(x_cat.clone(), x_cont.clone())\n",
    "\n",
    "# Collect attention maps\n",
    "n_objects = len(x_cat)\n",
    "n_blocks = len(model.transformer.blocks)\n",
    "n_heads = model.transformer.blocks[0].attention.fn.fn.n_heads\n",
    "\n",
    "attention_maps = torch.cat(hook.attention_maps)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "attention_maps = attention_maps.reshape(\n",
    "    n_objects * n_blocks * n_heads, num_features_cat, num_features_cat\n",
    ")\n",
    "assert attention_maps.shape == (\n",
    "    n_objects * n_blocks * n_heads,\n",
    "    num_features_cat,\n",
    "    num_features_cat,\n",
    ")\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "average_attention_map = attention_maps.mean(0)\n",
    "feature_importance = average_attention_map[-1]\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_ranks = scipy.stats.rankdata(-feature_importance)\n",
    "feature_indices_sorted_by_importance = feature_importance.argsort()[::-1]\n",
    "\n",
    "print(feature_importance)\n",
    "print(feature_ranks)\n",
    "print(feature_indices_sorted_by_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005bccf-f32e-490f-bb59-8b8d658f3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=feature_importance, y=[\"f1\", \"f2\", \"f3\"])\n",
    "ax.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af3d57-8812-43dc-99bd-ab1a45cbc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CategoricalFeatureTokenizer,\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    MultiheadAttention,\n",
    "    NumericalFeatureTokenizer,\n",
    "    Transformer,\n",
    ")\n",
    "\n",
    "num_features_cont = 5\n",
    "num_features_cat = 1\n",
    "cat_cardinalities = [2]\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(device)\n",
    "x_cont = torch.randn(batch_size, num_features_cont).float().to(device)\n",
    "expected_outputs = torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "\n",
    "params_feature_tokenizer = {\n",
    "    \"num_continous\": num_features_cont,\n",
    "    \"cat_cardinalities\": cat_cardinalities,\n",
    "    \"d_token\": 96,\n",
    "}\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "params_transformer = {\n",
    "    \"d_token\": 96,\n",
    "    \"n_blocks\": 3,\n",
    "    \"attention_n_heads\": 8,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": 0.1,\n",
    "    \"ffn_d_hidden\": 96 * 2,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.ReLU,\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,\n",
    "}\n",
    "\n",
    "transformer = Transformer(**params_transformer)\n",
    "\n",
    "model = FTTransformer(feature_tokenizer, transformer).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2a775-30cc-4d4c-9edd-59c018fc0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and model.\n",
    "n_objects = len(x_cat)  # 12\n",
    "n_features = num_features_cont + num_features_cat\n",
    "\n",
    "# The following hook will save all attention maps from all attention modules.\n",
    "hook = SaveAttentionMaps()\n",
    "for block in model.transformer.blocks:\n",
    "    block.attention.register_forward_hook(hook)\n",
    "\n",
    "# Apply the model to all objects.\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    model(x_cat, x_cont)\n",
    "\n",
    "# Collect attention maps\n",
    "n_blocks = len(model.transformer.blocks)\n",
    "n_heads = model.transformer.blocks[0].attention.n_heads\n",
    "n_tokens = n_features + 1\n",
    "attention_maps = torch.cat(hook.attention_maps)\n",
    "assert attention_maps.shape == (n_objects * n_blocks * n_heads, n_tokens, n_tokens)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "average_attention_map = attention_maps.mean(0)\n",
    "average_cls_attention_map = average_attention_map[-1]  # consider only the [CLS] token\n",
    "feature_importance = average_cls_attention_map[:-1]  # drop the [CLS] token importance\n",
    "assert feature_importance.shape == (n_features,)\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_ranks = scipy.stats.rankdata(-feature_importance)\n",
    "feature_indices_sorted_by_importance = feature_importance.argsort()[::-1]\n",
    "\n",
    "print(feature_importance)\n",
    "print(feature_ranks)\n",
    "print(feature_indices_sorted_by_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb4596-2d71-45ad-a746-a4f2cda29efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=feature_importance, y=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\"])\n",
    "ax.set(xlim=(0, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfclf-last",
   "language": "python",
   "name": "tfclf-last"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8ea8b642289b706932f10b33ee389827410dbaef0ce2c5bf73615e8d3267d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
