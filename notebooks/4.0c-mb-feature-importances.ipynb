{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b20cffb9-5df8-4606-8b33-f7abf7429842",
   "metadata": {},
   "source": [
    "Do custom install of `sage-importance`\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/karelze/sage.git\n",
    "cd sage\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44d4a0-223b-4117-ade7-5c5cfacfa28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.classical_classifier import ClassicalClassifier\n",
    "\n",
    "from sage import GroupedMarginalImputer, PermutationEstimator\n",
    "\n",
    "from otc.features.build_features import (\n",
    "    features_categorical,\n",
    "    features_classical,\n",
    "    features_classical_size,\n",
    "    features_ml,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22901e37-f5e0-43fa-a489-8581b094f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# set globally here\n",
    "EXCHANGE = \"ise\"  \n",
    "STRATEGY = \"supervised\"  \n",
    "SUBSET = \"test\"  \n",
    "\n",
    "# Change depending on model!\n",
    "FEATURES = features_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f00103-842c-473b-80d7-05e57f6ee25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set project name. Required to access files and artefacts\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64241e2b",
   "metadata": {},
   "source": [
    "## Sage Valuesüåµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22968e-d4e5-4473-bddb-71af4b8bbddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_groups(feature_names, feature_str):\n",
    "\n",
    "    fg_classical = {\n",
    "        'chg_all_lead (grouped)': ['price_all_lead', 'chg_all_lead'],\n",
    "        'chg_all_lag (grouped)': ['price_all_lag', 'chg_ex_lag'],\n",
    "        'chg_ex_lead (grouped)': ['price_ex_lead', 'chg_ex_lead', 'chg_all_lag'],\n",
    "        'chg_ex_lag (grouped)': ['price_ex_lag'],\n",
    "        'quote_best (grouped)': ['BEST_ASK', 'BEST_BID', 'prox_best'],\n",
    "        'quote_ex (grouped)': ['bid_ex', 'ask_ex','prox_ex' ],\n",
    "        'TRADE_PRICE': ['TRADE_PRICE'],\n",
    "        }\n",
    "    \n",
    "    fg_size = {'size_ex (grouped)': [ 'bid_ask_size_ratio_ex', 'rel_bid_size_ex',  'rel_ask_size_ex', 'bid_size_ex', 'ask_size_ex','depth_ex'], 'TRADE_SIZE': ['TRADE_SIZE']}\n",
    "    \n",
    "    fg_ml = {\n",
    "        \"STRK_PRC\": [\"STRK_PRC\"],\n",
    "        \"ttm\": [\"ttm\"],\n",
    "        \"option_type\": [\"option_type\"],\n",
    "        \"root\":[\"root\"],\n",
    "        \"myn\":[\"myn\"],\n",
    "        \"day_vol\":[\"day_vol\"], \n",
    "        \"issue_type\":[\"issue_type\"],\n",
    "    }\n",
    "    \n",
    "    if feature_str == \"classical\":\n",
    "        feature_groups = group_names = fg_classical    \n",
    "    if feature_str == \"classical-size\":\n",
    "        feature_groups = group_names = fg_classical | fg_size\n",
    "    if feature_str == \"ml\":\n",
    "        feature_groups = group_names = fg_classical | fg_size | fg_ml      \n",
    "    \n",
    "\n",
    "    # Group indices\n",
    "    groups = []\n",
    "    for _, group in feature_groups.items():\n",
    "        ind_list = []\n",
    "        for feature in group:\n",
    "            ind_list.append(feature_names.index(feature))\n",
    "        groups.append(ind_list)\n",
    "\n",
    "    return groups, group_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1d386-c864-4b9b-bf7b-15e683021531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load unscaled data for classical classifier\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_none:latest\"\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "data = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=[*features_classical_size, \"buy_sell\"])\n",
    "\n",
    "y_test = data[\"buy_sell\"]\n",
    "X_test = data.drop(columns=\"buy_sell\")\n",
    "\n",
    "feature_names = X_test.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88b949e",
   "metadata": {},
   "source": [
    "### Classical Classifierüè¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2d667-6fc4-4946-8445-389998654f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 1024 # 4x the recommended default\n",
    "\n",
    "idx = np.random.choice(y_test.index, size=sample_size, replace=False)\n",
    "\n",
    "X_importance = X_test.loc[idx]\n",
    "y_importance = y_test.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f011f3-149e-47ef-b565-ff60d6751c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare benchmarks\n",
    "configs = [\n",
    "    [(\"quote\", \"best\"), (\"quote\", \"ex\")],\n",
    "    [(\"trade_size\", \"ex\"), (\"quote\", \"best\"),  (\"quote\", \"ex\"), (\"depth\", \"best\"), (\"depth\", \"ex\"), (\"rev_tick\", \"all\")]  \n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    \n",
    "    groups, group_names = get_feature_groups(X_importance.columns.tolist(), \"classical-size\")\n",
    "    \n",
    "    clf = ClassicalClassifier(layers=config, random_state=SEED, strategy=\"random\")\n",
    "    # only set headers etc, no leakage\n",
    "    clf.fit(X=X_test.head(5), y=y_test.head(5))\n",
    "\n",
    "    # apply group based imputation + estimate importances in terms of zero-one loss\n",
    "    imputer = GroupedMarginalImputer(clf, X_importance.values, groups)\n",
    "    estimator = PermutationEstimator(imputer, \"zero one\")\n",
    "    \n",
    "    # calculate values over entire test set\n",
    "    sage_values = estimator(X_test.values, y_test.values)\n",
    "    \n",
    "    # save sage values + std deviation to data frame\n",
    "    result = pd.DataFrame(index=group_names, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ac26f-9e44-432c-beb2-0aacd91573f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate names for df\n",
    "names = []\n",
    "\n",
    "# generate human readable names like quote(best)->quote(ex)\n",
    "for r in tqdm(configs):\n",
    "    name = \"->\".join(\"%s(%s)\" % tup for tup in r)\n",
    "    names.append(name)\n",
    "\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "\n",
    "# flatten column names (required to save to parquet)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9c24d-2374-4ac5-a9fd-60d94243a5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65310c-da57-4ad7-a9c9-eeab69d4c5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}_classical_feature_importance\"\n",
    "\n",
    "URI_FI_CLASSICAL = f\"gs://thesis-bucket-option-trade-classification/data/results/{KEY}.parquet\"\n",
    "\n",
    "results_df.to_parquet(URI_FI_CLASSICAL)\n",
    "\n",
    "result_set = wandb.Artifact(name=KEY, type=\"results\")\n",
    "result_set.add_reference(URI_FI_CLASSICAL, name=\"results\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efa51927",
   "metadata": {},
   "source": [
    "### Gradient Boosting üêà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ab3d-f8bb-4fa0-9ea2-6d4505fd3e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_MAP = {\n",
    "    \"classical\": features_classical,\n",
    "    \"classical-size\": features_classical_size,\n",
    "    \"ml\": features_ml,\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "# load processed data for gradient-boosting\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_log_standardized_clipped:latest\"\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "data = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=[*features_ml, \"buy_sell\"])\n",
    "\n",
    "y_test = data[\"buy_sell\"]\n",
    "X_test = data.drop(columns=\"buy_sell\")\n",
    "\n",
    "feature_names = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28458daa-362a-47f1-ba78-a4cf0f05d40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 256\n",
    "\n",
    "idx = np.random.choice(y_test.index, size=sample_size, replace=False)\n",
    "\n",
    "X_importance = X_test.loc[idx]\n",
    "y_importance = y_test.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef3a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs = [(\"classical\", \"1gzk7msy_CatBoostClassifier_default.cbm:latest\"),\n",
    "    (\"classical-size\", \"3vntumoi_CatBoostClassifier_default.cbm:latest\"),\n",
    "    (\"ml\", \"2t5zo50f_CatBoostClassifier_default.cbm:latest\")]\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_str, model in configs:\n",
    "    \n",
    "    # get feature names and slice to subset\n",
    "    fs = FEATURE_MAP.get(feature_str)\n",
    "    X_importance_fs = X_importance.loc[:, fs]\n",
    "    X_importance_cols = X_importance_fs.columns.tolist()\n",
    "    \n",
    "    # calculate cat indices\n",
    "    if feature_str == \"ml\":\n",
    "        cat_features = [t[0] for t in features_categorical]\n",
    "        cat_idx = [X_importance_cols.index(f) for f in cat_features]\n",
    "    \n",
    "    # get groups\n",
    "    groups, group_names = get_feature_groups(X_importance_cols, feature_str)\n",
    "    \n",
    "    #  load model by identifier from wandb\n",
    "    model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "    \n",
    "    artifact = run.use_artifact(model)\n",
    "    model_dir = artifact.download()\n",
    "    clf = CatBoostClassifier()\n",
    "    clf.load_model(fname=Path(model_dir, model_name))\n",
    "\n",
    "    # use callable instead of default catboost as it doesn't work with categoricals otherwise\n",
    "    def call_catboost(X):\n",
    "        if feature_str == \"ml\":       \n",
    "            # convert categorical to int\n",
    "            X = pd.DataFrame(X, columns=X_importance.columns)\n",
    "            # Update the selected columns in the original DataFrame\n",
    "            X[cat_features] = X.iloc[:, cat_idx].astype(int)\n",
    "            # pass cat indices\n",
    "            return clf.predict_proba(Pool(X, cat_features=cat_idx))\n",
    "        else:\n",
    "            return clf.predict_proba(X)\n",
    "    \n",
    "    # apply group based imputation + estimate importances in terms of zero-one loss\n",
    "    imputer = GroupedMarginalImputer(call_catboost, X_importance_fs, groups)\n",
    "    estimator = PermutationEstimator(imputer, \"zero one\")\n",
    "    \n",
    "    # calculate values over entire test set\n",
    "    sage_values = estimator(X_test.loc[:,fs].values, y_test.values)\n",
    "    \n",
    "    # save sage values + std deviation to data frame\n",
    "    result = pd.DataFrame(index=group_names, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list to data frame + set human readable names\n",
    "names = [f\"gbm({feature_str[0]})\" for feature_str in configs]\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]\n",
    "\n",
    "# save to google clound and save identiifer\n",
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}_gbm_feature_importance\"\n",
    "\n",
    "URI_FI_GBM = f\"gs://thesis-bucket-option-trade-classification/data/results/{KEY}.parquet\"\n",
    "\n",
    "results_df.to_parquet(URI_FI_GBM)\n",
    "\n",
    "result_set = wandb.Artifact(name=KEY, type=\"results\")\n",
    "result_set.add_reference(URI_FI_GBM, name=\"results\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b26639ad",
   "metadata": {},
   "source": [
    "### Transformer Classifier ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad9295-78c8-44c0-9078-1994d3b048d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"classical\", \"3jpe46s1_TransformerClassifier_default.pkl:latest\"),\n",
    "    (\"classical-size\", \"1qx3ul4j_TransformerClassifier_default.pkl:latest\"), \n",
    "    (\"ml\", \"2h81aiow_TransformerClassifier_default.pkl:latest\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_str, model in configs:\n",
    "    # load model by identifier from wandb\n",
    "    model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "\n",
    "    # get feature names and slice to subset\n",
    "    fs = FEATURE_MAP.get(feature_str)\n",
    "    X_importance_fs = X_importance.loc[:, fs]\n",
    "    X_importance_cols = X_importance_fs.columns.tolist()\n",
    "    \n",
    "    # calculate cat indices\n",
    "    if feature_str == \"ml\":\n",
    "        cat_features = [t[0] for t in features_categorical]\n",
    "        cat_idx = [X_importance_cols.index(f) for f in cat_features]\n",
    "    \n",
    "    # get groups\n",
    "    groups, group_names = get_feature_groups(X_importance_cols, feature_str)\n",
    "    \n",
    "    model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "\n",
    "    artifact = run.use_artifact(model)\n",
    "    model_dir = artifact.download()\n",
    "\n",
    "    with open(Path(model_dir, model_name), 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "    \n",
    "    # apply group based imputation + estimate importances in terms of zero-one loss\n",
    "    imputer = GroupedMarginalImputer(clf, X_importance_fs, groups)\n",
    "    estimator = PermutationEstimator(imputer, \"zero one\")\n",
    "    \n",
    "    # calculate values over entire test set\n",
    "    sage_values = estimator(X_test.loc[:,fs].values, y_test.values)\n",
    "    \n",
    "    # save sage values + std deviation to data frame\n",
    "    result = pd.DataFrame(index=group_names, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b4c55-f424-47be-bb7e-0da20cd0b843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list to data frame + set human readable names\n",
    "names = [f\"fttransformer({feature_str[0]})\" for feature_str in configs]\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]\n",
    "\n",
    "# save to google clound and save identiifer\n",
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}_fttransformer_feature_importance\"\n",
    "\n",
    "URI_FI_FTTRANSFORMER = f\"gs://thesis-bucket-option-trade-classification/data/results/{KEY}.parquet\"\n",
    "\n",
    "results_df.to_parquet(URI_FI_FTTRANSFORMER)\n",
    "\n",
    "result_set = wandb.Artifact(name=KEY, type=\"results\")\n",
    "result_set.add_reference(URI_FI_FTTRANSFORMER, name=\"results\")\n",
    "run.log_artifact(result_set)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a13ba17c-50af-4f24-9cf5-b60a95057020",
   "metadata": {},
   "source": [
    "## Attention Maps for Transformers\n",
    "\n",
    "We calculate the average attention map from all transformer blocks, as done in the [here](https://github.com/hila-chefer/Transformer-MM-Explainability/blob/main/lxmert/lxmert/src/ExplanationGenerator.py#L26) and [here](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb#scrollTo=fWKGyu2YAeSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    Transformer,\n",
    ")\n",
    "\n",
    "num_features_cont = 5\n",
    "num_features_cat = 1\n",
    "cat_cardinalities = [2]\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(device)\n",
    "x_cont = torch.randn(batch_size, num_features_cont).float().to(device)\n",
    "expected_outputs = torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "\n",
    "x_cont.requires_grad_(True)\n",
    "expected_outputs.requires_grad_(True)\n",
    "\n",
    "params_feature_tokenizer = {\n",
    "    \"num_continous\": num_features_cont,\n",
    "    \"cat_cardinalities\": cat_cardinalities,\n",
    "    \"d_token\": 96,\n",
    "}\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "params_transformer = {\n",
    "    \"d_token\": 96,\n",
    "    \"n_blocks\": 3,\n",
    "    \"attention_n_heads\": 8,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": 0.1,\n",
    "    \"ffn_d_hidden\": 96 * 2,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.ReLU,\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,\n",
    "}\n",
    "\n",
    "transformer = Transformer(**params_transformer)\n",
    "\n",
    "model = FTTransformer(feature_tokenizer, transformer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459439c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Prepare data and model.\n",
    "n_objects = len(x_cat)  # 12\n",
    "n_features = num_features_cont + num_features_cat\n",
    "\n",
    "# apply the model to all objects.\n",
    "out = model(x_cat, x_cont)\n",
    "\n",
    "# calculate outputs\n",
    "logits = model(x_cat, x_cont)\n",
    "# zero gradients\n",
    "model.zero_grad()\n",
    "# loss + backward pass\n",
    "loss = criterion(logits, expected_outputs)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3065f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hila-chefer/Transformer-MM-Explainability/blob/main/lxmert/lxmert/src/ExplanationGenerator.py#L26\n",
    "# https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb#scrollTo=fWKGyu2YAeSV\n",
    "\n",
    "attn_block = model.transformer.blocks[0].attention.get_attn()\n",
    "# cat + cont + [CLS]\n",
    "n_tokens = attn_block.shape[-1]\n",
    "# residual connection. Repeat along batch dimension\n",
    "res = torch.eye(n_tokens, n_tokens).to(device)\n",
    "res = res.unsqueeze(0).expand(batch_size, n_tokens, n_tokens)\n",
    "\n",
    "# one_hot = expected_outputs.sum()\n",
    "# one_hot.backward(retain_graph=True)\n",
    "\n",
    "for i, block in enumerate(model.transformer.blocks):\n",
    "\n",
    "    grad = block.attention.get_attn_gradients().detach()\n",
    "    cam = block.attention.get_attn().detach()\n",
    "\n",
    "    # reshape to [batch_size x num_head, num_tokens, num_tokens]\n",
    "    cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "    grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "    \n",
    "    # dot product\n",
    "    cam = grad * cam\n",
    "    \n",
    "    # reshape to [batch_size, num_head, num_tokens, num_tokens]\n",
    "    cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "    # clamp negative values, calculate mean over heads\n",
    "    cam = cam.clamp(min=0).mean(dim=1)\n",
    "    res = res + torch.bmm(cam, res)\n",
    "\n",
    "relevancy = res\n",
    "# disregard the first token, which is the [CLS] token\n",
    "# relevancy[:,0,0] = 0\n",
    "\n",
    "# in FT-Transformer token is appended / preprended to the end of the sequenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90667b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first attention map from batch and visualize\n",
    "test = relevancy[0].detach().cpu().numpy()\n",
    "plt.imshow(test, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
