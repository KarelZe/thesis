{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20cffb9-5df8-4606-8b33-f7abf7429842",
   "metadata": {},
   "source": [
    "Do custom install of `sage-importance`\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/karelze/sage.git\n",
    "cd sage\n",
    "pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44d4a0-223b-4117-ade7-5c5cfacfa28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter, PercentFormatter\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.classical_classifier import ClassicalClassifier\n",
    "\n",
    "from sage import GroupedMarginalImputer, PermutationEstimator, MarginalImputer\n",
    "\n",
    "from otc.features.build_features import (\n",
    "    features_categorical,\n",
    "    features_classical,\n",
    "    features_classical_size,\n",
    "    features_ml,\n",
    ")\n",
    "\n",
    "from otc.models.fttransformer import FeatureTokenizer, FTTransformer, Transformer\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping\n",
    "from otc.optim.scheduler import CosineWarmupScheduler\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22901e37-f5e0-43fa-a489-8581b094f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "# set globally here\n",
    "EXCHANGE = \"ise\"  \n",
    "STRATEGY = \"supervised\"  \n",
    "SUBSET = \"test\"  \n",
    "\n",
    "\n",
    "# Change depending on model!\n",
    "FEATURES = features_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f00103-842c-473b-80d7-05e57f6ee25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set project name. Required to access files and artefacts\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64241e2b",
   "metadata": {},
   "source": [
    "## Sage Valuesüåµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22968e-d4e5-4473-bddb-71af4b8bbddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_feature_groups(feature_names, feature_str):\n",
    "\n",
    "    fg_classical = {\n",
    "        'chg_all_lead (grouped)': ['price_all_lead', 'chg_all_lead'],\n",
    "        'chg_all_lag (grouped)': ['price_all_lag', 'chg_ex_lag'],\n",
    "        'chg_ex_lead (grouped)': ['price_ex_lead', 'chg_ex_lead', 'chg_all_lag'],\n",
    "        'chg_ex_lag (grouped)': ['price_ex_lag'],\n",
    "        'quote_best (grouped)': ['BEST_ASK', 'BEST_BID', 'prox_best'],\n",
    "        'quote_ex (grouped)': ['bid_ex', 'ask_ex','prox_ex' ],\n",
    "        'TRADE_PRICE': ['TRADE_PRICE'],\n",
    "        }\n",
    "    \n",
    "    fg_size = {'size_ex (grouped)': [ 'bid_ask_size_ratio_ex', 'rel_bid_size_ex',  'rel_ask_size_ex', 'bid_size_ex', 'ask_size_ex','depth_ex'], 'TRADE_SIZE': ['TRADE_SIZE']}\n",
    "    \n",
    "    fg_ml = {\n",
    "        \"STRK_PRC\": [\"STRK_PRC\"],\n",
    "        \"ttm\": [\"ttm\"],\n",
    "        \"option_type\": [\"option_type\"],\n",
    "        \"root\":[\"root\"],\n",
    "        \"myn\":[\"myn\"],\n",
    "        \"day_vol\":[\"day_vol\"], \n",
    "        \"issue_type\":[\"issue_type\"],\n",
    "    }\n",
    "    \n",
    "    if feature_str == \"classical\":\n",
    "        feature_groups = group_names = fg_classical    \n",
    "    if feature_str == \"classical-size\":\n",
    "        feature_groups = group_names = fg_classical | fg_size\n",
    "    if feature_str == \"ml\":\n",
    "        feature_groups = group_names = fg_classical | fg_size | fg_ml      \n",
    "    \n",
    "\n",
    "    # Group indices\n",
    "    groups = []\n",
    "    for _, group in feature_groups.items():\n",
    "        ind_list = []\n",
    "        for feature in group:\n",
    "            ind_list.append(feature_names.index(feature))\n",
    "        groups.append(ind_list)\n",
    "\n",
    "    return groups, group_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1d386-c864-4b9b-bf7b-15e683021531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load unscaled data for classical classifier\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_none:latest\"\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "data = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=[*features_classical_size, \"buy_sell\"])\n",
    "\n",
    "y_test = data[\"buy_sell\"]\n",
    "X_test = data.drop(columns=\"buy_sell\")\n",
    "\n",
    "feature_names = X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b949e",
   "metadata": {},
   "source": [
    "### Classical Classifierüè¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de357b3-7b1b-4a19-8583-10274ecd0174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2d667-6fc4-4946-8445-389998654f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 1024 # default\n",
    "\n",
    "idx = np.random.choice(y_test.index, size=sample_size, replace=False)\n",
    "\n",
    "X_importance = X_test.loc[idx]\n",
    "y_importance = y_test.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b94c5-9fa2-47a2-9d5f-106b8b7398d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = [(\"trade_size\", \"ex\"), (\"quote\", \"best\"),  (\"quote\", \"ex\"), (\"depth\", \"best\"), (\"depth\", \"ex\"), (\"rev_tick\", \"all\")]  \n",
    "config = [(\"nan\", \"ex\")]\n",
    "clf = ClassicalClassifier(layers=config, random_state=SEED, strategy=\"random\")\n",
    "clf.fit(X=X_test.head(5), y=y_test.head(5))\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "pred_grauer = clf.predict_proba(X_test)\n",
    "pred_grauer_label = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd960bf-b998-4eb8-b99a-9f2d3d63d0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, zero_one_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb666f57-48fb-43e2-aa59-93cbe780a10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# samplewise_log_loss = []\n",
    "# for true_label, pred_probs in zip(y_importance, y_pred):\n",
    "#     sample_loss = log_loss([true_label], [pred_probs], labels=[0, 1])\n",
    "#     samplewise_log_loss.append(sample_loss)\n",
    "\n",
    "# print(\"Samplewise Log Loss:\", samplewise_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a51a2-b0ed-46b0-a83d-3fb0c6158d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# log_loss(y_test.clip(0), pred_grauer)\n",
    "\n",
    "zero_one_loss(y_test.clip(0), pred_grauer_label.clip(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f011f3-149e-47ef-b565-ff60d6751c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare benchmarks\n",
    "configs = [\n",
    "    [(\"quote\", \"best\"), (\"quote\", \"ex\"), (\"rev_tick\", \"all\")],\n",
    "    [(\"trade_size\", \"ex\"), (\"quote\", \"best\"),  (\"quote\", \"ex\"), (\"depth\", \"best\"), (\"depth\", \"ex\"), (\"rev_tick\", \"all\")]  \n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    \n",
    "    groups, group_names = get_feature_groups(X_importance.columns.tolist(), \"classical-size\")\n",
    "    \n",
    "    clf = ClassicalClassifier(layers=config, random_state=SEED, strategy=\"random\")\n",
    "    # only set headers etc, no leakage\n",
    "    clf.fit(X=X_test.head(5), y=y_test.head(5))\n",
    "    \n",
    "    def call_classical(X):\n",
    "        \n",
    "        pred = clf.predict_proba(X)\n",
    "        # max_class = np.argmax(pred, axis=-1)\n",
    "        # return max_class\n",
    "        return pred\n",
    "\n",
    "    # apply group based imputation + estimate importances in terms of zero-one loss\n",
    "    imputer = GroupedMarginalImputer(call_classical, X_importance.values, groups)\n",
    "    estimator = PermutationEstimator(imputer, \"zero one\")\n",
    "    \n",
    "    # calculate values over entire test set\n",
    "    #y_test_np = y_test.clip(lower=0).values\n",
    "    # print(y_test_np.shape)\n",
    "    sage_values = estimator(X_test.values, y_test.values)\n",
    "    \n",
    "    # save sage values + std deviation to data frame\n",
    "    result = pd.DataFrame(index=group_names, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ac26f-9e44-432c-beb2-0aacd91573f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate names for df\n",
    "names = []\n",
    "\n",
    "# generate human readable names like quote(best)->quote(ex)\n",
    "for r in tqdm(configs):\n",
    "    name = \"->\".join(\"%s(%s)\" % tup for tup in r)\n",
    "    names.append(name)\n",
    "\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "\n",
    "# flatten column names (required to save to parquet)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838b6f1-880c-428f-9efa-274327c1c4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9c24d-2374-4ac5-a9fd-60d94243a5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65310c-da57-4ad7-a9c9-eeab69d4c5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}_classical_feature_importance\"\n",
    "\n",
    "URI_FI_CLASSICAL = f\"gs://thesis-bucket-option-trade-classification/data/results/{KEY}.parquet\"\n",
    "\n",
    "results_df.to_parquet(URI_FI_CLASSICAL)\n",
    "\n",
    "result_set = wandb.Artifact(name=KEY, type=\"results\")\n",
    "result_set.add_reference(URI_FI_CLASSICAL, name=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c892b-01b3-4eaa-b12f-207ac2245bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa51927",
   "metadata": {},
   "source": [
    "### Gradient Boosting üêà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4ab3d-f8bb-4fa0-9ea2-6d4505fd3e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURE_MAP = {\n",
    "    \"classical\": features_classical,\n",
    "    \"classical-size\": features_classical_size,\n",
    "    \"ml\": features_ml,\n",
    "}\n",
    "\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "# load processed data for gradient-boosting\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_log_standardized_clipped:latest\"\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "data = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=[*features_ml, \"buy_sell\"])\n",
    "\n",
    "y_test = data[\"buy_sell\"]\n",
    "X_test = data.drop(columns=\"buy_sell\")\n",
    "\n",
    "feature_names = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7280b8-22f9-4761-ab74-64a3e845c106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841d43f-5954-46a1-847b-cd633db1fdb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28458daa-362a-47f1-ba78-a4cf0f05d40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(X_test.index, size=sample_size, replace=False)\n",
    "\n",
    "X_importance = X_test.loc[idx]\n",
    "y_importance = y_test.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4459d19-cff9-4cf2-8fa0-e16004dedf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392399b-d9ad-4e47-be90-492ab54f70ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups, group_names = get_feature_groups(X_importance.columns.tolist(), \"classical\")\n",
    "print(groups)\n",
    "print(group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef3a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs = [(\"classical\", \"1gzk7msy_CatBoostClassifier_default.cbm:latest\"),\n",
    "    (\"classical-size\", \"3vntumoi_CatBoostClassifier_default.cbm:latest\"),\n",
    "    (\"ml\", \"2t5zo50f_CatBoostClassifier_default.cbm:latest\")]\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_str, model in configs:\n",
    "    \n",
    "    # get feature names and slice to subset\n",
    "    fs = FEATURE_MAP.get(feature_str)\n",
    "    X_importance_fs = X_importance.loc[:, fs]\n",
    "    X_importance_cols = X_importance_fs.columns.tolist()\n",
    "    \n",
    "    # calculate cat indices\n",
    "    if feature_str == \"ml\":\n",
    "        cat_features = [t[0] for t in features_categorical]\n",
    "        cat_idx = [X_importance_cols.index(f) for f in cat_features]\n",
    "    \n",
    "    # get groups\n",
    "    groups, group_names = get_feature_groups(X_importance_cols, feature_str)\n",
    "    \n",
    "    #  load model by identifier from wandb\n",
    "    model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "\n",
    "    \n",
    "    artifact = run.use_artifact(model)\n",
    "    model_dir = artifact.download()\n",
    "    clf = CatBoostClassifier()\n",
    "    clf.load_model(fname=Path(model_dir, model_name))\n",
    "    \n",
    "    # print(clf.score(X_test, y_test))\n",
    "    \n",
    "    # use callable instead of default catboost as it doesn't work with categoricals otherwise\n",
    "    \n",
    "    pred=None\n",
    "    \n",
    "    def call_catboost(X):\n",
    "        if feature_str == \"ml\":       \n",
    "            # convert categorical to int\n",
    "            X = pd.DataFrame(X, columns=X_importance.columns)\n",
    "            # Update the selected columns in the original DataFrame\n",
    "            X[cat_features] = X.iloc[:, cat_idx].astype(int)\n",
    "            # pass cat indices\n",
    "            # return clf.predict(Pool(X, cat_features=cat_idx))\n",
    "            return clf.predict_proba(Pool(X, cat_features=cat_idx))\n",
    "        else:\n",
    "            # pred = clf.predict_proba(X)\n",
    "            # print(pred)\n",
    "            \n",
    "            # max_class = np.argmax(pred, axis=-1)\n",
    "            # print(max_class)\n",
    "            \n",
    "            # zeros = np.count_nonzero(max_class == 0)\n",
    "            #  print(zeros)\n",
    "            # ones = np.count_nonzero(max_class == 1)\n",
    "            # print(ones)\n",
    "            # return max_class\n",
    "            return clf.predict_proba(X)\n",
    "            \n",
    "    \n",
    "    # apply group based imputation + estimate importances in terms of zero-one loss\n",
    "    imputer = GroupedMarginalImputer(call_catboost, X_importance_fs, groups)\n",
    "    # imputer = MarginalImputer(call_catboost, X_importance_fs)\n",
    "    estimator = PermutationEstimator(imputer, \"zero one\")\n",
    "    \n",
    "    # print(X_test.loc[:,fs].shape)\n",
    "    # calculate values over entire test set\n",
    "    # y_test_np = y_test.clip(lower=0).values\n",
    "    sage_values = estimator(X_test.loc[:,fs].values, y_test.values)\n",
    "    \n",
    "    # save sage values + std deviation to data frame\n",
    "    result = pd.DataFrame(index=group_names, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    # result = pd.DataFrame(index=X_importance_cols, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b66f35-0127-4b4c-8060-c6a050de9e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde92de1-c037-492d-8664-ac8271370737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = clf.predict_proba(X_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f483e3-178f-4164-a61c-55dc492b0e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d5382-7bf4-443e-a30c-55a16a36ac71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438f1b5-fb2e-499f-b529-d684cb718e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = [f\"gbm({feature_str[0]})\" for feature_str in configs]\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0b84a-8b1c-4bd2-8712-8a3f34d50703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list to data frame + set human readable names\n",
    "names = [f\"gbm({feature_str[0]})\" for feature_str in configs]\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]\n",
    "\n",
    "# save to google clound and save identiifer\n",
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}_gbm_feature_importance_{sample_size}\"\n",
    "\n",
    "URI_FI_GBM = f\"gs://thesis-bucket-option-trade-classification/data/results/{KEY}.parquet\"\n",
    "\n",
    "results_df.to_parquet(URI_FI_GBM)\n",
    "\n",
    "result_set = wandb.Artifact(name=KEY, type=\"results\")\n",
    "result_set.add_reference(URI_FI_GBM, name=\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26639ad",
   "metadata": {},
   "source": [
    "### Transformer Classifier ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad9295-78c8-44c0-9078-1994d3b048d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"classical\", \"3jpe46s1_TransformerClassifier_default.pkl:latest\"),\n",
    "    (\"classical-size\", \"1qx3ul4j_TransformerClassifier_default.pkl:latest\"), \n",
    "    (\"ml\", \"2h81aiow_TransformerClassifier_default.pkl:latest\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_str, model in configs:\n",
    "    # load model by identifier from wandb\n",
    "    model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "\n",
    "    # get feature names and slice to subset\n",
    "    fs = FEATURE_MAP.get(feature_str)\n",
    "    X_importance_fs = X_importance.loc[:, fs]\n",
    "    X_importance_cols = X_importance_fs.columns.tolist()\n",
    "    \n",
    "    # calculate cat indices\n",
    "    if feature_str == \"ml\":\n",
    "        cat_features = [t[0] for t in features_categorical]\n",
    "        cat_idx = [X_importance_cols.index(f) for f in cat_features]\n",
    "    \n",
    "    # get groups\n",
    "    groups, group_names = get_feature_groups(X_importance_cols, feature_str)\n",
    "    \n",
    "    model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "\n",
    "    artifact = run.use_artifact(model)\n",
    "    model_dir = artifact.download()\n",
    "\n",
    "    with open(Path(model_dir, model_name), 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "    \n",
    "    # apply group based imputation + estimate importances in terms of zero-one loss\n",
    "    imputer = GroupedMarginalImputer(clf, X_importance_fs, groups)\n",
    "    estimator = PermutationEstimator(imputer, \"zero one\")\n",
    "    \n",
    "    # calculate values over entire test set\n",
    "    sage_values = estimator(X_test.loc[:,fs].values, y_test.values)\n",
    "    \n",
    "    # save sage values + std deviation to data frame\n",
    "    result = pd.DataFrame(index=group_names, data={\"values\": sage_values.values, \"std\": sage_values.std})\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c53cd2-d3cf-4dd5-84b1-2c2dc79a3f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b4c55-f424-47be-bb7e-0da20cd0b843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list to data frame + set human readable names\n",
    "names = [f\"fttransformer({feature_str[0]})\" for feature_str in configs]\n",
    "results_df = pd.concat(results, axis=1, keys=names)\n",
    "results_df.columns = [' '.join(col).strip() for col in results_df.columns.values]\n",
    "\n",
    "# save to google clound and save identiifer\n",
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}_fttransformer_feature_importance_{sample_size}\"\n",
    "\n",
    "URI_FI_FTTRANSFORMER = f\"gs://thesis-bucket-option-trade-classification/data/results/{KEY}.parquet\"\n",
    "\n",
    "results_df.to_parquet(URI_FI_FTTRANSFORMER)\n",
    "\n",
    "result_set = wandb.Artifact(name=KEY, type=\"results\")\n",
    "result_set.add_reference(URI_FI_FTTRANSFORMER, name=\"results\")\n",
    "run.log_artifact(result_set)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ba17c-50af-4f24-9cf5-b60a95057020",
   "metadata": {},
   "source": [
    "## Attention Maps for Transformers\n",
    "\n",
    "We calculate the average attention map from all transformer blocks, as done in the [here](https://github.com/hila-chefer/Transformer-MM-Explainability/blob/main/lxmert/lxmert/src/ExplanationGenerator.py#L26) and [here](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb#scrollTo=fWKGyu2YAeSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6591c20-afce-4eab-a3ac-c3681fbea9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"pgf.texsystem\": \"xelatex\",\n",
    "    \"pgf.rcfonts\": False,\n",
    "    \"font.serif\": [],\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.sans-serif\": [],\n",
    "    \"axes.labelsize\": 11,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}\\usepackage[utf8]{inputenc}')\n",
    "\n",
    "CM = 1 / 2.54\n",
    "\n",
    "cmap = mpl.colormaps.get_cmap(\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c4a8e-ae30-462f-bc20-cdbbd8dab12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"2h81aiow_TransformerClassifier_default.pkl:latest\"\n",
    "\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "model_name = MODEL.split(\"/\")[-1].split(\":\")[0]\n",
    "\n",
    "artifact = run.use_artifact(MODEL)\n",
    "model_dir = artifact.download()\n",
    "    \n",
    "with open(Path(model_dir, model_name), 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "clf = model.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e4e6d-3556-4948-92fe-31451e5ff10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_log_standardized:latest\"\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "data = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=[*features_ml, \"buy_sell\"])\n",
    "\n",
    "y_test = data[\"buy_sell\"]\n",
    "X_test = data.drop(columns=\"buy_sell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428224a5-742d-44fb-8a5c-9befd000cca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbad3f-f749-41bc-94a0-4708b8e22b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = \"ise_quotes_mid\"\n",
    "\n",
    "# first 16 at quotes, last 16 at mid\n",
    "idx =  [39342191, 39342189, 39342188, 39342175, 39342174, 39342171,\n",
    "            39342233, 39342241, 39342388, 39342239, 39342237, 39342193,\n",
    "            39342194, 39342199, 39342202, 39342204,\n",
    "        39342276, 39342363, 39342387, 39342437, 39342436, 39342428,\n",
    "            39342464, 39342540, 39342608, 39342598, 39342620, 39342632,\n",
    "            39342674, 39342781, 39342804, 39342824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f59cb-0eaa-4fb0-83b2-d606bb5e2c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b452ec-6f07-459f-9701-c4eaaa19fed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "device = \"cuda\"\n",
    "batch_size = len(idx)\n",
    "\n",
    "cat_features = model.module_params[\"cat_features\"]\n",
    "cat_unique_counts = model.module_params[\"cat_cardinalities\"]\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  \n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "test_data = TabDataset(X_test.loc[idx], y_test.loc[idx], cat_features=cat_features, cat_unique_counts=cat_unique_counts)\n",
    "\n",
    "\n",
    "test_loader = TabDataLoader(\n",
    "    test_data.x_cat,\n",
    "    test_data.x_cont,\n",
    "    test_data.weight,\n",
    "    test_data.y,\n",
    "    **dl_params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34969bc-74ff-4b30-9ee2-49530249011a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_cat, x_cont, weight, y = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4a58c-6109-4f72-814e-6874e4f9f4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# calculate outputs\n",
    "logits = clf(x_cat, x_cont).flatten()\n",
    "\n",
    "# zero gradients\n",
    "clf.zero_grad()\n",
    "\n",
    "# loss + backward pass\n",
    "loss = criterion(logits, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d3b73-783f-416a-a3d5-7d27ca520381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/hila-chefer/Transformer-MM-Explainability/blob/main/lxmert/lxmert/src/ExplanationGenerator.py#L26\n",
    "# https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb#scrollTo=fWKGyu2YAeSV\n",
    "\n",
    "attn_block = clf.transformer.blocks[0].attention.get_attn()\n",
    "# cat + cont + [CLS]\n",
    "n_tokens = attn_block.shape[-1]\n",
    "# residual connection. Repeat along batch dimension\n",
    "res = torch.eye(n_tokens, n_tokens).to(device)\n",
    "res = res.unsqueeze(0).expand(batch_size, n_tokens, n_tokens)\n",
    "\n",
    "# one_hot = expected_outputs.sum()\n",
    "# one_hot.backward(retain_graph=True)\n",
    "\n",
    "cams = []\n",
    "grads = []\n",
    "\n",
    "for i, block in enumerate(clf.transformer.blocks):\n",
    "\n",
    "    grad = block.attention.get_attn_gradients().detach()\n",
    "    cam = block.attention.get_attn().detach()\n",
    "    \n",
    "    cams.append(cam)\n",
    "    grads.append(grad)\n",
    "    \n",
    "    # reshape to [batch_size x num_head, num_tokens, num_tokens]\n",
    "    cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "    grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "    \n",
    "    # dot product\n",
    "    cam = grad * cam\n",
    "    \n",
    "    # reshape to [batch_size, num_head, num_tokens, num_tokens]\n",
    "    cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "    # clamp negative values, calculate mean over heads\n",
    "    cam = cam.clamp(min=0).mean(dim=1)\n",
    "    res = res + torch.bmm(cam, res)\n",
    "\n",
    "relevancy = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9daed-7e19-44c2-8fab-d1bd5278140d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get first attention map from batch and visualize\n",
    "batch_probs = relevancy.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793915da-c2a3-43f8-a813-505c53d0dc11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "stack = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    row = batch_probs[-i][0,1:]\n",
    "    # row = test[np.newaxis,...]\n",
    "    stack.append(row)\n",
    "    \n",
    "stack_np = np.vstack(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502aea9b-e57c-4147-a90b-7b1b6b4d45b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(stack_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c14065-c513-4f4d-8d97-1fae8d7dd6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_features = [f for f in X_test.columns.tolist() if f not in cat_features]\n",
    "# see feature tokenizer but without cls token\n",
    "labels = [*cont_features, *cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907fd4c-102a-4a35-b1fc-d7445377a098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_sanitized = ['trade price',\n",
    " 'bid (ex)',\n",
    " 'ask (ex)',\n",
    " 'ask (best)',\n",
    " 'bid (best)',\n",
    " 'price lag (ex)',\n",
    " 'price lead (ex)',\n",
    " 'price lag (all)',\n",
    " 'price lead (all)',\n",
    " 'chg lead (ex)',\n",
    " 'chg lag (ex)',\n",
    " 'chg lead (all)',\n",
    " 'chg lag (all)',\n",
    " 'prox (ex)',\n",
    " 'prox (best)',\n",
    " 'bid ask size ratio (ex)',\n",
    " 'rel. bid size (ex)',\n",
    " 'rel. ask size (ex)',\n",
    " 'trade size',\n",
    " 'bid size (ex)',\n",
    " 'ask size (ex)',\n",
    " 'depth (ex)',\n",
    " 'strike price',\n",
    " 'time to maturity',\n",
    " 'moneyness',\n",
    " 'day volume',\n",
    " 'option type',\n",
    " 'issue type',\n",
    " 'root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f08c4-1781-49d8-86a0-e78ca1189f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into trades at quotes + at mid\n",
    "stack_at_quotes, stack_at_mid = np.split(stack_np, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861fa9c-8311-4f44-bf3a-c1dbeb12a411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detail_idx_correct = 0 # 39342191\n",
    "detail_idx_false = 8 # 39342388\n",
    "alpha = 0.1\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14*CM,10*CM), sharey=True, sharex=True)\n",
    "ax[0].imshow(stack_at_quotes.T, cmap='Blues', interpolation='nearest')\n",
    "ax[0].set_yticks(list(range(len(labels_sanitized))))\n",
    "# ax[0].set_xticks(range(1, 17, 2))\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=\"x-small\")\n",
    "\n",
    "ax[0].axvspan(detail_idx_correct - 0.5, detail_idx_correct + 0.5, color='green', alpha=alpha)\n",
    "ax[0].axvspan(detail_idx_false - 0.5, detail_idx_false + 0.5, color='red', alpha=alpha)\n",
    "\n",
    "# ax[0].set_xticks(range(1, 17, 2), fontsize='x-small')\n",
    "ax[0].set_yticklabels(labels_sanitized)\n",
    "ax[0].set_xlabel(\"At Quotes\",size=\"small\")\n",
    "\n",
    "ax[1].imshow(stack_at_mid.T, cmap='Blues', interpolation='nearest')\n",
    "ax[1].set_yticks(list(range(len(labels_sanitized))))\n",
    "ax[1].set_xticks(range(0, 16, 2))\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=\"x-small\")\n",
    "ax[1].set_xlabel(\"At Mid\", size=\"small\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../reports/Graphs/attention_maps_{key}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396c145-0f7d-4de3-ba98-cb8bc2736e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_right = [\"$\\mathtt{[CLS]}$\", *labels_sanitized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e299f-e281-492c-a5ba-0718629c6bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_left = ['$\\\\mathtt{[CLS]}$', *[\"...\"]*(len(labels_right) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2284231-fd5f-4895-bb06-cb259de848fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3*CM,10*CM))\n",
    "\n",
    "\n",
    "yoffset = 0\n",
    "# xoffset = ei * width * example_sep\n",
    "xoffset = 0\n",
    "\n",
    "\n",
    "# width = 1\n",
    "# # example_sep = 3\n",
    "# word_height = 1\n",
    "# pad = 0.02\n",
    "\n",
    "\n",
    "width = 0.5\n",
    "example_sep = 3\n",
    "word_height = 0.01\n",
    "pad = 0.025\n",
    "\n",
    "# by index\n",
    "l = 3 # 3# 3\n",
    "h = 7 # 4 #6 #2 # 4\n",
    "batch_idx = 8 # or 8\n",
    "c = \"green\" if batch_idx == 0 else \"red\"\n",
    "\n",
    "# heads=8\n",
    "# layers=4\n",
    "\n",
    "cam = cams[l].reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "attention = cam[batch_idx,h,:,:]\n",
    "attention /= attention.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# print(attention)\n",
    "# color = iter(plt.cm.rainbow(np.linspace(0, 1, heads * layers)))\n",
    "\n",
    "for position, word in enumerate(labels_left):\n",
    "    plt.text(0, yoffset - position * word_height, word,\n",
    "                ha=\"right\", va=\"center\", size=\"xx-small\")\n",
    "for position, word in enumerate(labels_detail):\n",
    "    plt.text(width, yoffset - position * word_height, word,\n",
    "                ha=\"left\", va=\"center\", size=\"xx-small\")\n",
    "# focus on cls token\n",
    "# c = next(color)\n",
    "# CLS is prepended, get first row, similar to chefer\n",
    "for i, vec in enumerate(attention[0:1]):\n",
    "    for j, el in enumerate(vec):\n",
    "        plt.plot([xoffset + pad, xoffset + width - pad],\n",
    "                    [yoffset - word_height * i, yoffset - word_height * j],\n",
    "                    color=c, linewidth=2, alpha=el.item())\n",
    "plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "plt.savefig(f\"../reports/Graphs/attention_head_{h+1}_layer_{l+1}_color_{c}_{key}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa580f4f-9396-411a-8d97-fa5664707ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# by index\n",
    "batch_idx = 0 # or 8\n",
    "c = \"green\" if batch_idx == 0 else \"red\"\n",
    "\n",
    "heads=8\n",
    "layers=4\n",
    "\n",
    "# iterate over all layers\n",
    "for l in tqdm(range(layers)):\n",
    "    for h in range(heads):\n",
    "\n",
    "        plt.figure(figsize=(3*CM,10*CM))\n",
    "        \n",
    "        cam = cams[l].reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        attention = cam[batch_idx,h,:,:]\n",
    "        attention /= attention.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        \n",
    "        for position, word in enumerate(labels_left):\n",
    "            plt.text(0, yoffset - position * word_height, word,\n",
    "                        ha=\"right\", va=\"center\", size=\"xx-small\")\n",
    "        for position, word in enumerate(labels_right):\n",
    "            plt.text(width, yoffset - position * word_height, word,\n",
    "                        ha=\"left\", va=\"center\", size=\"xx-small\")\n",
    "            \n",
    "        # CLS is prepended, get first row, similar to chefer\n",
    "        for i, vec in enumerate(attention[0:1]):\n",
    "            for j, el in enumerate(vec):\n",
    "                plt.plot([xoffset + pad, xoffset + width - pad],\n",
    "                            [yoffset - word_height * i, yoffset - word_height * j],\n",
    "                            color=c, linewidth=2, alpha=el.item())\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../reports/Graphs/attention_head_{h+1}_layer_{l+1}_color_{c}_{key}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03553f7e-4df3-49eb-adb2-205b79cc9bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "\n",
    "plt.figure(figsize=(28,12))\n",
    "\n",
    "\n",
    "yoffset = 0\n",
    "# xoffset = ei * width * example_sep\n",
    "xoffset = 0\n",
    "\n",
    "\n",
    "# width = 1\n",
    "# # example_sep = 3\n",
    "# word_height = 1\n",
    "# pad = 0.02\n",
    "\n",
    "batch_idx = 0\n",
    "color = \"green\" if batch_idx else \"red\"\n",
    "\n",
    "width = 0.5\n",
    "example_sep = 3\n",
    "word_height = 1\n",
    "pad = 0.025\n",
    "\n",
    "layers = 4\n",
    "heads = 4\n",
    "\n",
    "fig, axes = plt.subplots(heads, layers)\n",
    "\n",
    "\n",
    "\n",
    "# cam = cams[3]\n",
    "\n",
    "# [batch x head x attn x dim attn]\n",
    "# attention = cam[0,7,:,:]\n",
    "\n",
    "# print(attention.shape)\n",
    "\n",
    "# # attention = cams[3][5,:,:]\n",
    "# attention /= attention.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# strengthen\n",
    "# attention = np.exp(attention.cpu())/np.exp(attention.cpu()).sum()\n",
    "\n",
    "# print(attention)\n",
    "# color = iter(cm.rainbow(np.linspace(0, 1, heads * layer)))\n",
    "\n",
    "for l in range(layer):\n",
    "\n",
    "    for h in range (heads):\n",
    "        # [batch x head x attn x dim attn]\n",
    "\n",
    "        cam = cams[l].reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "\n",
    "        # [first in batch, head h, :,:]\n",
    "        attention = cam[0,h,:,:]\n",
    "\n",
    "        attention /= attention.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        # yoffset = 1\n",
    "        # xoffset = h * width * example_sep\n",
    "\n",
    "        # for position, word in enumerate(labels_detail):\n",
    "        #     plt.text(xoffset + 0, yoffset - position * word_height, word,\n",
    "        #             ha=\"right\", va=\"center\")\n",
    "        #     plt.text(xoffset + width, yoffset - position * word_height, word,\n",
    "        #             ha=\"left\", va=\"center\")\n",
    "\n",
    "        # focus on cls token\n",
    "        # c = next(color)\n",
    "        for i, vec in enumerate(attention[0:1]):\n",
    "            for j, el in enumerate(vec):\n",
    "                axes[h, l].plot([pad, width - pad], # x axis\n",
    "                         [word_height * i, word_height * j],\n",
    "                         color=color, linewidth=2, alpha=el.item())\n",
    "\n",
    "        # axes[l,h].set_yticks(range(len(labels_left)), labels_left, size='xx-small', ha=\"right\", va=\"baseline\")\n",
    "        # axes_right.set_yticks(range(len(labels_detail)), labels_detail, size='xx-small')\n",
    "        # axes_right = axes[l,h].twinx()\n",
    "        # axes_right.set_yticks(range(len(labels_detail)), labels_detail, size='xx-small', ha=\"left\", va=\"baseline\")\n",
    "        # axes[l,h].set_xticks([])\n",
    "        axes[h,l].set_xlabel(f\"head ({l+1},{h+1})\", size='xx-small')\n",
    "# fig.tight_layout()\n",
    "        axes[h,l].set_xticks([])\n",
    "        axes[h,l].set_yticks([])\n",
    "        # axes[l,h].axis('off')\n",
    "\n",
    "plt.savefig(f\"../reports/Graphs/attention_heads_layer_all_{key}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324efd8b-fee1-4658-a9d7-c9662f7f9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\"grads\":grads, \"cams\":cams, \"final-scores\":stack_np_copy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277344d-7221-40df-b697-b2c954f946fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the pickle file\n",
    "file_path = 'data.pickle'\n",
    "\n",
    "# Open the file in binary mode and write the dictionary to it\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e85cd-f735-4f25-b254-57a15343845d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
