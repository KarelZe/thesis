{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from otc.models.fttransformer import FeatureTokenizer, FTTransformer, Transformer\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping\n",
    "from otc.optim.scheduler import CosineWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_supervised_log_standardized_clipped:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ache\n",
    "frac = 0.05\n",
    "\n",
    "# sample\n",
    "X_train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\")\n",
    "X_train = X_train.sample(frac=frac, random_state=42)\n",
    "y_train = X_train[\"buy_sell\"]\n",
    "X_train = X_train[features_classical_size]\n",
    "\n",
    "X_val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\").sample(\n",
    "    frac=frac, random_state=42\n",
    ")\n",
    "y_val = X_val[\"buy_sell\"]\n",
    "X_val = X_val[features_classical_size]\n",
    "\n",
    "X_test = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\")\n",
    "y_test = X_test[\"buy_sell\"]\n",
    "X_test = X_test[features_classical_size]\n",
    "\n",
    "# eps = 0.1\n",
    "# y_train[np.where(y_train == 0)] = eps\n",
    "# y_train[np.where(y_train == 1)] = 1.0 - eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Run Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frac = 1\n",
    "\n",
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "epochs = 100\n",
    "\n",
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "\n",
    "\n",
    "# clipping_value = 5\n",
    "reduction = \"mean\"\n",
    "\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "transformer_kwargs = {\n",
    "    \"d_token\": d_token,\n",
    "    \"n_blocks\": n_blocks,\n",
    "    \"attention_n_heads\": attention_heads,\n",
    "    \"attention_initialization\": \"kaiming\",\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"attention_normalization\": nn.LayerNorm,\n",
    "    \"ffn_normalization\": nn.LayerNorm,\n",
    "    \"ffn_dropout\": ffn_dropout,\n",
    "    # fix at 4/3, as activation (see search space B in\n",
    "    # https://arxiv.org/pdf/2106.11959v2.pdf)\n",
    "    # is static with ReGLU / GeGLU\n",
    "    \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "    \"attention_dropout\": attention_dropout,\n",
    "    \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "    \"prenormalization\": True,\n",
    "    \"first_prenormalization\": False,\n",
    "    \"last_layer_query_idx\": None,\n",
    "    \"n_tokens\": None,\n",
    "    \"kv_compression_ratio\": None,\n",
    "    \"kv_compression_sharing\": None,\n",
    "    \"head_activation\": nn.GELU,  # nn.ReLU\n",
    "    \"head_normalization\": nn.LayerNorm,\n",
    "    \"d_out\": 1,  # fix at 1, due to binary classification\n",
    "}\n",
    "\n",
    "\n",
    "optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "module_params = {\n",
    "    \"transformer\": Transformer(**transformer_kwargs),  \n",
    "    \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),   # noqa: E501\n",
    "    \"cat_features\": None,\n",
    "    \"cat_cardinalities\": [],\n",
    "}\n",
    "\n",
    "clf = FTTransformer(**module_params)\n",
    "# use multiple gpus, if available\n",
    "clf = nn.DataParallel(clf).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# wandb.log(other_kwargs)\n",
    "# wandb.log(transformer_kwargs)\n",
    "# wandb.log(optim_params)\n",
    "# wandb.log(feature_tokenizer_kwargs)\n",
    "# wandb.log(dl_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "val_data = TabDataset(X_val, y_val)\n",
    "test_data = TabDataset(X_test, y_test)\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    training_data.x_cat,\n",
    "    training_data.x_cont,\n",
    "    training_data.weight,\n",
    "    training_data.y,\n",
    "    **dl_params\n",
    ")\n",
    "val_loader = TabDataLoader(\n",
    "    val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    ")\n",
    "\n",
    "test_loader = TabDataLoader(\n",
    "    test_data.x_cat, test_data.x_cont, test_data.weight, test_data.y, **dl_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "max_iters = epochs * len(train_loader)\n",
    "# saw recommendation of 5 - 10 % of total training budget or 100 to 500 steps\n",
    "warmup = int(0.05 * max_iters)\n",
    "print(f\"warmup steps: {warmup}\")\n",
    "print(max_iters)\n",
    "\n",
    "\n",
    "scheduler = CosineWarmupScheduler(\n",
    "    optimizer=optimizer, warmup=warmup, max_iters=max_iters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkpoint(model, filename):\n",
    "\n",
    "    # remove old files\n",
    "    for filename in glob.glob(f\"checkpoints/{run.id}*\"):\n",
    "        os.remove(filename)\n",
    "\n",
    "    # create_dir\n",
    "    dir_checkpoints = \"checkpoints/\"\n",
    "    os.makedirs(dir_checkpoints, exist_ok=True)\n",
    "\n",
    "    # save new file\n",
    "    print(\"saving new checkpoints.\")\n",
    "    torch.save(model.state_dict(), os.path.join(dir_checkpoints, f\"{run.id}*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "# see https://stackoverflow.com/a/53628783/5755604\n",
    "# no sigmoid required; numerically more stable\n",
    "# do not reduce, calculate mean after multiplication with weight\n",
    "\n",
    "step = 0\n",
    "best_accuracy = -1\n",
    "best_step = -1\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    for x_cat, x_cont, _, targets in train_loader:\n",
    "\n",
    "        clf.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # for my implementation\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            train_loss = criterion(logits, targets)\n",
    "\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss_in_epoch_train += train_loss.item()\n",
    "        wandb.log({\"train_loss_step\": train_loss.item(), \"epoch\": epoch, \"batch\": batch})\n",
    "\n",
    "        batch += 1\n",
    "        step += 1\n",
    "\n",
    "    clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, _, targets in val_loader:\n",
    "\n",
    "            # for my implementation\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            logits = logits.flatten()\n",
    "\n",
    "            val_loss = criterion(logits, targets)\n",
    "\n",
    "            # get probabilities and round to nearest integer\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "            loss_in_epoch_val += val_loss.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss.item(), \"epoch\": epoch, \"batch\": batch})\n",
    "\n",
    "            batch += 1\n",
    "\n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "\n",
    "    # correct samples / no samples\n",
    "    val_accuracy = correct / len(X_val)\n",
    "    if best_accuracy < val_accuracy:\n",
    "        checkpoint(clf, f\"checkpoints/{run.id}-{step}.ptx\")\n",
    "        best_accuracy = val_accuracy\n",
    "        best_step = step\n",
    "\n",
    "    wandb.log({\"train_loss\": train_loss, \"epoch\": epoch})\n",
    "    wandb.log({\"val_loss\": val_loss, \"epoch\": epoch})\n",
    "\n",
    "    print(f\"train:{train_loss} val:{val_loss}\")\n",
    "    print(f\"val accuracy:{val_accuracy}\")\n",
    "\n",
    "    # return early if val accuracy doesn't improve. Minus to minimize.\n",
    "    early_stopping(-val_accuracy)\n",
    "    if early_stopping.early_stop or math.isnan(train_loss) or math.isnan(val_loss):\n",
    "        print(\"meh... early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp = glob.glob(f\"checkpoints/{run.id}*\")\n",
    "print(cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load(cp[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred, y_true = [], []\n",
    "\n",
    "for x_cat, x_cont, _, targets in test_loader:\n",
    "\n",
    "    logits = clf(x_cat, x_cont).flatten()\n",
    "    logits = logits.flatten()\n",
    "\n",
    "    # map between zero and one, sigmoid is otherwise included in loss already\n",
    "    # https://stackoverflow.com/a/66910866/5755604\n",
    "    preds = torch.sigmoid(logits.squeeze())\n",
    "    y_pred.append(preds.detach().cpu().numpy())\n",
    "    y_true.append(targets.detach().cpu().numpy())  \n",
    "\n",
    "# round prediction to nearest int\n",
    "y_pred = np.rint(np.concatenate(y_pred))\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "acc = (y_pred == y_true).sum() / len(y_true) \n",
    "print(acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "thesis2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
