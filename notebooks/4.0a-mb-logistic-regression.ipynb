{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.features.build_features import features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_supervised_log_standardized_clipped:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ace\n",
    "frac = 1\n",
    "\n",
    "# sample\n",
    "X_train = pd.read_parquet(\n",
    "    Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\"\n",
    ").sample(frac=frac)\n",
    "y_train = X_train[\"buy_sell\"]\n",
    "X_train = X_train[features_classical_size]\n",
    "\n",
    "X_val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\").sample(\n",
    "    frac=frac\n",
    ")\n",
    "y_val = X_val[\"buy_sell\"]\n",
    "X_val = X_val[features_classical_size]\n",
    "\n",
    "X_test = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\")\n",
    "y_test = X_test[\"buy_sell\"]\n",
    "X_test = X_test[features_classical_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if x_cat:\n",
    "            x = torch.cat((x_cat, x_cont), 1)\n",
    "        else:\n",
    "            x = x_cont\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "val_data = TabDataset(X_val, y_val)\n",
    "test_data = TabDataset(X_test, y_test)\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": 32768,\n",
    "    \"device\": \"cuda\",\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    training_data.x_cat,\n",
    "    training_data.x_cont,\n",
    "    training_data.weight,\n",
    "    training_data.y,\n",
    "    **dl_params,\n",
    ")\n",
    "val_loader = TabDataLoader(\n",
    "    val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    ")\n",
    "\n",
    "test_loader = TabDataLoader(\n",
    "    test_data.x_cat, test_data.x_cont, test_data.weight, test_data.y, **dl_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "clf = LogisticRegression(input_size=X_train.shape[1], num_classes=1).to(\"cuda\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "epochs = 100\n",
    "\n",
    "step = 0\n",
    "best_accuracy = -1\n",
    "best_step = -1\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    batch = 0\n",
    "\n",
    "    for x_cat, x_cont, weights, targets in train_loader:\n",
    "        clf.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            train_loss = criterion(logits, targets)\n",
    "\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss_in_epoch_train += train_loss  # .item()\n",
    "        wandb.log({\"train_loss_step\": train_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "\n",
    "        batch += 1\n",
    "        step += 1\n",
    "\n",
    "    clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, weights, targets in val_loader:\n",
    "            # for my implementation\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            logits = logits.flatten()\n",
    "\n",
    "            val_loss = criterion(logits, targets)\n",
    "\n",
    "            # get probabilities and round to nearest integer\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "            loss_in_epoch_val += val_loss  # val_loss #.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "\n",
    "            batch += 1\n",
    "\n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "\n",
    "    # correct samples / no samples\n",
    "    val_accuracy = correct / len(X_val)\n",
    "    if best_accuracy < val_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_step = step\n",
    "\n",
    "    wandb.log({\"train_loss\": train_loss, \"epoch\": epoch})\n",
    "    wandb.log({\"val_loss\": val_loss, \"epoch\": epoch})\n",
    "    # wandb.log({\"val_accuracy\": val_accuracy, 'epoch': epoch})\n",
    "\n",
    "    print(f\"train:{train_loss} val:{val_loss}\")\n",
    "    print(f\"val accuracy:{val_accuracy}\")\n",
    "\n",
    "    # return early if val accuracy doesn't improve. Minus to minimize.\n",
    "    early_stopping(-val_accuracy)\n",
    "    if early_stopping.early_stop or math.isnan(train_loss) or math.isnan(val_loss):\n",
    "        print(\"early stopping now.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred, y_true = [], []\n",
    "\n",
    "for x_cat, x_cont, weights, targets in test_loader:\n",
    "    logits = clf(x_cat, x_cont).flatten()\n",
    "    logits = logits.flatten()\n",
    "\n",
    "    # map between zero and one, sigmoid is otherwise included in loss already\n",
    "    # https://stackoverflow.com/a/66910866/5755604\n",
    "    preds = torch.sigmoid(logits.squeeze())\n",
    "    y_pred.append(preds.detach().cpu().numpy())\n",
    "    y_true.append(targets.detach().cpu().numpy())\n",
    "\n",
    "# round prediction to nearest int\n",
    "y_pred = np.rint(np.concatenate(y_pred))\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "# calculate accuracy on validation set\n",
    "acc = (y_pred == y_true).sum() / len(y_true)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
