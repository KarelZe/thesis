{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b853e9-a97e-4a0b-b1af-1bd0ede09c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9361c4b-0819-45a0-b9a4-5089914cd280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n",
    "# fs = gcsfs.GCSFileSystem(project=\"thesis\")\n",
    "# fs_prefix = \"gs://\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a63a56-6aff-4459-9765-1242403443b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarelze\u001b[0m (\u001b[33mfbv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/pfs/data5/home/kit/stud/uloak/thesis/notebooks/wandb/run-20230516_101839-23f06yxl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fbv/thesis/runs/23f06yxl\" target=\"_blank\">vibrant-cherry-1560</a></strong> to <a href=\"https://wandb.ai/fbv/thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ise_supervised_log_standardized_clipped:latest, 5205.52MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:6.8\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/ise_supervised_log_standardized_clipped:latest\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf7b797-6256-45c0-9bec-d4faf98d9daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical, features_classical_size\n",
    "from otc.optim.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd424255-737f-4590-93ee-e9e6dcfc3258",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2106.11959.pdf\n",
    "\n",
    "Layer count 3\n",
    "Feature embedding size 192\n",
    "Head count 8\n",
    "Activation & FFN size factor (ReGLU,\n",
    "4/3)\n",
    "Attention dropout 0.2\n",
    "FFN dropout 0.1\n",
    "Residual dropout 0.0\n",
    "Initialization Kaiming (He et al., 2015a)\n",
    "Parameter count 929K The value is given for 100 numerical features\n",
    "Optimizer AdamW\n",
    "Learning rate 1e−4\n",
    "Weight decay 1e−5 0.0 for Feature Tokenizer, LayerNorm and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bf75ce-b0b4-4198-9be7-f0da4e699f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preserve relative ordering, sample for testing ache\n",
    "\n",
    "frac = 1\n",
    "\n",
    "# sample\n",
    "X_train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\").sample(frac=frac)\n",
    "y_train = X_train[\"buy_sell\"]\n",
    "X_train = X_train[features_classical_size]\n",
    "\n",
    "X_val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\").sample(frac=frac)# .sample(frac=frac, random_state=42).sort_index()\n",
    "y_val = X_val[\"buy_sell\"]\n",
    "X_val = X_val[features_classical_size]\n",
    "\n",
    "X_test = pd.read_parquet(Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\")\n",
    "y_test = X_test[\"buy_sell\"]\n",
    "X_test = X_test[features_classical_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56360209-b30d-47a1-bb88-c34112178ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if x_cat:\n",
    "            x = torch.cat((x_cat, x_cont), 1)\n",
    "        else:\n",
    "            x = x_cont\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c271935-f54d-4b6d-a51c-423c5d5d2818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = TabDataset(X_train, y_train)\n",
    "val_data = TabDataset(X_val, y_val)\n",
    "test_data = TabDataset(X_test, y_test)\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": 32768, \n",
    "    \"device\": \"cuda\",\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "\n",
    "train_loader = TabDataLoader(\n",
    "    training_data.x_cat,\n",
    "    training_data.x_cont,\n",
    "    training_data.weight,\n",
    "    training_data.y,\n",
    "    **dl_params\n",
    ")\n",
    "val_loader = TabDataLoader(\n",
    "    val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    ")\n",
    "\n",
    "test_loader = TabDataLoader(\n",
    "    test_data.x_cat, test_data.x_cont, test_data.weight, test_data.y, **dl_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359c2ccf-8984-4ce1-ae41-f1dcb0ae3d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "clf = LogisticRegression(input_size=X_train.shape[1],num_classes=1).to(\"cuda\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.AdamW(clf.parameters(),\n",
    "    lr=optim_params[\"lr\"],\n",
    "    weight_decay=optim_params[\"weight_decay\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99743844-ba1e-4a9c-8650-8ce14ca48385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkpoint(model, filename):\n",
    "    \n",
    "    # remove old files\n",
    "    for filename in glob.glob(f\"checkpoints/{run.id}*\"):\n",
    "        os.remove(filename) \n",
    "    \n",
    "    # create_dir\n",
    "    dir_checkpoints = \"checkpoints/\"\n",
    "    os.makedirs(dir_checkpoints, exist_ok = True) \n",
    "    \n",
    "    # save new file\n",
    "    print(\"saving new checkpoints.\")\n",
    "    torch.save(model.state_dict(), os.path.join(dir_checkpoints,f\"{run.id}*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a57c959b-29fa-4329-a1d5-c05bea812dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434bbf5e6b4444c19623d8d6fb7b96d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving new checkpoints.\n",
      "train:0.6196326017379761 val:0.6251112222671509\n",
      "val accuracy:0.6699623499176299\n",
      "saving new checkpoints.\n",
      "train:0.5817510485649109 val:0.6106944680213928\n",
      "val accuracy:0.6789899480779357\n",
      "saving new checkpoints.\n",
      "train:0.557647705078125 val:0.6030252575874329\n",
      "val accuracy:0.6861189210454878\n",
      "saving new checkpoints.\n",
      "train:0.5399504899978638 val:0.5986984372138977\n",
      "val accuracy:0.6924417385902207\n",
      "saving new checkpoints.\n",
      "train:0.5262736678123474 val:0.5967432260513306\n",
      "val accuracy:0.6959893920280118\n",
      "saving new checkpoints.\n",
      "train:0.5157594680786133 val:0.5966953039169312\n",
      "val accuracy:0.6979309389452708\n",
      "saving new checkpoints.\n",
      "train:0.50788813829422 val:0.5979034900665283\n",
      "val accuracy:0.6987825588487865\n",
      "saving new checkpoints.\n",
      "train:0.5022001266479492 val:0.6003912091255188\n",
      "val accuracy:0.6991216608144285\n",
      "saving new checkpoints.\n",
      "train:0.49826887249946594 val:0.6031561493873596\n",
      "val accuracy:0.699231507881883\n",
      "train:0.49568480253219604 val:0.6058236360549927\n",
      "val accuracy:0.699048022595135\n",
      "train:0.4940405488014221 val:0.6080855131149292\n",
      "val accuracy:0.6988055453647538\n",
      "train:0.49298545718193054 val:0.6094672083854675\n",
      "val accuracy:0.6985291986219075\n",
      "train:0.49225252866744995 val:0.6101369857788086\n",
      "val accuracy:0.6984411175474486\n",
      "train:0.49167513847351074 val:0.6101831197738647\n",
      "val accuracy:0.698282856402116\n",
      "train:0.49123436212539673 val:0.6101393699645996\n",
      "val accuracy:0.6980749606559334\n",
      "train:0.49090588092803955 val:0.6100772023200989\n",
      "val accuracy:0.698000712175154\n",
      "train:0.49068230390548706 val:0.6099272966384888\n",
      "val accuracy:0.6976867326406798\n",
      "train:0.49053752422332764 val:0.6100823879241943\n",
      "val accuracy:0.6976187901952542\n",
      "train:0.49045029282569885 val:0.6097472310066223\n",
      "val accuracy:0.6973726513959579\n",
      "train:0.49039435386657715 val:0.6097056865692139\n",
      "val accuracy:0.6972928088515581\n",
      "train:0.4903591275215149 val:0.6096574664115906\n",
      "val accuracy:0.6971990320032311\n",
      "train:0.49033308029174805 val:0.6096330881118774\n",
      "val accuracy:0.6971901832116861\n",
      "train:0.4903136193752289 val:0.6097721457481384\n",
      "val accuracy:0.6971509230560959\n",
      "train:0.4902970790863037 val:0.6097301244735718\n",
      "val accuracy:0.6971653659112612\n",
      "meh... early stopping\n"
     ]
    }
   ],
   "source": [
    "# half precision, see https://pytorch.org/docs/stable/amp.html\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "early_stopping = EarlyStopping(patience=15)\n",
    "epochs = 100\n",
    "\n",
    "step = 0\n",
    "best_accuracy = -1\n",
    "best_step = -1\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # perform training\n",
    "    loss_in_epoch_train = 0\n",
    "\n",
    "    batch = 0\n",
    "    \n",
    "    for x_cat, x_cont, weights, targets in train_loader:\n",
    "    \n",
    "        clf.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            train_loss = criterion(logits, targets)\n",
    "\n",
    "        scaler.scale(train_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss_in_epoch_train += train_loss  # .item()\n",
    "        wandb.log({\"train_loss_step\": train_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "        batch += 1\n",
    "        step +=1\n",
    "\n",
    "    clf.eval()\n",
    "    loss_in_epoch_val = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, weights, targets in val_loader:\n",
    "            \n",
    "            # for my implementation\n",
    "            logits = clf(x_cat, x_cont).flatten()\n",
    "            logits = logits.flatten()\n",
    "\n",
    "            val_loss = criterion(logits, targets)\n",
    "            \n",
    "            # get probabilities and round to nearest integer\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            correct += (preds == targets).sum().item()\n",
    "\n",
    "            loss_in_epoch_val += val_loss  # val_loss #.item()\n",
    "            wandb.log({\"val_loss_step\": val_loss, \"epoch\": epoch, \"batch\": batch})\n",
    "            \n",
    "            batch +=1      \n",
    "\n",
    "    # loss average over all batches\n",
    "    train_loss = loss_in_epoch_train / len(train_loader)\n",
    "    val_loss = loss_in_epoch_val / len(val_loader)\n",
    "    \n",
    "    # correct samples / no samples\n",
    "    val_accuracy = correct / len(X_val)\n",
    "    if best_accuracy < val_accuracy:\n",
    "        checkpoint(clf, f\"checkpoints/{run.id}-{step}.ptx\")\n",
    "        best_accuracy = val_accuracy\n",
    "        best_step = step\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": train_loss, 'epoch': epoch})\n",
    "    wandb.log({\"val_loss\": val_loss, 'epoch': epoch})\n",
    "    # wandb.log({\"val_accuracy\": val_accuracy, 'epoch': epoch})    \n",
    "    \n",
    "    print(f\"train:{train_loss} val:{val_loss}\")\n",
    "    print(f\"val accuracy:{val_accuracy}\")\n",
    "\n",
    "    # return early if val accuracy doesn't improve. Minus to minimize.\n",
    "    early_stopping(-val_accuracy)\n",
    "    if early_stopping.early_stop or math.isnan(train_loss) or math.isnan(val_loss):\n",
    "        print(\"early stopping now.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ca46d5-7448-4e63-8d93-ff3773fe5753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp =  glob.glob(f\"checkpoints/{run.id}*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ca09c3b-6598-4a6f-be55-ce0cf7de61d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(cp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b73fd9d-9260-4866-9cb5-0928dcece535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668656308078952\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = [], []\n",
    "\n",
    "for x_cat, x_cont, weights, targets in test_loader:\n",
    "    # logits = clf(x_cont,x_cat).flatten() #\n",
    "    # for my implementation\n",
    "    logits = clf(x_cat, x_cont).flatten()\n",
    "    logits = logits.flatten()\n",
    "\n",
    "\n",
    "    # map between zero and one, sigmoid is otherwise included in loss already\n",
    "    # https://stackoverflow.com/a/66910866/5755604\n",
    "    preds = torch.sigmoid(logits.squeeze())\n",
    "    y_pred.append(preds.detach().cpu().numpy())\n",
    "    y_true.append(targets.detach().cpu().numpy())  # type: ignore\n",
    "\n",
    "# round prediction to nearest int\n",
    "y_pred = np.rint(np.concatenate(y_pred))\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "acc = (y_pred == y_true).sum() / len(y_true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c882d2a-6cad-4ff2-af4a-a2b58518cab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
