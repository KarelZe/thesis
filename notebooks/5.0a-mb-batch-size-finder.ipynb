{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CategoricalFeatureTokenizer,\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    MultiheadAttention,\n",
    "    NumericalFeatureTokenizer,\n",
    "    Transformer,\n",
    ")\n",
    "from otc.models.tabtransformer import TabTransformer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448,
     "referenced_widgets": [
      "cf721f54b254482482c152ac155f979d",
      "f28a4d5288574bef831604096f8810e5",
      "6c3df6ba8d034fbab73eaea6291cf22b",
      "95681fb6b27444568eac8ee363b0f9ac",
      "2b70cdcb5ce44b6e9a52e6b353ce3a5e",
      "9046f63b323141ec80ba215a5c6ea1bc",
      "8c9d70af3c184d78bb98819ca0aba1b9",
      "45e989c2906d4ae18acaebea3eccfbd5",
      "b3f8ec205c9c4f6ab0ab906b1ea80535",
      "04fbb71360224ea3a89f5105c7e30080",
      "d30a4cddbf324b1a866899dd18121fdd"
     ]
    },
    "id": "aUoXCM9yM0qw",
    "outputId": "8f89a6d1-292a-4310-d0b0-03c33cfc6724"
   },
   "outputs": [],
   "source": [
    "# code adapted from here:\n",
    "# https://towardsdatascience.com/a-batch-too-large-finding-the-batch-size-that-fits-on-gpus-aef70902a9f1\n",
    "\n",
    "\n",
    "# dataset information\n",
    "CAT_CARDINALITY = 9_000\n",
    "NUM_FEATURES_CAT = 3\n",
    "NUM_FEATURES_CONT = 41\n",
    "DATASET_SIZE = 50_000_000\n",
    "\n",
    "\n",
    "def get_batch_size(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    min_batch_size: int = 2,\n",
    "    max_batch_size: Optional[int] = None,\n",
    "    num_iterations: int = 5,\n",
    ") -> int:\n",
    "    #print(model)\n",
    "    model.to(device)\n",
    "    model.train(True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    print(\"Test batch size\")\n",
    "    batch_size = min_batch_size\n",
    "    while True:\n",
    "        if max_batch_size is not None and batch_size >= max_batch_size:\n",
    "            batch_size = max_batch_size\n",
    "            break\n",
    "        if batch_size >= DATASET_SIZE:\n",
    "            batch_size = batch_size // 2\n",
    "            break\n",
    "        try:\n",
    "            for _ in range(num_iterations):\n",
    "                # dummy inputs and targets\n",
    "\n",
    "                x_cat = torch.randint(\n",
    "                    1, CAT_CARDINALITY, (batch_size, NUM_FEATURES_CAT)\n",
    "                ).to(device)\n",
    "                x_cont = torch.rand((batch_size, NUM_FEATURES_CONT)).to(device)\n",
    "                targets = torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "                outputs = model(x_cat, x_cont)\n",
    "                loss = F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            batch_size *= 2\n",
    "            print(f\"\\tTesting batch size {batch_size}\")\n",
    "            sleep(3)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            print(f\"\\tOOM at batch size {batch_size}\")\n",
    "            batch_size //= 2\n",
    "            break\n",
    "    del model, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Final batch size {batch_size}\")\n",
    "    return batch_size\n",
    "\n",
    "\n",
    "def get_datasets(batch_size: int, num_workers: int = 2):\n",
    "\n",
    "    x_cat = torch.randint(0, CAT_CARDINALITY, (DATASET_SIZE, NUM_FEATURES_CAT))\n",
    "    x_cont = torch.rand((DATASET_SIZE, NUM_FEATURES_CONT))\n",
    "    weight = torch.ones((DATASET_SIZE, 1))\n",
    "    y = torch.randint(0, 1, (DATASET_SIZE, 1))\n",
    "\n",
    "    train_ds = TabDataLoader(\n",
    "        x_cat,\n",
    "        x_cont,\n",
    "        weight,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    test_ds = TabDataLoader(\n",
    "        x_cat,\n",
    "        x_cont,\n",
    "        weight,\n",
    "        y,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "def main(epochs: int = 2):\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # https://github.com/Yura52/rtdl/blob/main/rtdl/modules.py\n",
    "    # set max cardinality for all categorical features and max dimension from search space\n",
    "#     params_feature_tokenizer = {\n",
    "#         \"num_continous\": NUM_FEATURES_CONT,\n",
    "#         \"cat_cardinalities\": [CAT_CARDINALITY] * NUM_FEATURES_CAT,\n",
    "#         \"d_token\": 512,\n",
    "#     }\n",
    "#     feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "#     params_transformer = {\n",
    "#         \"d_token\": 512,\n",
    "#         \"n_blocks\": 6,\n",
    "#         \"attention_n_heads\": 8,\n",
    "#         \"attention_initialization\": \"kaiming\",\n",
    "#         \"ffn_activation\": ReGLU,\n",
    "#         \"attention_normalization\": nn.LayerNorm,\n",
    "#         \"ffn_normalization\": nn.LayerNorm,\n",
    "#         \"ffn_dropout\": 0.1,\n",
    "#         \"ffn_d_hidden\": int(512 * (4 / 3)),\n",
    "#         \"attention_dropout\": 0.1,\n",
    "#         \"residual_dropout\": 0.1,\n",
    "#         \"prenormalization\": True,\n",
    "#         \"first_prenormalization\": False,\n",
    "#         \"last_layer_query_idx\": None,\n",
    "#         \"n_tokens\": None,\n",
    "#         \"kv_compression_ratio\": None,\n",
    "#         \"kv_compression_sharing\": None,\n",
    "#         \"head_activation\": nn.ReLU,\n",
    "#         \"head_normalization\": nn.LayerNorm,\n",
    "#         \"d_out\": 1,\n",
    "#     }\n",
    "\n",
    "#     # FTTransformer\n",
    "#     transformer = Transformer(**params_transformer)\n",
    "\n",
    "#     model = FTTransformer(feature_tokenizer, transformer)\n",
    "\n",
    "#     batch_size = get_batch_size(\n",
    "#         model=model,\n",
    "#         device=device,\n",
    "#         min_batch_size=32,\n",
    "#         max_batch_size= 1024 * 1024,\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    # TabTransformer\n",
    "    module_params = {\n",
    "            \"depth\": 12,\n",
    "            \"heads\":8,\n",
    "            \"dim\": 256,\n",
    "            \"dim_out\": 1,\n",
    "            \"mlp_act\": nn.ReLU,\n",
    "            \"transformer_act\": F.gelu,\n",
    "            \"transformer_norm_first\": False,\n",
    "            \"mlp_hidden_mults\": (4, 2),\n",
    "            \"transformer_dropout\": 0.5,\n",
    "            \"cat_cardinalities\": [CAT_CARDINALITY] * NUM_FEATURES_CAT,\n",
    "            \"cat_features\": NUM_FEATURES_CAT,\n",
    "            \"num_continuous\": NUM_FEATURES_CONT,\n",
    "        }\n",
    "    \n",
    "    model = TabTransformer(\n",
    "        **module_params\n",
    "    )\n",
    "\n",
    "\n",
    "    batch_size = get_batch_size(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        min_batch_size=32,\n",
    "        max_batch_size= 1024 * 1024,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT Transformer\n",
    "# Test batch size\n",
    "# \tTesting batch size 64\n",
    "# \tTesting batch size 128\n",
    "# \tTesting batch size 256\n",
    "# \tTesting batch size 512\n",
    "# \tTesting batch size 1024\n",
    "# \tTesting batch size 2048\n",
    "# \tTesting batch size 4096\n",
    "# CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 31.74 GiB total capacity; 29.41 GiB already allocated; 309.25 MiB free; 30.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "# \tOOM at batch size 4096\n",
    "# Final batch size 2048\n",
    "\n",
    "# TabTransformer\n",
    "# Test batch size\n",
    "# \tTesting batch size 64\n",
    "# \tTesting batch size 128\n",
    "# \tTesting batch size 256\n",
    "# \tTesting batch size 512\n",
    "# \tTesting batch size 1024\n",
    "# \tTesting batch size 2048\n",
    "# \tTesting batch size 4096\n",
    "# \tTesting batch size 8192\n",
    "# \tTesting batch size 16384\n",
    "# \tTesting batch size 32768\n",
    "# \tTesting batch size 65536\n",
    "# CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 31.74 GiB total capacity; 30.24 GiB already allocated; 7.25 MiB free; 30.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "# \tOOM at batch size 65536\n",
    "# Final batch size 32768\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8ea8b642289b706932f10b33ee389827410dbaef0ce2c5bf73615e8d3267d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
