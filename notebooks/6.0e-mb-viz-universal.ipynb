{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqA-31WTmVb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter, PercentFormatter\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_c4GpJmndXz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"pgf.texsystem\": \"xelatex\",\n",
    "    \"pgf.rcfonts\": False,\n",
    "    \"font.serif\": [],\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.sans-serif\": [],\n",
    "    \"axes.labelsize\": 11,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "rc(\"text\", usetex=True)\n",
    "\n",
    "plt.rc('text.latex', preamble=r'\\usepackage{amsmath}\\usepackage[utf8]{inputenc}')\n",
    "\n",
    "CM = 1 / 2.54\n",
    "# cmap = plt.cm.get_cmap(\"viridis\")\n",
    "cmap = mpl.colormaps.get_cmap(\"plasma\")\n",
    "# plt.style.use(['science','nature'])\n",
    "\n",
    "# Bright color scheme\n",
    "# color-blind safe\n",
    "# from Paul Tot's website: https://personal.sron.nl/~pault/\n",
    "# Set color cycle\n",
    "# mpl.rcParams['axes.prop_cycle'] = mpl.cycler('color', ['4477AA', 'EE6677', '228833', 'CCBB44', '66CCEE', 'AA3377', 'BBBBBB'])\n",
    "\n",
    "\n",
    "# Standard SciencePlots color cycle\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(\n",
    "    \"color\", [\"0C5DA5\", \"00B945\", \"FF9500\", \"FF2C00\", \"845B97\", \"474747\", \"9e9e9e\"]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kNTG2a_kf5gS"
   },
   "source": [
    "## Comparsion of Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPtje60Kf5gU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8*CM,8*CM))  \n",
    "    \n",
    "# plot orientation line\n",
    "x = np.linspace(0, 10, num=100)\n",
    "y = np.linspace(0, 10, num=100)\n",
    "plt.plot(x,y,'--', c=\"black\", linewidth=0.5)\n",
    "    \n",
    "# Preparing dataset\n",
    "x = [x for x in range(10)]\n",
    "y = [5, 2, 4, 8, 5, 6, 8, 7, 1, 3]\n",
    "text = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\",\n",
    "        \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\"]\n",
    "\n",
    "# plotting scatter plot\n",
    "plt.scatter(x, y, zorder=10)\n",
    "  \n",
    "# Loop for annotation of all points\n",
    "for i in range(len(x)):\n",
    "    plt.annotate(text[i], (x[i]-0.8, y[i] + 0.4))\n",
    "\n",
    "\n",
    "# adjusting the scale of the axes\n",
    "plt.xlim((0, 10))\n",
    "plt.ylim((0, 10))\n",
    "\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Training Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"../reports/Graphs/training-validation-accuracy.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vVE2JK9Af5gW"
   },
   "source": [
    "## Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcOCYG4Of5gW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 100 linearly spaced numbers\n",
    "x = np.linspace(-2,2,100)\n",
    "# the function, which is y = x^2 here\n",
    "y = np.log(1 + np.exp(-2*x))\n",
    "\n",
    "# setting the axes at the centre\n",
    "fig = plt.figure(figsize=(12 * CM, 6 * CM))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "ax.set_xlabel(\"Margin $y-F_m(\\mathbf{x})$\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "# plot the function\n",
    "plt.plot(x,y, label=\"cross-entropy loss\")\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(\"../reports/Graphs/cross-entropy-loss.pdf\", bbox_inches=\"tight\")\n",
    "# show the p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h9mAHJU1f5gX"
   },
   "source": [
    "## Histogram Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC57KvgPf5gX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.6 - rng.rand(16))\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=1)\n",
    "# regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "# regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "# y_2 = regr_2.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8*CM,6*CM))\n",
    "plt.scatter(X, y, s=20, c=\"yellowgreen\", edgecolors=\"black\", linewidth=0.5, label=\"Data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"Approximation\", linewidth=1)\n",
    "# plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(\"../reports/Graphs/dt-decision-boundary.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rdBVk3fyf5gZ"
   },
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Wfu05Wrf5gZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_mpl(start: str, end: str):\n",
    "    mpl_start = mdates.date2num(pd.to_datetime(start))\n",
    "    mpl_end = mdates.date2num(pd.to_datetime(end))\n",
    "    return mpl_start, mpl_end - mpl_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6d0dXM-f5ga",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_pos(span: tuple):\n",
    "    return span[0] + 0.5 * span[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpm8MpP5f5gb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax, bx) = plt.subplots(\n",
    "    2, 1, sharey=\"none\", sharex=\"col\", figsize=(12 * CM, 6 * CM)\n",
    ")\n",
    "\n",
    "# ise\n",
    "ax.broken_barh([to_mpl(\"2005-05-02\", \"2017-05-31\")], (1, 5), facecolors=\"lightgray\")\n",
    "\n",
    "# ise pretraining\n",
    "span = [to_mpl(\"2012-10-23\", \"2013-10-24\")]\n",
    "ax.broken_barh(span, (2.5, 1), facecolors=(168/255,209/255,238/255), edgecolor=\"black\", linewidth=0.8)\n",
    "\n",
    "ax.text(\n",
    "    x=to_pos(span[0]),\n",
    "    y=3,\n",
    "    s=\"train\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    color=\"black\",\n",
    "    fontsize=\"small\",\n",
    ")\n",
    "\n",
    "spans = [\n",
    "    to_mpl(\"2005-05-02\", \"2013-10-24\"),\n",
    "    to_mpl(\"2013-10-25\", \"2015-11-05\"),\n",
    "    to_mpl(\"2015-11-06\", \"2017-05-31\"),\n",
    "]\n",
    "\n",
    "# ise supervised\n",
    "ax.broken_barh(\n",
    "    spans,\n",
    "    (1.2, 1),\n",
    "    facecolors=((168/255,209/255,238/255), (204/255,212/255,151/255), (239/255,171/255,170/255)),\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.8,\n",
    ")\n",
    "\n",
    "# add text labels manually\n",
    "labels = [\"train\", \"val\", \"test\"]\n",
    "for i, s in enumerate(spans):\n",
    "    ax.text(\n",
    "        x=to_pos(s),\n",
    "        y=1.7,\n",
    "        s=labels[i],\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        color=\"black\",\n",
    "        fontsize=\"small\",\n",
    "    )\n",
    "\n",
    "ax.xaxis_date()\n",
    "plt.setp(ax.get_xticklabels(), visible=True)\n",
    "\n",
    "# cboe\n",
    "bx.broken_barh([to_mpl(\"2011-01-01\", \"2017-10-31\")], (1, 3), facecolors=\"lightgray\")\n",
    "\n",
    "spans = [\n",
    "    to_mpl(\"2015-11-06\", \"2017-10-31\"),\n",
    "]\n",
    "\n",
    "# cboe supervised\n",
    "bx.broken_barh(\n",
    "    spans,\n",
    "    (1.85, 1),\n",
    "    facecolors=(239/255,171/255,170/255),\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.8,\n",
    ")\n",
    "\n",
    "# add text labels manually\n",
    "labels = [\"test\"]\n",
    "for i, s in enumerate(spans):\n",
    "    bx.text(\n",
    "        x=to_pos(s),\n",
    "        y=2.35,\n",
    "        s=labels[i],\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        color=\"black\",\n",
    "        fontsize=\"small\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Modify y-axis tick labels\n",
    "ax.set_yticks([1.7, 3], labels=[\"ISE\\n Supervised\", \"ISE\\n Pretraining\"])\n",
    "bx.set_yticks([2.35], labels=[\"CBOE\"])\n",
    "\n",
    "ax.set_ylim(1, 3.7)\n",
    "bx.set_ylim(1, 3.7)\n",
    "\n",
    "# into to date\n",
    "bx.xaxis_date()\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(\"../reports/Graphs/train-test-split.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SyA46Ie6f5gc"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiz8Mtuzf5ge"
   },
   "outputs": [],
   "source": [
    "# code from https://d2l.ai/d2l-en.pdf\n",
    "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap=cmap):\n",
    "    num_rows, num_cols, _, _ = matrices.shape\n",
    "    fig, axes = plt.subplots(\n",
    "        num_rows, num_cols, figsize=figsize, sharex=True, sharey=True, squeeze=False\n",
    "    )\n",
    "    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\n",
    "        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n",
    "            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)\n",
    "            if i == num_rows - 1:\n",
    "                ax.set_xlabel(xlabel)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(ylabel)\n",
    "            if titles:\n",
    "                ax.set_title(titles[j])\n",
    "    fig.colorbar(pcm, ax=axes)\n",
    "    plt.savefig(\"../reports/Graphs/attention-maps.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMBPYiltf5ge"
   },
   "outputs": [],
   "source": [
    "# attention_weights = torch.eye(10).reshape((1, 1, 10, 10))\n",
    "attention_weights = torch.rand(size=(2, 4, 10, 10))\n",
    "show_heatmaps(\n",
    "    attention_weights, xlabel=\"Keys\", ylabel=\"Queries\", figsize=(12 * CM, 6 * CM)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LeO039jmYXY"
   },
   "outputs": [],
   "source": [
    "# Code from https://www.tensorflow.org/tutorials/text/transformer\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return pos_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXDCDLyhmhJ2"
   },
   "outputs": [],
   "source": [
    "tokens = 64\n",
    "dimensions = 96\n",
    "\n",
    "\n",
    "pos_encoding = positional_encoding(tokens, dimensions)\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "plt.figure(figsize=(12 * CM, 6 * CM))\n",
    "plt.pcolormesh(pos_encoding[0], cmap=cmap)\n",
    "plt.xlabel(\"Embedding dimension $d_e$\")\n",
    "plt.xlim((0, dimensions))\n",
    "plt.ylim((tokens, 0))\n",
    "plt.ylabel(\"token position $t$\")\n",
    "plt.colorbar()\n",
    "plt.savefig(\"../reports/Graphs/positional-encoding.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KLKHwCjOf5gg"
   },
   "source": [
    "## Semi-Supervised Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f4rU7X8f5gg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authors: Clay Woolam   <clay@woolam.org>\n",
    "#          Oliver Rausch <rauscho@ethz.ch>\n",
    "# License: BSD\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator # needed for integer only on axis\n",
    "from matplotlib.lines import Line2D # for creating the custom legend\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# mask = y[y<2]\n",
    "\n",
    "# y = y[-mask]\n",
    "# X = X[-mask]\n",
    "\n",
    "# print(mask.astype(bool))\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.1\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "y_rand = rng.rand(y.shape[0])\n",
    "y_30 = np.copy(y)\n",
    "y_30[y_rand < 0.3] = -1  # set random samples to be unlabeled\n",
    "y_50 = np.copy(y)\n",
    "y_50[y_rand < 0.5] = -1\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "# ls30 = (LabelSpreading().fit(X, y_30), y_30, \"Label Spreading 30% data\")\n",
    "# ls50 = (LabelSpreading().fit(X, y_50), y_50, \"Label Spreading 50% data\")\n",
    "# ls100 = (LabelSpreading().fit(X, y), y, \"Label Spreading 100% data\")\n",
    "\n",
    "# the base classifier for self-training is identical to the SVC\n",
    "base_classifier = SVC(kernel=\"rbf\", gamma=0.5, probability=True)\n",
    "st30 = (\n",
    "    SelfTrainingClassifier(base_classifier).fit(X, y_30),\n",
    "    y_30,\n",
    "    \"Semi-Supervised Classifier\",\n",
    ")\n",
    "# st50 = (\n",
    "#     SelfTrainingClassifier(base_classifier).fit(X, y_50),\n",
    "#     y_50,\n",
    "#     \"Self-training 50% data\",\n",
    "# )\n",
    "\n",
    "rbf_svc = (base_classifier.fit(X, y), y, \"Supervised Classifier\")\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "PROB_DOT_SCALE = 40 # modifier to scale the probability dots\n",
    "PROB_DOT_SCALE_POWER = 3 # exponential used to increase/decrease size of prob dots\n",
    "TRUE_DOT_SIZE = 50 #\n",
    "\n",
    "redish = '#d73027'\n",
    "orangeish = '#fc8d59'\n",
    "yellowish = '#fee090'\n",
    "blueish = '#4575b4'\n",
    "colormap = np.array([redish,blueish,orangeish])\n",
    "\n",
    "color_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}\n",
    "\n",
    "ax = plt.figure(figsize=(12*CM, 6*CM))\n",
    "\n",
    "classifiers = (rbf_svc, st30)\n",
    "for i, (clf, y_train, title) in enumerate(classifiers):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    z_proba =  clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # the size of each probability dot\n",
    "    Z_size = np.max(Z_proba, axis=1) \n",
    "    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    tri = plt.tricontourf(xx.flatten(), yy.flatten(), z_proba[:,1], levels=14, cmap=\"RdBu_r\")\n",
    "    plt.contour(xx, yy, z_proba[:,1].reshape(xx.shape), 15, linewidths=0.5, colors=\"k\")\n",
    "    \n",
    "    \n",
    "    # Plot also the training points\n",
    "    colors = [color_map[y] for y in y_train]\n",
    "\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=20, edgecolors=\"black\", linewidth=0.5, zorder=10)\n",
    "\n",
    "    plt.title(title, y=-0.3)\n",
    "\n",
    "plt.savefig(\"../reports/Graphs/semi-supervised-decision-boundary.pdf\", bbox_inches=\"tight\")\n",
    "# plt.suptitle(\"Unlabeled points are colored white\", y=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jGL-HbYlf5gi"
   },
   "source": [
    "## Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mBAljLdf5gi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set study globally here\n",
    "# study = \"1gzk7msy.optuna:v49\" # gbm classical\n",
    "# study = \"3vntumoi.optuna:v49\" # gbm classical-size\n",
    "# study = \"2t5zo50f.optuna:v49\" # gbm ml\n",
    "\n",
    "# study = \"37lymmzc.optuna:v49\" # gbm semi-classical\n",
    "# study = \"1vmti6db.optuna:v49\" # gbm semi classical-size\n",
    "# study = \"t55nd8r0.optuna:v49\" # gbm semi ml\n",
    "\n",
    "# transformer \n",
    "# study = \"3jpe46s1.optuna:v9\" # transformer classical\n",
    "# study = \"1qx3ul4j.optuna:v9\" # transformer classical-size\n",
    "study = \"2h81aiow.optuna:v9\" # transformer ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "xO_7X5Mlf5gi",
    "outputId": "8d21f681-6dfd-4e13-ab6f-c5ff28277bb7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see https://wandb.ai/fbv/thesis/runs/kwlaw02g/overview?workspace=user-karelze\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "# model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "# study_id = model_name.split(\"_\")[0]\n",
    "\n",
    "study_id = study.split(\".\")[0]\n",
    "\n",
    "\n",
    "artifact = run.use_artifact(study)\n",
    "study_dir = artifact.download()\n",
    "\n",
    "# artifact = run.use_artifact(model)\n",
    "# model_dir = artifact.download()\n",
    "\n",
    "study_name, version = study.split(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf8yf8LBf5gj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file  = open(f\"./artifacts/{study_id}.optuna:{version}/{study_id}.optuna\",'rb')\n",
    "study = pickle.load(file)\n",
    "\n",
    "sampler = study.sampler\n",
    "storage = f\"sqlite:///artifacts/{study_id}.optuna:{version}/{study_id}.db\"\n",
    "\n",
    "study = optuna.load_study(study_name=study_id, storage=storage, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG5plnddf5gj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LUT_LABELS = {\"Objective Value\": \"Accuracy\", \"bagging_temperature\": \"Bagging Temp.\", \"depth\":\"Depth\", \"l2_leaf_reg\": \"$\\ell_2$ Leaf Reg.\" , \"learning_rate\" : \"$\\eta$\", \"random_strength\": \"Rand. Str.\", \"attention_dropout\": \"Att Dropout\", \"d_token\":\"$d_e$\",\"ffn_dropout\":\"FFN Dropout\", \"weight_decay\":\"$\\lambda$\",\"lr\": \"$\\eta$\",\"n_blocks\":\"$L$\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia3zNhBbf5gk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Sequence\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from optuna._experimental import experimental_func\n",
    "from optuna._imports import try_import\n",
    "from optuna.logging import get_logger\n",
    "from optuna.study import Study\n",
    "from optuna.trial import FrozenTrial\n",
    "from optuna.visualization._contour import _AxisInfo\n",
    "from optuna.visualization._contour import _ContourInfo\n",
    "from optuna.visualization._contour import _get_contour_info\n",
    "from optuna.visualization._contour import _SubContourInfo\n",
    "from optuna.visualization.matplotlib._matplotlib_imports import _imports\n",
    "\n",
    "\n",
    "with try_import() as _optuna_imports:\n",
    "    import scipy\n",
    "\n",
    "if _imports.is_successful():\n",
    "    from optuna.visualization.matplotlib._matplotlib_imports import Axes\n",
    "    from optuna.visualization.matplotlib._matplotlib_imports import Colormap\n",
    "    from optuna.visualization.matplotlib._matplotlib_imports import ContourSet\n",
    "    from optuna.visualization.matplotlib._matplotlib_imports import plt\n",
    "\n",
    "_logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "CONTOUR_POINT_NUM = 100\n",
    "\n",
    "\n",
    "def plot_contour(\n",
    "    study: Study,\n",
    "    params: Optional[List[str]] = None,\n",
    "    *,\n",
    "    target: Optional[Callable[[FrozenTrial], float]] = None,\n",
    "    target_name: str = \"Objective Value\",\n",
    ") -> \"Axes\":\n",
    "    \"\"\"Plot the parameter relationship as contour plot in a study with Matplotlib.\n",
    "\n",
    "    Note that, if a parameter contains missing values, a trial with missing values is not plotted.\n",
    "\n",
    "    .. seealso::\n",
    "        Please refer to :func:`optuna.visualization.plot_contour` for an example.\n",
    "\n",
    "    Warnings:\n",
    "        Output figures of this Matplotlib-based\n",
    "        :func:`~optuna.visualization.matplotlib.plot_contour` function would be different from\n",
    "        those of the Plotly-based :func:`~optuna.visualization.plot_contour`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        The following code snippet shows how to plot the parameter relationship as contour plot.\n",
    "\n",
    "        .. plot::\n",
    "\n",
    "            import optuna\n",
    "\n",
    "\n",
    "            def objective(trial):\n",
    "                x = trial.suggest_float(\"x\", -100, 100)\n",
    "                y = trial.suggest_categorical(\"y\", [-1, 0, 1])\n",
    "                return x ** 2 + y\n",
    "\n",
    "\n",
    "            sampler = optuna.samplers.TPESampler(seed=10)\n",
    "            study = optuna.create_study(sampler=sampler)\n",
    "            study.optimize(objective, n_trials=30)\n",
    "\n",
    "            optuna.visualization.matplotlib.plot_contour(study, params=[\"x\", \"y\"])\n",
    "\n",
    "    Args:\n",
    "        study:\n",
    "            A :class:`~optuna.study.Study` object whose trials are plotted for their target values.\n",
    "        params:\n",
    "            Parameter list to visualize. The default is all parameters.\n",
    "        target:\n",
    "            A function to specify the value to display. If it is :obj:`None` and ``study`` is being\n",
    "            used for single-objective optimization, the objective values are plotted.\n",
    "\n",
    "            .. note::\n",
    "                Specify this argument if ``study`` is being used for multi-objective optimization.\n",
    "        target_name:\n",
    "            Target's name to display on the color bar.\n",
    "\n",
    "    Returns:\n",
    "        A :class:`matplotlib.axes.Axes` object.\n",
    "\n",
    "    .. note::\n",
    "        The colormap is reversed when the ``target`` argument isn't :obj:`None` or ``direction``\n",
    "        of :class:`~optuna.study.Study` is ``minimize``.\n",
    "    \"\"\"\n",
    "\n",
    "    _imports.check()\n",
    "    info = _get_contour_info(study, params, target, target_name)\n",
    "    return _get_contour_plot(info)\n",
    "\n",
    "\n",
    "\n",
    "def _get_contour_plot(info: _ContourInfo) -> \"Axes\":\n",
    "\n",
    "    sorted_params = info.sorted_params\n",
    "    sub_plot_infos = info.sub_plot_infos\n",
    "    reverse_scale = info.reverse_scale\n",
    "    target_name = info.target_name\n",
    "\n",
    "    if len(sorted_params) <= 1:\n",
    "        _, ax = plt.subplots()\n",
    "        return ax\n",
    "    n_params = len(sorted_params)\n",
    "\n",
    "    if n_params == 2:\n",
    "        # Set up the graph style.\n",
    "        fig, axs = plt.subplots()\n",
    "        cmap = _set_cmap(reverse_scale)\n",
    "\n",
    "        cs = _generate_contour_subplot(sub_plot_infos[0][0], axs, cmap)\n",
    "        if isinstance(cs, ContourSet):\n",
    "            axcb = fig.colorbar(cs)\n",
    "            axcb.set_label(\"Accuracy\")\n",
    "    else:\n",
    "        # Set up the graph style.\n",
    "        fig, axs = plt.subplots(n_params, n_params, figsize=(15 *CM, 15 *CM))\n",
    "        cmap = _set_cmap(reverse_scale)\n",
    "\n",
    "        # Prepare data and draw contour plots.\n",
    "        cs_list = []\n",
    "        for x_i in range(len(sorted_params)):\n",
    "            for y_i in range(len(sorted_params)):\n",
    "                ax = axs[y_i, x_i]\n",
    "                cs = _generate_contour_subplot(sub_plot_infos[y_i][x_i], ax, cmap)\n",
    "                if isinstance(cs, ContourSet):\n",
    "                    cs_list.append(cs)\n",
    "        if cs_list:\n",
    "            axcb = fig.colorbar(cs_list[0], ax=axs, aspect=50)\n",
    "            axcb.set_label(\"Accuracy\")\n",
    "\n",
    "    return axs\n",
    "\n",
    "\n",
    "def _set_cmap(reverse_scale: bool) -> \"Colormap\":\n",
    "    cmap = \"Blues_r\" if not reverse_scale else \"Blues\"\n",
    "    return plt.get_cmap(cmap)\n",
    "\n",
    "\n",
    "class _LabelEncoder:\n",
    "    def __init__(self) -> None:\n",
    "        self.labels: List[str] = []\n",
    "\n",
    "    def fit(self, labels: List[str]) -> \"_LabelEncoder\":\n",
    "        self.labels = sorted(set(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, labels: List[str]) -> List[int]:\n",
    "        return [self.labels.index(label) for label in labels]\n",
    "\n",
    "    def fit_transform(self, labels: List[str]) -> List[int]:\n",
    "        return self.fit(labels).transform(labels)\n",
    "\n",
    "    def get_labels(self) -> List[str]:\n",
    "        return self.labels\n",
    "\n",
    "    def get_indices(self) -> List[int]:\n",
    "        return list(range(len(self.labels)))\n",
    "\n",
    "\n",
    "def _calculate_griddata(\n",
    "    xaxis: _AxisInfo,\n",
    "    yaxis: _AxisInfo,\n",
    "    z_values_dict: Dict[Tuple[int, int], float],\n",
    ") -> Tuple[\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    np.ndarray,\n",
    "    List[int],\n",
    "    List[str],\n",
    "    List[int],\n",
    "    List[str],\n",
    "    List[Union[int, float]],\n",
    "    List[Union[int, float]],\n",
    "    List[Union[int, float]],\n",
    "]:\n",
    "\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    z_values = []\n",
    "    for x_value, y_value in zip(xaxis.values, yaxis.values):\n",
    "        if x_value is not None and y_value is not None:\n",
    "            x_values.append(x_value)\n",
    "            y_values.append(y_value)\n",
    "            x_i = xaxis.indices.index(x_value)\n",
    "            y_i = yaxis.indices.index(y_value)\n",
    "            z_values.append(z_values_dict[(x_i, y_i)])\n",
    "\n",
    "    # Return empty values when x or y has no value.\n",
    "    if len(x_values) == 0 or len(y_values) == 0:\n",
    "        return np.array([]), np.array([]), np.array([]), [], [], [], [], [], []\n",
    "\n",
    "    def _calculate_axis_data(\n",
    "        axis: _AxisInfo,\n",
    "        values: Sequence[Union[str, float]],\n",
    "    ) -> Tuple[np.ndarray, List[str], List[int], List[Union[int, float]]]:\n",
    "\n",
    "        # Convert categorical values to int.\n",
    "        cat_param_labels = []  # type: List[str]\n",
    "        cat_param_pos = []  # type: List[int]\n",
    "        returned_values: Sequence[Union[int, float]]\n",
    "        if axis.is_cat:\n",
    "            enc = _LabelEncoder()\n",
    "            returned_values = enc.fit_transform(list(map(str, values)))\n",
    "            cat_param_labels = enc.get_labels()\n",
    "            cat_param_pos = enc.get_indices()\n",
    "        else:\n",
    "            returned_values = list(map(lambda x: float(x), values))\n",
    "\n",
    "        # For x and y, create 1-D array of evenly spaced coordinates on linear or log scale.\n",
    "        if axis.is_log:\n",
    "            ci = np.logspace(np.log10(axis.range[0]), np.log10(axis.range[1]), CONTOUR_POINT_NUM)\n",
    "        else:\n",
    "            ci = np.linspace(axis.range[0], axis.range[1], CONTOUR_POINT_NUM)\n",
    "\n",
    "        return ci, cat_param_labels, cat_param_pos, list(returned_values)\n",
    "\n",
    "    xi, cat_param_labels_x, cat_param_pos_x, transformed_x_values = _calculate_axis_data(\n",
    "        xaxis,\n",
    "        x_values,\n",
    "    )\n",
    "    yi, cat_param_labels_y, cat_param_pos_y, transformed_y_values = _calculate_axis_data(\n",
    "        yaxis,\n",
    "        y_values,\n",
    "    )\n",
    "\n",
    "    # Calculate grid data points.\n",
    "    zi: np.ndarray = np.array([])\n",
    "    # Create irregularly spaced map of trial values\n",
    "    # and interpolate it with Plotly's interpolation formulation.\n",
    "    if xaxis.name != yaxis.name:\n",
    "        zmap = _create_zmap(transformed_x_values, transformed_y_values, z_values, xi, yi)\n",
    "        zi = _interpolate_zmap(zmap, CONTOUR_POINT_NUM)\n",
    "\n",
    "    return (\n",
    "        xi,\n",
    "        yi,\n",
    "        zi,\n",
    "        cat_param_pos_x,\n",
    "        cat_param_labels_x,\n",
    "        cat_param_pos_y,\n",
    "        cat_param_labels_y,\n",
    "        transformed_x_values,\n",
    "        transformed_y_values,\n",
    "        z_values,\n",
    "    )\n",
    "\n",
    "\n",
    "def _generate_contour_subplot(info: _SubContourInfo, ax: \"Axes\", cmap: \"Colormap\") -> \"ContourSet\":\n",
    "\n",
    "    if len(info.xaxis.indices) < 2 or len(info.yaxis.indices) < 2:\n",
    "        ax.label_outer()\n",
    "        return ax\n",
    "\n",
    "    # replace with lut values\n",
    "    # ax.set(xlabel=info.xaxis.name, ylabel=info.yaxis.name)\n",
    "    ax.set(xlabel=LUT_LABELS[info.xaxis.name], ylabel=LUT_LABELS[info.yaxis.name])\n",
    "    ax.set_xlim(info.xaxis.range[0], info.xaxis.range[1])\n",
    "    ax.set_ylim(info.yaxis.range[0], info.yaxis.range[1])\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=\"small\")\n",
    "    # ax.tick_params(axis='both', which='minor', labelsize=\"x-small\")\n",
    "    \n",
    "    if info.xaxis.name == info.yaxis.name:\n",
    "        ax.label_outer()\n",
    "        return ax\n",
    "\n",
    "    (\n",
    "        xi,\n",
    "        yi,\n",
    "        zi,\n",
    "        x_cat_param_pos,\n",
    "        x_cat_param_label,\n",
    "        y_cat_param_pos,\n",
    "        y_cat_param_label,\n",
    "        x_values,\n",
    "        y_values,\n",
    "        z_values,\n",
    "    ) = _calculate_griddata(info.xaxis, info.yaxis, info.z_values)\n",
    "    \n",
    "    # https://stackoverflow.com/a/55929839/5755604\n",
    "    max_value = max(z_values)\n",
    "    order = np.argsort(z_values)\n",
    "#             print(order)\n",
    "#             print(np.take(x_values, order))\n",
    "            \n",
    "#             print(np.arrange(x_values[order]))\n",
    "            \n",
    "    colors = ['black' if z < max_value else 'white' for z in z_values]\n",
    "            # marker = [\"o\" if z != max_value else \"x\" for z in z_values]\n",
    "    x_values = np.take(x_values, order)\n",
    "    y_values = np.take(y_values, order)\n",
    "    colors = np.take(colors, order)\n",
    "    \n",
    "    # print(x_values)\n",
    "    \n",
    "    cs = None\n",
    "    if len(zi) > 0:\n",
    "        if info.xaxis.is_log:\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.tick_params(axis='x', which='major', labelsize=\"xx-small\")\n",
    "        if info.yaxis.is_log:\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.tick_params(axis='y', which='major', labelsize=\"xx-small\")\n",
    "        if info.xaxis.name != info.yaxis.name:\n",
    "            # Contour the gridded data.\n",
    "            ax.contour(xi, yi, zi, 15, linewidths=0.5, colors=\"k\")\n",
    "            cs = ax.contourf(xi, yi, zi, 15, cmap=cmap.reversed())\n",
    "            # Plot data points.\n",
    "\n",
    "            ax.scatter(\n",
    "                x_values,\n",
    "                y_values,\n",
    "                marker=\"o\",\n",
    "                c=colors,\n",
    "                s=10,\n",
    "                edgecolors=\"grey\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "    if info.xaxis.is_cat:\n",
    "        ax.set_xticks(x_cat_param_pos)\n",
    "        ax.set_xticklabels(x_cat_param_label)\n",
    "    if info.yaxis.is_cat:\n",
    "        ax.set_yticks(y_cat_param_pos)\n",
    "        ax.set_yticklabels(y_cat_param_label)\n",
    "    ax.label_outer()\n",
    "    return cs\n",
    "\n",
    "\n",
    "def _create_zmap(\n",
    "    x_values: List[Union[int, float]],\n",
    "    y_values: List[Union[int, float]],\n",
    "    z_values: List[float],\n",
    "    xi: np.ndarray,\n",
    "    yi: np.ndarray,\n",
    ") -> Dict[Tuple[int, int], float]:\n",
    "\n",
    "    # Creates z-map from trial values and params.\n",
    "    # z-map is represented by hashmap of coordinate and trial value pairs.\n",
    "    #\n",
    "    # Coordinates are represented by tuple of integers, where the first item\n",
    "    # indicates x-axis index and the second item indicates y-axis index\n",
    "    # and refer to a position of trial value on irregular param grid.\n",
    "    #\n",
    "    # Since params were resampled either with linspace or logspace\n",
    "    # original params might not be on the x and y axes anymore\n",
    "    # so we are going with close approximations of trial value positions.\n",
    "    zmap = dict()\n",
    "    for x, y, z in zip(x_values, y_values, z_values):\n",
    "        xindex = int(np.argmin(np.abs(xi - x)))\n",
    "        yindex = int(np.argmin(np.abs(yi - y)))\n",
    "        zmap[(xindex, yindex)] = z\n",
    "\n",
    "    return zmap\n",
    "\n",
    "\n",
    "def _interpolate_zmap(zmap: Dict[Tuple[int, int], float], contour_plot_num: int) -> np.ndarray:\n",
    "\n",
    "    # Implements interpolation formulation used in Plotly\n",
    "    # to interpolate heatmaps and contour plots\n",
    "    # https://github.com/plotly/plotly.js/blob/95b3bd1bb19d8dc226627442f8f66bce9576def8/src/traces/heatmap/interp2d.js#L15-L20\n",
    "    # citing their doc:\n",
    "    #\n",
    "    # > Fill in missing data from a 2D array using an iterative\n",
    "    # > poisson equation solver with zero-derivative BC at edges.\n",
    "    # > Amazingly, this just amounts to repeatedly averaging all the existing\n",
    "    # > nearest neighbors\n",
    "    #\n",
    "    # Plotly's algorithm is equivalent to solve the following linear simultaneous equation.\n",
    "    # It is discretization form of the Poisson equation.\n",
    "    #\n",
    "    #     z[x, y] = zmap[(x, y)]                                  (if zmap[(x, y)] is given)\n",
    "    # 4 * z[x, y] = z[x-1, y] + z[x+1, y] + z[x, y-1] + z[x, y+1] (if zmap[(x, y)] is not given)\n",
    "\n",
    "    a_data = []\n",
    "    a_row = []\n",
    "    a_col = []\n",
    "    b = np.zeros(contour_plot_num**2)\n",
    "    for x in range(contour_plot_num):\n",
    "        for y in range(contour_plot_num):\n",
    "            grid_index = y * contour_plot_num + x\n",
    "            if (x, y) in zmap:\n",
    "                a_data.append(1)\n",
    "                a_row.append(grid_index)\n",
    "                a_col.append(grid_index)\n",
    "                b[grid_index] = zmap[(x, y)]\n",
    "            else:\n",
    "                for dx, dy in ((-1, 0), (1, 0), (0, -1), (0, 1)):\n",
    "                    if 0 <= x + dx < contour_plot_num and 0 <= y + dy < contour_plot_num:\n",
    "                        a_data.append(1)\n",
    "                        a_row.append(grid_index)\n",
    "                        a_col.append(grid_index)\n",
    "                        a_data.append(-1)\n",
    "                        a_row.append(grid_index)\n",
    "                        a_col.append(grid_index + dy * contour_plot_num + dx)\n",
    "\n",
    "    z = scipy.sparse.linalg.spsolve(scipy.sparse.csc_matrix((a_data, (a_row, a_col))), b)\n",
    "\n",
    "    return z.reshape((contour_plot_num, contour_plot_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfsZaU3Af5gl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "axes = plot_contour(study)\n",
    "plt.savefig(f\"../reports/Graphs/{study_id}-hyperparam-search-space.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "roRmlg_nf5gl"
   },
   "source": [
    "## Training and Validation Loss of GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aJWetuuf5gm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "model = \"3laathab_CatBoostClassifier_default.cbm:v7\"\n",
    "\n",
    "model_name = model.split(\"/\")[-1].split(\":\")[0]\n",
    "study_id = model_name.split(\"_\")[0]\n",
    "\n",
    "artifact = run.use_artifact(model)\n",
    "model_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6BsBFvcf5gm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "with open(Path(model_dir,model_name[:-4]+\"_training.json\"), 'r') as j:\n",
    "     contents = json.loads(j.read())\n",
    "    \n",
    "# extract relevant keys\n",
    "iterations = contents.get(\"iterations\")\n",
    "test_metrics = [d['name'] for d in contents['meta']['test_metrics'] ]\n",
    "test_results = [d['test'] for d in iterations]\n",
    "learn_metrics = [d['name'] for d in contents['meta']['learn_metrics'] ]\n",
    "learn_results = [d['learn'] for d in iterations]\n",
    "\n",
    "metrics_learn = pd.DataFrame(learn_results, columns=learn_metrics).add_suffix(\" (train)\")\n",
    "metrics_test = pd.DataFrame(test_results, columns=test_metrics).add_suffix(\" (val)\")\n",
    "\n",
    "learning_metrics = pd.concat([metrics_learn, metrics_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UBq2BTQg0jl"
   },
   "outputs": [],
   "source": [
    "# viz transformer\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "model = \"fbv/thesis/viz_fttransformer:latest\"\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "artifact = run.use_artifact(model)\n",
    "model_dir = artifact.download()\n",
    "\n",
    "learning_metrics = pd.read_parquet(Path(model_dir, \"results\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLZi87lqGvdI"
   },
   "outputs": [],
   "source": [
    "fig, (ax2, ax4, ax3, ax1) = plt.subplots(4, 1, figsize=(12*CM, 15*CM))\n",
    "\n",
    "loss_train = learning_metrics[[\"default_train_loss\", \"activation_train_loss\", \"lr_scheduler_train_loss\", \"sample_weighting_train_loss\", \"label_smoothing_train_loss\"]].dropna(how=\"any\").reset_index(drop=True).rolling(20).mean()\n",
    "\n",
    "ax2.plot(loss_train.index,loss_train[\"default_train_loss\"], label=\"Default\", linewidth=1, zorder=100)\n",
    "ax2.plot(loss_train.index,loss_train[\"activation_train_loss\"], label=\"Activation\", linewidth=1)\n",
    "ax2.plot(loss_train.index,loss_train[\"label_smoothing_train_loss\"], label=\"Label Smoothing\", linewidth=1)\n",
    "ax2.plot(loss_train.index,loss_train[\"lr_scheduler_train_loss\"], label=\"Lr Schedule\", linewidth=1)\n",
    "ax2.plot(loss_train.index,loss_train[\"sample_weighting_train_loss\"], label=\"Sample Weighting\", linewidth=1)\n",
    "ax2.set_ylabel(\"Log Loss (Train)\")\n",
    "\n",
    "ax2.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "# ax2.set_xlabel(\"Iteration\")\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "step_size = int(max(learning_metrics[\"default_train_step\"]) / n_epochs)\n",
    "\n",
    "for i in range(step_size, step_size * n_epochs + 1, step_size):\n",
    "    ax2.axvline(x=i, linestyle='--', color='grey', linewidth=0.5)\n",
    "\n",
    "ax2.set_xlim(0, step_size * n_epochs - 1)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "acc_train = learning_metrics[[\"default_train_accuracy\", \"activation_train_accuracy\", \"lr_scheduler_train_accuracy\", \"sample_weighting_train_accuracy\", \"label_smoothing_train_accuracy\"]].dropna(how=\"any\").reset_index(drop=True).rolling(20).mean()\n",
    "ax4.plot(acc_train.index, acc_train[\"default_train_accuracy\"], label=\"Default\", linewidth=1, zorder=100)\n",
    "ax4.plot(acc_train.index, acc_train[\"activation_train_accuracy\"], label=\"Activation\", linewidth=1)\n",
    "ax4.plot(acc_train.index, acc_train[\"label_smoothing_train_accuracy\"], label=\"Label Smoothing\", linewidth=1)\n",
    "ax4.plot(acc_train.index, acc_train[\"lr_scheduler_train_accuracy\"], label=\"Lr Schedule\", linewidth=1)\n",
    "ax4.plot(acc_train.index, acc_train[\"sample_weighting_train_accuracy\"], label=\"Sample Weighting\", linewidth=1)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "step_size = int(len(acc_train) / n_epochs)\n",
    "\n",
    "for i in range(step_size, step_size * n_epochs + 1, step_size):\n",
    "    ax4.axvline(x=i, linestyle='--', color='grey', linewidth=0.5)\n",
    "\n",
    "ax4.set_xlim(0, step_size * n_epochs - 1)\n",
    "ax4.set_ylim(0.67, 0.80)\n",
    "\n",
    "\n",
    "ax4.set_ylabel(\"Accuracy (Train)\")\n",
    "ax4.set_xlabel(\"Iteration\")\n",
    "ax4.yaxis.set_major_formatter(PercentFormatter(1.0,decimals=2))\n",
    "ax4.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "loss_val = learning_metrics.groupby(\"default_epoch\")[[\"default_val_loss\", \"activation_val_loss\", \"lr_scheduler_val_loss\", \"sample_weighting_val_loss\", \"label_smoothing_val_loss\"]].mean()\n",
    "\n",
    "\n",
    "ax3.plot(loss_val.index,loss_val[\"default_val_loss\"], label=\"Default\", linewidth=1, zorder=100)\n",
    "ax3.plot(loss_val.index,loss_val[\"activation_val_loss\"], label=\"Activation\", linewidth=1)\n",
    "ax3.plot(loss_val.index,loss_val[\"label_smoothing_val_loss\"], label=\"Label Smoothing\", linewidth=1)\n",
    "ax3.plot(loss_val.index,loss_val[\"lr_scheduler_val_loss\"], label=\"Lr Schedule\", linewidth=1)\n",
    "ax3.plot(loss_val.index,loss_val[\"sample_weighting_val_loss\"], label=\"Sample Weighting\", linewidth=1)\n",
    "ax3.set_ylabel(\"Log Loss (Val)\")\n",
    "# ax3.set_xlabel(\"Step\")\n",
    "n_epochs = 10\n",
    "\n",
    "step_size = int(len(loss_val) / n_epochs)\n",
    "\n",
    "for i in range(step_size, step_size * n_epochs + 1, step_size):\n",
    "    ax3.axvline(x=i, linestyle='--', color='grey', linewidth=0.5)\n",
    "\n",
    "# ax.set_ylim(0.6, 0.72)\n",
    "\n",
    "# ax3.set_xlabel(\"Step\")\n",
    "ax3.set_ylabel(\"Log Loss (Val)\")\n",
    "ax3.set_xlim(0, step_size * n_epochs - 1)\n",
    "\n",
    "ax3.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "acc_val = learning_metrics.groupby(\"default_epoch\")[[\"default_val_accuracy\", \"activation_val_accuracy\", \"lr_scheduler_val_accuracy\", \"sample_weighting_val_accuracy\", \"label_smoothing_val_accuracy\"]].mean()\n",
    "ax1.plot(acc_val.index,acc_val[\"default_val_accuracy\"], label=\"Default\", linewidth=1, zorder=100)\n",
    "ax1.plot(acc_val.index,acc_val[\"activation_val_accuracy\"], label=\"Activation\", linewidth=1)\n",
    "ax1.plot(acc_val.index,acc_val[\"label_smoothing_val_accuracy\"], label=\"Label Smoothing\", linewidth=1)\n",
    "ax1.plot(acc_val.index,acc_val[\"lr_scheduler_val_accuracy\"], label=\"Lr Schedule\", linewidth=1)\n",
    "ax1.plot(acc_val.index,acc_val[\"sample_weighting_val_accuracy\"], label=\"Sample Weighting\", linewidth=1)\n",
    "ax1.set_ylabel(\"Log Loss (Val)\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "n_epochs = 10\n",
    "\n",
    "step_size = int(len(loss_val) / n_epochs)\n",
    "\n",
    "for i in range(step_size, step_size * n_epochs + 1, step_size):\n",
    "    ax1.axvline(x=i, linestyle='--', color='grey', linewidth=0.5)\n",
    "\n",
    "ax1.set_xlim(0, step_size * n_epochs - 1)\n",
    "# ax.set_ylim(0.6, 0.72)\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Accuracy (Val)\")\n",
    "ax1.yaxis.set_major_formatter(PercentFormatter(1.0,decimals=2))\n",
    "ax1.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', frameon=False, ncol=3, bbox_to_anchor = (0, -0.07, 1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"fttransformer-optimisations-loss-acc.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vqcUXygFfrX"
   },
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"default_val_acc\"],'--', label=\"default\", linewidth=1, zorder=100)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"early_stopping_val_acc\"],'--', label=\"early stopping\", linewidth=1)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"grow_policy_val_acc\"],'--', label=\"grow policy\", linewidth=1)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"exp_weighting_val_acc\"],'--', label=\"sample weighting\", linewidth=1)\n",
    "\n",
    "ax1.yaxis.set_major_formatter(PercentFormatter(1.0,decimals=2))\n",
    "ax1.set_ylabel(\"Accuracy (Val)\")\n",
    "ax1.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "ax1.set_ylim(0.57, 0.64)\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "\n",
    "# plot log loss\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"default_val_log\"], label=\"default\", linewidth=1, zorder=100)\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"early_stopping_val_log\"], label=\"early stopping\", linewidth=1)\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"grow_policy_val_log\"], label=\"grow policy\", linewidth=1)\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"exp_weighting_val_log\"], label=\"sample weighting\", linewidth=1)\n",
    "ax2.set_ylabel(\"Log Loss (Val)\")\n",
    "ax2.set_ylim(0.55, 0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax1.legend(frameon=False, loc='lower right')\n",
    "ax2.legend(frameon=False, loc='lower right')\n",
    "\n",
    "plt.savefig(f\"../reports/Graphs/gbm-optimisations-loss-acc.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "FNsp-_q3yEO0",
    "outputId": "209206c7-9f3f-4164-ed8d-6c59e3c0435b"
   },
   "outputs": [],
   "source": [
    "# viz gradient boosting\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "model = \"exp_weighting:latest\"\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "artifact = run.use_artifact(model)\n",
    "model_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "UhfLwS2rVzDa",
    "outputId": "4fcfac69-165d-476d-c686-b309f86b6dae"
   },
   "outputs": [],
   "source": [
    "learning_metrics = pd.read_parquet(Path(model_dir, \"results\"))\n",
    "learning_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "IILG4fssf5gn",
    "outputId": "b942d8cf-872e-4dd2-ca3a-4cb5ec3dcb03",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = plt.subplots(2, 1, figsize=(12*CM,7.5*CM), sharex=True)\n",
    "\n",
    "# plot accuracy\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"default_learn_acc\"], label=\"Train\", linewidth=1)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"default_val_acc\"], label=\"Val\",linewidth=1)\n",
    "ax1.yaxis.set_major_formatter(PercentFormatter(1.0,decimals=2))\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "\n",
    "# plot log loss\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"default_learn_log\"], label=\"Train\",linewidth=1)    \n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"default_val_log\"], label=\"Val\",linewidth=1)   \n",
    "ax2.set_ylabel(\"Log Loss\")\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', frameon=False, ncol=4, bbox_to_anchor = (0, -0.03, 1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"gbm-train-val-loss-acc.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "9bzql0oSf5gn",
    "outputId": "0efd90c8-1cad-4b94-cdd6-b82d39a5ef68"
   },
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = plt.subplots(2, 1, figsize=(12*CM,7.5*CM), sharex=True)\n",
    "\n",
    "# [\"default\", \"depth\", \"early_stopping\", \"border_count\", \"grow_policy\", \"exp_weighting\"]\n",
    "\n",
    "# plot accuracy\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"default_val_acc\"], label=\"Default\", linewidth=1, zorder=100)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"early_stopping_val_acc\"], label=\"Early Stopping\", linewidth=1)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"grow_policy_val_acc\"], label=\"Grow Policy\", linewidth=1)\n",
    "ax1.plot(learning_metrics.index,learning_metrics[\"exp_weighting_val_acc\"], label=\"Sample Weighting\", linewidth=1)\n",
    "\n",
    "ax1.yaxis.set_major_formatter(PercentFormatter(1.0,decimals=2))\n",
    "ax1.set_ylabel(\"Accuracy (Val)\")\n",
    "ax1.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "#ax1.set_ylim(0.57, 0.64)\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "\n",
    "# plot log loss\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"default_val_log\"], label=\"Default\", linewidth=1, zorder=100)\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"early_stopping_val_log\"], label=\"Early Stopping\", linewidth=1)\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"grow_policy_val_log\"], label=\"Grow Policy\", linewidth=1)\n",
    "ax2.plot(learning_metrics.index,learning_metrics[\"exp_weighting_val_log\"], label=\"Sample Weighting\", linewidth=1)\n",
    "ax2.set_ylabel(\"Log Loss (Val)\")\n",
    "# ax2.set_ylim(0.55, 0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', frameon=False, ncol=4, bbox_to_anchor = (0, -0.03, 1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"gbm-optimisations-loss-acc.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r5ZnoZIG26K_"
   },
   "source": [
    "## Warm-up Cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKtsTTxZ3c0J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNDCa3N02Rem",
    "outputId": "37e07f6a-e51f-4247-c832-eaeadf9d64b4"
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(10, 1)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    layer.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "max_iters = 2**10\n",
    "# saw recommendation of 5 - 10 % of total training budget or 100 to 500 steps\n",
    "warmup = int(0.05 * max_iters)\n",
    "print(f\"warmup steps: {warmup}\")\n",
    "print(max_iters)\n",
    "\n",
    "\n",
    "scheduler = CosineWarmupScheduler(\n",
    "    optimizer=optimizer, warmup=warmup, max_iters=max_iters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "tPDwPL7I33RK",
    "outputId": "a6ad56fb-dd8a-47ae-b747-6e2d4f090f31"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "factor = [scheduler.get_lr_factor(i) * lr  for i in range(0, max_iters)]\n",
    "\n",
    "fig = plt.figure(figsize=(12 * CM, 3.5 * CM))\n",
    "\n",
    "plt.plot(factor, linewidth=1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "\n",
    "plt.savefig(f\"lr-lin-warmup-cosine-decay.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hc7pkVqe4qNw"
   },
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: replace with versioned results\n",
    "\n",
    "fi_classical = pd.read_parquet(f\"gs://thesis-bucket-option-trade-classification/data/results/ise_supervised_test_classical_feature_importance.parquet\")\n",
    "fi_gbm = pd.read_parquet(f\"gs://thesis-bucket-option-trade-classification/data/results/ise_supervised_test_gbm_feature_importance.parquet\")\n",
    "fi_transformer = pd.read_parquet(f\"gs://thesis-bucket-option-trade-classification/data/results/ise_supervised_test_fttransformer_feature_importance.parquet\")\n",
    "\n",
    "# set features to nan that are not part of dataset\n",
    "fi_classical.loc[[\"size_ex (grouped)\", \"TRADE_SIZE\"],[\"quote(best)->quote(ex) values\",\"quote(best)->quote(ex) std\"]] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fi = pd.concat([fi_classical, fi_gbm, fi_transformer], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18*CM, 12*CM), sharex=False, sharey=True)\n",
    "\n",
    "# adapted from here: https://stackoverflow.com/a/15214551/5755604\n",
    "ind = np.arange(len(fi))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].barh(ind, fi[\"quote(best)->quote(ex) values\"], width, xerr=fi[\"quote(best)->quote(ex) std\"], label=\"Classical\")\n",
    "axes[0].barh(ind+width, fi[\"gbm(classical) values\"], width, xerr=fi[\"gbm(classical) std\"], label=\"GBRT\")\n",
    "axes[0].barh(ind+width + width, fi[\"fttransformer(classical) values\"], width, xerr=fi[\"fttransformer(classical) std\"], label=\"Transformer\")\n",
    "axes[0].axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[0].set_xlim([-0.15,0.15])\n",
    "\n",
    "axes[1].barh(ind, fi[\"trade_size(ex)->quote(best)->quote(ex)->depth(best)->depth(ex)->rev_tick(all) values\"], width, xerr=fi[\"trade_size(ex)->quote(best)->quote(ex)->depth(best)->depth(ex)->rev_tick(all) std\"], label=\"Classical\")\n",
    "axes[1].barh(ind+width, fi[\"gbm(classical-size) values\"], width, xerr=fi[\"gbm(classical-size) std\"], label=\"GBRT\")\n",
    "axes[1].barh(ind+width + width, fi[\"fttransformer(classical-size) values\"], width, xerr=fi[\"fttransformer(classical-size) std\"], label=\"Transformer\")\n",
    "axes[1].axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[1].set_xlim([-0.15,0.15])\n",
    "\n",
    "axes[2].barh(ind, fi[\"trade_size(ex)->quote(best)->quote(ex)->depth(best)->depth(ex)->rev_tick(all) values\"], width, xerr=fi[\"trade_size(ex)->quote(best)->quote(ex)->depth(best)->depth(ex)->rev_tick(all) std\"], label=\"Classical\")\n",
    "axes[2].barh(ind+width, fi[\"gbm(ml) values\"], width, xerr=fi[\"gbm(ml) std\"], label=\"GBRT\")\n",
    "axes[2].barh(ind+width + width, fi[\"fttransformer(ml) values\"], width, xerr=fi[\"fttransformer(ml) std\"], label=\"Transformer\")\n",
    "axes[2].axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[2].set_xlim([-0.15,0.15])\n",
    "\n",
    "\n",
    "# set y-labels\n",
    "labels = ['Price Lead All', 'Price Lag All', 'Price Lead Ex', 'Price  Lag Ex', 'Quotes NBBO', 'Quotes Ex', 'Trade Price', \"Quotes Size\", 'Trade Size', 'Strike Price', 'Time To Maturity', 'Option Type', 'Root', 'Moneyness', \"Day Volume\", 'Issue Type']\n",
    "axes[0].set(yticks=ind + width, yticklabels=labels, ylim=[2*width - 1, len(fi)])\n",
    "\n",
    "# set x-labels\n",
    "axes[0].set_xlabel(\"SAGE Value\")\n",
    "axes[1].set_xlabel(\"SAGE Value\")\n",
    "axes[2].set_xlabel(\"SAGE Value\")\n",
    "\n",
    "# set y-labels\n",
    "axes[0].set_title(\"Set Classical\")\n",
    "axes[1].set_title(\"Set Classical-Size\")\n",
    "axes[2].set_title(\"Set Options\")\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc = \"lower center\", frameon=False, bbox_to_anchor=(0.5, -0.05), ncols=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"../reports/Graphs/sage-importances.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
