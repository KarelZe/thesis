{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Tue_Mar__8_18:18:20_PST_2022\n",
            "Cuda compilation tools, release 11.6, V11.6.124\n",
            "Build cuda_11.6.r11.6/compiler.31057947_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vajs4UZ4V3-N",
        "outputId": "973dbdd5-5772-4262-ea0b-93d21ab5c350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.8/dist-packages (2022.11.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/lib/python3/dist-packages (from gcsfs) (1.5.1)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/lib/python3/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.3)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from gcsfs) (2.22.0)\n",
            "Requirement already satisfied: fsspec==2022.11.0 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2022.11.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/lib/python3/dist-packages (from gcsfs) (0.4.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (19.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage->gcsfs) (2.4.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage->gcsfs) (2.3.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage->gcsfs) (2.11.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.8)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.21.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.57.0)\n",
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.8/dist-packages (2022.11.0)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (2.6.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (1.24.0rc1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (1.5.2)\n",
            "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from fastparquet) (20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from fastparquet) (2022.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.5.0->fastparquet) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.14.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.8/dist-packages (11.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu116\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.24.0rc1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting torch[dynamo]\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu116/torch-1.14.0.dev20221203%2Bcu116-cp38-cp38-linux_x86_64.whl (1991.1 MB)\n",
            "Collecting typing-extensions\n",
            "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.0rc1-py3-none-any.whl (2.0 MB)\n",
            "Collecting torchtriton==2.0.0+0d7e753227\n",
            "  Using cached https://download.pytorch.org/whl/nightly/torchtriton-2.0.0%2B0d7e753227-cp38-cp38-linux_x86_64.whl (18.7 MB)\n",
            "Collecting sympy\n",
            "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "Collecting jinja2; extra == \"dynamo\"\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting cmake\n",
            "  Using cached cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Using cached MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: numpy, typing-extensions, networkx, cmake, filelock, torchtriton, mpmath, sympy, MarkupSafe, jinja2, torch\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.0rc1\n",
            "    Uninstalling numpy-1.24.0rc1:\n",
            "      Successfully uninstalled numpy-1.24.0rc1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.4.0\n",
            "    Uninstalling typing-extensions-4.4.0:\n",
            "      Successfully uninstalled typing-extensions-4.4.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0rc1\n",
            "    Uninstalling networkx-3.0rc1:\n",
            "      Successfully uninstalled networkx-3.0rc1\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.25.0\n",
            "    Uninstalling cmake-3.25.0:\n",
            "      Successfully uninstalled cmake-3.25.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: torchtriton\n",
            "    Found existing installation: torchtriton 2.0.0+0d7e753227\n",
            "    Uninstalling torchtriton-2.0.0+0d7e753227:\n",
            "      Successfully uninstalled torchtriton-2.0.0+0d7e753227\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.2.1\n",
            "    Uninstalling mpmath-1.2.1:\n",
            "      Successfully uninstalled mpmath-1.2.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11.1\n",
            "    Uninstalling sympy-1.11.1:\n",
            "      Successfully uninstalled sympy-1.11.1\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.1\n",
            "    Uninstalling MarkupSafe-2.1.1:\n",
            "      Successfully uninstalled MarkupSafe-2.1.1\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.14.0.dev20221203+cu116\n",
            "    Uninstalling torch-1.14.0.dev20221203+cu116:\n",
            "      Successfully uninstalled torch-1.14.0.dev20221203+cu116\n",
            "Successfully installed MarkupSafe-2.1.1 cmake-3.25.0 filelock-3.8.0 jinja2-3.1.2 mpmath-1.2.1 networkx-3.0rc1 numpy-1.24.0rc1 sympy-1.11.1 torch-1.14.0.dev20221203+cu116 torchtriton-2.0.0+0d7e753227 typing-extensions-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install gcsfs\n",
        "!pip install fastparquet\n",
        "!pip install pynvml\n",
        "# https://pytorch.org/get-started/pytorch-2.0/#faqs\n",
        "!pip install numpy --pre torch[dynamo] --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu116"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NRm1-RKDHY2C"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, List, Optional, Tuple, Union\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "from typing import List, Optional,Tuple, Union, Callable\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch import nn, optim, tensor, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
        "import logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qxmlWlnPHYyp"
      },
      "outputs": [],
      "source": [
        "class TabDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for fitting timeseries models.\n",
        "    Args:\n",
        "        Dataset (Dataset): dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        X: pd.DataFrame,\n",
        "        y: pd.Series,\n",
        "        cat_features: Optional[List[str]] = None,\n",
        "        cat_unique_counts: Optional[List[int]] = None,\n",
        "        threshold: float = 1e-7,\n",
        "        device:str=\"cpu\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Tabular data set holding data for the model.\n",
        "        Args:\n",
        "            X (pd.DataFrame): feature matrix.\n",
        "            y (pd.Series): target.\n",
        "            cat_features (Optional[List[str]], optional): List with categorical columns.\n",
        "            Defaults to None.\n",
        "            cat_unique_counts (Optional[List[int]], optional): Number of categories per\n",
        "            categorical feature. Defaults to None.\n",
        "            threshold (float, optional): threshold for z-standardization.\n",
        "            Defaults to 1e-7.\n",
        "        \"\"\"\n",
        "        self._cat_unique_counts: Union[\n",
        "            Optional[List[int]], Tuple[()]\n",
        "        ] = cat_unique_counts\n",
        "\n",
        "        # calculate cat indices\n",
        "        features = X.columns.tolist()\n",
        "        cat_features = [] if not cat_features else cat_features\n",
        "        self._cat_idx = [features.index(i) for i in cat_features if i in features]\n",
        "\n",
        "        # calculate cont indices\n",
        "        cont_features = [x for x in features if x not in cat_features]\n",
        "        self._cont_idx = [features.index(i) for i in cont_features if i in features]\n",
        "\n",
        "        if not self._cat_unique_counts:\n",
        "            self._cat_unique_counts = ()\n",
        "\n",
        "        assert (\n",
        "            X.shape[0] == y.shape[0]\n",
        "        ), \"Length of feature matrix must match length of target.\"\n",
        "        assert len(cat_features) == len(\n",
        "            self._cat_unique_counts\n",
        "        ), \"For all categorical features the number of unique entries must be provided.\"\n",
        "\n",
        "        # adjust target to be either 0 or 1\n",
        "        self._y = torch.tensor(y.values).float()\n",
        "        self._y[self._y < 0] = 0\n",
        "\n",
        "        # cut into continous and categorical tensor\n",
        "        self._X_cat = torch.tensor(X.iloc[:, self._cat_idx].values).int()\n",
        "        self._X_cont = torch.tensor(X.iloc[:, self._cont_idx].values).float()\n",
        "\n",
        "        # pre-fetch dataset to device\n",
        "        # https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609/15\n",
        "        self._y = self._y.to(device)\n",
        "        self._X_cat = self._X_cat.to(device)\n",
        "        self._X_cont = self._X_cont.to(device)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Length of dataset.\n",
        "        Returns:\n",
        "            int: length\n",
        "        \"\"\"\n",
        "        return len(self._X_cont)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get sample for model.\n",
        "        Args:\n",
        "            idx (int): index of prediction (between ``0`` and ``len(dataset) - 1``)\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: X_cat, X_cont and y.\n",
        "        \"\"\"\n",
        "        return self._X_cat[idx], self._X_cont[idx], self._y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HUjGdNsxJiHF"
      },
      "outputs": [],
      "source": [
        "class TabDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a set of tensors that can be much faster than\n",
        "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
        "    the dataset and calls cat (slow).\n",
        "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
        "    \"\"\"\n",
        "    def __init__(self, *tensors, batch_size=32, shuffle=False, **kwargs: Any):\n",
        "        \"\"\"\n",
        "        Initialize a TabDataLoader.\n",
        "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "        :returns: A FastTensorDataLoader.\n",
        "        \"\"\"\n",
        "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
        "        self.tensors = tensors\n",
        "\n",
        "        self.dataset_len = self.tensors[0].shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            r = torch.randperm(self.dataset_len)\n",
        "            self.tensors = [t[r] for t in self.tensors]\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EeBiVlxWHW2z"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of residual connections.\n",
        "    Args:\n",
        "        nn (nn.Module): module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fn: nn.Module):\n",
        "        \"\"\"\n",
        "        Residual connection.\n",
        "        Args:\n",
        "            fn (nn.Module): network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of residual connections.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of pre-normalization.\n",
        "    Args:\n",
        "        nn (nn.module): module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, fn: nn.Module):\n",
        "        \"\"\"\n",
        "        Pre-normalization.\n",
        "        Consists of layer for layer normalization followed by another network.\n",
        "        Args:\n",
        "            dim (int): Number of dimensions of normalized shape.\n",
        "            fn (nn.Module): network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of pre-normalization layers.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    r\"\"\"\n",
        "    Implementation of the GeGLU activation function.\n",
        "    Given by:\n",
        "    $\\operatorname{GeGLU}(x, W, V, b, c)=\\operatorname{GELU}(x W+b) \\otimes(x V+c)$\n",
        "    Proposed in https://arxiv.org/pdf/2002.05202v1.pdf.\n",
        "    Args:\n",
        "        nn (torch.Tensor): module\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of GeGlU activation.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        x, gates = x.chunk(2, dim=-1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of feed forward network.\n",
        "    Args:\n",
        "        nn (nn.module): module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, mult: int = 4, dropout: float = 0.0):\n",
        "        \"\"\"\n",
        "        Feed forward network.\n",
        "        Network consists of input layer, GEGLU activation, dropout layer,\n",
        "        and output layer.\n",
        "        Args:\n",
        "            dim (int): dimension of input and output layer.\n",
        "            mult (int, optional): Scaling factor for output dimension of input layer or\n",
        "            input dimension of output layer. Defaults to 4.\n",
        "            dropout (float, optional): Degree of dropout. Defaults to 0.0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of feed forward network.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch implementation of attention.\n",
        "    Args:\n",
        "        nn (nn.Module): module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dim: int, heads: int = 8, dim_head: int = 16, dropout: float = 0.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Attention.\n",
        "        Args:\n",
        "            dim (int): Number of dimensions.\n",
        "            heads (int, optional): Number of attention heads. Defaults to 8.\n",
        "            dim_head (int, optional): Dimension of attention heads. Defaults to 16.\n",
        "            dropout (float, optional): Degree of dropout. Defaults to 0.0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head**-0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of attention module.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n",
        "        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\", h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer.\n",
        "    Based on paper:\n",
        "    https://arxiv.org/abs/1706.03762\n",
        "    Args:\n",
        "        nn (nn.Module): Module with transformer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens: int,\n",
        "        dim: int,\n",
        "        depth: int,\n",
        "        heads: int,\n",
        "        dim_head: int,\n",
        "        attn_dropout: float,\n",
        "        ff_dropout: float,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Classical transformer.\n",
        "        Args:\n",
        "            num_tokens (int): Number of tokens i. e., unique classes + special tokens.\n",
        "            dim (int): Number of dimensions.\n",
        "            depth (int): Depth of encoder / decoder.\n",
        "            heads (int): Number of attention heads.\n",
        "            dim_head (int): Dimensions of attention heads.\n",
        "            attn_dropout (float): Degree of dropout in attention.\n",
        "            ff_dropout (float): Degree of dropout in feed-forward network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeds = nn.Embedding(num_tokens, dim)  # (Embed the categorical features.)\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        Residual(\n",
        "                            PreNorm(\n",
        "                                dim,\n",
        "                                Attention(\n",
        "                                    dim,\n",
        "                                    heads=heads,\n",
        "                                    dim_head=dim_head,\n",
        "                                    dropout=attn_dropout,\n",
        "                                ),\n",
        "                            )\n",
        "                        ),\n",
        "                        Residual(PreNorm(dim, FeedForward(dim, dropout=ff_dropout))),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of transformer.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        x = self.embeds(x)\n",
        "\n",
        "        for attn, ff in self.layers:  # type: ignore\n",
        "            x = attn(x)\n",
        "            x = ff(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Pytorch model of a vanilla multi-layer perceptron.\n",
        "    Args:\n",
        "        nn (nn.Module): module with implementation of MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dims: List[int], act: Union[str, Callable[..., nn.Module]]):\n",
        "        \"\"\"\n",
        "        Multilayer perceptron.\n",
        "        Depth of network is given by `len(dims)`. Capacity is given by entries\n",
        "        of `dim`. Activation function is used after each linear layer. There is\n",
        "        no activation function for the final linear layer, as it is sometimes part\n",
        "        of the loss function already e. g., `nn.BCEWithLogitsLoss()`.\n",
        "        Args:\n",
        "            dims (List[int]): List with dimensions of layers.\n",
        "            act (Union[str, Callable[..., nn.Module]]): Activation function of each linear layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
        "        layers = []\n",
        "        for dim_in, dim_out in dims_pairs:\n",
        "            linear = nn.Linear(dim_in, dim_out)\n",
        "            layers.append(linear)\n",
        "            layers.append(act)\n",
        "\n",
        "        # drop last layer, as a sigmoid layer is included from BCELogitLoss\n",
        "        del layers[-1]\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward propagate tensor through MLP.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor.\n",
        "        Returns:\n",
        "            torch.Tensor: output tensor.\n",
        "        \"\"\"\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class TabTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch model of TabTransformer.\n",
        "    Based on paper:\n",
        "    https://arxiv.org/abs/2012.06678\n",
        "    Args:\n",
        "        nn (nn.Module): Module with implementation of TabTransformer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        categories: Union[List[int], Tuple[()]],\n",
        "        num_continuous: int,\n",
        "        dim: int = 32,\n",
        "        depth: int = 4,\n",
        "        heads: int = 8,\n",
        "        dim_head: int = 16,\n",
        "        dim_out: int = 1,\n",
        "        mlp_hidden_mults: Tuple[(int, int)] = (4, 2),\n",
        "        mlp_act: Union[str, Callable[..., nn.Module]] = nn.ReLU,\n",
        "        num_special_tokens: int = 2,\n",
        "        continuous_mean_std: Optional[torch.Tensor] = None,\n",
        "        attn_dropout: float = 0.0,\n",
        "        ff_dropout: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TabTransformer.\n",
        "        Originally introduced in https://arxiv.org/abs/2012.06678.\n",
        "        Args:\n",
        "            categories (Union[List[int],Tuple[()]]): List with number of categories\n",
        "            for each categorical feature. If no categorical variables are present,\n",
        "            use empty tuple. For categorical variables e. g., option type ('C' or 'P'),\n",
        "            the list would be `[1]`.\n",
        "            num_continuous (int): Number of continous features.\n",
        "            dim (int, optional): Dimensionality of transformer. Defaults to 32.\n",
        "            depth (int, optional): Depth of encoder / decoder of transformer.\n",
        "            Defaults to 4.\n",
        "            heads (int, optional): Number of attention heads. Defaults to 8.\n",
        "            dim_head (int, optional): Dimensionality of attention head. Defaults to 16.\n",
        "            dim_out (int, optional): Dimension of output layer of MLP. Set to one for\n",
        "            binary classification. Defaults to 1.\n",
        "            mlp_hidden_mults (Tuple[(int, int)], optional): multipliers for dimensions\n",
        "            of hidden layer in MLP. Defaults to (4, 2).\n",
        "            mlp_act (Union[str, Callable[..., nn.Module]], optional): Activation function used in MLP.\n",
        "            Defaults to nn.ReLU().\n",
        "            num_special_tokens (int, optional): Number of special tokens in transformer.\n",
        "            Defaults to 2.\n",
        "            continuous_mean_std (Optional[torch.Tensor]): List with mean and\n",
        "            std deviation of each continous feature. Shape eq. `[num_continous x 2]`.\n",
        "            Defaults to None.\n",
        "            attn_dropout (float, optional): Degree of attention dropout used in\n",
        "            transformer. Defaults to 0.0.\n",
        "            ff_dropout (float, optional): Dropout in feed forward net. Defaults to 0.0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert all(\n",
        "            map(lambda n: n > 0, categories)\n",
        "        ), \"number of each category must be positive\"\n",
        "\n",
        "        # categories related calculations\n",
        "\n",
        "        self.num_categories = len(categories)\n",
        "        self.num_unique_categories = sum(categories)\n",
        "\n",
        "        # create category embeddings table\n",
        "\n",
        "        self.num_special_tokens = num_special_tokens\n",
        "        total_tokens = self.num_unique_categories + num_special_tokens\n",
        "\n",
        "        # for automatically offsetting unique category ids to the correct position\n",
        "        #  in the categories embedding table\n",
        "\n",
        "        categories_offset = F.pad(\n",
        "            torch.tensor(list(categories)), (1, 0), value=num_special_tokens\n",
        "        )  # Prepend num_special_tokens.\n",
        "        categories_offset = categories_offset.cumsum(dim=-1)[:-1]\n",
        "        self.register_buffer(\"categories_offset\", categories_offset)\n",
        "\n",
        "        # continuous\n",
        "\n",
        "        if continuous_mean_std is not None:\n",
        "            assert continuous_mean_std.shape == (num_continuous, 2,), (\n",
        "                f\"continuous_mean_std must have a shape of ({num_continuous}, 2)\"\n",
        "                f\"where the last dimension contains the mean and variance respectively\"\n",
        "            )\n",
        "        self.register_buffer(\"continuous_mean_std\", continuous_mean_std)\n",
        "\n",
        "        self.norm = nn.LayerNorm(num_continuous)\n",
        "        self.num_continuous = num_continuous\n",
        "\n",
        "        # transformer\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            num_tokens=total_tokens,\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            dim_head=dim_head,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "        )\n",
        "\n",
        "        # mlp to logits\n",
        "\n",
        "        input_size = (dim * self.num_categories) + num_continuous\n",
        "        j = input_size // 8\n",
        "\n",
        "        hidden_dimensions = list(map(lambda t: j * t, mlp_hidden_mults))\n",
        "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
        "\n",
        "        self.mlp = MLP(all_dimensions, act=mlp_act)\n",
        "\n",
        "    def forward(self, x_categ: torch.Tensor, x_cont: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of TabTransformer.\n",
        "        Args:\n",
        "            x_categ (torch.Tensor): tensor with categorical data.\n",
        "            x_cont (torch.Tensor): tensor with continous data.\n",
        "        Returns:\n",
        "            torch.Tensor: predictions with shape [batch, 1]\n",
        "        \"\"\"\n",
        "        # Adaptation to work without categorical data\n",
        "        if x_categ is not None:\n",
        "            assert x_categ.shape[-1] == self.num_categories, (\n",
        "                f\"you must pass in {self.num_categories} \"\n",
        "                f\"values for your categories input\"\n",
        "            )\n",
        "            x_categ += self.categories_offset\n",
        "            x = self.transformer(x_categ)\n",
        "            flat_categ = x.flatten(1)\n",
        "\n",
        "        assert x_cont.shape[1] == self.num_continuous, (\n",
        "            f\"you must pass in {self.num_continuous} \"\n",
        "            f\"values for your continuous input\"\n",
        "        )\n",
        "\n",
        "        if self.continuous_mean_std is not None:\n",
        "            mean, std = self.continuous_mean_std.unbind(dim=-1)\n",
        "            x_cont = (x_cont - mean) / std\n",
        "\n",
        "        normed_cont = self.norm(x_cont)\n",
        "\n",
        "        # Adaptation to work without categorical data\n",
        "        if x_categ is not None:\n",
        "            x = torch.cat((flat_categ, normed_cont), dim=-1)\n",
        "        else:\n",
        "            x = normed_cont\n",
        "\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dx-aWaOpCQwr"
      },
      "outputs": [],
      "source": [
        "# https://svn.blender.org/svnroot/bf-blender/trunk/blender/build_files/scons/tools/bcolors.py\n",
        "# https://stackoverflow.com/a/287944/5755604\n",
        "class colors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "\n",
        "    def disable(self):\n",
        "        self.HEADER = ''\n",
        "        self.OKBLUE = ''\n",
        "        self.OKGREEN = ''\n",
        "        self.OKCYAN = ''\n",
        "        self.WARNING = ''\n",
        "        self.FAIL = ''\n",
        "        self.ENDC = ''\n",
        "        self.BOLD = ''\n",
        "        self.UNDERLINE = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-T5Ki40INJso"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "columns = [\n",
        "    \"TRADE_SIZE\",\n",
        "    \"TRADE_PRICE\",\n",
        "    \"BEST_ASK\",\n",
        "    \"BEST_BID\",\n",
        "    \"price_ex_lag\",\n",
        "    \"price_ex_lead\",\n",
        "    \"price_all_lag\",\n",
        "    \"price_all_lead\",\n",
        "    \"bid_ex\",\n",
        "    \"ask_ex\",\n",
        "    \"bid_size_ex\",\n",
        "    \"ask_size_ex\",\n",
        "    \"OPTION_TYPE\",\n",
        "    \"buy_sell\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "X = pd.read_parquet(\n",
        "    f\"gs://thesis-bucket-option-trade-classification/data/preprocessed/val_set_20.parquet\",\n",
        "    engine=\"fastparquet\", columns=columns\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "23PR7UObWvyG",
        "outputId": "96ca6a2d-f0a1-4b71-d83f-ef8aa190fc38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TRADE_SIZE</th>\n",
              "      <th>TRADE_PRICE</th>\n",
              "      <th>BEST_ASK</th>\n",
              "      <th>BEST_BID</th>\n",
              "      <th>price_ex_lag</th>\n",
              "      <th>price_ex_lead</th>\n",
              "      <th>price_all_lag</th>\n",
              "      <th>price_all_lead</th>\n",
              "      <th>bid_ex</th>\n",
              "      <th>ask_ex</th>\n",
              "      <th>bid_size_ex</th>\n",
              "      <th>ask_size_ex</th>\n",
              "      <th>OPTION_TYPE</th>\n",
              "      <th>buy_sell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29510320</th>\n",
              "      <td>20</td>\n",
              "      <td>1.47</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.38</td>\n",
              "      <td>2.73</td>\n",
              "      <td>1.12</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.60</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>P</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29510321</th>\n",
              "      <td>20</td>\n",
              "      <td>6.27</td>\n",
              "      <td>6.31</td>\n",
              "      <td>5.85</td>\n",
              "      <td>10.29</td>\n",
              "      <td>5.92</td>\n",
              "      <td>7.69</td>\n",
              "      <td>6.32</td>\n",
              "      <td>5.85</td>\n",
              "      <td>6.31</td>\n",
              "      <td>115.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>P</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29510322</th>\n",
              "      <td>2</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.44</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.02</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.30</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.44</td>\n",
              "      <td>82.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29510323</th>\n",
              "      <td>20</td>\n",
              "      <td>1.66</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.62</td>\n",
              "      <td>1.70</td>\n",
              "      <td>99.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>P</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29510324</th>\n",
              "      <td>1</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>P</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          TRADE_SIZE  TRADE_PRICE  BEST_ASK  BEST_BID  price_ex_lag  \\\n",
              "29510320          20         1.47      1.62      1.38          2.73   \n",
              "29510321          20         6.27      6.31      5.85         10.29   \n",
              "29510322           2         1.32      1.44      1.19          1.19   \n",
              "29510323          20         1.66      1.70      1.62          1.60   \n",
              "29510324           1         0.85      0.00      0.00          0.86   \n",
              "\n",
              "          price_ex_lead  price_all_lag  price_all_lead  bid_ex  ask_ex  \\\n",
              "29510320           1.12           1.62            1.60     NaN     NaN   \n",
              "29510321           5.92           7.69            6.32    5.85    6.31   \n",
              "29510322           1.02           1.25            1.30    1.19    1.44   \n",
              "29510323           1.62           1.60            1.62    1.62    1.70   \n",
              "29510324           0.65           0.86            0.50     NaN     NaN   \n",
              "\n",
              "          bid_size_ex  ask_size_ex OPTION_TYPE  buy_sell  \n",
              "29510320          NaN          NaN           P        -1  \n",
              "29510321        115.0         11.0           P         1  \n",
              "29510322         82.0         82.0           C         1  \n",
              "29510323         99.0        172.0           P         1  \n",
              "29510324          NaN          NaN           P         1  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "haMEXGBAXQcD"
      },
      "outputs": [],
      "source": [
        "# select categorical e. g., option type and strings e. g., ticker\n",
        "cat_columns = X.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "\n",
        "# binarize categorical similar to Borisov et al.\n",
        "X[cat_columns] = X[cat_columns].apply(lambda x: pd.factorize(x)[0])\n",
        "\n",
        "X.fillna(-1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RQWxsVKX0Sy5"
      },
      "outputs": [],
      "source": [
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSwU1h3OHicX",
        "outputId": "b42704ce-7ff0-4271-83cb-fdfdfd7e703b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-8b7c47d56998>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x_train.drop(columns=['buy_sell'], inplace=True)\n",
            "<ipython-input-12-8b7c47d56998>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x_val.drop(columns=['buy_sell'], inplace=True)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:98: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda_available: cpu\n",
            "num of cores:24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/'\n",
            "/usr/lib/python3/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/lib/python3/dist-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        }
      ],
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "x_train = X.head(100000)\n",
        "y_train = x_train['buy_sell']\n",
        "x_train.drop(columns=['buy_sell'], inplace=True)\n",
        "\n",
        "x_val = X.tail(50000)\n",
        "y_val = x_val['buy_sell']\n",
        "x_val.drop(columns=['buy_sell'], inplace=True)\n",
        "\n",
        "features = x_train.columns.tolist()\n",
        "cat_features = [\"OPTION_TYPE\"]\n",
        "\n",
        "_cat_unique = [2]\n",
        "if not _cat_unique:\n",
        "    _cat_unique = ()\n",
        "# assume columns are duplicate free, which is standard in pandas\n",
        "cont_features = [x for x in x_train.columns.tolist() if x not in cat_features]\n",
        "\n",
        "# print(cat_features)\n",
        "\n",
        "# static params\n",
        "epochs = 8\n",
        "\n",
        "\n",
        "#  use gpu if available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"cuda_available: {device}\")\n",
        "print(f\"num of cores:{os.cpu_count()}\")\n",
        "\n",
        "\n",
        "\n",
        "# create training and val set\n",
        "training_data = TabDataset(x_train,y_train,cat_features,_cat_unique)\n",
        "val_data = TabDataset(x_val,y_val,cat_features,_cat_unique)\n",
        "\n",
        "dim: int = 64 # type: ignore\n",
        "\n",
        "depth: int = 3 \n",
        "heads: int = 8\n",
        "weight_decay: float = 1e-5\n",
        "lr = 4e-3\n",
        "dropout = 0.2\n",
        "batch_size: int = 8192\n",
        "\n",
        "# span as many workers as cores\n",
        "dl_kwargs = {'num_workers': os.cpu_count(), 'pin_memory': True, 'batch_size':batch_size, 'shuffle':False} if use_cuda else {'batch_size':batch_size, 'shuffle':False}\n",
        "\n",
        "\n",
        "train_loader = TabDataLoader(training_data._X_cat, training_data._X_cont, training_data._y, **dl_kwargs)\n",
        "val_loader = TabDataLoader(val_data._X_cat, val_data._X_cont, val_data._y, **dl_kwargs)\n",
        "\n",
        "\n",
        "_clf = TabTransformer(\n",
        "    categories=_cat_unique,\n",
        "    num_continuous=len(cont_features),  # number of continuous values\n",
        "    dim_out=1,\n",
        "    mlp_act=nn.ReLU(),  # sigmoid of last layer already included in loss.\n",
        "    dim=dim,\n",
        "    depth=depth,\n",
        "    heads=heads,\n",
        "    attn_dropout=dropout,\n",
        "    ff_dropout=dropout,\n",
        "    mlp_hidden_mults=(4, 2),\n",
        ").to(device)\n",
        "\n",
        "# # API NOT FINAL\n",
        "# # default: optimizes for large models, low compile-time\n",
        "# #          and no extra memory usage\n",
        "# torch.compile(model)\n",
        "\n",
        "# # reduce-overhead: optimizes to reduce the framework overhead\n",
        "# #                and uses some extra memory. Helps speed up small models\n",
        "# torch.compile(model, mode=\"reduce-overhead\")\n",
        "\n",
        "# max-autotune: optimizes to produce the fastest model,\n",
        "#               but takes a very long time to compile\n",
        "_clf = torch.compile(_clf, mode=\"max-autotune\")\n",
        "\n",
        "\n",
        "# prof = torch.profiler.profile(\n",
        "#         schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
        "#         on_trace_ready=torch.profiler.tensorboard_trace_handler('./drive/MyDrive/log/tabtransformer'),\n",
        "#         record_shapes=True,\n",
        "#         with_stack=True)\n",
        "# prof.start()\n",
        "\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Generate the optimizers\n",
        "optimizer = optim.AdamW(\n",
        "    _clf.parameters(), lr=lr, weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# see https://stackoverflow.com/a/53628783/5755604\n",
        "# no sigmoid required; numerically more stable\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def our()->None:\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      # perform training\n",
        "      loss_in_epoch_train = 0\n",
        "\n",
        "      _clf.train()\n",
        "\n",
        "      for x_cat, x_cont, targets in train_loader:\n",
        "\n",
        "          x_cat = x_cat.to(device)\n",
        "          x_cont = x_cont.to(device)\n",
        "          targets = targets.to(device)\n",
        "          # print(x_cat.is_cuda)\n",
        "\n",
        "          # reset the gradients back to zero\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = _clf(x_cat, x_cont)\n",
        "          outputs = outputs.flatten()\n",
        "\n",
        "          with torch.cuda.amp.autocast():\n",
        "            train_loss = criterion(outputs, targets)\n",
        "\n",
        "          # compute accumulated gradients\n",
        "          scaler.scale(train_loss).backward()\n",
        "\n",
        "          # perform parameter update based on current gradients\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "\n",
        "          # add the mini-batch training loss to epoch loss\n",
        "          loss_in_epoch_train += train_loss.item()\n",
        "\n",
        "      #     prof.step()\n",
        "      # prof.stop()\n",
        "\n",
        "      _clf.eval()\n",
        "\n",
        "      loss_in_epoch_val = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for x_cat, x_cont, targets in val_loader:\n",
        "          x_cat = x_cat.to(device)\n",
        "          x_cont = x_cont.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          outputs = _clf(x_cat, x_cont)\n",
        "\n",
        "          outputs = outputs.flatten()\n",
        "\n",
        "          val_loss = criterion(outputs, targets)\n",
        "          loss_in_epoch_val += val_loss.item()\n",
        "\n",
        "      train_loss = loss_in_epoch_train / len(train_loader)\n",
        "      val_loss = loss_in_epoch_val / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3mUGlf-boHh",
        "outputId": "e80448e1-ace7-46e9-e294-25fc4b784891"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "[2022-12-04 06:54:54,991] torch._inductor.lowering: [WARNING] using triton random, expect difference from eager\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "[2022-12-04 06:54:55,506] torch._inductor.lowering: [WARNING] using triton random, expect difference from eager\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "[2022-12-04 06:54:56,150] torch._inductor.lowering: [WARNING] using triton random, expect difference from eager\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "[2022-12-04 06:55:01,641] torch._inductor.lowering: [WARNING] using triton random, expect difference from eager\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "[2022-12-04 06:55:02,109] torch._inductor.lowering: [WARNING] using triton random, expect difference from eager\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "[2022-12-04 06:55:02,578] torch._inductor.lowering: [WARNING] using triton random, expect difference from eager\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
            "  warnings.warn(message, UserWarning)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/passes/shape_prop.py\", line 116, in run_node\n",
            "    result = super().run_node(n)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 171, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\", line 288, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1480, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
            "    return F.embedding(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 2179, in embedding\n",
            "    return handle_torch_function(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/overrides.py\", line 1520, in handle_torch_function\n",
            "    result = mode.__torch_function__(public_api, types, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\", line 36, in __torch_function__\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 2210, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "IndexError: index out of range in self\n"
          ]
        },
        {
          "ename": "BackendCompilerFailed",
          "evalue": "_compile_fn raised RuntimeError: ShapeProp error for: node=%self_embeds : [#users=1] = call_module[target=self_embeds](args = (%x,), kwargs = {}) with meta={'nn_module_stack': OrderedDict([('self_embeds', \"<class 'torch.nn.modules.sparse.Embedding'>\")]), 'stack_trace': 'Module stack: {\\'self_embeds\\': <class \\'torch.nn.modules.sparse.Embedding\\'>}\\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\\n    x = self.embeds(x)\\n\\nGradient addition node due to multiple use of tensor around:\\nModule stack: {\\'self_embeds\\': <class \\'torch.nn.modules.sparse.Embedding\\'>}\\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\\n    x = self.embeds(x)\\n'}\n\nWhile executing %self_embeds : [#users=1] = call_module[target=self_embeds](args = (%x,), kwargs = {})\nOriginal traceback:\nModule stack: {'self_embeds': <class 'torch.nn.modules.sparse.Embedding'>}\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\n    x = self.embeds(x)\n\nGradient addition node due to multiple use of tensor around:\nModule stack: {'self_embeds': <class 'torch.nn.modules.sparse.Embedding'>}\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\n    x = self.embeds(x)\n\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mcall_module\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msubmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2179\u001b[0;31m         return handle_torch_function(\n\u001b[0m\u001b[1;32m   2180\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreplacements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mcompiler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_compiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m             \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"done compiler function {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/debug_utils.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_compile_fn\u001b[0;34m(model_, inputs_)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mmodel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mmodel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0mnum_example_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_inductor/overrides.py\u001b[0m in \u001b[0;36mfuse_fx\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;31m# the binary inputs have same tensor info(device, dtype, and layout).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0mShapeProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m     \u001b[0mgm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuse_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;34mf\"ShapeProp error for: node={n.format_node()} with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ShapeProp error for: node=%self_embeds : [#users=1] = call_module[target=self_embeds](args = (%x,), kwargs = {}) with meta={'nn_module_stack': OrderedDict([('self_embeds', \"<class 'torch.nn.modules.sparse.Embedding'>\")]), 'stack_trace': 'Module stack: {\\'self_embeds\\': <class \\'torch.nn.modules.sparse.Embedding\\'>}\\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\\n    x = self.embeds(x)\\n\\nGradient addition node due to multiple use of tensor around:\\nModule stack: {\\'self_embeds\\': <class \\'torch.nn.modules.sparse.Embedding\\'>}\\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\\n    x = self.embeds(x)\\n'}\n\nWhile executing %self_embeds : [#users=1] = call_module[target=self_embeds](args = (%x,), kwargs = {})\nOriginal traceback:\nModule stack: {'self_embeds': <class 'torch.nn.modules.sparse.Embedding'>}\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\n    x = self.embeds(x)\n\nGradient addition node due to multiple use of tensor around:\nModule stack: {'self_embeds': <class 'torch.nn.modules.sparse.Embedding'>}\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\n    x = self.embeds(x)\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b3c3b452fad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# warm up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menable_timing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menable_timing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8b7c47d56998>\u001b[0m in \u001b[0;36mour\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m           \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamo_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mdynamic_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fe7e4b98c04e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_categ, x_cont)\u001b[0m\n\u001b[1;32m    406\u001b[0m             )\n\u001b[1;32m    407\u001b[0m             \u001b[0mx_categ\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_categ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mflat_categ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcatch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mcatch_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchdynamo_orig_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_from_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfx_forward_from_src_skip_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_grad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mtime_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mcompilation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mlatency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_convert_frame_assert\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0minitial_grad_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, guard_export_fn, frame)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0morig_code_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_code\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mfix_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         )\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"torchdynamo start tracing {self.f_code.co_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch_nested_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_pointer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             ):\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mcontinue_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_graphstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_subgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_convert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         self.output.add_output_instructions(\n\u001b[1;32m    475\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mcreate_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JUMP_ABSOLUTE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinue_inst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# optimization to generate better code in a common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             self.add_output_instructions(\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_call_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UNPACK_SEQUENCE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0massert_no_fake_params_or_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBackendCompilerFailed\u001b[0m: _compile_fn raised RuntimeError: ShapeProp error for: node=%self_embeds : [#users=1] = call_module[target=self_embeds](args = (%x,), kwargs = {}) with meta={'nn_module_stack': OrderedDict([('self_embeds', \"<class 'torch.nn.modules.sparse.Embedding'>\")]), 'stack_trace': 'Module stack: {\\'self_embeds\\': <class \\'torch.nn.modules.sparse.Embedding\\'>}\\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\\n    x = self.embeds(x)\\n\\nGradient addition node due to multiple use of tensor around:\\nModule stack: {\\'self_embeds\\': <class \\'torch.nn.modules.sparse.Embedding\\'>}\\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\\n    x = self.embeds(x)\\n'}\n\nWhile executing %self_embeds : [#users=1] = call_module[target=self_embeds](args = (%x,), kwargs = {})\nOriginal traceback:\nModule stack: {'self_embeds': <class 'torch.nn.modules.sparse.Embedding'>}\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\n    x = self.embeds(x)\n\nGradient addition node due to multiple use of tensor around:\nModule stack: {'self_embeds': <class 'torch.nn.modules.sparse.Embedding'>}\n  File \"<ipython-input-6-fe7e4b98c04e>\", line 228, in forward\n    x = self.embeds(x)\n\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
          ]
        }
      ],
      "source": [
        "# warm up\n",
        "our()\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "our()\n",
        "\n",
        "torch.cuda.synchronize()  # wait for all_reduce to complete\n",
        "end.record()\n",
        "\n",
        "torch.cuda.synchronize()  # need to wait once more for op to finish\n",
        "\n",
        "our_time = start.elapsed_time(end)\n",
        "\n",
        "print(f\"our: {our_time :>5.1f} ms\")  # milliseconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBIA3T7ndo7J",
        "outputId": "dcb6c6ff-45a5-4c71-d545-cec9b5be15ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['OPTION_TYPE']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x_train = X.head(100000)\n",
        "y_train = x_train['buy_sell']\n",
        "x_train.drop(columns=['buy_sell'], inplace=True)\n",
        "\n",
        "x_val = X.tail(50000)\n",
        "y_val = x_val['buy_sell']\n",
        "x_val.drop(columns=['buy_sell'], inplace=True)\n",
        "\n",
        "features = x_train.columns.tolist()\n",
        "cat_features = [\"OPTION_TYPE\"]\n",
        "\n",
        "\n",
        "_cat_idx = [features.index(i) for i in cat_features if i in features]\n",
        "\n",
        "# assume columns are duplicate free, which is standard in pandas\n",
        "cont_features = [x for x in x_train.columns.tolist() if x not in cat_features]\n",
        "_cont_idx = [features.index(i) for i in cont_features if i in features]\n",
        "\n",
        "_cat_unique = [2]\n",
        "if not _cat_unique:\n",
        "    _cat_unique = ()\n",
        "# assume columns are duplicate free, which is standard in pandas\n",
        "cont_features = [x for x in x_train.columns.tolist() if x not in cat_features]\n",
        "\n",
        "print(cat_features)\n",
        "\n",
        "# static params\n",
        "epochs = 8\n",
        "\n",
        "# FIXME: fix embedding lookup for ROOT / Symbol.\n",
        "# convert to tensor\n",
        "x_train = tensor(x_train.values).float()\n",
        "# FIXME: Integrate at another part of the code e. g., pre-processing / data set.\n",
        "x_train = torch.nan_to_num(x_train, nan=0)\n",
        "\n",
        "y_train = tensor(y_train.values).float()\n",
        "# FIXME: set -1 to 0, due to rounding before output + binary classification\n",
        "y_train[y_train < 0] = 0\n",
        "\n",
        "x_val = tensor(x_val.values).float()\n",
        "x_val = torch.nan_to_num(x_val, nan=0)\n",
        "y_val = tensor(y_val.values).float()\n",
        "y_val[y_val < 0] = 0\n",
        "\n",
        "# create training and val set\n",
        "training_data = TensorDataset(x_train, y_train)\n",
        "val_data = TensorDataset(x_val, y_val)\n",
        "\n",
        "dim: int = 64 # type: ignore\n",
        "\n",
        "depth: int = 3 \n",
        "heads: int = 8\n",
        "weight_decay: float = 1e-5\n",
        "lr = 4e-3\n",
        "dropout = 0.2\n",
        "batch_size: int = 1024\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    training_data, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_data, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")\n",
        "\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "_clf = TabTransformer(\n",
        "    categories=_cat_unique,\n",
        "    num_continuous=len(_cont_idx),  # number of continuous values\n",
        "    dim_out=1,\n",
        "    mlp_act=nn.ReLU(),  # sigmoid of last layer already included in loss.\n",
        "    dim=dim,\n",
        "    depth=depth,\n",
        "    heads=heads,\n",
        "    attn_dropout=dropout,\n",
        "    ff_dropout=dropout,\n",
        "    mlp_hidden_mults=(4, 2),\n",
        ").to(device)\n",
        "\n",
        "# Generate the optimizers\n",
        "optimizer = optim.AdamW(\n",
        "    _clf.parameters(), lr=lr, weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# see https://stackoverflow.com/a/53628783/5755604\n",
        "# no sigmoid required; numerically more stable\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def their()->None:\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      # perform training\n",
        "      loss_in_epoch_train = 0\n",
        "\n",
        "      _clf.train()\n",
        "\n",
        "      for inputs, targets in train_loader:\n",
        "\n",
        "          # FIXME: refactor to custom data loader\n",
        "          x_cat = (\n",
        "              inputs[:, _cat_idx].int().to(device) if _cat_idx else None\n",
        "          )\n",
        "\n",
        "          x_cont = inputs[:, _cont_idx].to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          # reset the gradients back to zero\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = _clf(x_cat, x_cont)\n",
        "          outputs = outputs.flatten()\n",
        "\n",
        "          train_loss = criterion(outputs, targets)\n",
        "\n",
        "          # compute accumulated gradients\n",
        "          train_loss.backward()\n",
        "\n",
        "          # perform parameter update based on current gradients\n",
        "          optimizer.step()\n",
        "\n",
        "          # add the mini-batch training loss to epoch loss\n",
        "          loss_in_epoch_train += train_loss.item()\n",
        "\n",
        "      _clf.eval()\n",
        "\n",
        "      loss_in_epoch_val = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for inputs, targets in val_loader:\n",
        "\n",
        "              x_cat = (\n",
        "                  inputs[:, _cat_idx].int().to(device)\n",
        "                  if _cat_idx\n",
        "                  else None\n",
        "              )\n",
        "              x_cont = inputs[:, _cont_idx].to(device)\n",
        "              targets = targets.to(device)\n",
        "\n",
        "              outputs = _clf(x_cat, x_cont)\n",
        "\n",
        "              outputs = outputs.flatten()\n",
        "\n",
        "              val_loss = criterion(outputs, targets)\n",
        "              loss_in_epoch_val += val_loss.item()\n",
        "\n",
        "      train_loss = loss_in_epoch_train / len(train_loader)\n",
        "      val_loss = loss_in_epoch_val / len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74CfZOHchR3C",
        "outputId": "2a2d8abf-9aa2-4aca-eff2-bfa71eb07ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "our: 22346.5 ms\n",
            "speedup: 9.843617139917821\n"
          ]
        }
      ],
      "source": [
        "# warm up\n",
        "their()\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "their()\n",
        "\n",
        "torch.cuda.synchronize()  # wait for all_reduce to complete\n",
        "end.record()\n",
        "\n",
        "torch.cuda.synchronize()  # need to wait once more for op to finish\n",
        "\n",
        "their_time = start.elapsed_time(end)\n",
        "print(f\"our: {their_time :>5.1f} ms\")  # milliseconds\n",
        "print(f\"speedup: {their_time / our_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhRbE6wUOqcI"
      },
      "outputs": [],
      "source": [
        "# fused layer norm has been integrated to pytorch\n",
        "# https://github.com/pytorch/pytorch/pull/27634\n",
        "# also compare https://gist.github.com/ptrblck/8b1c6a7efd97604a7dedbf2c3edd1019"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMagLePMs2PZPiLGh1CoNWC",
      "include_colab_link": true,
      "mount_file_id": "https://github.com/KarelZe/thesis/blob/transformer/notebooks/4.0d-mb-transformer-performance-sol.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
