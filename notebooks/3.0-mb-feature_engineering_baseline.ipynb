{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarelZe/thesis/blob/baseline/notebooks/3.0-mb-feature_engineering_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQwYMQAr-0vu"
      },
      "outputs": [],
      "source": [
        "!pip install catboost==1.1\n",
        "!pip install gcsfs==2022.10.0\n",
        "!pip install ipywidgets==8.0.2\n",
        "!pip install numpy==1.23.4\n",
        "!pip install pandas==1.5.1\n",
        "!pip install optuna==3.0.3\n",
        "!pip install scikit-learn==1.1.3\n",
        "!pip install seaborn==0.12.1\n",
        "!pip install shap==0.41.0\n",
        "!pip install wandb==0.13.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WXF7w4VyVgG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "import gcsfs\n",
        "import google.auth\n",
        "from google.colab import auth, output\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import seaborn as sns\n",
        "import shap\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOtLR4B0R-ql"
      },
      "outputs": [],
      "source": [
        "# connect to weights and biases\n",
        "run = wandb.init(project=\"thesis\", job_type=\"baseline\", entity=\"fbv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init shap\n",
        "shap.initjs()"
      ],
      "metadata": {
        "id": "qCzoR4qrH4Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-eGOVujp_TN"
      },
      "outputs": [],
      "source": [
        "# connect to google cloud storage\n",
        "auth.authenticate_user()\n",
        "credentials, _ = google.auth.default()\n",
        "fs = gcsfs.GCSFileSystem(project=\"thesis\", token=credentials)\n",
        "fs_prefix = \"gs://\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUZrGE8U_kDA"
      },
      "outputs": [],
      "source": [
        "output.enable_custom_widget_manager()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF0_Mz8DYjGz"
      },
      "outputs": [],
      "source": [
        "# set fixed seed\n",
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproducibility of results\n",
        "    \"\"\"\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "seed = 42\n",
        "seed_everything(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmXtH-PEqyQE"
      },
      "outputs": [],
      "source": [
        "train = pd.read_parquet(\n",
        "    f\"gs://thesis-bucket-option-trade-classification/data/preprocessed/train_set_60.parquet\"\n",
        ")\n",
        "val = pd.read_parquet(\n",
        "    f\"gs://thesis-bucket-option-trade-classification/data/preprocessed/val_set_20.parquet\"\n",
        ")\n",
        "test = pd.read_parquet(\n",
        "    f\"gs://thesis-bucket-option-trade-classification/data/preprocessed/test_set_20.parquet\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYosl7qJwYFS"
      },
      "outputs": [],
      "source": [
        "train.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2ZHX3MpqzKD"
      },
      "outputs": [],
      "source": [
        "# randomly sample frac of rows\n",
        "# frac = 0.02\n",
        "\n",
        "# train = train.sample(frac=frac, random_state=seed)\n",
        "# val = val.sample(frac=frac, random_state=seed)\n",
        "# test = test.sample(frac=frac, random_state=seed)\n",
        "\n",
        "# unify for common preprocessing\n",
        "X = pd.concat([train, val, test])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4RRb08pyo6p"
      },
      "outputs": [],
      "source": [
        "# calculate days to maturity\n",
        "X[\"time_to_maturity\"] = (X[\"EXPIRATION\"] - X[\"QUOTE_DATETIME\"]).dt.days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDU5Wy02WWT"
      },
      "outputs": [],
      "source": [
        "# apply positional encoding to dates\n",
        "X[\"date_month_sin\"] = np.sin(2 * np.pi * X[\"QUOTE_DATETIME\"].dt.year / 12)\n",
        "X[\"date_month_cos\"] = np.cos(2 * np.pi * X[\"QUOTE_DATETIME\"].dt.year / 12)\n",
        "\n",
        "# apply positional encoding to dates\n",
        "X[\"date_month_sin\"] = np.sin(2 * np.pi * X[\"QUOTE_DATETIME\"].dt.year / 12)\n",
        "X[\"date_month_cos\"] = np.cos(2 * np.pi * X[\"QUOTE_DATETIME\"].dt.year / 12)\n",
        "\n",
        "seconds_in_day = 24*60*60\n",
        "\n",
        "seconds = (X[\"QUOTE_DATETIME\"] - X[\"QUOTE_DATETIME\"].dt.normalize()).dt.total_seconds()\n",
        "X[\"date_time_sin\"] = np.sin(2*  np.pi*seconds.dt.seconds/ seconds_in_day)\n",
        "X[\"date_time_cos\"] = np.cos(2 * np.pi*seconds.dt.seconds/ seconds_in_day)\n",
        "\n",
        "# add year\n",
        "X[\"date_year\"] = (X[\"QUOTE_DATETIME\"].dt.year - 2005) / (2017 - 2005)\n",
        "\n",
        "date_columns = [\"date_month_sin\", \"date_month_cos\",\"date_time_sin\", \"date_time_cos\", \"date_year\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj79d4i90rhm"
      },
      "outputs": [],
      "source": [
        "X.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlTiomjqw_QD"
      },
      "outputs": [],
      "source": [
        "# remove problematic features -> see notebook on aversarial validation\n",
        "X = X.drop(\n",
        "    [\"SEQUENCE_NUMBER\", \"order_id\", \"optionid\", \"EXPIRATION\", \"QUOTE_DATETIME\", \"ROOT\", \"UNDERLYING_SYMBOL\"],\n",
        "    axis=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2kABLCA2CEb"
      },
      "outputs": [],
      "source": [
        "# isolate target\n",
        "y = X[[\"buy_sell\"]]\n",
        "X = X.drop([\"buy_sell\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUN8LxqeyX8n"
      },
      "outputs": [],
      "source": [
        "corr = X.corr()\n",
        "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manual deletion of columns that are highly correlated with other columns\n",
        "X = X.drop(\n",
        "    [\"day_vol\", \"BEST_ASK\", \"BEST_BID\", \"price_all_lead\", \"price_all_lag\"],\n",
        "    axis=1,\n",
        ")"
      ],
      "metadata": {
        "id": "ys4hQk8IN6oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbIFNIc_3i_6"
      },
      "outputs": [],
      "source": [
        "# Midspread\n",
        "mid = 0.5 * (X[\"ask_ex\"] + X[\"bid_ex\"])\n",
        "# X[\"mid_ex\"] = mid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "546ZQQnm2lYx"
      },
      "outputs": [],
      "source": [
        "# Absolute distance from mid\n",
        "X[\"abs_mid_ex\"] = X[\"TRADE_PRICE\"] - mid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx1Thx0Y3sU-"
      },
      "outputs": [],
      "source": [
        "# Degree how much trade size is filled\n",
        "X[\"rel_bid_size_ex\"] = X[\"TRADE_SIZE\"] / X[\"bid_size_ex\"]\n",
        "X[\"rel_ask_size_ex\"] = X[\"TRADE_SIZE\"] / X[\"ask_size_ex\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Dj2dd33Vwy"
      },
      "outputs": [],
      "source": [
        "# Calculate change similar to tick rule\n",
        "X[\"chg_ex_lead\"] = X[\"TRADE_PRICE\"] - X[\"price_ex_lead\"]\n",
        "\n",
        "# Calculate change similar to reverse tick rule\n",
        "X[\"chg_ex_lag\"] = X[\"TRADE_PRICE\"] - X[\"price_ex_lag\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KmaiY0m5i8X"
      },
      "outputs": [],
      "source": [
        "# select categorical e. g., option type and strings e. g., ticker\n",
        "cat_columns = X.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
        "print(cat_columns)\n",
        "\n",
        "# binarize categorical similar to Borisov et al.\n",
        "X[cat_columns] = X[cat_columns].apply(lambda x: pd.factorize(x)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngZUqSLW_M_8"
      },
      "outputs": [],
      "source": [
        "# treat inf as nan\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2HBaXLP5L4I"
      },
      "outputs": [],
      "source": [
        "# Do not fill, let CatBoost Handle NaNs\n",
        "# X = X.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIXLmMcZ5-aC"
      },
      "outputs": [],
      "source": [
        "# separate again for training scaling\n",
        "X_train = X.loc[train.index, :]\n",
        "X_val = X.loc[val.index, :]\n",
        "X_test = X.loc[test.index, :]\n",
        "\n",
        "y_train = y.loc[train.index, :]\n",
        "y_val = y.loc[val.index, :]\n",
        "y_test = y.loc[test.index, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV5a5st_2vrt"
      },
      "outputs": [],
      "source": [
        "# Standardize numerical values\n",
        "num_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_columns = [x for x in num_columns if x not in cat_columns]\n",
        "num_columns = [x for x in num_columns if x not in date_columns]\n",
        "\n",
        "# use scaler due to outlying observations > dataset notebook.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train[num_columns] = scaler.fit_transform(X_train[num_columns])\n",
        "X_val[num_columns] = scaler.transform(X_val[num_columns])\n",
        "X_test[num_columns] = scaler.transform(X_test[num_columns])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMIOV1jA_ImH"
      },
      "source": [
        "## CatBoost Baseline 🐈‍⬛"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmbM8by1WO5J"
      },
      "source": [
        "### Hyperparameter Search Baseline🗃️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4hMoRaVQa64"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.Trial):\n",
        "    # See docs for recommendations on tuning hyperparameters\n",
        "    #  https://catboost.ai/en/docs/concepts/parameter-tuning\n",
        "    iterations = trial.suggest_int(\"iterations\", 10, 1000, log=False)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 1, log=True)\n",
        "    random_strength = trial.suggest_float(\"random_strength\", 1e-9, 10, log=True)\n",
        "    depth = trial.suggest_int(\"depth\", 1, 8, log=False)\n",
        "    grow_policy = trial.suggest_categorical(\n",
        "        \"grow_policy\", [\"SymmetricTree\", \"Depthwise\"]\n",
        "    )\n",
        "    params = {\n",
        "        \"iterations\": iterations,\n",
        "        \"depth\": depth,\n",
        "        \"grow_policy\": grow_policy,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"random_strength\": random_strength,\n",
        "        \"od_type\": \"Iter\",\n",
        "        \"logging_level\": \"Silent\",\n",
        "        \"task_type\": \"GPU\",\n",
        "        \"cat_features\":cat_columns,\n",
        "    }\n",
        "\n",
        "    model = CatBoostClassifier(**params)\n",
        "\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_val, prediction_type=\"Class\")\n",
        "    return accuracy_score(y_val, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfchJ_dGYfHe"
      },
      "outputs": [],
      "source": [
        "wandb_kwargs = {\"project\": \"thesis\"}\n",
        "wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgdvLxBI3Cs3"
      },
      "outputs": [],
      "source": [
        "# Implement hyperparameter search\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    sampler=optuna.samplers.TPESampler(seed=seed),\n",
        "    study_name=\"baseline_gbm\",\n",
        ")\n",
        "study.optimize(objective, n_trials=200, callbacks=[wandbc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujM_aozLILeQ"
      },
      "outputs": [],
      "source": [
        "ax_history = optuna.visualization.matplotlib.plot_optimization_history(study)\n",
        "ax_param_importance = optuna.visualization.matplotlib.plot_param_importances(study)\n",
        "fig_contour = optuna.visualization.matplotlib.plot_contour(\n",
        "    study, [\"iterations\", \"depth\", \"grow_policy\", \"learning_rate\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVfXVspgQia0"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of finished trials: {len(study.trials)}\")\n",
        "\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"Best trial: {trial}\")\n",
        "\n",
        "print(f\"Value: {trial.value}\")\n",
        "\n",
        "print(\"Params:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6cNgPVLnPFu"
      },
      "outputs": [],
      "source": [
        "# use CPU to plot learning curves\n",
        "# see https://catboost.ai/en/docs/concepts/python-reference_catboost_fit\n",
        "static_params = {\"od_type\": \"Iter\", \"logging_level\": \"Silent\", \"task_type\": \"CPU\",\"cat_features\":cat_columns}\n",
        "\n",
        "params = {**static_params, **trial.params}\n",
        "print(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thy1Ic73l4Ri"
      },
      "outputs": [],
      "source": [
        "model = CatBoostClassifier(**params)\n",
        "model.fit(X_train, y_train, plot=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use shap instead of feature importance to maintain consistency throughout the work\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(Pool(X, y, cat_features=cat_columns))\n",
        "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "o4fvxZ41IKDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h7GTJBmmFXq"
      },
      "outputs": [],
      "source": [
        "acc_train = model.score(X_train, y_train)\n",
        "acc_val = model.score(X_val, y_val)\n",
        "acc_test = model.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy (train): {acc_train}, (val) {acc_val}, and (test) {acc_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3vzAVSc_DfD"
      },
      "source": [
        "### Robustness Baseline🥊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3evMG-KVA2eX"
      },
      "outputs": [],
      "source": [
        "# Copy unscaled columns\n",
        "X_print = test.copy()\n",
        "\n",
        "# add baseline results\n",
        "X_print[\"rule\"] = \"Baseline\"\n",
        "X_print[\"buy_sell_predicted\"] = 0 # model.predict(X_test)\n",
        "\n",
        "# prepare columns for printing\n",
        "X_print[\"ttm\"] = (\n",
        "    X_print[\"EXPIRATION\"].dt.to_period(\"M\")\n",
        "    - X_print[\"QUOTE_DATETIME\"].dt.to_period(\"M\")\n",
        ").apply(lambda x: x.n)\n",
        "X_print[\"year\"] = X_print[\"QUOTE_DATETIME\"].dt.year\n",
        "\n",
        "bins_tradesize = [0, 1, 3, 5, 11, np.inf]\n",
        "trade_size_labels = [\"(0,1]\", \"(1,3]\", \"(3,5]\", \"(5,11]\", \">11\"]\n",
        "X_print[\"TRADE_SIZE_binned\"] = pd.cut(\n",
        "    X_print[\"TRADE_SIZE\"], bins_tradesize, labels=trade_size_labels\n",
        ")\n",
        "\n",
        "bins_years = [2005, 2007, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
        "year_labels = [\n",
        "    \"2005-2007\",\n",
        "    \"2008-2010\",\n",
        "    \"2011\",\n",
        "    \"2012\",\n",
        "    \"2013\",\n",
        "    \"2014\",\n",
        "    \"2015\",\n",
        "    \"2016\",\n",
        "    \"2017\",\n",
        "]\n",
        "X_print[\"year_binned\"] = pd.cut(X_print[\"year\"], bins_years, labels=year_labels)\n",
        "\n",
        "bins_ttm = [0, 1, 2, 3, 6, 12, np.inf]\n",
        "ttm_labels = [\n",
        "    \"ttm <= 1 month\",\n",
        "    \"ttm (1-2] month\",\n",
        "    \"ttm (2-3] month\",\n",
        "    \"ttm (3-6] month\",\n",
        "    \"ttm (6-12] month\",\n",
        "    \"ttm > 12 month\",\n",
        "]\n",
        "X_print[\"ttm_binned\"] = pd.cut(X_print[\"ttm\"], bins_ttm, labels=ttm_labels)\n",
        "\n",
        "# TODO: Security type\n",
        "# TODO: Moneyness\n",
        "# TODO: time from previous trade; same underlying or any?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clDZ4Z95_0jj"
      },
      "outputs": [],
      "source": [
        "def check_robustness(criterion: str = \"year_binned\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Check robustness of rules by calculating the accuracy for a given\n",
        "    criterion and rules.\n",
        "\n",
        "    Example:\n",
        "    rule\t\tBaseline\n",
        "    TRADE_SIZE_binned\n",
        "    (0,1]\t  0.710966\n",
        "    (1,3]\t  0.717664\n",
        "    (3,5]\t  0.715195\n",
        "    (5,11]\t0.699428\n",
        "    >11\t  \t0.688348\n",
        "\n",
        "    Args:\n",
        "        criterion (str, optional): criterion to check robustness for.\n",
        "        Defaults to \"year_binned\".\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with accuracy of rules. Rule in columns and\n",
        "        criterion values in rows.\n",
        "    \"\"\"\n",
        "    results = (\n",
        "        X_print.groupby([\"rule\", criterion])[[\"buy_sell\", \"buy_sell_predicted\"]]\n",
        "        .apply(lambda x: accuracy_score(x[\"buy_sell\"], x[\"buy_sell_predicted\"]))\n",
        "        .unstack(level=0)\n",
        "    )\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KRw_J0IJiFU"
      },
      "outputs": [],
      "source": [
        "check_robustness(\"year_binned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCey7G_tE6zt"
      },
      "outputs": [],
      "source": [
        "check_robustness(\"OPTION_TYPE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpg1yY2MEGFa"
      },
      "outputs": [],
      "source": [
        "check_robustness(\"TRADE_SIZE_binned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8624JR8wEN4D"
      },
      "outputs": [],
      "source": [
        "check_robustness(\"ttm_binned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P3UVsQb2T_V"
      },
      "source": [
        "## Classical rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXOz7eU-T0dj"
      },
      "outputs": [],
      "source": [
        "# tick rule\n",
        "# FIXME: Discuss with Grauer et al what is used in table 9 ISE. Probably all lag?\n",
        "tt = np.where(X_print[\"TRADE_PRICE\"] > X_print[\"price_all_lag\"], 1.0, -1.0)\n",
        "X_print[\"buy_sell_predicted\"] = tt\n",
        "X_print[\"rule\"] = \"tick rule\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WalF1CT1RpmB"
      },
      "outputs": [],
      "source": [
        "check_robustness(\"year_binned\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse tick rule\n",
        "X_print[\"buy_sell_predicted\"] = np.where(X_print[\"TRADE_PRICE\"] > X_print[\"price_all_lag\"], 1.0, -1.0)\n",
        "X_print[\"rule\"] = \"reverse tick rule\""
      ],
      "metadata": {
        "id": "CpZTx7tzSxmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_robustness(\"year_binned\")"
      ],
      "metadata": {
        "id": "QjxlCgTkVAb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTr6C0gPEAlh"
      },
      "outputs": [],
      "source": [
        "# quote rule\n",
        "mid = 0.5 * (X_print[\"ask_ex\"] + X_print[\"bid_ex\"])\n",
        "qr = np.where(X_print[\"TRADE_PRICE\"] > mid, 1, np.where(X_print[\"TRADE_PRICE\"] < mid, -1, np.nan))\n",
        "X_print[\"buy_sell_predicted\"] = qr\n",
        "\n",
        "# fill others random\n",
        "X_print[\"buy_sell_predicted\"] = X_print[\"buy_sell_predicted\"].map(\n",
        "    lambda l: l if not np.isnan(l) else np.random.choice([-1, 1])\n",
        ")\n",
        "\n",
        "X_print[\"rule\"] = \"quote rule\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_robustness(\"year_binned\")"
      ],
      "metadata": {
        "id": "S3tVWvFtV66V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKFUz1AHxFHQ"
      },
      "outputs": [],
      "source": [
        "# trade size tick rule\n",
        "ts_eq_bid = X_print[\"TRADE_SIZE\"] == X_print[\"bid_size_ex\"]\n",
        "ts_eq_ask = X_print[\"TRADE_SIZE\"] == X_print[\"ask_size_ex\"]\n",
        "\n",
        "X_print[\"buy_sell_predicted\"] = np.where(\n",
        "    ts_eq_bid, 1.0, np.where(ts_eq_ask, -1.0, tt)\n",
        ")\n",
        "X_print[\"rule\"] = \"trade size + tick rule\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_robustness(\"year_binned\")"
      ],
      "metadata": {
        "id": "5WMY1YC5XnO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_print[\"buy_sell_predicted\"] = np.where(\n",
        "    ts_eq_bid, 1.0, np.where(ts_eq_ask, -1.0, qr)\n",
        ")\n",
        "\n",
        "# fill others random\n",
        "X_print[\"buy_sell_predicted\"] = X_print[\"buy_sell_predicted\"].map(\n",
        "    lambda l: l if not np.isnan(l) else np.random.choice([-1, 1])\n",
        ")\n",
        "\n",
        "X_print[\"rule\"] = \"trade size + quote rule\""
      ],
      "metadata": {
        "id": "mkGlIJxVXlFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_robustness(\"year_binned\")"
      ],
      "metadata": {
        "id": "ad4VLhDVZGmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym31mXboWRUL"
      },
      "outputs": [],
      "source": [
        "# depth rule p. 14\n",
        "dr = np.where(\n",
        "    (X_print[\"TRADE_PRICE\"] == mid) & (X_print[\"ask_size_ex\"] > X_print[\"bid_size_ex\"]),\n",
        "    1,\n",
        "    np.where(\n",
        "        (X_print[\"TRADE_PRICE\"] == mid) & (X_print[\"ask_size_ex\"] < X_print[\"bid_size_ex\"]), -1, np.nan\n",
        "    ),\n",
        ")\n",
        "X_print[\"buy_sell_predicted\"] = dr\n",
        "\n",
        "X_print[\"rule\"] = \"depth rule\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_print[\"buy_sell_predicted\"] = X_print[\"buy_sell_predicted\"].map(\n",
        "    lambda l: l if not np.isnan(l) else np.random.choice([-1, 1])\n",
        ")"
      ],
      "metadata": {
        "id": "KCPamSRxa5y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_robustness(\"year_binned\")"
      ],
      "metadata": {
        "id": "vQEGKJStb9SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_print[\"buy_sell_predicted\"] = np.where(\n",
        "    ts_eq_bid, 1.0, np.where(ts_eq_ask, -1.0, dr)\n",
        ")\n",
        "\n",
        "X_print[\"rule\"] = \"trade size + depth rule\""
      ],
      "metadata": {
        "id": "mZToz_urayRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_print[\"buy_sell_predicted\"] = X_print[\"buy_sell_predicted\"].map(\n",
        "    lambda l: l if not np.isnan(l) else np.random.choice([-1, 1])\n",
        ")"
      ],
      "metadata": {
        "id": "MkrC5H3ScGDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_robustness(\"year_binned\")"
      ],
      "metadata": {
        "id": "xUL24ZqCb-da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classical Rules Sklearn Implementation 📦"
      ],
      "metadata": {
        "id": "6vzzLA34Z4Bu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV4JDAz_RpmG"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.base import MultiOutputMixin\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.utils.validation import _num_samples\n",
        "from sklearn.utils.validation import check_consistent_length\n",
        "from sklearn.utils.validation import check_is_fitted, _check_sample_weight\n",
        "from sklearn.utils.random import _random_choice_csc\n",
        "from sklearn.utils.multiclass import class_distribution\n",
        "\n",
        "\n",
        "class TRClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n",
        "\n",
        "    def __init__(self, *, strategy=\"standard\", random_state=None, constant=None):\n",
        "        self.strategy = strategy\n",
        "        self.random_state = random_state\n",
        "        self.constant = constant\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"Fit the baseline classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data.\n",
        "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "            Target values.\n",
        "        sample_weight : array-like of shape (n_samples,), default=None\n",
        "            Sample weights.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns the instance itself.\n",
        "        \"\"\"\n",
        "        allowed_strategies = (\"standard\", \"tradesize\")\n",
        "\n",
        "        if self.strategy not in allowed_strategies:\n",
        "            raise ValueError(\n",
        "                \"Unknown strategy type: %s, expected one of %s.\"\n",
        "                % (self.strategy, allowed_strategies)\n",
        "            )\n",
        "\n",
        "        self._strategy = self.strategy\n",
        "\n",
        "        if self._strategy == \"uniform\" and sp.issparse(y):\n",
        "            y = y.toarray()\n",
        "            warnings.warn(\n",
        "                \"A local copy of the target data has been converted \"\n",
        "                \"to a numpy array. Predicting on sparse target data \"\n",
        "                \"with the uniform strategy would not save memory \"\n",
        "                \"and would be slower.\",\n",
        "                UserWarning,\n",
        "            )\n",
        "\n",
        "        self.sparse_output_ = sp.issparse(y)\n",
        "\n",
        "        if not self.sparse_output_:\n",
        "            y = np.asarray(y)\n",
        "            y = np.atleast_1d(y)\n",
        "\n",
        "        if y.ndim == 1:\n",
        "            y = np.reshape(y, (-1, 1))\n",
        "\n",
        "        self.n_outputs_ = y.shape[1]\n",
        "\n",
        "        check_consistent_length(X, y)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = _check_sample_weight(sample_weight, X)\n",
        "\n",
        "        if self._strategy == \"constant\":\n",
        "            if self.constant is None:\n",
        "                raise ValueError(\n",
        "                    \"Constant target value has to be specified \"\n",
        "                    \"when the constant strategy is used.\"\n",
        "                )\n",
        "            else:\n",
        "                constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))\n",
        "                if constant.shape[0] != self.n_outputs_:\n",
        "                    raise ValueError(\n",
        "                        \"Constant target value should have shape (%d, 1).\"\n",
        "                        % self.n_outputs_\n",
        "                    )\n",
        "\n",
        "        (self.classes_, self.n_classes_, self.class_prior_) = class_distribution(\n",
        "            y, sample_weight\n",
        "        )\n",
        "\n",
        "        if self._strategy == \"constant\":\n",
        "            for k in range(self.n_outputs_):\n",
        "                if not any(constant[k][0] == c for c in self.classes_[k]):\n",
        "                    # Checking in case of constant strategy if the constant\n",
        "                    # provided by the user is in y.\n",
        "                    err_msg = (\n",
        "                        \"The constant target value must be present in \"\n",
        "                        \"the training data. You provided constant={}. \"\n",
        "                        \"Possible values are: {}.\".format(\n",
        "                            self.constant, list(self.classes_[k])\n",
        "                        )\n",
        "                    )\n",
        "                    raise ValueError(err_msg)\n",
        "\n",
        "        if self.n_outputs_ == 1:\n",
        "            self.n_classes_ = self.n_classes_[0]\n",
        "            self.classes_ = self.classes_[0]\n",
        "            self.class_prior_ = self.class_prior_[0]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Perform classification on test vectors X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data.\n",
        "        Returns\n",
        "        -------\n",
        "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "            Predicted target values for X.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "\n",
        "        # numpy random_state expects Python int and not long as size argument\n",
        "        # under Windows\n",
        "        n_samples = _num_samples(X)\n",
        "        rs = check_random_state(self.random_state)\n",
        "\n",
        "        n_classes_ = self.n_classes_\n",
        "        classes_ = self.classes_\n",
        "        class_prior_ = self.class_prior_\n",
        "        constant = self.constant\n",
        "        if self.n_outputs_ == 1:\n",
        "            # Get same type even for self.n_outputs_ == 1\n",
        "            n_classes_ = [n_classes_]\n",
        "            classes_ = [classes_]\n",
        "            class_prior_ = [class_prior_]\n",
        "            constant = [constant]\n",
        "        # Compute probability only once\n",
        "        if self._strategy == \"stratified\":\n",
        "            proba = self.predict_proba(X)\n",
        "            if self.n_outputs_ == 1:\n",
        "                proba = [proba]\n",
        "\n",
        "        if self.sparse_output_:\n",
        "            class_prob = None\n",
        "            if self._strategy in (\"most_frequent\", \"prior\"):\n",
        "                classes_ = [np.array([cp.argmax()]) for cp in class_prior_]\n",
        "\n",
        "            elif self._strategy == \"stratified\":\n",
        "                class_prob = class_prior_\n",
        "\n",
        "            elif self._strategy == \"uniform\":\n",
        "                raise ValueError(\n",
        "                    \"Sparse target prediction is not \"\n",
        "                    \"supported with the uniform strategy\"\n",
        "                )\n",
        "\n",
        "            elif self._strategy == \"constant\":\n",
        "                classes_ = [np.array([c]) for c in constant]\n",
        "\n",
        "            y = _random_choice_csc(n_samples, classes_, class_prob, self.random_state)\n",
        "        else:\n",
        "            if self._strategy in (\"most_frequent\", \"prior\"):\n",
        "                y = np.tile(\n",
        "                    [\n",
        "                        classes_[k][class_prior_[k].argmax()]\n",
        "                        for k in range(self.n_outputs_)\n",
        "                    ],\n",
        "                    [n_samples, 1],\n",
        "                )\n",
        "\n",
        "            elif self._strategy == \"stratified\":\n",
        "                y = np.vstack(\n",
        "                    [\n",
        "                        classes_[k][proba[k].argmax(axis=1)]\n",
        "                        for k in range(self.n_outputs_)\n",
        "                    ]\n",
        "                ).T\n",
        "\n",
        "            elif self._strategy == \"uniform\":\n",
        "                ret = [\n",
        "                    classes_[k][rs.randint(n_classes_[k], size=n_samples)]\n",
        "                    for k in range(self.n_outputs_)\n",
        "                ]\n",
        "                y = np.vstack(ret).T\n",
        "\n",
        "            elif self._strategy == \"constant\":\n",
        "                y = np.tile(self.constant, (n_samples, 1))\n",
        "\n",
        "            if self.n_outputs_ == 1:\n",
        "                y = np.ravel(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        pass\n",
        "\n",
        "    def predict_log_proba(self, X):\n",
        "        pass\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\n",
        "            \"poor_score\": True,\n",
        "            \"no_validation\": True,\n",
        "            \"_xfail_checks\": {\n",
        "                \"check_methods_subset_invariance\": \"fails for the predict method\",\n",
        "                \"check_methods_sample_order_invariance\": \"fails for the predict method\",\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def score(self, X, y, sample_weight=None):\n",
        "        \"\"\"Return the mean accuracy on the given test data and labels.\n",
        "        In multi-label classification, this is the subset accuracy\n",
        "        which is a harsh metric since you require for each sample that\n",
        "        each label set be correctly predicted.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : None or array-like of shape (n_samples, n_features)\n",
        "            Test samples. Passing None as test samples gives the same result\n",
        "            as passing real test samples, since DummyClassifier\n",
        "            operates independently of the sampled observations.\n",
        "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "            True labels for X.\n",
        "        sample_weight : array-like of shape (n_samples,), default=None\n",
        "            Sample weights.\n",
        "        Returns\n",
        "        -------\n",
        "        score : float\n",
        "            Mean accuracy of self.predict(X) wrt. y.\n",
        "        \"\"\"\n",
        "        if X is None:\n",
        "            X = np.zeros(shape=(len(y), 1))\n",
        "        return super().score(X, y, sample_weight)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}