{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44d4a0-223b-4117-ade7-5c5cfacfa28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f2b8-dd9a-405c-be61-1df3f7adc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, model.predict(X_test)))\n",
    "print(model.predict_proba(X_test))\n",
    "\n",
    "\n",
    "# shap values with kernel explainer\n",
    "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1311117-fe54-4665-8b79-dbe2b1bd185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap values with tree explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904df82-7bb3-4655-8dd8-bf46216666bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://catboost.ai/en/docs/concepts/shap-values\n",
    "shap_values = model.get_feature_importance(data=Pool(X_test, y_test), type=\"ShapValues\")\n",
    "# shape (observations, features + 1 * expected_value)shap_values = model.get_feature_importance(data=Pool(X_test, y_test), type=\"ShapValues\")\n",
    "shap.summary_plot(shap_values[:,0,:-1], X_test, plot_type=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9586d12-4dd8-405c-91f7-9e795ed6f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to random feature permutation\n",
    "# https://catboost.ai/en/docs/concepts/fstr#regular-feature-importance\n",
    "model.get_feature_importance(data=Pool(X_test, y_test), type=\"FeatureImportance\", prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a276069-0b78-44e7-8268-9ea43b1ba006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random feature permutation sklearn\n",
    "r = permutation_importance(model, X_test, y_test,\n",
    "                            n_repeats=30,\n",
    "                            random_state=0)\n",
    "# results are average; obviously not normalized to one.\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(f\"{X_train.columns[i]}\"\n",
    "        f\"{r.importances_mean[i]:.3f}\"\n",
    "        f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ba17c-50af-4f24-9cf5-b60a95057020",
   "metadata": {},
   "source": [
    "## Attention Maps for Transformers\n",
    "\n",
    "We calculate the average attention map from all transformer blocks, as done in the Gorishniy paper (see [here](https://github.com/Yura52/tabular-dl-revisiting-models/issues/2)). This is different from the Borisov paper (see [here](https://github.com/kathrinse/TabSurvey/blob/main/models/basemodel_torch.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2dad110-fac4-4b88-920c-0f4d5690eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.tabtransformer import TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c252711f-a31e-4ee6-91be-be4163e94723",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_cont = 5\n",
    "num_features_cat = 3\n",
    "num_unique_cat = tuple([2,2, 2])\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(\n",
    "            device\n",
    "        )\n",
    "x_cont = (\n",
    "            torch.randn(batch_size, num_features_cont).float().to(device)\n",
    "        )\n",
    "expected_outputs = (\n",
    "            torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    "        )\n",
    "\n",
    "model = TabTransformer(\n",
    "            cat_cardinalities=num_unique_cat,\n",
    "            num_continuous=num_features_cont,\n",
    "            dim_out=1,\n",
    "            mlp_act=nn.ReLU,\n",
    "            dim=32,\n",
    "            depth=2,\n",
    "            heads=6,\n",
    "            attn_dropout=0.1,\n",
    "            ff_dropout=0.1,\n",
    "            mlp_hidden_mults=(4, 2)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd5018b6-3993-4cd2-a06e-efc37e62f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveAttentionMaps:\n",
    "    \"\"\"\n",
    "    Hook for attention maps.\n",
    "    \n",
    "    Inspired by:\n",
    "    https://github.com/Yura52/tabular-dl-revisiting-models/issues/2#issuecomment-1068123629\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.attention_maps: List[torch.Tensor] = []\n",
    "\n",
    "    def __call__(self, _, __, output):\n",
    "        print(output[1][\"attention_probs\"].shape)\n",
    "        self.attention_maps.append(output[1]['attention_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "759907c7-07e5-4675-9cbc-5f9c0cee1057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6, 3, 3])\n",
      "torch.Size([64, 6, 3, 3])\n",
      "[0.35401863 0.31725085 0.32873052]\n",
      "[1. 3. 2.]\n",
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "# The following hook will save all attention maps from all attention modules.\n",
    "hook = SaveAttentionMaps()\n",
    "for block in model.transformer.blocks:\n",
    "    block.attention.fn.fn.register_forward_hook(hook)\n",
    "\n",
    "# Apply the model to all objects.\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    model(x_cat.clone(), x_cont.clone())\n",
    "\n",
    "# Collect attention maps\n",
    "n_objects = len(x_cat)\n",
    "n_blocks = len(model.transformer.blocks)\n",
    "n_heads = model.transformer.blocks[0].attention.fn.fn.n_heads\n",
    "\n",
    "attention_maps = torch.cat(hook.attention_maps)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "attention_maps = attention_maps.reshape(n_objects * n_blocks * n_heads, num_features_cat, num_features_cat)\n",
    "assert attention_maps.shape == (n_objects * n_blocks * n_heads, num_features_cat, num_features_cat)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "average_attention_map = attention_maps.mean(0)\n",
    "feature_importance = average_attention_map[-1]\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_ranks = scipy.stats.rankdata(-feature_importance)\n",
    "feature_indices_sorted_by_importance = feature_importance.argsort()[::-1]\n",
    "\n",
    "print(feature_importance)\n",
    "print(feature_ranks)\n",
    "print(feature_indices_sorted_by_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c005bccf-f32e-490f-bb59-8b8d658f3a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x = feature_importance, y = [\"f1\", \"f2\", \"f3\"])\n",
    "ax.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "56af3d57-8812-43dc-99bd-ab1a45cbc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from otc.models.activation import ReGLU\n",
    "from otc.models.fttransformer import (\n",
    "    CategoricalFeatureTokenizer,\n",
    "    CLSToken,\n",
    "    FeatureTokenizer,\n",
    "    FTTransformer,\n",
    "    MultiheadAttention,\n",
    "    NumericalFeatureTokenizer,\n",
    "    Transformer,\n",
    ")\n",
    "\n",
    "num_features_cont = 5\n",
    "num_features_cat = 1\n",
    "cat_cardinalities = [2]\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_cat = torch.randint(0, 1, (batch_size, num_features_cat)).to(\n",
    "    device\n",
    ")\n",
    "x_cont = (\n",
    "      torch.randn(batch_size, num_features_cont).float().to(device)\n",
    ")\n",
    "expected_outputs = (\n",
    "        torch.randint(0, 1, (batch_size, 1)).float().to(device)\n",
    ")\n",
    "\n",
    "params_feature_tokenizer= {\n",
    "        \"num_continous\": num_features_cont,\n",
    "        \"cat_cardinalities\": cat_cardinalities,\n",
    "        \"d_token\": 96,\n",
    "}\n",
    "feature_tokenizer = FeatureTokenizer(**params_feature_tokenizer)\n",
    "params_transformer = {\n",
    "            \"d_token\": 96,\n",
    "            \"n_blocks\": 3,\n",
    "            \"attention_n_heads\": 8,\n",
    "            \"attention_initialization\": \"kaiming\",\n",
    "            \"ffn_activation\": ReGLU,\n",
    "            \"attention_normalization\": nn.LayerNorm,\n",
    "            \"ffn_normalization\": nn.LayerNorm,\n",
    "            \"ffn_dropout\": 0.1,\n",
    "            \"ffn_d_hidden\": 96 * 2,\n",
    "            \"attention_dropout\": 0.1,\n",
    "            \"residual_dropout\": 0.1,\n",
    "            \"prenormalization\": True,\n",
    "            \"first_prenormalization\": False,\n",
    "            \"last_layer_query_idx\": None,\n",
    "            \"n_tokens\": None,\n",
    "            \"kv_compression_ratio\": None,\n",
    "            \"kv_compression_sharing\": None,\n",
    "            \"head_activation\": nn.ReLU,\n",
    "            \"head_normalization\": nn.LayerNorm,\n",
    "            \"d_out\": 1,\n",
    "        }\n",
    "\n",
    "transformer = Transformer(**params_transformer)\n",
    "\n",
    "model = FTTransformer(feature_tokenizer, transformer).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32c2a775-30cc-4d4c-9edd-59c018fc0cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 7, 7])\n",
      "torch.Size([512, 7, 7])\n",
      "torch.Size([512, 7, 7])\n",
      "[0.1375918  0.1466848  0.14887066 0.14371951 0.1427471  0.14100587]\n",
      "[6. 2. 1. 3. 4. 5.]\n",
      "[2 1 3 4 5 0]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data and model.\n",
    "n_objects = len(x_cat)# 12\n",
    "n_features = num_features_cont + num_features_cat\n",
    "\n",
    "# The following hook will save all attention maps from all attention modules.\n",
    "hook = SaveAttentionMaps()\n",
    "for block in model.transformer.blocks:\n",
    "    block.attention.register_forward_hook(hook)\n",
    "\n",
    "# Apply the model to all objects.\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    model(x_cat, x_cont)\n",
    "\n",
    "# Collect attention maps\n",
    "n_blocks = len(model.transformer.blocks)\n",
    "n_heads = model.transformer.blocks[0].attention.n_heads\n",
    "n_tokens = n_features + 1\n",
    "attention_maps = torch.cat(hook.attention_maps)\n",
    "assert attention_maps.shape == (n_objects * n_blocks * n_heads, n_tokens, n_tokens)\n",
    "\n",
    "# Calculate feature importance and ranks.\n",
    "average_attention_map = attention_maps.mean(0)\n",
    "average_cls_attention_map = average_attention_map[-1]  # consider only the [CLS] token\n",
    "feature_importance = average_cls_attention_map[:-1]  # drop the [CLS] token importance\n",
    "assert feature_importance.shape == (n_features,)\n",
    "\n",
    "feature_importance = feature_importance.cpu().numpy()\n",
    "feature_ranks = scipy.stats.rankdata(-feature_importance)\n",
    "feature_indices_sorted_by_importance = feature_importance.argsort()[::-1]\n",
    "\n",
    "print(feature_importance)\n",
    "print(feature_ranks)\n",
    "print(feature_indices_sorted_by_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dddb4596-2d71-45ad-a746-a4f2cda29efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.0)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x = feature_importance, y = [\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\"])\n",
    "ax.set(xlim=(0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
