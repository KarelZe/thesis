{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WXF7w4VyVgG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from otc.features.build_features import features_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"thesis\", job_type=\"dataset-creation\", entity=\"fbv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exchange = \"cboe\"\n",
    "strategy = \"transfer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = f\"fbv/thesis/{exchange}_{strategy}_none:latest\"\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "# load unscaled data\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, val, test = None, None, None\n",
    "columns = [*features_ml, \"buy_sell\"]\n",
    "\n",
    "if strategy == \"supervised\":\n",
    "    train = pd.read_parquet(\n",
    "        Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\", columns=columns\n",
    "    )\n",
    "    val = pd.read_parquet(\n",
    "        Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\", columns=columns\n",
    "    )\n",
    "    test = pd.read_parquet(\n",
    "        Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=columns\n",
    "    )\n",
    "\n",
    "elif strategy == \"transfer\":\n",
    "    # load test set\n",
    "    test = pd.read_parquet(\n",
    "        Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=columns\n",
    "    )\n",
    "\n",
    "\n",
    "elif strategy == \"unsupervised\":\n",
    "    # load unlabelled training set\n",
    "    train = pd.read_parquet(\n",
    "        Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\", columns=columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_stats(df):\n",
    "    summary_stats = pd.DataFrame(\n",
    "        index=df.columns\n",
    "    )  # Create an empty DataFrame with column names as index\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    # summary_stats['Count'] = df.count()\n",
    "    # summary_stats['Nunique'] = df.nunique()\n",
    "\n",
    "    summary_stats = df.describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "    summary_stats = (\n",
    "        summary_stats.transpose()\n",
    "    )  # Transpose the table to have columns as variables\n",
    "\n",
    "    # Rename the columns\n",
    "    summary_stats.rename(\n",
    "        columns={\"mean\": \"Mean\", \"std\": \"SD\", \"50%\": \"Median\"}, inplace=True\n",
    "    )\n",
    "    return summary_stats[\n",
    "        [\"Mean\", \"SD\", \"1%\", \"5%\", \"25%\", \"Median\", \"75%\", \"95%\", \"99%\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_sets = pd.concat([train, val, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = summarize_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KEY = f\"{exchange}-{strategy}-test\"\n",
    "FNAME = \"../reports/Content/\" + KEY + \"-summary-stats.tex\"\n",
    "\n",
    "summary.style.to_latex(\n",
    "    FNAME,\n",
    "    siunitx=True,\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    "    clines=\"skip-last;data\",\n",
    "    label=f\"tab:{KEY}\",\n",
    "    caption=(f\"long{KEY}\", f\"short{KEY}\"),\n",
    "    convert_css=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights into distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set here globally\n",
    "EXCHANGE = \"ise\"  # \"ise\"\n",
    "MODELS = [\"classical\"]  # \"classical\", \"fttransformer\", \"gbm\"\n",
    "SUBSET = \"all\"  # \"all\"\n",
    "STRATEGY = \"supervised\"  # \"supervised\"\n",
    "\n",
    "RETRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KEY = f\"{EXCHANGE}_{STRATEGY}_{SUBSET}\"\n",
    "DATASET = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_none:latest\"\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n",
    "\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "# load unscaled data\n",
    "artifact = run.use_artifact(DATASET)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "# load results\n",
    "result_dirs = []\n",
    "for model in MODELS:\n",
    "    # retraining is only possible for gbm\n",
    "    if model == \"gbm\" and RETRAIN:\n",
    "        results = f\"fbv/thesis/{EXCHANGE}_{model}_{STRATEGY}_{SUBSET}_retrain:latest\"\n",
    "    else:\n",
    "        results = f\"fbv/thesis/{EXCHANGE}_{model}_{STRATEGY}_{SUBSET}:latest\"\n",
    "    artifact = run.use_artifact(results)\n",
    "    result_dir = artifact.download()\n",
    "    result_dirs.append(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmXtH-PEqyQE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p. 35-38\n",
    "COLUMNS = [\n",
    "    \"buy_sell\",\n",
    "    \"ttm\",\n",
    "    \"TRADE_SIZE\",\n",
    "    \"TRADE_PRICE\",\n",
    "    \"ask_ex\",\n",
    "    \"ask_size_ex\",\n",
    "    \"bid_ex\",\n",
    "    \"bid_size_ex\",\n",
    "    \"myn\",\n",
    "    \"option_type\",\n",
    "    \"issue_type\",\n",
    "    \"prox_ex\",\n",
    "    # \"price_ex_lag\",\n",
    "    # \"price_all_lag\",\n",
    "    # \"BEST_BID\",\n",
    "    # \"BEST_ASK\",\n",
    "]\n",
    "\n",
    "\n",
    "if SUBSET == \"all\":\n",
    "    train = pd.read_parquet(\n",
    "        Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\", columns=COLUMNS\n",
    "    )\n",
    "    val = pd.read_parquet(\n",
    "        Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\", columns=COLUMNS\n",
    "    )\n",
    "    test = pd.read_parquet(\n",
    "        Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=COLUMNS\n",
    "    )\n",
    "    eval_data = pd.concat([train, val, test])\n",
    "    del train, val, test\n",
    "\n",
    "elif SUBSET == \"test\":\n",
    "    eval_data = pd.read_parquet(\n",
    "        Path(data_dir, \"test_set.parquet\"), engine=\"fastparquet\", columns=COLUMNS\n",
    "    )\n",
    "\n",
    "\n",
    "results = []\n",
    "for i, model in tqdm(enumerate(MODELS)):\n",
    "    result = pd.read_parquet(Path(result_dirs[i], \"results\"), engine=\"fastparquet\")\n",
    "    result.columns = pd.MultiIndex.from_product([[model], result.columns])\n",
    "    results.append(result)\n",
    "\n",
    "results_data = pd.concat(results, axis=1, names=MODELS)\n",
    "\n",
    "assert len(eval_data) == len(results_data)\n",
    "\n",
    "X_print = eval_data\n",
    "\n",
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_print[\"issue_type\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bins_tradesize = [-1, 1, 3, 5, 11, np.inf]\n",
    "trade_size_labels = [\"(0,1]\", \"(1,3]\", \"(3,5]\", \"(5,11]\", \">11\"]\n",
    "X_print[\"TRADE_SIZE_binned\"] = pd.cut(\n",
    "    X_print[\"TRADE_SIZE\"], bins_tradesize, labels=trade_size_labels\n",
    ")\n",
    "\n",
    "# p. 37\n",
    "bins_ttm = [-1, 1, 2, 3, 6, 12, np.inf]\n",
    "ttm_labels = [\n",
    "    \"<= 1\",\n",
    "    \"(1-2]\",\n",
    "    \"(2-3]\",\n",
    "    \"(3-6]\",\n",
    "    \"(6-12]\",\n",
    "    \"> 12\",\n",
    "]\n",
    "X_print[\"ttm_binned\"] = pd.cut(X_print[\"ttm\"], bins_ttm, labels=ttm_labels)\n",
    "\n",
    "# Security type\n",
    "# see 3.0a-mb-explanatory-data-analysis.ipynb\n",
    "X_print[\"issue_type\"] = X_print[\"issue_type\"].map(\n",
    "    {\n",
    "        \"0\": \"Stock option\",\n",
    "        \"A\": \"Index option\",\n",
    "        \"7\": \"Others\",\n",
    "        \"F\": \"Others\",\n",
    "        \"%\": \"Others\",\n",
    "        \" \": \"Others\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Moneyness p. 38\n",
    "bins_myn = [-1, 0.7, 0.9, 1.1, 1.3, np.inf]\n",
    "myn_labels = [\n",
    "    \"<= 0.7\",\n",
    "    \"(0.7-0.9]\",\n",
    "    \"(0.9-1.1]\",\n",
    "    \"(1.1-1.3]\",\n",
    "    \"> 1.3\",\n",
    "]\n",
    "X_print[\"myn_binned\"] = pd.cut(X_print[\"myn\"], bins_myn, labels=myn_labels)\n",
    "\n",
    "# mid p. 31 + extra category for unknowns\n",
    "ask = X_print[\"ask_ex\"]\n",
    "bid = X_print[\"bid_ex\"]\n",
    "trade_price = X_print[\"TRADE_PRICE\"]\n",
    "\n",
    "# require ask >= bid\n",
    "mid = np.where(ask >= bid, (ask + bid) * 0.5, np.nan)\n",
    "\n",
    "prox_quotes = np.where(\n",
    "    trade_price == mid,\n",
    "    0,  # at mid\n",
    "    np.where(\n",
    "        (bid < trade_price) & (trade_price < ask),\n",
    "        1,  # inside\n",
    "        np.where(\n",
    "            (trade_price == bid) | (ask == trade_price),\n",
    "            2,  # at quotes\n",
    "            np.where((trade_price < bid) | (ask < trade_price), 3, 4),\n",
    "        ),\n",
    "    ),\n",
    ")  # outside + unclassifiable\n",
    "\n",
    "bins_prox = [-np.inf, 0, 1, 2, 3, 4]\n",
    "prox_labels = [\n",
    "    \"at mid\",\n",
    "    \"inside\",\n",
    "    \"at quotes\",\n",
    "    \"outside\",\n",
    "    \"unknown\",\n",
    "]\n",
    "\n",
    "X_print[\"prox_q_binned\"] = pd.cut(prox_quotes, bins_prox, labels=prox_labels)\n",
    "X_print[\"mid\"] = mid\n",
    "\n",
    "# clean up empty buckets, as it causes empty grouping in result set generation\n",
    "X_print[\"myn_binned\"] = X_print[\"myn_binned\"].cat.remove_unused_categories()\n",
    "X_print[\"ttm_binned\"] = X_print[\"ttm_binned\"].cat.remove_unused_categories()\n",
    "X_print[\"prox_q_binned\"] = X_print[\"prox_q_binned\"].cat.remove_unused_categories()\n",
    "\n",
    "X_print[\"values\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_print.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_table = pd.pivot_table(\n",
    "    X_print,\n",
    "    values=\"values\",\n",
    "    columns=\"prox_q_binned\",\n",
    "    index=\"TRADE_SIZE_binned\",\n",
    "    aggfunc=sum,\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    ")\n",
    "pivot_table.div(pivot_table.iloc[:, -1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moneyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_table = pd.pivot_table(\n",
    "    X_print,\n",
    "    values=\"values\",\n",
    "    columns=\"myn_binned\",\n",
    "    index=\"TRADE_SIZE_binned\",\n",
    "    aggfunc=sum,\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    ")\n",
    "pivot_table.div(pivot_table.iloc[:, -1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-to-maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# savickas: trades with longer maturity tend to be smaller\n",
    "pivot_table = pd.pivot_table(\n",
    "    X_print,\n",
    "    values=\"values\",\n",
    "    index=\"ttm_binned\",\n",
    "    columns=\"TRADE_SIZE_binned\",\n",
    "    aggfunc=sum,\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    ")\n",
    "pivot_table.div(pivot_table.iloc[:, -1], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_table = pd.pivot_table(\n",
    "    X_print,\n",
    "    values=\"values\",\n",
    "    index=\"issue_type\",\n",
    "    columns=None,\n",
    "    aggfunc=sum,\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    ")\n",
    "pivot_table.div(pivot_table.iloc[-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_table = pd.pivot_table(\n",
    "    X_print,\n",
    "    values=\"values\",\n",
    "    index=\"prox_q_binned\",\n",
    "    columns=None,\n",
    "    aggfunc=sum,\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    ")\n",
    "pivot_table.div(pivot_table.iloc[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mid p. 31 + extra category for unknowns\n",
    "ask = X_print[\"ask_ex\"]\n",
    "bid = X_print[\"bid_ex\"]\n",
    "trade_price = X_print[\"TRADE_PRICE\"]\n",
    "\n",
    "# require ask >= bid\n",
    "mid = np.where(ask >= bid, (ask + bid) * 0.5, np.nan)\n",
    "\n",
    "results = []\n",
    "\n",
    "# calculate true rel effective spread but not aggregated, convert to %\n",
    "es_true = effective_spread(\n",
    "    X_print[\"buy_sell\"], X_print[\"TRADE_PRICE\"], mid, mode=\"none\"\n",
    ")\n",
    "nom_true = np.nanmean(es_true)\n",
    "\n",
    "eps_true = np.empty(es_true.shape)\n",
    "np.divide(es_true, mid, out=eps_true, where=mid != 0)\n",
    "rel_true = np.nanmean(eps_true)\n",
    "\n",
    "\n",
    "for classifier in tqdm(classifiers):\n",
    "    # calculate pred rel effective spread but not aggregated convert to %\n",
    "    es_pred = effective_spread(\n",
    "        X_print[classifier], X_print[\"TRADE_PRICE\"], mid, mode=\"none\"\n",
    "    )\n",
    "\n",
    "    eps_pred = np.empty(es_pred.shape)\n",
    "    np.divide(es_pred, mid, out=eps_pred, where=mid != 0)\n",
    "\n",
    "    wilcoxon_res = wilcoxon(eps_pred, eps_true, nan_policy=\"omit\", zero_method=\"zsplit\")\n",
    "\n",
    "    res = pd.Series(\n",
    "        {\n",
    "            \"nom_pred\": np.nanmean(es_pred),\n",
    "            \"rel_pred\": np.nanmean(eps_pred),\n",
    "            \"statistic\": wilcoxon_res.statistic,\n",
    "            \"pvalue\": wilcoxon_res.pvalue,\n",
    "        },\n",
    "        name=classifier,\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "true_eff = pd.Series(\n",
    "    {\"nom_pred\": nom_true, \"rel_pred\": rel_true, \"statistic\": np.nan, \"pvalue\": np.nan},\n",
    "    name=\"true_eff\",\n",
    ")\n",
    "\n",
    "results.append(true_eff)\n",
    "\n",
    "results = pd.concat(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.T.style.format(\"{:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.T.style.to_latex(\n",
    "    f\"../reports/Content/{KEY}-eff-spread.tex\",\n",
    "    siunitx=True,\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    "    clines=\"skip-last;data\",\n",
    "    label=f\"tab:eff-{KEY}\",\n",
    "    caption=(f\"long-eff-{KEY}\", f\"short-eff-{KEY}\"),\n",
    "    convert_css=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8ea8b642289b706932f10b33ee389827410dbaef0ce2c5bf73615e8d3267d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
