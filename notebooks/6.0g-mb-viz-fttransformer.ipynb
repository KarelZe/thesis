{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.features.build_features import (\n",
    "    features_classical,\n",
    ")\n",
    "from otc.models.fttransformer import FeatureTokenizer, FTTransformer, Transformer\n",
    "from otc.models.activation import ReGLU, GeGLU\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical\n",
    "from otc.optim.early_stopping import EarlyStopping\n",
    "from otc.optim.scheduler import CosineWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globally here\n",
    "EXCHANGE = \"ise\"  # \"cboe\"\n",
    "STRATEGY = \"supervised\"  # \"transfer\"\n",
    "SUBSET = \"test\"  # \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key used for files and artefacts\n",
    "key = f\"{EXCHANGE}_fttransformer_{STRATEGY}_{SUBSET}_viz\"\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_log_standardized_clipped:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_7CKpqcONOy"
   },
   "outputs": [],
   "source": [
    "# set project name. Required to access files and artefacts\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmXtH-PEqyQE"
   },
   "outputs": [],
   "source": [
    "# see https://wandb.ai/fbv/thesis/runs/kwlaw02g/overview?workspace=user-karelze\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\")\n",
    "y_train = train[\"buy_sell\"]\n",
    "X_train = train.drop(columns=\"buy_sell\")\n",
    "X_train = X_train.loc[:, features_classical]\n",
    "\n",
    "val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\")\n",
    "y_val = val[\"buy_sell\"]\n",
    "X_val = val.drop(columns=\"buy_sell\")\n",
    "X_val = X_val.loc[:, features_classical]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zMIOV1jA_ImH"
   },
   "source": [
    "## FT-TransformerðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters varied\n",
    "kwargs_activation = {\"ffn_activation\": GeGLU, \"head_activation\": nn.GELU}\n",
    "kwargs_sample_weighting = {\"sample_weighting\": True}\n",
    "kwargs_label_smoothing = {\"label_smoothing\": True}\n",
    "kwargs_lr_scheduler = {\"lr_scheduler\": True}\n",
    "\n",
    "kwargs_default = {\"ffn_activation\": ReGLU, \"head_activation\": nn.ReLU, \"sample_weighting\": False, \"lr_scheduler\": False, \"label_smoothing\": False}\n",
    "\n",
    "# complete config\n",
    "settings = [{}, kwargs_activation, kwargs_sample_weighting, kwargs_label_smoothing, kwargs_lr_scheduler]\n",
    "# use default or overwrite\n",
    "settings = [{**kwargs_default, **setting} for setting in settings] \n",
    "\n",
    "identifier = [\"default\", \"activation\", \"sample_weighting\", \"label_smoothing\", \"lr_scheduler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "epochs = 10\n",
    "\n",
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "\n",
    "\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": True,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, setting in enumerate(tqdm(settings)):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    transformer_kwargs = {\n",
    "        \"d_token\": d_token,\n",
    "        \"n_blocks\": n_blocks,\n",
    "        \"attention_n_heads\": attention_heads,\n",
    "        \"attention_initialization\": \"kaiming\",\n",
    "        \"ffn_activation\": setting[\"ffn_activation\"],\n",
    "        \"attention_normalization\": nn.LayerNorm,\n",
    "        \"ffn_normalization\": nn.LayerNorm,\n",
    "        \"ffn_dropout\": ffn_dropout,\n",
    "        \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "        \"attention_dropout\": attention_dropout,\n",
    "        \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "        \"prenormalization\": True,\n",
    "        \"first_prenormalization\": False,\n",
    "        \"last_layer_query_idx\": None,\n",
    "        \"n_tokens\": None,\n",
    "        \"kv_compression_ratio\": None,\n",
    "        \"kv_compression_sharing\": None,\n",
    "        \"head_activation\": setting[\"head_activation\"],\n",
    "        \"head_normalization\": nn.LayerNorm,\n",
    "        \"d_out\": 1,\n",
    "    }\n",
    "\n",
    "\n",
    "    optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "    module_params = {\n",
    "        \"transformer\": Transformer(**transformer_kwargs),\n",
    "        \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs), \n",
    "        \"cat_features\": None,\n",
    "        \"cat_cardinalities\": [],\n",
    "    }\n",
    "\n",
    "    clf = FTTransformer(**module_params)\n",
    "    clf.to(device)\n",
    "\n",
    "    if setting[\"sample_weighting\"]:\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        weight = np.geomspace(0.001, 1, num=len(y_train))\n",
    "    else:    \n",
    "        criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        weight = None\n",
    "\n",
    "    training_data = TabDataset(X_train, y_train, weight=weight)\n",
    "    val_data = TabDataset(X_val, y_val)\n",
    "\n",
    "    # apply label smoothing but only on training data\n",
    "    if setting[\"label_smoothing\"]:\n",
    "        eps = 0.1\n",
    "        training_data.y = (1 - 2 * eps) * training_data.y + eps\n",
    "\n",
    "    train_loader = TabDataLoader(\n",
    "        training_data.x_cat,\n",
    "        training_data.x_cont,\n",
    "        training_data.weight,\n",
    "        training_data.y,\n",
    "        **dl_params\n",
    "    )\n",
    "    \n",
    "    val_loader = TabDataLoader(\n",
    "        val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        clf.parameters(),\n",
    "        lr=optim_params[\"lr\"],\n",
    "        weight_decay=optim_params[\"weight_decay\"],\n",
    "    )\n",
    "\n",
    "    if setting[\"lr_scheduler\"]:\n",
    "        max_iters = epochs * len(train_loader)\n",
    "        warmup = int(0.05 * max_iters) + 1\n",
    "        scheduler = CosineWarmupScheduler(\n",
    "            optimizer=optimizer, warmup=warmup, max_iters=max_iters\n",
    "        )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_step = 0\n",
    "    val_step = 0\n",
    "    best_accuracy = -1\n",
    "    best_step = -1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss_in_epoch_train = 0.0\n",
    "        train_batch = 0\n",
    "\n",
    "        results_epoch = []\n",
    "\n",
    "        for x_cat, x_cont, weights, targets in train_loader:\n",
    "\n",
    "            clf.train()\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                logits = clf(x_cat, x_cont).flatten()\n",
    "                \n",
    "                if setting[\"sample_weighting\"]:\n",
    "                    intermediate_loss = criterion(logits, targets)\n",
    "                    train_loss = torch.mean(weights * intermediate_loss)\n",
    "                else:\n",
    "                    train_loss = criterion(logits, targets)\n",
    "\n",
    "            scaler.scale(train_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if setting[\"lr_scheduler\"]:\n",
    "                scheduler.step()\n",
    "\n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            loss_in_epoch_train += train_loss.item()\n",
    "            results_epoch.append({\"train_loss\": train_loss.item(), \"epoch\": epoch, \"train_step\": train_step})\n",
    "\n",
    "            train_batch += 1\n",
    "            train_step += 1\n",
    "\n",
    "        clf.eval()\n",
    "        loss_in_epoch_val = 0.0\n",
    "        val_step = 0\n",
    "        correct_sum = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for x_cat, x_cont, weights, targets in val_loader:\n",
    "\n",
    "                # for my implementation\n",
    "                logits = clf(x_cat, x_cont).flatten()\n",
    "                logits = logits.flatten()\n",
    "\n",
    "                if setting[\"sample_weighting\"]:\n",
    "                    intermediate_loss = criterion(logits, targets)\n",
    "                    val_loss = torch.mean(weights * intermediate_loss)\n",
    "                else:\n",
    "                    val_loss = criterion(logits, targets)\n",
    "\n",
    "                # get probabilities and round to nearest integer\n",
    "                preds = torch.sigmoid(logits).round()\n",
    "                correct = (preds == targets).sum().item()\n",
    "                \n",
    "                correct_sum += correct\n",
    "                val_accuracy = correct / len(targets)\n",
    "\n",
    "                loss_in_epoch_val += val_loss.item()\n",
    "                results_epoch.append({\"val_loss\": val_loss.item(),\"val_accuracy\":val_accuracy, \"epoch\": epoch, \"val_step\": val_step})\n",
    "\n",
    "                val_step += 1\n",
    "\n",
    "        # loss average over all batches\n",
    "        train_loss = loss_in_epoch_train / len(train_loader)\n",
    "        val_loss = loss_in_epoch_val / len(val_loader)\n",
    "        val_accuracy = correct_sum / len(X_val)\n",
    "        \n",
    "        print(f\"train:{train_loss} val:{val_loss}\")\n",
    "        print(f\"val accuracy:{val_accuracy}\")\n",
    "\n",
    "        result.extend(results_epoch)\n",
    "\n",
    "    del train_loader, val_loader, clf, training_data, val_data\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    results.append({identifier[i]: result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for result in results:\n",
    "    key = list(result.keys())[0]\n",
    "    df = pd.DataFrame(result[key])\n",
    "    df.name=key\n",
    "    dfs.append(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = pd.concat(dfs, axis=1, keys = identifier)\n",
    "output_path = (\n",
    "    f\"gs://thesis-bucket-option-trade-classification/data/results/{key}-viz-losses.parquet\"\n",
    ")\n",
    "dfs.columns = ['_'.join(col).rstrip('_') for col in dfs.columns.values]\n",
    "dfs.to_parquet(output_path)\n",
    "\n",
    "name = \"viz_fttransformer\"\n",
    "\n",
    "# Log the artifact to save it as an output of this run\n",
    "result_set = wandb.Artifact(name=name, type=\"results\")\n",
    "result_set.add_reference(output_path, name=\"results\")\n",
    "run.log_artifact(result_set)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
