{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.features.build_features import (\n",
    "    features_classical,\n",
    ")\n",
    "from otc.models.fttransformer import FeatureTokenizer, FTTransformer, Transformer\n",
    "from otc.models.activation import ReGLU, GeGLU\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader\n",
    "from otc.features.build_features import features_classical\n",
    "from otc.optim.early_stopping import EarlyStopping\n",
    "from otc.optim.scheduler import CosineWarmupScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globally here\n",
    "EXCHANGE = \"ise\"  # \"cboe\"\n",
    "STRATEGY = \"supervised\"  # \"transfer\"\n",
    "SUBSET = \"test\"  # \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key used for files and artefacts\n",
    "key = f\"{EXCHANGE}_fttransformer_{STRATEGY}_{SUBSET}_viz\"\n",
    "dataset = f\"fbv/thesis/{EXCHANGE}_{STRATEGY}_log_standardized_clipped:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_7CKpqcONOy"
   },
   "outputs": [],
   "source": [
    "# set project name. Required to access files and artefacts\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmXtH-PEqyQE"
   },
   "outputs": [],
   "source": [
    "# see https://wandb.ai/fbv/thesis/runs/kwlaw02g/overview?workspace=user-karelze\n",
    "run = wandb.init(project=\"thesis\", entity=\"fbv\")\n",
    "\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "train = pd.read_parquet(Path(data_dir, \"train_set.parquet\"), engine=\"fastparquet\").sample(frac=0.02)\n",
    "y_train = train[\"buy_sell\"]\n",
    "X_train = train.drop(columns=\"buy_sell\")\n",
    "X_train = X_train.loc[:, features_classical]\n",
    "\n",
    "val = pd.read_parquet(Path(data_dir, \"val_set.parquet\"), engine=\"fastparquet\").sample(frac=0.02)\n",
    "y_val = val[\"buy_sell\"]\n",
    "X_val = val.drop(columns=\"buy_sell\")\n",
    "X_val = X_val.loc[:, features_classical]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMIOV1jA_ImH"
   },
   "source": [
    "## FT-TransformerðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters varied\n",
    "kwargs_activation = {\"ffn_activation\": GeGLU, \"head_activation\": nn.GELU}\n",
    "kwargs_sample_weighting = {\"sample_weighting\": True}\n",
    "kwargs_label_smoothing = {\"label_smoothing\": True}\n",
    "kwargs_lr_scheduler = {\"lr_scheduler\": True}\n",
    "\n",
    "kwargs_default = {\n",
    "    \"ffn_activation\": ReGLU,\n",
    "    \"head_activation\": nn.ReLU,\n",
    "    \"sample_weighting\": False,\n",
    "    \"lr_scheduler\": False,\n",
    "    \"label_smoothing\": False,\n",
    "}\n",
    "\n",
    "# complete config\n",
    "settings = [\n",
    "    {},\n",
    "    kwargs_activation,\n",
    "    kwargs_sample_weighting,\n",
    "    kwargs_label_smoothing,\n",
    "    kwargs_lr_scheduler,\n",
    "]\n",
    "# use default or overwrite\n",
    "settings = [{**kwargs_default, **setting} for setting in settings]\n",
    "\n",
    "identifier = [\n",
    "    \"default\",\n",
    "    \"activation\",\n",
    "    \"sample_weighting\",\n",
    "    \"label_smoothing\",\n",
    "    \"lr_scheduler\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "device = \"cuda\"\n",
    "batch_size = 16192\n",
    "epochs = 10\n",
    "eval_interval = 128\n",
    "\n",
    "d_token = 192\n",
    "n_blocks = 3\n",
    "attention_dropout = 0.2\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.0\n",
    "attention_heads = 8\n",
    "\n",
    "\n",
    "feature_tokenizer_kwargs = {\n",
    "    \"num_continous\": len(X_train.columns.tolist()),\n",
    "    \"cat_cardinalities\": (),\n",
    "    \"d_token\": d_token,\n",
    "}\n",
    "\n",
    "dl_params = {\n",
    "    \"batch_size\": batch_size,  # dataprallel splits batches across devices\n",
    "    \"shuffle\": True,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, setting in enumerate(tqdm(settings)):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    transformer_kwargs = {\n",
    "        \"d_token\": d_token,\n",
    "        \"n_blocks\": n_blocks,\n",
    "        \"attention_n_heads\": attention_heads,\n",
    "        \"attention_initialization\": \"kaiming\",\n",
    "        \"ffn_activation\": setting[\"ffn_activation\"],\n",
    "        \"attention_normalization\": nn.LayerNorm,\n",
    "        \"ffn_normalization\": nn.LayerNorm,\n",
    "        \"ffn_dropout\": ffn_dropout,\n",
    "        \"ffn_d_hidden\": int(d_token * (4 / 3)),\n",
    "        \"attention_dropout\": attention_dropout,\n",
    "        \"residual_dropout\": residual_dropout,  # see search space (B)\n",
    "        \"prenormalization\": True,\n",
    "        \"first_prenormalization\": False,\n",
    "        \"last_layer_query_idx\": None,\n",
    "        \"n_tokens\": None,\n",
    "        \"kv_compression_ratio\": None,\n",
    "        \"kv_compression_sharing\": None,\n",
    "        \"head_activation\": setting[\"head_activation\"],\n",
    "        \"head_normalization\": nn.LayerNorm,\n",
    "        \"d_out\": 1,\n",
    "    }\n",
    "\n",
    "    optim_params = {\"lr\": 1e-4, \"weight_decay\": 0.00001}\n",
    "\n",
    "    module_params = {\n",
    "        \"transformer\": Transformer(**transformer_kwargs),\n",
    "        \"feature_tokenizer\": FeatureTokenizer(**feature_tokenizer_kwargs),\n",
    "        \"cat_features\": None,\n",
    "        \"cat_cardinalities\": [],\n",
    "    }\n",
    "\n",
    "    clf = FTTransformer(**module_params)\n",
    "    clf.to(device)\n",
    "\n",
    "    if setting[\"sample_weighting\"]:\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        weight = np.geomspace(0.001, 1, num=len(y_train))\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        weight = None\n",
    "\n",
    "    training_data = TabDataset(X_train, y_train, weight=weight)\n",
    "    val_data = TabDataset(X_val, y_val)\n",
    "\n",
    "    # apply label smoothing but only on training data\n",
    "    if setting[\"label_smoothing\"]:\n",
    "        eps = 0.1\n",
    "        training_data.y = (1 - 2 * eps) * training_data.y + eps\n",
    "\n",
    "    train_loader = TabDataLoader(\n",
    "        training_data.x_cat,\n",
    "        training_data.x_cont,\n",
    "        training_data.weight,\n",
    "        training_data.y,\n",
    "        **dl_params\n",
    "    )\n",
    "\n",
    "    val_loader = TabDataLoader(\n",
    "        val_data.x_cat, val_data.x_cont, val_data.weight, val_data.y, **dl_params\n",
    "    )\n",
    "\n",
    "    # Specify parameters for which weight decay should be disabled\n",
    "    no_decay = [\"tokenizer\", \".norm\", \".bias\"]\n",
    "\n",
    "    # Create a list of parameter groups\n",
    "    param_groups = [\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p\n",
    "                        for n, p in clf.named_parameters()\n",
    "                        if not any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": optim_params[\"weight_decay\"],\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p\n",
    "                        for n, p in clf.named_parameters()\n",
    "                        if any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "    ]\n",
    "\n",
    "    # Generate the optimizers\n",
    "    optimizer = optim.AdamW(\n",
    "        param_groups,\n",
    "        lr=optim_params[\"lr\"],\n",
    "    )\n",
    "\n",
    "    max_steps = epochs * len(train_loader)\n",
    "    \n",
    "    if setting[\"lr_scheduler\"]:\n",
    "        max_iters = max_steps\n",
    "        warmup = int(0.05 * max_iters) + 1\n",
    "        scheduler = CosineWarmupScheduler(\n",
    "            optimizer=optimizer, warmup=warmup, max_iters=max_iters\n",
    "        )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        results_step = []\n",
    "\n",
    "        for batch_idx, (x_cat, x_cont, weights, targets) in enumerate(train_loader):\n",
    "\n",
    "            clf.train()\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                logits = clf(x_cat, x_cont).flatten()\n",
    "\n",
    "                if setting[\"sample_weighting\"]:\n",
    "                    intermediate_loss = criterion(logits, targets)\n",
    "                    train_loss = torch.sum(weights * intermediate_loss) / torch.sum(\n",
    "                        weights\n",
    "                    )\n",
    "                else:\n",
    "                    train_loss = criterion(logits, targets)\n",
    "\n",
    "            scaler.scale(train_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if setting[\"lr_scheduler\"]:\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "            preds = torch.sigmoid(logits).round()\n",
    "            # round back if label smoothing was applied\n",
    "            correct = (preds == targets.round()).sum().item()\n",
    "            train_accuracy = correct / len(targets)\n",
    "\n",
    "            results_step.append(\n",
    "                {\n",
    "                    \"train_loss\": train_loss.item(),\n",
    "                    \"train_accuracy\": train_accuracy,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_step\": batch_idx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "            if (batch_idx + 1) % eval_interval == 0:  \n",
    "\n",
    "                clf.eval()\n",
    "                \n",
    "                correct = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for x_cat, x_cont, weights, targets in val_loader:\n",
    "\n",
    "                        # for my implementation\n",
    "                        logits = clf(x_cat, x_cont).flatten()\n",
    "                        logits = logits.flatten()\n",
    "\n",
    "                        if setting[\"sample_weighting\"]:\n",
    "                            intermediate_loss = criterion(logits, targets)\n",
    "                            val_loss = torch.sum(weights * intermediate_loss) / torch.sum(\n",
    "                                weights\n",
    "                            )\n",
    "                        else:\n",
    "                            val_loss = criterion(logits, targets)\n",
    "\n",
    "                        # get probabilities and round to nearest integer\n",
    "                        preds = torch.sigmoid(logits).round()\n",
    "                        correct += (preds == targets).sum().item()\n",
    "\n",
    "                    \n",
    "                    val_accuracy = correct / len(val_data)\n",
    "\n",
    "                    results_step.append(\n",
    "                            {\n",
    "                                \"val_loss\": val_loss.item(),\n",
    "                                \"val_accuracy\": val_accuracy,\n",
    "                                \"epoch\": epoch,\n",
    "                                \"val_step\": batch_idx,\n",
    "                            }\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "        result.extend(results_step)\n",
    "\n",
    "    del train_loader, val_loader, clf, training_data, val_data\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    results.append({identifier[i]: result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for result in results:\n",
    "    key = list(result.keys())[0]\n",
    "    df = pd.DataFrame(result[key])\n",
    "    df.name = key\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = pd.concat(dfs, axis=1, keys=identifier)\n",
    "output_path = f\"gs://thesis-bucket-option-trade-classification/data/results/{key}-viz-losses-frequent.parquet\"\n",
    "dfs.columns = [\"_\".join(col).rstrip(\"_\") for col in dfs.columns.values]\n",
    "dfs.to_parquet(output_path)\n",
    "\n",
    "name = \"viz_fttransformer_frequent\"\n",
    "\n",
    "# Log the artifact to save it as an output of this run\n",
    "result_set = wandb.Artifact(name=name, type=\"results\")\n",
    "result_set.add_reference(output_path, name=\"results\")\n",
    "run.log_artifact(result_set)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in dfs if col.endswith('val_loss')]\n",
    "dfs[filter_col].dropna().reset_index(drop=True).plot()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
