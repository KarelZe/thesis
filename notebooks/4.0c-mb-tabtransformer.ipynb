{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7WXF7w4VyVgG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import gcsfs\n",
    "import google.auth\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from google.colab import auth, output\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DMpV9NTt25pj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/stud/uloak/.local/lib/python3.8/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# connect to google cloud storage\n",
    "# auth.authenticate_user()\n",
    "#credentials, _ = google.auth.default()\n",
    "os.environ[\"GCLOUD_PROJECT\"] = \"flowing-mantis-239216\"\n",
    "fs = gcsfs.GCSFileSystem(project=\"thesis\")\n",
    "fs_prefix = \"gs://\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vFMHPh-nTcSI"
   },
   "outputs": [],
   "source": [
    "features_classical_size = [\n",
    "    'TRADE_PRICE', 'bid_ask_size_ratio_ex', 'rel_bid_size_ex',\n",
    "       'rel_ask_size_ex', 'TRADE_SIZE', 'bid_size_ex', 'ask_size_ex',\n",
    "       'rel_ask_ex', 'rel_bid_ex', 'BEST_rel_bid', 'BEST_rel_ask',\n",
    "       'bid_ask_ratio_ex', 'chg_ex_lead', 'chg_ex_lag', 'chg_all_lead',\n",
    "       'chg_all_lag', 'ask_ex', 'bid_ex', 'BEST_ASK', 'BEST_BID',\n",
    "       'price_all_lag', 'price_all_lead', 'price_ex_lag', 'price_ex_lead'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "ah1dofx3TdDj",
    "outputId": "fb566d1a-ccae-4ba5-c113-6eb2f30e1c49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarelze\u001b[0m (\u001b[33mfbv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/pfs/data5/home/kit/stud/uloak/thesis/notebooks/wandb/run-20221222_160504-1zukwl2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/fbv/thesis/runs/1zukwl2f\" target=\"_blank\">wobbly-deluge-598</a></strong> to <a href=\"https://wandb.ai/fbv/thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact classical_size_features_log_normalized:v0, 2564.04MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "# see https://wandb.ai/fbv/thesis/runs/kwlaw02g/overview?workspace=user-karelze\n",
    "# for refs\n",
    "\n",
    "run = wandb.init(project=\"thesis\",entity=\"fbv\")\n",
    "\n",
    "dataset = \"fbv/thesis/classical_size_features_log_normalized:v0\"\n",
    "artifact = run.use_artifact(dataset)\n",
    "data_dir = artifact.download()\n",
    "\n",
    "model = \"fbv/thesis/3lfsbuby_TabTransformer_default_trial_82.pth:v0\"\n",
    "artifact = run.use_artifact(model)\n",
    "model_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WmXtH-PEqyQE"
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_parquet(Path(data_dir, \"test_set_20.parquet\"), engine=\"fastparquet\")\n",
    "\n",
    "y_test = X_test[\"buy_sell\"]\n",
    "X_test = X_test[features_classical_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ypC-A70pYQ7z",
    "outputId": "69af7468-67d4-4901-882e-33eab47a5b8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRADE_PRICE</th>\n",
       "      <th>bid_ask_size_ratio_ex</th>\n",
       "      <th>rel_bid_size_ex</th>\n",
       "      <th>rel_ask_size_ex</th>\n",
       "      <th>TRADE_SIZE</th>\n",
       "      <th>bid_size_ex</th>\n",
       "      <th>ask_size_ex</th>\n",
       "      <th>rel_ask_ex</th>\n",
       "      <th>rel_bid_ex</th>\n",
       "      <th>BEST_rel_bid</th>\n",
       "      <th>...</th>\n",
       "      <th>chg_all_lead</th>\n",
       "      <th>chg_all_lag</th>\n",
       "      <th>ask_ex</th>\n",
       "      <th>bid_ex</th>\n",
       "      <th>BEST_ASK</th>\n",
       "      <th>BEST_BID</th>\n",
       "      <th>price_all_lag</th>\n",
       "      <th>price_all_lead</th>\n",
       "      <th>price_ex_lag</th>\n",
       "      <th>price_ex_lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39342171</th>\n",
       "      <td>-0.886605</td>\n",
       "      <td>-0.999944</td>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.999959</td>\n",
       "      <td>-0.920668</td>\n",
       "      <td>-0.397940</td>\n",
       "      <td>-0.568327</td>\n",
       "      <td>-0.450753</td>\n",
       "      <td>0.450753</td>\n",
       "      <td>-0.222609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979614</td>\n",
       "      <td>0.887864</td>\n",
       "      <td>-0.869878</td>\n",
       "      <td>-0.883935</td>\n",
       "      <td>-0.922989</td>\n",
       "      <td>-0.883936</td>\n",
       "      <td>-0.917824</td>\n",
       "      <td>-0.921640</td>\n",
       "      <td>-0.897066</td>\n",
       "      <td>-0.875702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39342172</th>\n",
       "      <td>-0.398830</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.999660</td>\n",
       "      <td>-0.999773</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.879588</td>\n",
       "      <td>-0.879588</td>\n",
       "      <td>-0.449826</td>\n",
       "      <td>0.449826</td>\n",
       "      <td>-0.221914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979609</td>\n",
       "      <td>0.887724</td>\n",
       "      <td>-0.387113</td>\n",
       "      <td>-0.403539</td>\n",
       "      <td>-0.637270</td>\n",
       "      <td>-0.403545</td>\n",
       "      <td>-0.511138</td>\n",
       "      <td>-0.615242</td>\n",
       "      <td>-0.380144</td>\n",
       "      <td>-0.521084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39342173</th>\n",
       "      <td>-0.060248</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.999660</td>\n",
       "      <td>-0.999773</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.879588</td>\n",
       "      <td>-0.879588</td>\n",
       "      <td>-0.450689</td>\n",
       "      <td>0.450689</td>\n",
       "      <td>-0.222561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979589</td>\n",
       "      <td>0.887492</td>\n",
       "      <td>-0.031559</td>\n",
       "      <td>-0.059053</td>\n",
       "      <td>-0.426840</td>\n",
       "      <td>-0.059063</td>\n",
       "      <td>-0.248786</td>\n",
       "      <td>-0.402171</td>\n",
       "      <td>-0.057769</td>\n",
       "      <td>-0.080426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39342174</th>\n",
       "      <td>-0.675788</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.583443</td>\n",
       "      <td>-0.583443</td>\n",
       "      <td>-0.448436</td>\n",
       "      <td>0.448436</td>\n",
       "      <td>-0.220290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979647</td>\n",
       "      <td>0.887998</td>\n",
       "      <td>-0.673685</td>\n",
       "      <td>-0.709687</td>\n",
       "      <td>-0.811995</td>\n",
       "      <td>-0.709690</td>\n",
       "      <td>-0.802774</td>\n",
       "      <td>-0.914062</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39342175</th>\n",
       "      <td>-0.727221</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.583443</td>\n",
       "      <td>-0.583443</td>\n",
       "      <td>-0.450753</td>\n",
       "      <td>0.450753</td>\n",
       "      <td>-0.222609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979636</td>\n",
       "      <td>0.887758</td>\n",
       "      <td>-0.700498</td>\n",
       "      <td>-0.724674</td>\n",
       "      <td>-0.822743</td>\n",
       "      <td>-0.724677</td>\n",
       "      <td>-0.747425</td>\n",
       "      <td>-0.913006</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          TRADE_PRICE  bid_ask_size_ratio_ex  rel_bid_size_ex  \\\n",
       "39342171    -0.886605              -0.999944        -0.999978   \n",
       "39342172    -0.398830              -0.999980        -0.999660   \n",
       "39342173    -0.060248              -0.999980        -0.999660   \n",
       "39342174    -0.675788              -0.999980        -0.999966   \n",
       "39342175    -0.727221              -0.999980        -0.999966   \n",
       "\n",
       "          rel_ask_size_ex  TRADE_SIZE  bid_size_ex  ask_size_ex  rel_ask_ex  \\\n",
       "39342171        -0.999959   -0.920668    -0.397940    -0.568327   -0.450753   \n",
       "39342172        -0.999773   -1.000000    -0.879588    -0.879588   -0.449826   \n",
       "39342173        -0.999773   -1.000000    -0.879588    -0.879588   -0.450689   \n",
       "39342174        -0.999977   -1.000000    -0.583443    -0.583443   -0.448436   \n",
       "39342175        -0.999977   -1.000000    -0.583443    -0.583443   -0.450753   \n",
       "\n",
       "          rel_bid_ex  BEST_rel_bid  ...  chg_all_lead  chg_all_lag    ask_ex  \\\n",
       "39342171    0.450753     -0.222609  ...      0.979614     0.887864 -0.869878   \n",
       "39342172    0.449826     -0.221914  ...      0.979609     0.887724 -0.387113   \n",
       "39342173    0.450689     -0.222561  ...      0.979589     0.887492 -0.031559   \n",
       "39342174    0.448436     -0.220290  ...      0.979647     0.887998 -0.673685   \n",
       "39342175    0.450753     -0.222609  ...      0.979636     0.887758 -0.700498   \n",
       "\n",
       "            bid_ex  BEST_ASK  BEST_BID  price_all_lag  price_all_lead  \\\n",
       "39342171 -0.883935 -0.922989 -0.883936      -0.917824       -0.921640   \n",
       "39342172 -0.403539 -0.637270 -0.403545      -0.511138       -0.615242   \n",
       "39342173 -0.059053 -0.426840 -0.059063      -0.248786       -0.402171   \n",
       "39342174 -0.709687 -0.811995 -0.709690      -0.802774       -0.914062   \n",
       "39342175 -0.724674 -0.822743 -0.724677      -0.747425       -0.913006   \n",
       "\n",
       "          price_ex_lag  price_ex_lead  \n",
       "39342171     -0.897066      -0.875702  \n",
       "39342172     -0.380144      -0.521084  \n",
       "39342173     -0.057769      -0.080426  \n",
       "39342174     -1.000000      -1.000000  \n",
       "39342175     -1.000000      -1.000000  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMIOV1jA_ImH"
   },
   "source": [
    "## TabTransformer Baseline 🦾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from otc.models.tabtransformer import TabTransformer\n",
    "from otc.data.dataset import TabDataset\n",
    "from otc.data.dataloader import TabDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_QQ7sa9OJA2X"
   },
   "outputs": [],
   "source": [
    "# https://wandb.ai/fbv/thesis/runs/4fmccjm7/files/wandb-summary.json\n",
    "# https://wandb.ai/fbv/thesis/artifacts/model/3lfsbuby_TabTransformer_default_trial_82.pth/3a1937a3e6ec748d45a3/metadata\n",
    "params = {  \"dim\": 32,\n",
    "  \"depth\": 3,\n",
    "  \"heads\": 2,\n",
    "  \"weight_decay\": 0.00835620489462654,\n",
    "  \"lr\": 0.0015514372468568292,\n",
    "  \"dropout\": 0.1,\n",
    "  \"batch_size\": 32768}\n",
    "\n",
    "training_data = TabDataset(X_test, y_test, [], [])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")        \n",
    "\n",
    "# differentiate between continous features only and mixed.\n",
    "test_loader = TabDataLoader(\n",
    "training_data.x_cat, training_data.x_cont, training_data.y, batch_size=params['batch_size'], device=device\n",
    ")\n",
    "\n",
    "       \n",
    "model = TabTransformer(\n",
    "            cat_cardinalities=[],\n",
    "            num_continuous=len(features_classical_size),\n",
    "            dim_out=1,\n",
    "            mlp_act=nn.ReLU,\n",
    "            dim=params[\"dim\"],\n",
    "            depth=params[\"depth\"],\n",
    "            heads=params[\"heads\"],\n",
    "            attn_dropout=params[\"dropout\"],\n",
    "            ff_dropout=params[\"dropout\"],\n",
    "            mlp_hidden_mults=(4, 2),\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(Path(model_dir,\"3lfsbuby_TabTransformer_default_trial_82.pth\")))\n",
    "model.eval()\n",
    "\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "for x_cat, x_cont, targets in test_loader:\n",
    "  output = model(x_cat, x_cont)\n",
    "\n",
    "  # map between zero and one, sigmoid is otherwise included in loss already\n",
    "  # https://stackoverflow.com/a/66910866/5755604\n",
    "  output = torch.sigmoid(output.squeeze())\n",
    "  y_pred.append(output.detach().cpu().numpy())\n",
    "  y_true.append(targets.detach().cpu().numpy())  # type: ignore\n",
    "\n",
    "# round prediction to nearest int\n",
    "y_pred = np.rint(np.concatenate(y_pred))\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "# map zeros back to -1\n",
    "y_pred[y_pred == 0] = -1\n",
    "y_true[y_true == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD8uTBuBN9ai",
    "outputId": "de97533b-2724-43b1-98d5-d2e724893f95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.672905426069829"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AMoxcj4YZXOs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/stud/uloak/.local/lib/python3.8/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# load default data to use unscaled version with all possible columns\n",
    "test_orig = pd.read_parquet(\n",
    "    f\"gs://thesis-bucket-option-trade-classification/data/preprocessed/test_set_extended_20.parquet\",\n",
    "    engine=\"fastparquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3evMG-KVA2eX"
   },
   "outputs": [],
   "source": [
    "# Copy unscaled columns\n",
    "X_print = test_orig.copy()\n",
    "\n",
    "# add baseline results\n",
    "X_print[\"rule\"] = \"Baseline\"\n",
    "X_print[\"buy_sell_predicted\"] = y_pred\n",
    "\n",
    "# prepare columns for printing\n",
    "X_print[\"ttm\"] = (\n",
    "    X_print[\"EXPIRATION\"].dt.to_period(\"M\")\n",
    "    - X_print[\"QUOTE_DATETIME\"].dt.to_period(\"M\")\n",
    ").apply(lambda x: x.n)\n",
    "X_print[\"year\"] = X_print[\"QUOTE_DATETIME\"].dt.year\n",
    "\n",
    "bins_tradesize = [-np.inf, 1, 3, 5, 11, np.inf]\n",
    "trade_size_labels = [\"(0,1]\", \"(1,3]\", \"(3,5]\", \"(5,11]\", \">11\"]\n",
    "X_print[\"TRADE_SIZE_binned\"] = pd.cut(\n",
    "    X_print[\"TRADE_SIZE\"], bins_tradesize, labels=trade_size_labels\n",
    ")\n",
    "\n",
    "bins_years = [2004, 2007, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
    "year_labels = [\n",
    "    \"2005-2007\",\n",
    "    \"2008-2010\",\n",
    "    \"2011\",\n",
    "    \"2012\",\n",
    "    \"2013\",\n",
    "    \"2014\",\n",
    "    \"2015\",\n",
    "    \"2016\",\n",
    "    \"2017\",\n",
    "]\n",
    "X_print[\"year_binned\"] = pd.cut(X_print[\"year\"], bins_years, labels=year_labels)\n",
    "\n",
    "bins_ttm = [-np.inf, 1, 2, 3, 6, 12, np.inf]\n",
    "ttm_labels = [\n",
    "    \"ttm <= 1 month\",\n",
    "    \"ttm (1-2] month\",\n",
    "    \"ttm (2-3] month\",\n",
    "    \"ttm (3-6] month\",\n",
    "    \"ttm (6-12] month\",\n",
    "    \"ttm > 12 month\",\n",
    "]\n",
    "X_print[\"ttm_binned\"] = pd.cut(X_print[\"ttm\"], bins_ttm, labels=ttm_labels)\n",
    "\n",
    "\n",
    "bins_myn = [-np.inf, 0.7, 0.9, 1.1, 1.3, np.inf]\n",
    "myn_labels = [\n",
    "    \"mny <=0.7\",\n",
    "    \"mny (0.7-0.9]\",\n",
    "    \"mny (0.9-1.1]\",\n",
    "    \"mny (1.1-1.3]\",\n",
    "    \"mny > 1.3\",\n",
    "]\n",
    "X_print[\"myn_binned\"] = pd.cut(X_print[\"myn\"], bins_myn, labels=myn_labels)\n",
    "\n",
    "X_print[\"issue_type_binned\"] = X_print[\"issue_type\"].replace(\n",
    "    {\"0\": 'Stock options', 'A': 'Index options', '7': 'Others',\n",
    "     'F': 'Others', '%': 'Others', ' ': 'Others'})\n",
    "\n",
    "\n",
    "# TODO: time from previous trade; same underlying or any?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "clDZ4Z95_0jj"
   },
   "outputs": [],
   "source": [
    "def check_robustness(criterion: str = \"year_binned\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check robustness of rules by calculating the accuracy for a given\n",
    "    criterion and rules.\n",
    "\n",
    "    Example:\n",
    "    rule\t\tBaseline\n",
    "    TRADE_SIZE_binned\n",
    "    (0,1]\t  0.710966\n",
    "    (1,3]\t  0.717664\n",
    "    (3,5]\t  0.715195\n",
    "    (5,11]\t0.699428\n",
    "    >11\t  \t0.688348\n",
    "\n",
    "    Args:\n",
    "        criterion (str, optional): criterion to check robustness for.\n",
    "        Defaults to \"year_binned\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with accuracy of rules. Rule in columns and\n",
    "        criterion values in rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # fill others randomly with equal weight for every class.\n",
    "    X_print[\"buy_sell_predicted\"] = X_print[\"buy_sell_predicted\"].map(\n",
    "        lambda l: l if not np.isnan(l) else np.random.choice([-1, 1])\n",
    "    )\n",
    "\n",
    "    # cuculate average over columns if multiple subsets are combined\n",
    "    results = (\n",
    "        X_print.groupby([\"rule\", criterion])[[\"buy_sell\", \"buy_sell_predicted\"]]\n",
    "        .apply(lambda x: accuracy_score(x[\"buy_sell\"], x[\"buy_sell_predicted\"]))\n",
    "        .unstack(level=0)\n",
    "        .assign(avg=lambda x: x.mean(axis=1))\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "4KRw_J0IJiFU",
    "outputId": "374fac9f-97cb-4338-bf79-b289332a0b40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rule</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_binned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0.645190</td>\n",
       "      <td>0.645190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.676413</td>\n",
       "      <td>0.676413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.674097</td>\n",
       "      <td>0.674097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rule         Baseline       avg\n",
       "year_binned                    \n",
       "2015         0.645190  0.645190\n",
       "2016         0.676413  0.676413\n",
       "2017         0.674097  0.674097"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_robustness(\"year_binned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "fCey7G_tE6zt",
    "outputId": "95762483-bc9e-406c-9024-ee7d8b968e68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rule</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OPTION_TYPE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.675753</td>\n",
       "      <td>0.675753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.669654</td>\n",
       "      <td>0.669654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rule         Baseline       avg\n",
       "OPTION_TYPE                    \n",
       "C            0.675753  0.675753\n",
       "P            0.669654  0.669654"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_robustness(\"OPTION_TYPE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "AGbchxWfds_Y",
    "outputId": "6b5646cb-65e4-4e59-979a-febc33b9be67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rule</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_type_binned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Index options</th>\n",
       "      <td>0.524634</td>\n",
       "      <td>0.524634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.702792</td>\n",
       "      <td>0.702792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stock options</th>\n",
       "      <td>0.663093</td>\n",
       "      <td>0.663093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rule               Baseline       avg\n",
       "issue_type_binned                    \n",
       "Index options      0.524634  0.524634\n",
       "Others             0.702792  0.702792\n",
       "Stock options      0.663093  0.663093"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_robustness(\"issue_type_binned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "Zpg1yY2MEGFa",
    "outputId": "63d6b69b-4238-4831-a6a1-8cfc9f4ec39f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rule</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRADE_SIZE_binned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0,1]</th>\n",
       "      <td>0.683563</td>\n",
       "      <td>0.683563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1,3]</th>\n",
       "      <td>0.690546</td>\n",
       "      <td>0.690546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3,5]</th>\n",
       "      <td>0.688925</td>\n",
       "      <td>0.688925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5,11]</th>\n",
       "      <td>0.653614</td>\n",
       "      <td>0.653614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;11</th>\n",
       "      <td>0.634339</td>\n",
       "      <td>0.634339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rule               Baseline       avg\n",
       "TRADE_SIZE_binned                    \n",
       "(0,1]              0.683563  0.683563\n",
       "(1,3]              0.690546  0.690546\n",
       "(3,5]              0.688925  0.688925\n",
       "(5,11]             0.653614  0.653614\n",
       ">11                0.634339  0.634339"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_robustness(\"TRADE_SIZE_binned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "8624JR8wEN4D",
    "outputId": "e7037e24-d07d-485f-a393-d513cbc07143"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rule</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttm_binned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ttm &lt;= 1 month</th>\n",
       "      <td>0.669447</td>\n",
       "      <td>0.669447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttm (1-2] month</th>\n",
       "      <td>0.691056</td>\n",
       "      <td>0.691056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttm (2-3] month</th>\n",
       "      <td>0.681317</td>\n",
       "      <td>0.681317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttm (3-6] month</th>\n",
       "      <td>0.675536</td>\n",
       "      <td>0.675536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttm (6-12] month</th>\n",
       "      <td>0.675719</td>\n",
       "      <td>0.675719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttm &gt; 12 month</th>\n",
       "      <td>0.665156</td>\n",
       "      <td>0.665156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rule              Baseline       avg\n",
       "ttm_binned                          \n",
       "ttm <= 1 month    0.669447  0.669447\n",
       "ttm (1-2] month   0.691056  0.691056\n",
       "ttm (2-3] month   0.681317  0.681317\n",
       "ttm (3-6] month   0.675536  0.675536\n",
       "ttm (6-12] month  0.675719  0.675719\n",
       "ttm > 12 month    0.665156  0.665156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_robustness(\"ttm_binned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "Z7Vn96hibEvM",
    "outputId": "4034575f-4c21-4f91-e621-817db6f3b332"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rule</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myn_binned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mny &lt;=0.7</th>\n",
       "      <td>0.665539</td>\n",
       "      <td>0.665539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mny (0.7-0.9]</th>\n",
       "      <td>0.665949</td>\n",
       "      <td>0.665949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mny (0.9-1.1]</th>\n",
       "      <td>0.683752</td>\n",
       "      <td>0.683752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mny (1.1-1.3]</th>\n",
       "      <td>0.626057</td>\n",
       "      <td>0.626057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mny &gt; 1.3</th>\n",
       "      <td>0.603918</td>\n",
       "      <td>0.603918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rule           Baseline       avg\n",
       "myn_binned                       \n",
       "mny <=0.7      0.665539  0.665539\n",
       "mny (0.7-0.9]  0.665949  0.665949\n",
       "mny (0.9-1.1]  0.683752  0.683752\n",
       "mny (1.1-1.3]  0.626057  0.626057\n",
       "mny > 1.3      0.603918  0.603918"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_robustness(\"myn_binned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabModel:\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        self._model = model\n",
    "        \n",
    "    \n",
    "    def predict(self,X:np.ndarray):\n",
    "        \n",
    "\n",
    "        \n",
    "        # TODO: infer correct cat columns\n",
    "        X = pd.DataFrame(X)\n",
    "        y = pd.Series(range(len(X)))\n",
    "        test_data = TabDataset(X, y, [], [])\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")        \n",
    "\n",
    "        # differentiate between continous features only and mixed.\n",
    "        sample_loader = TabDataLoader(\n",
    "        test_data.x_cat, test_data.x_cont, batch_size=params['batch_size'], device=device\n",
    "        )\n",
    "\n",
    "        y_pred= []\n",
    "\n",
    "        for x_cat, x_cont in sample_loader:\n",
    "          output = self._model(x_cat, x_cont)\n",
    "\n",
    "          # map between zero and one, sigmoid is otherwise included in loss already\n",
    "          # https://stackoverflow.com/a/66910866/5755604\n",
    "          output = torch.sigmoid(output.squeeze())\n",
    "          y_pred.append(output.detach().cpu().numpy())\n",
    "\n",
    "        # get probabilities\n",
    "        return y_pred[0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probilistic_predictions(model, X):\n",
    "    #X_sample = X_test.sample(n=50)\n",
    "\n",
    "    tabmodel = TabModel(model)\n",
    "    return tabmodel.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5f0e7f2c2146898a0572c07e3a0bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.44628912,  0.01191375,  0.        , ..., -0.02196888,\n",
       "          0.        , -0.00792286],\n",
       "        [ 0.22639358, -0.0367014 ,  0.08083702, ..., -0.03510106,\n",
       "          0.02744904,  0.0633191 ],\n",
       "        [-0.25610831,  0.        ,  0.        , ...,  0.02919581,\n",
       "          0.        , -0.01127516],\n",
       "        ...,\n",
       "        [ 0.25891763,  0.02913972,  0.02278852, ...,  0.        ,\n",
       "         -0.01200252, -0.03767283],\n",
       "        [-0.01431246,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.24529249,  0.        , -0.01396425, ..., -0.02078385,\n",
       "         -0.01523253, -0.01563378]])]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: get_probilistic_predictions(model, x)\n",
    "X_sample = X_test.sample(n=50)\n",
    "\n",
    "kernelshap = shap.KernelExplainer(f, shap.sample(X_test, 52));\n",
    "shap_values = kernelshap.shap_values(X_sample, nsamples=256);  # nsamples = no. of feature coalitions\n",
    "#print(shap_values.shape, shap_values.dtype)\n",
    "\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = pd.DataFrame(shap_values[0], columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attrs_abs = np.abs(shap_values)\n",
    "attrs_abs -= np.min(attrs_abs)\n",
    "attrs_abs /= np.max(attrs_abs)\n",
    "plt.ioff()\n",
    "plt.matshow(attrs_abs)\n",
    "plt.xticks(np.arange(len(X_test.columns)), X_test.columns, rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8ea8b642289b706932f10b33ee389827410dbaef0ce2c5bf73615e8d3267d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
