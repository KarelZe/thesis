\documentclass[oneside,a4paper,12pt]{article} % Specifies the page format and font size.


% -------------------------------------- Integration of packages --------------------------------------
% Literature and language
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[style=apa,backend=biber]{biblatex}
\DeclareLanguageMapping{british}{british-apa}
\addbibresource{Content/bibliography.bib}

% Format and layout
\usepackage[left=3cm,right=3cm,bottom=3cm]{geometry} % Specifies left and right side margins.
\usepackage{setspace} % Package that enables modifying the line spacing.
\setstretch{1.3} % Sets a line spacing of 1.3.
\parindent0pt % Sets the left indent at a new paragraph.
 \parskip10pt % Sets the space between two paragraphs.
\usepackage{footmisc} % Implements a range of footnote options.
\renewcommand{\footnotelayout}{\setstretch{1}} % Sets a line spacing of 1 for the footnotes.
\pagestyle{headings} % Creates a header using the page number and the heading of the current section.
\usepackage{eurosym} % Usage of €

\usepackage{pdflscape}

% Colors
\usepackage{xcolor} % Enables the definition of more colors.

\definecolor{viz-red}{HTML}{FF2C00}
\definecolor{viz-gray}{HTML}{D6DCE5}

\usepackage{colorprofiles} % load colour profiles for pdf/a standard

% https://tex.stackexchange.com/questions/188533/how-to-draw-squares-circles-and-triangles
\usepackage{tikz}
\usetikzlibrary{shapes}

\newcommand{\mysquare}[1]{\tikz{\node[draw=#1,fill=#1,rectangle,minimum
width=0.2cm,minimum height=0.2cm,inner sep=0pt] at (0,0) {};}}

\newcommand{\mycircle}[1]{\tikz{\node[draw=#1,fill=#1,circle,minimum
width=0.2cm,minimum height=0.2cm,inner sep=0pt] at (0,0) {};}}

\newcommand{\mytriangle}[1]{\tikz{\node[draw=#1,fill=#1,isosceles
triangle,isosceles triangle stretches,shape border rotate=90,minimum
width=0.2cm,minimum height=0.2cm,inner sep=0pt] at (0,0) {};}}

\newcommand{\bestcircle}{\tikz{\node[circle,draw=darkgray, fill=black, line width=2pt, minimum width=0.2cm,minimum height=0.2cm, inner sep=0pt, draw opacity=.2] at (0,0){};}}

% Tables and Graphs
\usepackage{booktabs} % Improves the design of the tables
\usepackage{longtable} % Allows tables to be longer than one page.
\usepackage{threeparttable} % footnotes in tables
\usepackage{multirow,multicol} % With this package it is now possible to combine columns and rows within tables.
\usepackage{graphicx} % Allows to implement graphics.
% \usepackage[font={sf, small}]{subfig} % Enables graphs consisting of several figures.
\usepackage[font={small}]{subfig}
% \usepackage[font={sf, small}]{floatrow} % enables to format tables
\usepackage[font={small}]{floatrow}

\graphicspath{{./Graphs/}} % Tells LATEX that the images are kept in a folder named images under the directory of the main document.
\usepackage[hypcap=false,font={small}]{caption} % Provides many ways to customise captions.
%\usepackage[hypcap=false,font={sf, small}]{caption} % Provides many ways to customise captions.

\usepackage{siunitx} % Enables the use of SI units e. g., proper handling of percentage

% manually define opening bracket which is otherwise parsed by sunitx
% https://tex.stackexchange.com/q/450026/169093
\usepackage{etoolbox} 
\newrobustcmd{\parl}{(}
\newrobustcmd{\parr}{)}

\newcommand*{\tabindent}{ \hspace{2mm}} % Indentation for tables

\sisetup{round-mode=places,round-precision=2, group-separator={,},output-decimal-marker={.}, round-pad = false, input-symbols = {(-)}, % separate-uncertainty=false,
group-minimum-digits=4, % 1,000 instead of 1000
% table-space-text-pre={(},
% table-align-text-pre=false,
% table-space-text-post={$^{***}$},
table-align-text-post=false,
detect-weight=true,
detect-inline-weight=math,
retain-explicit-plus=true,
% round-integer-to-decimal=false,
round-precision=2,
table-format = 1.2, 
} % round to 2 decimal places

\usepackage[super]{nth} % 1st, 2nd etc.
\usepackage{import} % path for inkscape graphics
% Mathematics
 \usepackage{amscd,amsfonts,amsmath,amssymb,amsthm,amscd,bbm} % Extends the maths set.

% PDF/a standard
\usepackage[a-2b,mathxmp]{pdfx}

% Depth
\setcounter{secnumdepth}{3}


% prevent footnotes from being split
% https://texfaq.org/FAQ-splitfoot
\interfootnotelinepenalty=10000


% --------------------------------- Information on thesis ---------------------------------
% Please fill in this information once at the beginning. This way, gaps will be filled in automatically in the following.
\newcommand{\name}{Markus Bilz} % Enter your name.
\newcommand{\dateofthesis}{30 May 2023} % Enter the submission date of your thesis.
\newcommand{\titleofthesis}{Forget About the Rules: Improving Option Trade Classification With Machine Learning} % Enter the title of your thesis.
\newcommand{\streetadress}{Mathystr.~14-16 // XI-11} % Enter your street address.
\newcommand{\postalcode}{76133} % Enter your postal code.
\newcommand{\city}{Karlsruhe} % Enter your city/town.
\newcommand{\email}{markus.bilz@student.kit.edu} % Enter your email address.
\newcommand{\typeofthesis}{Master's Thesis} % specify the type of thesis: Seminar Thesis, Bachelor Thesis, Master Thesis

%--------------------------------- Information in xmpdata ---------------------------------
% definition from macro doesn't seem to work.
\begin{filecontents*}[overwrite]{\jobname.xmpdata}
\Title{Forget About the Rules: Improving Option Trade Classification With Machine Learning}
\Author{Markus Bilz}
\Language{en-GB}
\Keywords{trade-classification\sep machine-learning\sep transformer}
\end{filecontents*}

% --------------------------------- Information in description ---------------------------------
\pdfinfo {
 /Title (\titleofthesis)
 /Author (\name)
 /Subject (Trade Classification using machine learning)
 /Keywords (trade-classification machine-learning transformer) 
}

% --------------------------------- Definition of hyperlinks ---------------------------------
% Hyperreferences
\usepackage{hyperref}
\definecolor{darkblue}{rgb}{0,0,.5}
\hypersetup{
 pdfstartview={FitH},
colorlinks=true,
linkcolor=black,
citecolor=darkblue,
urlcolor=black,
bookmarksopen
}



% --------------------------------- Algorithm ---------------------------------
% load before cref https://tex.stackexchange.com/a/393202/169093
\usepackage[linesnumbered, ruled]{algorithm2e}

% change algorithm font size
\SetAlFnt{\normalsize}

% change algorithm caption style
\newcommand{\xAlCapSty}[1]{\small\sffamily\bfseries#1}
\SetAlCapSty{xAlCapSty}

\SetAlCapSkip{1em}
\SetKwInput{KwParam}{Parameters}
\SetKwInput{KwHyper}{Hyperparameters}

% comment style (algorithms)
\newcommand{\xCommentSty}[1]{\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{xCommentSty}

% change line number style
\newcommand\mynlfont[1]{\sffamily\textcolor{gray}{#1}}
\SetNlSty{mynlfont}{}{}

 % add the line numbers
\LinesNumbered

% comments right justified
\SetSideCommentRight

% don't print semicolon
\DontPrintSemicolon

% ruled algorithm
\RestyleAlgo{algoruled}



\usepackage[capitalise, noabbrev]{cleveref} % Enables the use of \cref{} to refer to figures, etc. with Fig.
\creflabelformat{equation}{#2#1#3} % omit round brackets
% https://tex.stackexchange.com/a/121055/169093
\Crefname{appsec}{appendix}{appendices}


\usepackage[symbols,acronyms,automake,savewrites=true,toc,section=section,nopostdot]{glossaries}

\glsaddkey
 {unit}
 {}
 {\glsentryunit}
 {\Glsentryunit}
 {\glsunit}
 {\Glsunit}
 {\GLSunit}

\makeglossaries

% https://tex.stackexchange.com/questions/98494/glossaries-dont-print-single-occurences/230664#230664
\glsenableentrycount % if only used once, dont abbreviate

\newacronym{ANN}{ANN}{artificial neural network}
\newacronym{AUC}{AUC}{area under the curve}
\newacronym{BVC}{BVC}{bulk volume classification}
\newacronym{CRSP}{CRSP}{Center for Research in Securities Prices}
\newacronym{CBOE}{CBOE}{Chicago Board Options Exchange}
\newacronym{CLNV}{CLNV}{Chakrabarty-Li-Nguyen-Van-Ness}
\newacronym{EMO}{EMO}{Ellis-Michaely-O’Hara}
\newacronym{FFN}{FFN}{feed-forward network}
\newacronym{ISE}{ISE}{International Securities Exchange}
\newacronym{GBM}{GBM}{gradient boosting machine}
\newacronym{GSUH}{GSUH}{Grauer-Schuster-Uhrig-Homburg}
\newacronym{LR}{LR}{Lee-Ready}
\newacronym{MCC}{MCC}{Matthews correlation coefficient}
\newacronym{MLP}{MLP}{multi-layer perceptron}
\newacronym[first={national best bid and offer}]{NBBO}{NBBO}{national best bid and offer}
\newacronym{NYSE}{NYSE}{New York Stock Exchange}
\newacronym{NASDAQ}{NASDAQ}{National Association of Securities Dealers Automated Quotations}
\newacronym{RMSE}{RMSE}{root mean squared error}
\newacronym{RF}{RF}{random forest}
\newacronym{ReLU}{ReLU}{Rectified Linear Units}
\newacronym{SSE}{SSE}{sum of squared errors}
\newacronym{SHAP}{SHAP}{SHapley Additive exPlanations}
\newacronym{TRACE}{TRACE}{Trade Reporting and Compliance Engine}

% see https://tex.stackexchange.com/questions/347586/symbols-as-glossary-entry-indicators
%Symbols
\newglossaryentry{V}{type=symbols,name={\ensuremath{\cong[N_\t{V}]}},sort=V, description={vocabulary}}
\newglossaryentry{A}{type=symbols,name={\ensuremath{A}},sort=A, description={sequence of ask prices}, unit={\ensuremath{=\langle A_{1},A_{2},\dots,A_{T}\rangle}}}
\newglossaryentry{B}{type=symbols,name={\ensuremath{B}},sort=B, description={sequence of bid prices}, unit={\ensuremath{=\langle B_{1},B_{2},\dots,B_{T}\rangle}}}
\newglossaryentry{A-tilde}{type=symbols,name={\ensuremath{\tilde{A}}},sort=A-tilde, description={sequence of ask sizes}, unit={\ensuremath{=\langle \tilde{A}_{1},\tilde{A}_{2},\dots,\tilde{A}_{T}\rangle}}}
\newglossaryentry{B-tilde}{type=symbols,name={\ensuremath{\tilde{B}}},sort=B-tilde, description={sequence of bid sizes}, unit={\ensuremath{=\langle \tilde{B}_{1},\tilde{B}_{2},\dots,\tilde{B}_{T}\rangle}}}
\newglossaryentry{ell}{type=symbols,name={\ensuremath{d_e}}, sort=ell, description={length of token sequence}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{e}{type=symbols,name={\ensuremath{\mathbf{e}}}, sort=e, description={vector representation / embedding of a token}, unit={\ensuremath{\in \mathbb{R}^{d_e}}}}
\newglossaryentry{d}{type=symbols,name={\ensuremath{d}}, sort=d, description={dimension of a vector}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{ellmax}{type=symbols,name={\ensuremath{\ell_{\max}}},sort=ell-max, description={maximum sequence length}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{h}{type=symbols,name={\ensuremath{h}},sort=h, description={index of attention heads}}
\newglossaryentry{H}{type=symbols,name={\ensuremath{H}},sort=H, description={number of attention heads}}
\newglossaryentry{P}{type=symbols,name={\ensuremath{P}},sort=p, description={sequence of trade prices}, unit={\ensuremath{=\langle P_{1},P_{2},\dots,P_{T}\rangle}}}
\newglossaryentry{P-tilde}{type=symbols,name={\ensuremath{\tilde{P}}},sort=P-tilde, description={sequence of trade sizes}, unit={\ensuremath{=\langle \tilde{P}_{1},\tilde{P}_{2},\dots,\tilde{P}_{T}\rangle}}}
\newglossaryentry{L}{type=symbols,name={\ensuremath{L}},sort=L, description={number of layers in the encoder / decoder}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{M}{type=symbols,name={\ensuremath{M}},sort=m, description={sequence of spread midpoints}, unit={\ensuremath{\langle m_{1},m_{2},\dots,m_{T}\rangle}}}
\newglossaryentry{t}{type=symbols,name={\ensuremath{t}},sort=t, description={index of token in sequence},unit={\ensuremath{\in\left[\ell_{\max }\right]}}}
\newglossaryentry{x}{type=symbols,name={\ensuremath{x}},sort=x, description={primary token sequence}, unit={\ensuremath{\equiv x[1] x[2] \ldots x[\ell] \in V^{\ell}}}}
\newglossaryentry{y}{type=symbols,name={\ensuremath{y}},sort=y, description={trade initiator / target}, unit={\ensuremath{\in \mathcal{Y}}}}
\newglossaryentry{X}{type=symbols,name={\ensuremath{\mathbf{X}}},sort=X, description={encoded primary token sequence }, unit={\ensuremath{\in \mathbb{R}^{d_e \times \ell_x}}}}

\newglossaryentry{W-e}{type=symbols,name={\ensuremath{\mathbf{W}_e}},sort=W-e, description={token embedding matrix}, unit={\ensuremath{\in \mathbb{R} d_{\mathrm{e}} \times N_{\mathrm{V}}}}}
\newglossaryentry{W-p}{type=symbols,name={\ensuremath{\mathbf{W}_p}},sort=W-p, description={positional embedding matrix}, unit={\ensuremath{\in \mathbb{R} d_{\mathrm{e}} \times \ell_{\max}}}}


\newglossarystyle{dotglos}{%
    \setglossarystyle{list}%
    \renewcommand*{\glossentry}[2]{%
        \item[\glsentryitem{##1}\glstarget{##1}{\glossentryname{##1}}]
        \ifglshassymbol{##1}{\glossentrysymbol{##1}}{}%
		\glsentryunit{##1}\quad\parindent20mm%
		\glossentrydesc{##1}%
        \unskip\leaders\hbox to 2.9mm{\hss.}\hfill##2}%
    \renewcommand*{\glsgroupskip}{}%
}

% \renewcommand*{\glossentry}[2]{%
% \item[]\makebox[\glslistdottedwidth][l]{%
% \glsentryitem{##1}%
% \glstarget{##1}{\glossentryname{##1}}%
% \unskip\leaders\hbox to 2.9mm{\hss.}\hfill\strut}\glossentrydesc{##1}}%

% \renewcommand{\glossentry}[2]{%
% \glsentryitem{##1}\glstarget{##1}{\glossentryname{##1}} & $\acem{##1}$ & \glossentrydesc{##1}\tabularnewline
% }%

% 	{\bf Symbol }      \> {\bf Type}     \> {\bf Explanation}                                      \\[0ex]
% 	$[N]$              \> $:=\{1,...,N\}$  \> set of integers $1,2,...,N-1,N$                  \\[0ex]
% 	$i,j$              \> $âˆˆâ„•$           \> generic integer indices       \\[0ex]  
% 	$V$                \> $\cong[N_\t{V}]$      \> vocabulary                               \\[0ex]
% 	$N_\t{V}$          \> $âˆˆ\mathbb{N}$  \> vocabulary size                                 \\[0ex]
% 	$V^*$              \> $=\bigcup_{\ell=0}^âˆž V^{\ell}$ \> set of token sequences; elements include e.g.\ sentences or documents  \\[0ex]
% 	$\ell_{\max}$      \> $âˆˆ\mathbb{N}$     \> maximum sequence length                          \\[0ex]
% 	$\ell$             \> $âˆˆ[\ell_{\max}]$      \> length of token sequence                           \\[0ex]  
% 	$t$                \> $âˆˆ[\ell]$           \> index of token in a sequence                       \\[0ex]
% 	$d_{...}$          \> $âˆˆâ„•$           \> dimension of various vectors                       \\[0ex]  
% 	$\v{x}$            \> $â‰¡x[1:\ell]$   \> $â‰¡x[1]x[2]...x[\ell]âˆˆV^\ell$ ~~ primary token sequence  \\[0ex]
% 	$\v{z}$            \> $â‰¡z[1:\ell]$   \> $â‰¡z[1]z[2]...z[\ell]âˆˆV^\ell$ ~~ context token sequence  \\[0ex]
% 	$M[i,j]$           \> $âˆˆâ„$            \> entry $M_{ij}$ of matrix $Mâˆˆâ„^{dÃ—d'}$        \\[0ex]
% 	$M[i,:]\!â‰¡\!M[i]$  \> $âˆˆâ„^{d'}$      \> $i$-th row of matrix $Mâˆˆâ„^{dÃ—d'}$          \\[0ex]
% 	$M[:,j]$           \> $âˆˆâ„^d$         \> $j$-th column of matrix $Mâˆˆâ„^{dÃ—d'}$        \\[0ex]
% 	$\v{e}$            \> $âˆˆâ„^{d_\t{e}}$    \> vector representation / embedding of a token            \\[0ex]
% 	$\m{X}$              \> $âˆˆâ„^{d_\t{e} Ã— \ell_x}$    \> encoded primary token sequence                             \\[0ex]
% 	$\m{Z}$              \> $âˆˆâ„^{d_\t{e} Ã— \ell_z}$     \> encoded context token sequence                             \\[0ex]  
% 	$\t{Mask}$         \> $âˆˆâ„^{\ell_z\times \ell_x}$   \> masking matrix, it determines the attention context for each token \\[0ex]
% 	$L, L_\t{enc}, L_\t{dec}$ \> $âˆˆâ„•$       \> number of network (encoder, decoder) layers                   \\[0ex]  
% 	$l$                \> $âˆˆ[L]$           \> index of network layer                             \\[0ex]
% 	$H$                \> $âˆˆ\mathbb{N}$     \> number of attention heads                        \\[0ex]
% 	$h$                \> $âˆˆ[H]$         \> index of attention head                            \\[0ex]
% 	$N_\t{data}$       \> $âˆˆâ„•$           \> (i.i.d.) sample size                               \\[0ex]  
% 	$n$                \> $âˆˆ[N_\t{data}]$     \> index of sample sequence                         \\[0ex]
% 	$Î·$                \> $âˆˆ(0,âˆž)$           \> learning rate                                      \\[0ex]
% 	$Ï„$                \> $âˆˆ(0,âˆž)$           \> temperature; it controls the diversity-plausibility trade-off at inference  \\[0ex]
% 	$\m{W_e}$     \> $âˆˆâ„^{d_\t{e}Ã—N_\t{V}}$ \> token embedding matrix       \\[0ex]  
% 	$\m{W_p}$     \> $âˆˆâ„^{d_\t{e}Ã—\ell_{\max}}$ \> positional embedding matrix       \\[0ex]  
% 	$\m{W_u}$     \> $âˆˆâ„^{N_\t{V}Ã—d_\t{e}}$ \> unembedding matrix       \\[0ex]  
% 	$\m{W_q}$     \> $âˆˆâ„^{d_\t{attn}Ã—d_\t{x}}$ \> query weight matrix \\[0ex]
% 	$\v{b_q}$     \> $âˆˆâ„^{d_\t{attn}}$ \> query bias \\[0ex]
% 	$\m{W_k}$     \> $âˆˆâ„^{d_\t{attn}Ã—d_\t{z}}$ \> key weight matrix \\[0ex]
% 	$\v{b_k}$     \> $âˆˆâ„^{d_\t{attn}}$ \> key bias \\[0ex]
% 	$\m{W_v}$     \> $âˆˆâ„^{d_\t{out}Ã—d_\t{z}}$ \> value weight matrix \\[0ex]
% 	$\v{b_v}$     \> $âˆˆâ„^{d_\t{out}}$ \> value bias \\[0ex]
% 	$\bmcWqkv$     \>  \> collection of above parameters of a single-head attention layer \\[0ex]  
% 	$\m{W_o}$     \> $âˆˆâ„^{d_\t{out}Ã—Hd_\t{mid}}$ \> output weight matrix \\[0ex]
% 	$\v{b_o}$     \> $âˆˆâ„^{d_\t{out}}$ \> output bias \\[0ex]
% 	$\bmcW$       \>  \> collection of above parameters of a multi-head attention layer, see \cref{eq:Wall} \\[0ex]
% 	$\m{W_\t{mlp}}$    \> $âˆˆâ„^{d_1Ã—d_2}$ \> weight matrix corresponding to an MLP layer in a Transformer \\[0ex]
% 	$\v{b_\t{mlp}}$    \> $âˆˆâ„^{d_1}$ \> bias corresponding to an MLP layer in a Transformer \\[0ex]  
% 	$\vga$            \> $âˆˆâ„^{d_\t{e}}$ \> layer-norm learnable scale parameter \\[0ex]
% 	$\vbe$                \> $âˆˆâ„^{d_\t{e}}$  \> layer-norm learnable offset parameter \\[0ex]
% 	$\vth,\hat{\vth}$        \> $âˆˆâ„^d$        \> collection of all learnable / learned Transformer parameters \\[0ex]

%Glossary
\newglossaryentry{activation-function}{name={activation function},plural={activation functions}, description={An activation function is a function, that breaks up the linearity of the neural network. It determines if neurons are activated or not. Common variants include \gls{ReLU} and Softmax.}}
\newglossaryentry{embedding}{name={embedding},plural={embeddings},description={Numerical vector representation of the input, e.g. a word, category, or scalar.}}
\newglossaryentry{feed-forward-network}{name={feed-forward network},plural={feed-forward networks},description={Neural networks without recursion. Well-known variants are \glspl{MLP}.}}
\newglossaryentry{overfitting}{name={overfitting},plural={overfitting},description={Creating a model that fits the training data closely, but does not generalize on unseen data.}}
\newglossaryentry{token}{name={token},plural={tokens},description={Item in a vocabulary. For textual data, tokens can be an individual character, sub-word, a word. 
For tabular data, a token corresponds to a column in the data set.}}
\newglossaryentry{exploding-gradient}{name={exploding gradient},plural={exploding gradients},description={Exploding gradients is a problem encountered in training deep neural networks with backpropagation. Error gradients can accumulate, and result in very large parameter updates and unstable training of the network. The opposite is the vanishing gradient problem, whereby gradients become successively smaller during backpropagation, resulting in no or small parameter updates of the network. In both cases, the network does not converge.}}

% ----------------------------------- Start of document -----------------------------------
\begin{document}
\setcounter{page}{2} % Cover pages and title page are not numbered. Start numbering from page 2.

% Title page
\input{Content/Titlepage_Thesis} % Exclude title page (with %) that is not being used.

% Table of contents
\setcounter{page}{1}\renewcommand{\thepage}{\roman{page}} % Sets the numbering to roman small.
\newpage
\tableofcontents

% List of Figures (comment out if there are no figures in the thesis)
\newpage
\listoffigures % Inserts the list of figures.
\addcontentsline{toc}{section}{List of Figures} % Adds the list of figures to the table of contents.

% List of tables (comment out if there are no tables in the thesis)
\newpage
\listoftables % Inserts the list of figures.
\addcontentsline{toc}{section}{List of Tables} % Adds the list of tables to the table of contents.

\newpage
\printglossary[title={List of Symbols},type=symbols,style=dotglos]

% Main text section
\newpage
\setcounter{page}{1}\renewcommand{\thepage}{\arabic{page}} % Sets the numbering to Arabic.
\include{Content/main}

% Bibliography
\newpage
\printbibliography

% Glossary
\newpage
\printglossary[type=main,title=Glossary,style=altlist]


% Appendix
\newpage
\appendix % Enumerates appendix with letters.
\include{Content/Appendix}

\end{document}