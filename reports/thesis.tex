\documentclass[oneside,a4paper,12pt]{article} % Specifies the page format and font size.


% -------------------------------------- Integration of packages --------------------------------------
% Literature and language
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[style=apa,backend=biber]{biblatex}
\DeclareLanguageMapping{british}{british-apa}
\addbibresource{Content/bibliography.bib}

% Format and layout
\usepackage[left=3cm,right=3cm,bottom=3cm]{geometry} % Specifies left and right side margins.
\usepackage{setspace} % Package that enables modifing the line spacing.
\setstretch{1.3} % Sets a line spacing of 1.3.
\parindent0pt % Sets the left indent at new paragraph.
 \parskip10pt % Sets the space between two paragraphs.
\usepackage{footmisc} % Implements a range of footnotes options.
\renewcommand{\footnotelayout}{\setstretch{1}} % Sets a line spacing of 1 for the footnotes.
\pagestyle{headings} % Creates a header using the page number and the heading of the current section.
\usepackage{eurosym} % Usage of €

\usepackage{pdflscape}

% Colors
\usepackage{xcolor} % Enables the definition of more colors.
\usepackage{colorprofiles} % load colour profiles for pdf/a standard

% Tables and Graphs
\usepackage{booktabs} % Improves the design of the tables
\usepackage{longtable} % Allows tables to be longer than one page.
\usepackage{multirow,multicol} % With this package it is now possible to combine columns and rows within tables.
\usepackage{graphicx} % Allows to implement graphics.
\usepackage[font={sf, small}]{subfig} % Enables graphs consisting of several figures.
\usepackage[font={sf, small}]{floatrow} % enables to format tables

\graphicspath{{./Graphs/}} % Tells LATEX that the images are kept in a folder named images under the directory of the main document.
\usepackage[hypcap=false,font={sf, small}]{caption} % Provides many ways to customise captions.
\usepackage{siunitx} % Enables the use of SI units e. g., proper handling of percent
\usepackage[super]{nth} % 1st, 2nd etc.
\usepackage{import} % path for inkscape graphics
% Mathematics
 \usepackage{amscd,amsfonts,amsmath,amssymb,amsthm,amscd,bbm} % Extends the maths set.

% PDF/a standard
\usepackage[a-2b,mathxmp]{pdfx}

% Depth
\setcounter{secnumdepth}{3}

% --------------------------------- Information on thesis ---------------------------------
% Please fill in this information once at the beginning. This way, gaps will be filled in automatically in the following.
\newcommand{\name}{Markus Bilz} % Enter your name.
\newcommand{\dateofthesis}{15 April 2023} % Enter the submission date of your thesis.
\newcommand{\titleofthesis}{Forget About the Rules: Improving Option Trade Classification With Machine Learning} % Enter the title of your thesis.
\newcommand{\streetadress}{Mathystr.~14-16 // XI-11} % Enter your street adress.
\newcommand{\postalcode}{76133} % Enter your postal code.
\newcommand{\city}{Karlsruhe} % Enter your city/town.
\newcommand{\email}{markus.bilz@student.kit.edu} % Enter your email adress.
\newcommand{\typeofthesis}{Master's Thesis} % specify the type of thesis: Seminar Thesis, Bachelor Thesis, Master Thesis

%--------------------------------- Information in xmpdata ---------------------------------
% definition from macro doesn't seem to work.
\begin{filecontents*}[overwrite]{\jobname.xmpdata}
\Title{Forget About the Rules: Improving Option Trade Classification With Machine Learning}
\Author{Markus Bilz}
\Language{en-GB}
\Keywords{trade-classification\sep machine-learning\sep transformer}
\end{filecontents*}

% --------------------------------- Information in desription ---------------------------------
\pdfinfo {
 /Title (\titleofthesis)
 /Author (\name)
 /Subject (Trade Classification using machine learning)
 /Keywords (trade-classification machine-learning transformer) 
}

% --------------------------------- Definition of hyperlinks ---------------------------------
% Hyperreferences
\usepackage{hyperref}
\definecolor{darkblue}{rgb}{0,0,.5}
\hypersetup{
 pdfstartview={FitH},
colorlinks=true,
linkcolor=black,
citecolor=darkblue,
urlcolor=black,
bookmarksopen
}
\usepackage[capitalise, noabbrev]{cleveref} % Enables the use of \cref{} to refer to figures, etc. with Fig.
\creflabelformat{equation}{#2#1#3} % omit round brackets

\usepackage[symbols,acronyms,automake,savewrites=true,toc,section=section,nopostdot]{glossaries}

\glsaddkey
 {unit}
 {}
 {\glsentryunit}
 {\Glsentryunit}
 {\glsunit}
 {\Glsunit}
 {\GLSunit}

\makeglossaries

% https://tex.stackexchange.com/questions/98494/glossaries-dont-print-single-occurences/230664#230664
\glsenableentrycount % if only used once, dont abbrevate

\newacronym{ANN}{ANN}{artificial neural network}
\newacronym{AUC}{AUC}{area under the curve}
\newacronym{BVC}{BVC}{bulk volume classification}
\newacronym{CRSP}{CRSP}{Center for Research in Securities Prices}
\newacronym{CBOE}{CBOE}{Chicago Board Options Exchange}
\newacronym{CLNV}{CLNV}{Chakrabarty-Li-Nguyen-Van-Ness}
\newacronym{EMO}{EMO}{Ellis-Michaely-O’Hara}
\newacronym{FFN}{FFN}{feed-forward network}
\newacronym{ISE}{ISE}{International Securities Exchange}
\newacronym{GBM}{GBM}{gradient boosting machine}
\newacronym{LR}{LR}{Lee-Ready}
\newacronym{MLP}{MLP}{multi-layer perceptron}
\newacronym[first={national best bid and offer}]{NBBO}{NBBO}{national best bid and offer}
\newacronym{NYSE}{NYSE}{New York Stock Exchange}
\newacronym{NASDAQ}{NASDAQ}{National Association of Securities Dealers Automated Quotations}
\newacronym{RMSE}{RMSE}{root mean squared error}
\newacronym{RF}{RF}{random forest}
\newacronym{ReLU}{ReLU}{Rectified Linear Units}
\newacronym{SSE}{SSE}{sum of squared errors}
\newacronym{SHAP}{SHAP}{SHapley Additive exPlanations}

% see https://tex.stackexchange.com/questions/347586/symbols-as-glossary-entry-indicators
%Symbols
\newglossaryentry{V}{type=symbols,name={\ensuremath{\cong[N_\t{V}]}},sort=V, description={vocabulary}}
\newglossaryentry{ell}{type=symbols,name={\ensuremath{d_e}}, sort=ell, description={length of token sequence}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{e}{type=symbols,name={\ensuremath{e}}, sort=e, description={vector representation / embedding of a token}, unit={\ensuremath{\in \mathbb{R}^{d_e}}}}
\newglossaryentry{d}{type=symbols,name={\ensuremath{d}}, sort=d, description={dimension of a vector}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{ellmax}{type=symbols,name={\ensuremath{\ell_{\max}}},sort=ell-max, description={maximum sequence length}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{h}{type=symbols,name={\ensuremath{h}},sort=h, description={index of attention heads}}
\newglossaryentry{H}{type=symbols,name={\ensuremath{H}},sort=H, description={number of attention heads}}
\newglossaryentry{L}{type=symbols,name={\ensuremath{L}},sort=L, description={number of layers in the encoder / decoder}, unit={\ensuremath{\in \mathbb{N}}}}
\newglossaryentry{t}{type=symbols,name={\ensuremath{t}},sort=t, description={index of token in sequence},unit={\ensuremath{\in\left[\ell_{\max }\right]}}}
\newglossaryentry{x}{type=symbols,name={\ensuremath{x}},sort=x, description={primary token sequence}, unit={\ensuremath{\equiv x[1] x[2] \ldots x[\ell] \in V^{\ell}}}}
\newglossaryentry{y}{type=symbols,name={\ensuremath{y}},sort=y, description={trade initiator / target}, unit={\ensuremath{\in \{0,1\}}}}
\newglossaryentry{X}{type=symbols,name={\ensuremath{\boldsymbol{X}}},sort=X, description={encoded primary token sequence }, unit={\ensuremath{\in \mathbb{R}^{d_e \times \ell_x}}}}

\newglossarystyle{dotglos}{%
    \setglossarystyle{list}%
    \renewcommand*{\glossentry}[2]{%
        \item[\glsentryitem{##1}\glstarget{##1}{\glossentryname{##1}}]
        \ifglshassymbol{##1}{\glossentrysymbol{##1}}{}%
		\glsentryunit{##1}\quad\parindent20mm%
		\glossentrydesc{##1}%
        \unskip\leaders\hbox to 2.9mm{\hss.}\hfill##2}%
    \renewcommand*{\glsgroupskip}{}%
}

% \renewcommand*{\glossentry}[2]{%
% \item[]\makebox[\glslistdottedwidth][l]{%
% \glsentryitem{##1}%
% \glstarget{##1}{\glossentryname{##1}}%
% \unskip\leaders\hbox to 2.9mm{\hss.}\hfill\strut}\glossentrydesc{##1}}%

% \renewcommand{\glossentry}[2]{%
% \glsentryitem{##1}\glstarget{##1}{\glossentryname{##1}} & $\acem{##1}$ & \glossentrydesc{##1}\tabularnewline
% }%

% \begin{tabbing}
% 	\hspace{0.13\textwidth} \= \hspace{0.13\textwidth}\= \hspace{0.60\textwidth} \= \kill
% 	{\bf Symbol }      \> {\bf Type}     \> {\bf Explanation}                                      \\[0ex]
% 	$[N]$              \> $:=\{1,...,N\}$  \> set of integers $1,2,...,N-1,N$                  \\[0ex]
% 	$i,j$              \> $âˆˆâ„•$           \> generic integer indices       \\[0ex]  
% 	$V$                \> $\cong[N_\t{V}]$      \> vocabulary                               \\[0ex]
% 	$N_\t{V}$          \> $âˆˆ\mathbb{N}$  \> vocabulary size                                 \\[0ex]
% 	$V^*$              \> $=\bigcup_{\ell=0}^âˆž V^{\ell}$ \> set of token sequences; elements include e.g.\ sentences or documents  \\[0ex]
% 	$\ell_{\max}$      \> $âˆˆ\mathbb{N}$     \> maximum sequence length                          \\[0ex]
% 	$\ell$             \> $âˆˆ[\ell_{\max}]$      \> length of token sequence                           \\[0ex]  
% 	$t$                \> $âˆˆ[\ell]$           \> index of token in a sequence                       \\[0ex]
% 	$d_{...}$          \> $âˆˆâ„•$           \> dimension of various vectors                       \\[0ex]  
% 	$\v{x}$            \> $â‰¡x[1:\ell]$   \> $â‰¡x[1]x[2]...x[\ell]âˆˆV^\ell$ ~~ primary token sequence  \\[0ex]
% 	$\v{z}$            \> $â‰¡z[1:\ell]$   \> $â‰¡z[1]z[2]...z[\ell]âˆˆV^\ell$ ~~ context token sequence  \\[0ex]
% 	$M[i,j]$           \> $âˆˆâ„$            \> entry $M_{ij}$ of matrix $Mâˆˆâ„^{dÃ—d'}$        \\[0ex]
% 	$M[i,:]\!â‰¡\!M[i]$  \> $âˆˆâ„^{d'}$      \> $i$-th row of matrix $Mâˆˆâ„^{dÃ—d'}$          \\[0ex]
% 	$M[:,j]$           \> $âˆˆâ„^d$         \> $j$-th column of matrix $Mâˆˆâ„^{dÃ—d'}$        \\[0ex]
% 	$\v{e}$            \> $âˆˆâ„^{d_\t{e}}$    \> vector representation / embedding of a token            \\[0ex]
% 	$\m{X}$              \> $âˆˆâ„^{d_\t{e} Ã— \ell_x}$    \> encoded primary token sequence                             \\[0ex]
% 	$\m{Z}$              \> $âˆˆâ„^{d_\t{e} Ã— \ell_z}$     \> encoded context token sequence                             \\[0ex]  
% 	$\t{Mask}$         \> $âˆˆâ„^{\ell_z\times \ell_x}$   \> masking matrix, it determines the attention context for each token \\[0ex]
% 	$L, L_\t{enc}, L_\t{dec}$ \> $âˆˆâ„•$       \> number of network (encoder, decoder) layers                   \\[0ex]  
% 	$l$                \> $âˆˆ[L]$           \> index of network layer                             \\[0ex]
% 	$H$                \> $âˆˆ\mathbb{N}$     \> number of attention heads                        \\[0ex]
% 	$h$                \> $âˆˆ[H]$         \> index of attention head                            \\[0ex]
% 	$N_\t{data}$       \> $âˆˆâ„•$           \> (i.i.d.) sample size                               \\[0ex]  
% 	$n$                \> $âˆˆ[N_\t{data}]$     \> index of sample sequence                         \\[0ex]
% 	$Î·$                \> $âˆˆ(0,âˆž)$           \> learning rate                                      \\[0ex]
% 	$Ï„$                \> $âˆˆ(0,âˆž)$           \> temperature; it controls the diversity-plausibility trade-off at inference  \\[0ex]
% 	$\m{W_e}$     \> $âˆˆâ„^{d_\t{e}Ã—N_\t{V}}$ \> token embedding matrix       \\[0ex]  
% 	$\m{W_p}$     \> $âˆˆâ„^{d_\t{e}Ã—\ell_{\max}}$ \> positional embedding matrix       \\[0ex]  
% 	$\m{W_u}$     \> $âˆˆâ„^{N_\t{V}Ã—d_\t{e}}$ \> unembedding matrix       \\[0ex]  
% 	$\m{W_q}$     \> $âˆˆâ„^{d_\t{attn}Ã—d_\t{x}}$ \> query weight matrix \\[0ex]
% 	$\v{b_q}$     \> $âˆˆâ„^{d_\t{attn}}$ \> query bias \\[0ex]
% 	$\m{W_k}$     \> $âˆˆâ„^{d_\t{attn}Ã—d_\t{z}}$ \> key weight matrix \\[0ex]
% 	$\v{b_k}$     \> $âˆˆâ„^{d_\t{attn}}$ \> key bias \\[0ex]
% 	$\m{W_v}$     \> $âˆˆâ„^{d_\t{out}Ã—d_\t{z}}$ \> value weight matrix \\[0ex]
% 	$\v{b_v}$     \> $âˆˆâ„^{d_\t{out}}$ \> value bias \\[0ex]
% 	$\bmcWqkv$     \>  \> collection of above parameters of a single-head attention layer \\[0ex]  
% 	$\m{W_o}$     \> $âˆˆâ„^{d_\t{out}Ã—Hd_\t{mid}}$ \> output weight matrix \\[0ex]
% 	$\v{b_o}$     \> $âˆˆâ„^{d_\t{out}}$ \> output bias \\[0ex]
% 	$\bmcW$       \>  \> collection of above parameters of a multi-head attention layer, see \cref{eq:Wall} \\[0ex]
% 	$\m{W_\t{mlp}}$    \> $âˆˆâ„^{d_1Ã—d_2}$ \> weight matrix corresponding to an MLP layer in a Transformer \\[0ex]
% 	$\v{b_\t{mlp}}$    \> $âˆˆâ„^{d_1}$ \> bias corresponding to an MLP layer in a Transformer \\[0ex]  
% 	$\vga$            \> $âˆˆâ„^{d_\t{e}}$ \> layer-norm learnable scale parameter \\[0ex]
% 	$\vbe$                \> $âˆˆâ„^{d_\t{e}}$  \> layer-norm learnable offset parameter \\[0ex]
% 	$\vth,\hat{\vth}$        \> $âˆˆâ„^d$        \> collection of all learnable / learned Transformer parameters \\[0ex]
%   \end{tabbing}
%   \end{table*}
  
%   \end{document}

%Glossary
\newglossaryentry{activation-function}{name={activation function},plural={activation functions}, description={An activation function is a function, that breaks up the linearity of the neural network. It determines if neurons are activated or not. Common variants include \gls{ReLU} and Softmax.}}
\newglossaryentry{embedding}{name={embedding},plural={embeddings},description={Numerical vector representation of the input, e.g. a word, category, or scalar.}}
\newglossaryentry{feed-forward-network}{name={feed-forward network},plural={feed-forward networks},description={Neural networks without recursion. Well-known variants are \glspl{MLP}.}}
\newglossaryentry{overfitting}{name={overfitting},plural={overfitting},description={Creating a model that fits the training data closely, but does not generalize on unseen data.}}
\newglossaryentry{token}{name={token},plural={tokens},description={Item in a vocabulary. For textual data, tokens can be an individual character, sub-word, a word. 
For tabular data, a token corresponds to a column in the data set.}}
\newglossaryentry{exploding-gradient}{name={exploding gradient},plural={exploding gradients},description={Exploding gradients is a problem encountered in training deep neural networks with backpropagation. Error gradients can accumulate, and result in very large parameter updates and unstable training of the network. The opposite is the vanishing gradient problem, whereby gradients become successively smaller during backpropagation, resulting in no or small parameter updates of the network. In both cases, the network does not converge.}}
% ----------------------------------- Start of document -----------------------------------
\begin{document}
\setcounter{page}{2} % Cover pages and title page are not numbered. Start numbering from page 2.

% Title page
\input{Content/Titlepage_Thesis} % Exclude title page (with %) that is not being used.

% Table of contents
\setcounter{page}{1}\renewcommand{\thepage}{\roman{page}} % Sets the numbering to roman small.
\newpage
\tableofcontents

% List of Figures (comment out if there are no figures in the thesis)
\newpage
\listoffigures % Inserts the list of figures.
\addcontentsline{toc}{section}{List of Figures} % Adds the list of figures to the table of contents.

% List of tables (comment out if there are no tables in the thesis)
\newpage
\listoftables % Inserts the list of figures.
\addcontentsline{toc}{section}{List of Tables} % Adds the list of tables to the table of contents.

\newpage
\printglossary[title={List of Symbols},type=symbols,style=dotglos]

% Main text section
\newpage
\setcounter{page}{1}\renewcommand{\thepage}{\arabic{page}} % Sets the numbering to arabic.
\include{Content/main}

% Bibliography
\newpage
\printbibliography

% Glossary
\newpage
\printglossary[type=main,title=Glossary,style=index]


% Appendix
\newpage
\appendix % Enumerates appendix with letters.
\include{Content/Appendix}

\end{document}