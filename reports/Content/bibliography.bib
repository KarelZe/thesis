@incollection{abeDeepLearningForecasting2018,
  title = {Deep Learning for Forecasting Stock Returns in the Cross-Section},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Abe, Masaya and Nakayama, Hideki},
  editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
  year = {2018},
  volume = {10937},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93034-3_22}
}

@misc{abnarQuantifyingAttentionFlow2020,
  title = {Quantifying Attention Flow in Transformers},
  author = {Abnar, Samira and Zuidema, Willem},
  year = {2020},
  number = {arXiv:2005.00928},
  publisher = {{arXiv}}
}

@misc{adaloglouHowPositionalEmbeddings2021,
  title = {How Positional Embeddings Work in Self-Attention (Code in Pytorch)},
  author = {Adaloglou, Nikolas},
  year = {2021},
  journal = {AI Summer},
  howpublished = {https://theaisummer.com/positional-embeddings/}
}

@misc{agarapImplementingAutoencoderPyTorch2020,
  title = {Implementing an Autoencoder in {{PyTorch}}},
  author = {Agarap, Abien Fred},
  year = {2020},
  journal = {PyTorch}
}

@article{aggarwalFrameworkProjectedClustering,
  title = {A Framework for Projected Clustering of High Dimensional Data Streams},
  author = {Aggarwal, Charu C and Han, Jiawei and Wang, Jianyong and Yu, Philip S},
  doi = {10.1016/B978-012088469-8.50075-9}
}

@book{aggarwalRecommenderSystems2016,
  title = {Recommender Systems},
  author = {Aggarwal, Charu C.},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-29659-3}
}

@article{AÃ¯t-Sahalia_2009,
  title = {Estimating the Degree of Activity of Jumps in High Frequency Data},
  author = {{A{\"i}t-Sahalia}, Yacine and Jacod, Jean},
  year = {2009},
  journal = {Annals of Statistics},
  doi = {10.1214/08-aos640},
  mag_id = {3106171293},
  pmcid = {null},
  pmid = {null}
}

@article{Aitken_1996,
  title = {The Accuracy of the Tick Test : Evidence from the Australian Stock Exchange},
  author = {Aitken, Michael J. and Frino, Alex},
  year = {1996},
  journal = {Journal of Banking and Finance},
  doi = {10.1016/s0378-4266(96)00008-8},
  mag_id = {2015189325},
  pmcid = {null},
  pmid = {null}
}

@article{aitkenIntradayAnalysisProbability1995,
  title = {An Intraday Analysis of the Probability of Trading on the Asx at the Asking Price},
  author = {Aitken, Michael and Kua, Amaryllis and Brown, Philip and Watter, Terry and Y. Izan, H.},
  year = {1995},
  journal = {Australian Journal of Management},
  volume = {20},
  number = {2},
  doi = {10.1177/031289629502000202},
  note = {\section{Annotations\\
(02/11/2022, 07:41:47)}

\par
``We explain the probability of a trade at the asking price across time.'' (Aitken et al., 1995, p. 115)
\par
``Our study is based heavily on intraday data.'' (Aitken et al., 1995, p. 117)
\par
``Systematic trading patterns contribute to returns anomalies. Portfolio returns reflect the intraspread movements if systematic trading patterns lead to more frequent trading at ask or bid prices. The wider the bid-ask spread, the larger the intraspread movement.'' (Aitken et al., 1995, p. 122)
\par
``But according to Schwartz (1988), the bid-ask spread has four components: activity, risk, information and competition.'' (Aitken et al., 1995, p. 124)
\par
``Previous empirical studies (Brock and Kleidon 1992; McInish and Wood 1992) have shown a direct relationship between the number of trades and the bid-ask spread.'' (Aitken et al., 1995, p. 125)
\par
``O ur database consists of the complete set of SEATS transactions obtained from the ASX. The data include price, quantity and time signature information for all live bids and asks, all trades, and all bids and asks entered or amended in any way.'' (Aitken et al., 1995, p. 126)
\par
``The data used in this analysis are all regular trades on SEATS during the three year period from 4 September, 1990 to 3 September, 1993.'' (Aitken et al., 1995, p. 127)
\par
``Bid-ask spread: a. Number of price steps between the best ask and best bid (SPRDSTEP).20 b. The relative spread, defined to be the ratio of the bid-ask spread to the simple average of the best bid and best ask (SPRDPERC).'' (Aitken et al., 1995, p. 129)
\par
``Order Imbalance between Bids and Asks: a. The ratio of the number of bid orders to the number of ask orders just before a trade occurs (IMBAL 1). b. The ratio of the volume of bid orders to the volume of ask orders just before a trade occurs (IMBAL 2).'' (Aitken et al., 1995, p. 129)
\par
``We use univariate tests to examine the relationship between each independent variable and a trade at the asking price. One-tailed t-tests are employed to indicate if there is any significant directional difference between the means of independent continuous variables across trades at the asking price and trades at the bidding price'' (Aitken et al., 1995, p. 130)
\par
``The following logistic regression model is estimated in our multivariate analysis: {$\Omega$}i = {$\delta$}0 + t =1 2 {$\Sigma$}0 {$\delta$}t TIMEi,t + d =1 4 {$\Sigma$} {$\delta$}20+d DAYi,d + {$\delta$}25 PRICEi + {$\delta$}26 DOLVOLi + {$\delta$}27 IMBAL 1(IMBAL 2)i + {$\delta$}28 SPRDPERC (SPRDSTEP)i + {$\delta$}29 APPROSECi + {$\delta$}30 FREQLOGi + {$\delta$}31 SIZELOGi + ei'' (Aitken et al., 1995, p. 130)
\par
``We investigate if systematic trading patterns affect the frequency with which a trade occurs at the asking price. For each minute, the proportion of trades at the asking price is computed'' (Aitken et al., 1995, p. 132)
\par
``A chi-square test was used for the categorical variables. Table 3 presents two-dimensional chi-square tests of association between whether a trade was or was not at the asking price and the time of day.24 The test in Table 3 is whether, within the various categories, the proportion of trades at the ask differ significantly from 0.5.'' (Aitken et al., 1995, p. 134)
\par
``We employ a dichotomous logistic regression to conduct a simultaneous test of our hypotheses. It is particularly suited to this study, as none of the continuous explanatory variables is Normally distributed.'' (Aitken et al., 1995, p. 138)
\par
``Our main aim was to explore time related differences in the probability of a trade at the asking price, for instance across time and days of the week, and their relationship to intraday average rates of return. In addition, other determinants of the probability are examined.'' (Aitken et al., 1995, p. 151)
\par
``The probability of a trade at the asking price was significantly related to seven factors other than the time of day: it was inversely related to dollar volume of the trade, buy order imbalance, bid-ask spread and firm size; and directly related to the average trading frequency in the stock, its price level, and whether the stock is approved for short selling.'' (Aitken et al., 1995, p. 151)}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: A next-Generation Hyperparameter Optimization Framework},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  series = {{{KDD}} '19},
  publisher = {{Association for Computing Machinery}},
  address = {{Anchorage, AK}},
  doi = {10.1145/3292500.3330701},
  note = {Comment: 10 pages, Accepted at KDD 2019 Applied Data Science track}
}

@article{aktasTradeClassificationAccuracy2014,
  title = {Trade Classification Accuracy for the Bist},
  author = {Aktas, Osman Ulas and Kryzanowski, Lawrence},
  year = {2014},
  journal = {Journal of International Financial Markets, Institutions and Money},
  volume = {33},
  doi = {10.1016/j.intfin.2014.08.003},
  note = {\section{Annotations\\
(25/10/2022, 06:03:00)}

\par
``When trade IDs are not available, a trade is assumed to be initiated by the trader whose market order has been executed against a standing limit order. The advantage of this immediacy approach is that it considers both dimensions of liquidity for trades that match a market with a limit order that are on opposite sides of the market. However, as noted by Odders-White (2000), this approach cannot identify the actual trade initiator for crossed market orders, limit\textendash limit order matches and stopped market orders.'' (Aktas and Kryzanowski, 2014, p. 267)
\par
``When trade IDs are available, the chronological approach is used where the trade initiator is identified as being the trader who places an order last chronologically. The two-part rationale behind this approach is that: (i) the first-in party to the trade acts as the liquidity provider at its chosen price; and (ii) the last-in party pays the ``immediacy premium'' for the rapid execution of the trade. The advantage of this approach is that it considers both dimensions of liquidity for a wider set of order type pairings.'' (Aktas and Kryzanowski, 2014, p. 267)
\par
``Given different conceptual benchmarks, it is not surprising that they arrive at different conclusions about the reliability of using the LR algorithm for classifying trades when the LR algorithm classifies most (majority of) trades involving short sales as being buyer-initiated for stocks (not) subject to either the uptick or inside bid rule depending upon the trading venue examined.'' (Aktas and Kryzanowski, 2014, p. 267)}
}

@article{amel-zadehMachineLearningBasedFinancial2020,
  title = {Machine Learning-Based Financial Statement Analysis},
  author = {{Amel-Zadeh}, Amir and Calliess, Jan-Peter and Kaiser, Daniel and Roberts, Stephen},
  year = {2020},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3520684}
}

@article{antoniouLognormalDistributionStock2004,
  title = {On the Log-Normal Distribution of Stock Market Data},
  author = {Antoniou, I and Ivanov, Vi.V and Ivanov, Va.V and Zrelov, P.V},
  year = {2004},
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {331},
  number = {3-4},
  doi = {10.1016/j.physa.2003.09.034},
  note = {\section{Annotations\\
(04/12/2022, 11:37:44)}

\par
``In Section 3, we show that for some stock market data the statistical distributionof the closing prices normalized by corresponding traded volumes (``price/volume'' ratio) 1ts well the log-normal law.'' (Antoniou et al., 2004, p. 618)
\par
``Aiming to minimize the in=uence of the trend, we introduce a new stochastic variable , which is the closing price normalized by the corresponding traded volume (price/volume). The variable  (see left bottom plots inFigs. 3\textendash 9) has a relatively stable mean value and dispersion, compared to the original time series of closing prices and traded volumes.'' (Antoniou et al., 2004, p. 620)
\par
``In order to test the hypothesis on the correspondence of these statistical distributions to the log-normal law, they are approximated by (see, for example, Refs. [3,10]) f(x)= A {$\surd$}2x exp-(1=22)(ln x-	)2 ;'' (Antoniou et al., 2004, p. 621)
\par
``The quality of approximationis represented by the value \\
2=ndf , where ``ndf'' is the number of degrees of freedom.'' (Antoniou et al., 2004, p. 622)}
}

@misc{arikTabNetAttentiveInterpretable2020,
  title = {Tabnet: Attentive Interpretable Tabular Learning},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = {2020},
  number = {arXiv:1908.07442},
  eprint = {1908.07442},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 15:30:55)}

\par
``First, because DT-based approaches have certain benefits: (i) they are representionally efficient for decision manifolds with approximately hyperplane boundaries which are common in tabular data; and (ii) they are highly interpretable in their basic form (e.g. by tracking decision nodes) and there are popular post-hoc explainability methods for their ensemble form, e.g. (Lundberg, Erion, and Lee 2018) \textendash{} this is an important concern in many real-world applications; (iii) they are fast to train. Second, because previously-proposed DNN architectures are not well-suited for tabular data: e.g. stacked convolutional layers or multi-layer perceptrons (MLPs) are vastly overparametrized \textendash{} the lack of appropriate inductive bias often causes them to fail to find optimal solutions for tabular decision manifolds (Goodfellow, Bengio, and Courville 2016; Shavitt and Segal 2018; Xu et al. 2019).'' (Arik and Pfister, 2020, p. 1)
\par
``Finally, for the first time for tabular data, we show significant performance improvements by using unsupervised pre-training to predict masked features (see Fig. 2'' (Arik and Pfister, 2020, p. 1)
\par
``Self-supervised learning: Unsupervised representation learning improves supervised learning especially in small data regime (Raina et al. 2007). Recent work for text (Devlin et al. 2018) and image (Trinh, Luong, and Le 2019) data has shown significant advances \textendash{} driven by the judicious choice of the unsupervised learning objective (masked input prediction) and attention-based deep learning'' (Arik and Pfister, 2020, p. 3)
\par
``Tabular self-supervised learning: We propose a decoder architecture to reconstruct tabular features from the TabNet encoded representations. The decoder is composed of feature transformers, followed by FC layers at each decision step. The outputs are summed to obtain the reconstructed features.'' (Arik and Pfister, 2020, p. 5)
\par
``Table 10: Self-supervised tabular learning results. Mean and std. of accuracy (over 15 runs) on Forest Cover Type, varying the size of the training dataset for supervised fine-tunin'' (Arik and Pfister, 2020, p. 9)}
}

@article{arpitWhyRegularizedAutoEncoders2016,
  title = {Why Regularized Auto-Encoders Learn Sparse Representation?},
  author = {Arpit, Devansh and Zhou, Yingbo and Ngo, Hung and Govindaraju, Venu},
  year = {2016},
  journal = {arXiv:1505.05561 [cs, stat]},
  eprint = {1505.05561},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  note = {Comment: 8 pages of content, 1 page of reference, 4 pages of supplementary. ICML 2016; bug fix in lemma 1}
}

@article{Asquith_2010,
  title = {Short Sales and Trade Classification Algorithms},
  author = {Asquith, Paul and Oman, Rebecca and Safaya, Christopher},
  year = {2010},
  journal = {Journal of Financial Markets},
  doi = {10.2139/ssrn.951420},
  mag_id = {2052362834},
  pmcid = {null},
  pmid = {null}
}

@article{averyRecommenderSystemsEvaluating1997,
  title = {Recommender Systems for Evaluating Computer Messages},
  author = {Avery, Christopher and Zeckhauser, Richard},
  year = {1997},
  journal = {Communications of the ACM},
  volume = {40},
  number = {3},
  doi = {10.1145/245108.245127}
}

@article{Back_1993,
  title = {Asymmetric Information and Options},
  author = {Back, Kerry},
  year = {1993},
  journal = {Review of Financial Studies},
  doi = {10.1093/rfs/5.3.435},
  mag_id = {1986440955},
  pmcid = {null},
  pmid = {null}
}

@article{badaroTransformersTabularData,
  title = {Transformers for Tabular Data Representation: A Survey of Models and Applications},
  author = {Badaro, Gilbert and Saeed, Mohammed and Papotti, Paolo}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Accepted at ICLR 2015 as oral presentation}
}

@misc{bahriSCARFSelfSupervisedContrastive2022,
  title = {{{SCARF}}: Self-Supervised Contrastive Learning Using Random Feature Corruption},
  author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  year = {2022},
  number = {arXiv:2106.15147},
  eprint = {2106.15147},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(24/01/2023, 10:20:00)}

\par
``In many machine learning tasks, unlabeled data is abundant but labeled data is costly to collect, requiring manual human labelers. The goal of self-supervised learning is to leverage large amounts of unlabeled data to learn useful representations for downstream tasks such as classification.'' (Bahri et al., 2022, p. 1)
\par
``Despite the importance of self-supervised learning, there is surprisingly little work done in finding methods that are applicable across domains and in particular, ones that can be applied to tabular data.'' (Bahri et al., 2022, p. 1)
\par
``In this paper, we propose SCARF, a simple and versatile contrastive pre-training procedure. We generate a view for a given input by selecting a random subset of its features and replacing them by random draws from the features' respective empirical marginal distributions.'' (Bahri et al., 2022, p. 1)
\par
``We show that not only does SCARF pre-training improve classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. Moreover, we show that combining SCARF pre-training with other solutions to these problems further improves them, demonstrating the versatility of SCARF and its ability to'' (Bahri et al., 2022, p. 1)
\par
``A number of self-supervised learning techniques have been proposed in computer vision (Zhai et al., 2019; Tung et al., 2017; Jing \& Tian, 2020). One framework involves learning features based on generated images through various methods, including using a GAN (Goodfellow et al., 2014; Donahue et al., 2016; Radford et al., 2015; Chen et al., 2018), predicting pixels (Larsson et al., 2016), predicting colorizations (Zhang et al., 2016; Larsson et al., 2017), ensuring local and global consistency (Iizuka et al., 2017), and learning synthetic artifacts (Jenni \& Favaro, 2018). Most related to our approach are contrastive learning ones (Tian et al., 2019; Hassani \& Khasahmadi, 2020; Oord et al., 2018; Henaff, 2020; Li et al., 2016; He et al., 2020; Bojanowski \& Joulin, 2017; Wang \& Gupta, 2015; Gidaris et al., 2018). In particular, our framework is similar to SimCLR (Chen et al., 2020), which involves generating views of a single image via image-based corruptions like random cropping, color distortion and blurring; however, we generate views that are applicable to tabular data. Self-supervised learning has had an especially large impact in language modeling (Qiu et al., 2020). One popular approach is masked language modeling, wherein the model is trained to predict input tokens that have been intentionally masked out (Devlin et al., 2018; Raffel et al., 2019; Song et al., 2019) as well as enhancements to this approach (Liu et al., 2019; Dong et al., 2019; Bao et al., 2020; Lample \& Conneau, 2019; Joshi et al., 2020) and variations involving permuting the tokens (Yang et al., 2019; Song et al., 2020). Denoising autoencoders have been used by training them to reconstruct the input from a corrupted version (produced by, for example, token masking, deletion, and infilling) (Lewis et al., 2019; Wang et al., 2019; Freitag \& Roy, 2018). Contrastive approaches'' (Bahri et al., 2022, p. 2)
\par
``include randomly replacing words and distinguishing between real and fake phrases (Collobert et al., 2011; Mnih \& Kavukcuoglu, 2013), random token replacement (Mikolov et al., 2013; Clark et al., 2020), and adjacent sentences (Joshi et al., 2020; Lan et al., 2019; de Vries et al., 2019).'' (Bahri et al., 2022, p. 3)
\par
``Recently, Yao et al. (2020) adapted the contrastive framework to large-scale recommendation systems in a way similar to our approach. The key difference is in the way the methods generate multiple views. Yao et al. (2020) proposes masking random features in a correlated manner and applying a dropout for categorical features, while our approach involves randomizing random features based on the features' respective empirical marginal distribution (in an uncorrelated way).'' (Bahri et al., 2022, p. 3)
\par
``Lastly, also similar to our work is VIME (Yoon et al., 2020), which proposes the same corruption technique for tabular data that we do. They pre-train an encoder network on unlabeled data by attaching ``mask estimator'' and ``feature estimator'' heads on top of the encoder state and teaching the model to recover both the binary mask that was used for corruption as well as the original uncorrupted input, given the corrupted input. The pre-trained encoder network is subsequently used for semisupervised learning via attachment of a task-specific head and minimization of the supervised loss as well as an auto-encoder reconstruction loss. VIME was shown to achieve state-of-art results on genomics and clinical datasets. The key differences with our work is that we pre-train using a contrastive loss, which we show to be more effective than the denoising auto-encoder loss that partly constitutes VIME. Furthermore, after pre-training we fine-tune all model weights, including the encoder (unlike VIME, which only fine-tunes the task head), and we do so using task supervision only'' (Bahri et al., 2022, p. 3)}
}

@article{baLayerNormalization2016,
  title = {Layer Normalization},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  journal = {arXiv:1607.06450},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@book{banachewiczKaggleBookData2022,
  title = {The Kaggle Book: Data Analysis and Machine Learning for Competitive Data Science},
  author = {Banachewicz, Konrad and Massaron, Luca},
  year = {2022},
  edition = {[First edition]},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}}
}

@article{baptistaRelationPrognosticsPredictor2022,
  title = {Relation between Prognostics Predictor Evaluation Metrics and Local Interpretability {{SHAP}} Values},
  author = {Baptista, Marcia L. and Goebel, Kai and Henriques, Elsa M.P.},
  year = {2022},
  journal = {Artificial Intelligence},
  volume = {306},
  doi = {10.1016/j.artint.2022.103667}
}

@article{Barndorff-Nielsen_2005,
  title = {Econometrics of Testing for Jumps in Financial Economics Using Bipower Variation},
  author = {{Barndorff-Nielsen}, Ole E. and Shephard, Neil},
  year = {2005},
  journal = {Journal of Financial Econometrics},
  doi = {10.1093/jjfinec/nbi022},
  mag_id = {2136215272},
  pmcid = {null},
  pmid = {null}
}

@article{basakPredictingDirectionStock2019,
  title = {Predicting the Direction of Stock Market Prices Using Tree-Based Classifiers},
  author = {Basak, Suryoday and Kar, Saibal and Saha, Snehanshu and Khaidem, Luckyson and Dey, Sudeepa Roy},
  year = {2019},
  journal = {The North American Journal of Economics and Finance},
  volume = {47},
  doi = {10.1016/j.najef.2018.06.013},
  note = {\section{Annotations\\
(12/07/2022, 17:09:12)}

\par
``. Two algorithms, random forests, and gradient boosted decisio`n trees (using XGBoost) facilitate this connection by using ensembles of decision trees. We test our approach and report the accuracies for a variety of companies as improvement over existing predictions. A novelty of the current work is about the selection of technical indicators and their use as features, with high accuracy for medium to long-run prediction of stock price direction.'' (Basak et al., 2019, p. 1)
\par
``The focus of the current paper is therefore to implement random forests, and gradient boosted trees on stock data, and to discuss its advantages over non-ensemble techniques. These models have been trained on time intervals of 3, 5, 10, 15, 30, 60, and 90 days days and the results are impressive. Moreover, majority of the related work focused on the time window of 10 to 44 days on an average as most of the previous studies preferred to use metric classifiers on time series data which was not smoothened'' (Basak et al., 2019, p. 3)
\par
``The results are based on data for ten companies (see Appendix A) and all of the data available have been used, i.e., starting from the day they went public till 3rd February 2017'' (Basak et al., 2019, p. 6)
\par
``Here, too, the classification accuracy and F-score increase with the increase in the window width. Moreover, the goodness of classification observed for a certain window-width in the case of XGBoost is comparable to the goodness of classification for the same windowwidth in the case of random forests. These results are presented in Table 3 (seeFigs. 1\textendash{} 3'' (Basak et al., 2019, p. 7)
\par
``In this paper, we have used random forests and XGBoost classifiers, as two useful algorithms to build our predictive model, which produced impressive results. The model is found to be robust in predicting the direction of stock movements. The robustness of our model has been evaluated by calculating various parameters such as accuracy, precision, recall, specificity, and F-score'' (Basak et al., 2019, p. 11)}
}

@inproceedings{bastingsElephantInterpretabilityRoom2020,
  title = {The Elephant in the Interpretability Room: Why Use Attention as Explanation When We Have Saliency Methods?},
  booktitle = {Proceedings of the {{Third BlackboxNLP Workshop}} on {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Bastings, Jasmijn and Filippova, Katja},
  year = {2020},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.blackboxnlp-1.14}
}

@misc{batesCrossvalidationWhatDoes2022,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  year = {2022},
  number = {arXiv:2104.00673},
  eprint = {2104.00673},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(27/10/2022, 18:09:38)}

\par
``In this work, we show that the the estimand of CV is not the accuracy of the model fit on the data at hand, but is instead the average accuracy over many hypothetical data sets.'' (Bates et al., 2022, p. 1)
\par
``Turning to confidence intervals for prediction error, we show that na{\" }\i ve intervals based on CV can fail badly, giving coverage far below the nominal level'' (Bates et al., 2022, p. 1)
\par
``The source of this behavior is the estimation of the variance used to compute the width of the interval: it does not account for the correlation between the error estimates in different folds, which arises because each data point is used for both training and testing.'' (Bates et al., 2022, p. 1)
\par
``As a result, the estimate of variance is too small and the intervals are too narrow. To address this issue, we develop a modification of cross-validation, nested cross-validation (NCV), that achieves coverage near the nominal level, even in challenging cases where the usual cross-validation intervals have miscoverage rates two to three times larger than the nominal rate.'' (Bates et al., 2022, p. 1)}
}

@article{Battalio_2006,
  title = {Options and the Bubble},
  author = {Battalio, Robert H. and Schultz, Paul H.},
  year = {2006},
  journal = {Journal of Finance},
  doi = {10.2139/ssrn.558543},
  mag_id = {1984118538},
  pmcid = {null},
  pmid = {null}
}

@article{bengioNeuralProbabilisticLanguage,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian}
}

@incollection{bengioPracticalRecommendationsGradientBased2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Bengio, Yoshua},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_26}
}

@article{bessembinderIssuesAssessingTrade2003,
  title = {Issues in Assessing Trade Execution Costs},
  author = {Bessembinder, Hendrik},
  year = {2003},
  journal = {Journal of Financial Markets},
  volume = {6},
  number = {3},
  doi = {10.1016/S1386-4181(02)00064-2},
  note = {\section{Annotations\\
(23/10/2022, 12:22:26)}

\par
``One shortcoming of trade and quote data is that whether a trade was initiated by a buyer or a seller must be imperfectly inferred from the data. A second issue is that, although trade prices can be readily compared to quotes in effect at the trade report time, the appropriate comparison might be to quotes in effect at an earlier timesuch as the time of the trading decision or at the time the order arrived at the market, and these times are generally not known. This paper evaluates the practical impact of two methodological choices made when estimating trading costs from public data: the method used to classify trades as resulting from customer buy versus sell orders, and the relationship between trade report times and the time of the quote chosen as a reference point.'' (Bessembinder, 2003, p. 234)
\par
``Researchers who have adopted the five second (or other fixed time) lag recommended by Lee and Ready generally use the adjustment both when inferring whether trades are buyer or seller-initiated and for selecting a quote to be used as a benchmark to measuring effective trade execution costs. However, the optimal amount by which to adjust trade times before comparing trade prices to quotes could differ depending on the application. Suppose, for example, that trade report times lag actual trade execution times by five seconds, and that quotes tend to be systematically revised in the fifteen seconds before trades are completed. Then, comparing trade prices to quotes five seconds before the trade report time would be optimal for inferring trade direction. However, comparing trades to quotes in effect twenty seconds before the trade report time would provide a more accurate measure of trade execution cost, including the effect of pre-trade price impact'' (Bessembinder, 2003, p. 235)
\par
``This study assess the effect on measured trading costs of comparing trade prices to quotes in effect from zero to thirty seconds prior to the trade report time'' (Bessembinder, 2003, p. 235)
\par
``The results of the study can be summarized as follows. First, measures of rates at which trades are executed at prices better (trades receive ``price improvement'') or worse (trades receive ``price disimprovement'') than the quotations are quite sensitive to whether trade prices are compared to contemporaneous or previous quotes. Comparing trade prices to earlier quotes decreases the percentage of trades that appear to receive price improvement while sharply increasing the percentage of trades that appear to be disimproved.'' (Bessembinder, 2003, p. 236)
\par
``Several studies have recently emerged that use specialized datasets containing order information to assess the accuracy of the Lee and Ready algorithm. Finucane (2000), Odders-White (2000) and Lee and Radhakrishna (2000) all use the TORQ dataset, which provides trade, order, and quote data for a sample of 144 NYSElisted stocks during a three-month period in 1990 and 1991. Ellis et al. (2000) use a proprietary sample that includes order data following initial public offerings in 313 Nasdaq stocks during a twelve-month period during 1996 and 1997. Peterson and Sirri use recent NYSE systemorder data. These papers indicate that, while the Lee and Ready algorithmworks fairly well overall, classifying about 85\% of trades correctly, alternative algorithms may perform better.'' (Bessembinder, 2003, p. 240)
\par
``The usual rationale is that trade reports are sometimes delayed, so that report times lag actual execution times.'' (Bessembinder, 2003, p. 241)}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}}
}

@article{Black_1975,
  title = {Fact and Fantasy in the Use of Options},
  author = {Black, Fischer},
  year = {1975},
  journal = {Financial Analysts Journal},
  doi = {10.2469/faj.v31.n4.36},
  mag_id = {1816654404},
  pmcid = {null},
  pmid = {null}
}

@article{blackPricingOptionsCorporate1973,
  title = {The Pricing of Options and Corporate Liabilities},
  author = {Black, Fischer and Scholes, Myron},
  year = {1973},
  journal = {The Journal of Political Economy},
  volume = {81},
  number = {3},
  doi = {10.1086/260062}
}

@article{blazejewskiLocalNonParametricModel2005,
  title = {A Local Non-Parametric Model for Trade Sign Inference},
  author = {Blazejewski, Adam and Coggins, Richard},
  year = {2005},
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {348},
  doi = {10.1016/j.physa.2004.09.033},
  note = {\section{Annotations\\
(25/10/2022, 10:43:09)}

\par
``The k-nearest neighbor with three predictor variables achieves an average out-of-sample classification accuracy of 71.40\%, compared to 63.32\% for the linear logistic regression with seven predictor variables.'' (Blazejewski and Coggins, 2005, p. 481)
\par
``The result suggests that a non-linear approach may produce a more parsimonious trade sign inference model with a higher out-of-sample classification accuracy.'' (Blazejewski and Coggins, 2005, p. 481)
\par
``A buyer-initiated trade (a buy) is a trade triggered by a buy market order matched against one or more sell limit orders in the order book. The opposite holds for a seller-initiated trade (a sell), where a sell market order is matched against one or more buy limit orders in the order book. Submitters of market orders are called liquidity demanders, while submitters of limit orders stored in the book are called liquidity providers.'' (Blazejewski and Coggins, 2005, p. 482)
\par
``The trade initiator variable is alternatively referred to as a trade sign, trade direction, trade indicator, or buy/sell indicator. We will use the second term, trade sign, throughout the rest of this paper.'' (Blazejewski and Coggins, 2005, p. 482)
\par
``Aitken et al. [18] analyze the intraday probability of trading at the asking price on the Australian Stock Exchange. They use limit order book and other data to build a logistic regression model for a set of over 3 million trades, and manage to correctly classify 53.3\% of trades, while 51.58\% of all trades in their data set are at the asking price.'' (Blazejewski and Coggins, 2005, p. 483)
\par
``Our study explores a regularity in market order submission strategies on the Australian Stock Exchange (ASX).'' (Blazejewski and Coggins, 2005, p. 483)
\par
``We demonstrate this predictability by developing an empirical trade sign inference model to classify trades into buyer-initiated and sellerinitiated. The model is based on a local non-parametric method, k-nearest-neighbor (k-NN).'' (Blazejewski and Coggins, 2005, p. 483)
\par
``Quote and trade prices are not used.'' (Blazejewski and Coggins, 2005, p. 483)
\par
``Classification accuracy is determined through out-of-sample testing. The classification performance of the kNN classifier is compared against the performance of three other classifiers: linear logistic regression, trade continuation, and majority vote. We show that the k-NN classifier is superior to the other classifiers and can separate buyer-initiated and seller-initiated trades in our data set with an average accuracy of over 71\%.'' (Blazejewski and Coggins, 2005, p. 483)
\par
``Variable sets are ranked by classification accuracy across all stocks, and the best sets are selected for the logistic regression and the knearest-neighbor.'' (Blazejewski and Coggins, 2005, p. 484)
\par
``Two simple classifiers, a trade continuation and a majority vote, based on lagged values of the trade sign only, are used for performance comparison. The models are estimated and tested with a moving window method.'' (Blazejewski and Coggins, 2005, p. 484)
\par
``Before model estimation all predictor variables are preprocessed by calculating their natural logarithms.'' (Blazejewski and Coggins, 2005, p. 486)
\par
``The result of the testing procedure is a single classification accuracy value for the test day. The estimation and testing are then repeated for a new pair of training and test intervals, obtained by shifting the previous pair of intervals one day forward.'' (Blazejewski and Coggins, 2005, p. 486)
\par
``Statistical significance was not determined because it is not used by our selection procedure'' (Blazejewski and Coggins, 2005, p. 489)
\par
``Depending on the value of k; the k-nearest-neighbor has the mean accuracy approximately 9\% to 11\% higher than the logistic regression, while its standard deviation varies from 2.89\% to 3.28\%. Furthermore, the higher the value of k the better the performance of the k-NN classifier, even though an improvement between k {$\frac{1}{4}$} 5 and 9 is minimal.'' (Blazejewski and Coggins, 2005, p. 490)
\par
``(1) Among the k-NN classifiers, the higher the value of k the greater the mean accuracy. The difference between accuracies for k {$\frac{1}{4}$} 9 and 5, however, can be minimal and sometimes negative, but on average k {$\frac{1}{4}$} 9 is the best (12). (2) The mean accuracy of the k-NN classifier, where k {$\frac{1}{4}$} 9; is a monotonically increasing function of the training interval length. The rate of the increase, however, rapidly declines. Small, negligible fluctuations are sometimes present (10). (3) The mean accuracy of the k-NN classifier, where k {$\frac{1}{4}$} 9; is greater than the mean accuracy of the logistic regression classifier for all training timescales (8).'' (Blazejewski and Coggins, 2005, p. 491)
\par
``The mean accuracy of the k-NN classifier, where k {$\frac{1}{4}$} 9; is greater than the mean accuracies of the trade continuation and the majority vote classifiers, for all training timescales (12).'' (Blazejewski and Coggins, 2005, p. 492)
\par
``We proposed ARTICLE IN PRESS 9The total number of models constructed for each stock was 145: 2  3  16 k-NN, 2  16 logistic regression, 1 trade continuation, and 16 majority vote models. A. Blazejewski, R. Coggins / Physica A 348 (2005) 481\textendash 495 49'' (Blazejewski and Coggins, 2005, p. 493)
\par
``the k-nearest-neighbor classifier as an alternative to the linear logistic regression. The average classification accuracy of the k-NN \dh k {$\frac{1}{4}$} 9\TH{} classifier, across all stocks and allowing contemporaneous predictor variables, was found to be 71.40\% (SD {$\frac{1}{4}$} 4:01\%), or 8.08\% higher than the corresponding accuracy of 63.32\% (SD {$\frac{1}{4}$} 4:27\%) for the logistic regression. When compared with the trade continuation and the majority vote classifiers, the k-nearest-neighbor was 14.08\% and 17.87\% better, respectively.'' (Blazejewski and Coggins, 2005, p. 494)
\par
``These results suggest that a non-linear approach may produce a more parsimonious trade sign inference model with a higher out-of-sample classification accuracy. Furthermore, for most of our stocks the classification accuracy of the k-nearest-neighbor \dh k {$\frac{1}{4}$} 9\TH{} with contemporaneous predictor variables is a monotonically increasing function of the training interval length, with 30 days being the best interval.'' (Blazejewski and Coggins, 2005, p. 494)}
}

@article{Bloomfield_1999,
  title = {Market Transparency: Who Wins and Who Loses?},
  author = {Bloomfield, Robert J. and O'Hara, Maureen},
  year = {1999},
  journal = {Review of Financial Studies},
  doi = {10.1093/rfs/12.1.5},
  mag_id = {2022730840},
  pmcid = {null},
  pmid = {null}
}

@misc{boardofgovernorsofthefederalreservesystemus1YearTreasuryBill1959,
  title = {1-Year Treasury Bill Secondary Market Rate},
  author = {{Board of Governors of the Federal Reserve System (US)}},
  year = {1959},
  journal = {FRED, Federal Reserve Bank of St. Louis},
  publisher = {{FRED, Federal Reserve Bank of St. Louis}},
  howpublished = {https://fred.stlouisfed.org/series/DTB1YR},
  note = {Last updated: 2021-10-29 15:23:12-05}
}

@article{Boehmer_2007,
  title = {Estimating the Probability of Informed Trading - Does Trade Misclassification Matter?},
  author = {Boehmer, Ekkehart and Grammig, Joachim and Theissen, Erik},
  year = {2007},
  journal = {Journal of Financial Markets},
  doi = {10.1016/j.finmar.2006.07.002},
  mag_id = {2132988645},
  pmcid = {null},
  pmid = {null}
}

@misc{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  number = {arXiv:1607.04606},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Accepted to TACL. The two first authors contributed equally}
}

@article{bojerLearningsKaggleForecasting,
  title = {Learnings from {{Kaggle}}'s {{Forecasting Competitions}}},
  author = {Bojer, Casper Solheim and Meldgaard, Jens Peder}
}

@misc{borisovDeepNeuralNetworks2022,
  title = {Deep Neural Networks and Tabular Data: A Survey},
  author = {Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2022},
  number = {arXiv:2110.01889},
  eprint = {2110.01889},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{Bouchaud_2008,
  title = {How Markets Slowly Digest Changes in Supply and Demand},
  author = {Bouchaud, Jean-Philippe and Farmer, J. Doyne and Lillo, Fabrizio},
  year = {2008},
  journal = {arXiv: Trading and Market Microstructure},
  doi = {10.2139/ssrn.1266681},
  mag_id = {2103368690},
  pmcid = {null},
  pmid = {null}
}

@article{boweNewClassicalBayesian,
  title = {New Classical and Bayesian Estimators for Classifying Trade Direction in the Absence of Quotes},
  author = {Bowe, Michael and Cho, Sungjun and Hyde, Stuart and Sung, Iljin},
  year = {2018},
  note = {\section{Annotations\\
(02/11/2022, 09:39:10)}

\par
``We propose new methods for estimating the effective bid-ask spread and classifying trading intentions without access to quotes.'' (Bowe et al., 2018, p. 1)
\par
``Our state space approach utilizes both classical and Bayesian estimators'' (Bowe et al., 2018, p. 1)
\par
``For illustrative purposes, we apply our approach to an analysis of the trading patterns in the CME's gold futures contract during a period incorporating uncertainty in financial markets as a result of the UK's 2016 Brexit referendum.'' (Bowe et al., 2018, p. 1)
\par
``The second major contribution of this paper is to provide trade direction classification mechanism without recourse to quotes. These classification systems utilise both Bayesian MCMC methods and classical filtering and smoothing algorithms for latent trade direction indicators.'' (Bowe et al., 2018, p. 4)
\par
``Recently, Easley, Lopez de Prado, O'Hara (2016) propose a new conceptual framework for classifying trades, taking the perspective of a Bayesian statistician with priors on the unobservable information (buy or sell indicator), who is trying to extract trading intentions from observable trade data. They compare the strengths and weakness of several rules against an ideal Bayesian rule.'' (Bowe et al., 2018, p. 4)
\par
``We propose that certain familiar structural empirical market microstructure models, such as those we employ in this analysis, provide plausible approximations to their ideal Bayesian trade classification approach. In particular, these models employ a Markov switching process as the underlying process governing the dynamics of the unobservable buy-sell indicator, and treat the measurement equations as a plausible data generating process for the observed data relating to the indicator.'' (Bowe et al., 2018, p. 4)
\par
``For purposes of illustration, we apply our proposed approach to analyse trading behaviour in the gold futures contract trading on the CME over the two month period from May 2016 to June 2016, a timeframe incorporating the UK Brexit referendum.'' (Bowe et al., 2018, p. 4)
\par
``However, in the presence of greater uncertainty when trading potentially generates a greater price impact (relating from to order flow imbalances), our trade classification indicator often diverges significantly from those we obtain using the Tick rule.'' (Bowe et al., 2018, p. 5)
\par
``As Easley, Lopez de Prado, and O'Hara (2016) maintain that Tick rule classifications appear particularly problematic in periods of high volatility exhibiting imbalances in order flow, we believe the approach to trade classification we propose shows some promise.'' (Bowe et al., 2018, p. 5)
\par
``As Easley, Lopez de Prado, and O'Hara (2016) note, each trade classification rule may demonstrate both strengths and weakness, depending on the underlying market characteristics.'' (Bowe et al., 2018, p. 5)
\par
``They adopt the perspective of Bayesian statisticians with priors on the unobservable information (here t q ), who are trying to extract trading intentions from observable trading data. Ideally, we would like to specify the data generating processes for both the underlying unobservable variables and subsequently for the observed data, conditional on the realizations of the underlying unobservable data.'' (Bowe et al., 2018, p. 14)
\par
``They claim that every trade classification algorithm can be regarded as an approximation to this Bayesian approach, and that their bulk volume classification (BVC) methodology is conceptually closer to this ideal than traditional approaches such as the Tick rule, since BVC assigns a probability to a given trade being either a buy or sell.'' (Bowe et al., 2018, p. 14)
\par
``We conduct the empirical implementation of our proposed trade classification methods using data from gold futures trading on the Chicago Mercantile Exchange (CME) during May and June 2016.'' (Bowe et al., 2018, p. 15)
\par
``Specifically, we select our sample data from the gold futures contract trading on CME's Globex electronic trading platform during the period from May 1, 2016 to June 30, 2016.'' (Bowe et al., 2018, p. 15)
\par
``In order to provide appropriate benchmarks with which to compare our results on the classified trades, we proceed to classify trades using the standard Tick rule19 and generate daily correlation estimates of classified trades using the Tick rule and our model consistent rules'' (Bowe et al., 2018, p. 25)
\par
``We conjecture a potential explanation for the second finding is as follows. Easley, Lopez de Prado, O'Hara (2016) maintain that when the underlying data is less noisy, Tick rule classifications can be superior to other rules. However, they also show that in situations where underlying data noise is substantial or order flow is imbalanced, such as when private information motivates trading, trade classifications using the Tick rule may be unreliable.'' (Bowe et al., 2018, p. 26)
\par
``In summary, the model consistent trade direction classification algorithm based on the extended GH formulation generates very similar results to the Tick rule during normal trading periods, but in periods characterised by higher uncertainty and the existence of a potentially larger price impact of trades (closely related to order imbalances), the classifications obtained from the two methods diverge significantly. As these are precisely the circumstances under which Easley, Lopez de Prado, and O'Hara (2016) argue that the Tick rule appears most problematic in classifying trades, this suggest our proposed extended GH methods may be useful in such an environment.'' (Bowe et al., 2018, p. 27)
\par
``However, in the presence of greater uncertainty when trading potentially generates a greater price impact (resulting from order flow imbalances), our trade classification indicator often diverges significantly from those using the Tick rule. Easley, Lopez de Prado, and O'Hara (2016) maintain that Tick rule classifications appear particularly problematic in periods of high volatility exhibiting imbalances in order flow.'' (Bowe et al., 2018, p. 30)}
}

@article{boxAnalysisTransformations2022,
  title = {An Analysis of Transformations},
  author = {Box, G E P and Cox, D R},
  year = {2022}
}

@article{breedenPricesStateContingentClaims1978,
  title = {Prices of State-Contingent Claims Implicit in Option Prices},
  author = {Breeden, Douglas T. and Litzenberger, Robert H.},
  year = {1978},
  journal = {The Journal of Business},
  volume = {51},
  number = {4},
  doi = {10.1086/296025}
}

@article{breimanBaggingPredictors1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = {1996},
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  doi = {10.1007/BF00058655}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {1984},
  edition = {First},
  publisher = {{CLC Press}},
  address = {{Boca Raton, FL}}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  doi = {10.1023/A:1010933404324}
}

@misc{breuelEffectsHyperparametersSGD2015,
  title = {The Effects of Hyperparameters on {{SGD}} Training of Neural Networks},
  author = {Breuel, Thomas M.},
  year = {2015},
  number = {arXiv:1508.02788},
  publisher = {{arXiv}}
}

@misc{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 40+32 pages}
}

@article{burkeHybridRecommenderSystems2002,
  title = {Hybrid Recommender Systems: Survey and Experiments\textdagger},
  author = {Burke, Robin},
  year = {2002},
  journal = {User Modeling and User-Adapted Interaction},
  volume = {12},
  number = {4},
  doi = {10.1023/A:1021240730564}
}

@article{busariCrudeOilPrice2021,
  title = {Crude Oil Price Prediction: A Comparison between {{AdaBoost-LSTM}} and {{AdaBoost-GRU}} for Improving Forecasting Performance},
  author = {Busari, Ganiyu Adewale and Lim, Dong Hoon},
  year = {2021},
  journal = {Computers \& Chemical Engineering},
  volume = {155},
  doi = {10.1016/j.compchemeng.2021.107513}
}

@article{Cao_2005,
  title = {Informational Content of Option Volume Prior to Takeovers},
  author = {Cao, Charles and Chen, Zhiwu and Griffin, John M.},
  year = {2005},
  journal = {The Journal of Business},
  doi = {10.1086/429654},
  mag_id = {2078301694},
  pmcid = {null},
  pmid = {null}
}

@misc{carionEndtoEndObjectDetection2020,
  title = {End-to-End Object Detection with Transformers},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  year = {2020},
  number = {arXiv:2005.12872},
  eprint = {2005.12872},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{carrionTradeSigningFast2020,
  title = {Trade Signing in Fast Markets},
  author = {Carrion, Allen and Kolay, Madhuparna},
  year = {2020},
  journal = {Financial Review},
  volume = {55},
  number = {3},
  doi = {10.1111/fire.12218}
}

@misc{caruanaObtainingCalibratedProbabilities,
  title = {Obtaining Calibrated Probabilities from Boosting},
  author = {Caruana, Alexandru Niculescu-Mizil Rich},
  note = {\section{Annotations\\
(27/10/2022, 17:39:25)}

\par
``In a recent evaluation of learning algorithms (Caruana \& Niculescu-Mizil 2006), boosted decision trees had excellent performance on metrics such as accuracy, lift, area under the ROC curve, average precision, and precision/recall break even point. However, boosted decision trees had poor squared error and cross-entropy because AdaBoost does not produce good probability estimates.'' (Caruana, p. 28)
\par
``Friedman, Hastie, and Tibshirani (2000) provide an explanation for why boosting makes poorly calibrated predictions. They show that boosting can be viewed as an additive logistic regression model. A consequence of this is that the predictions made by boosting are trying to fit a logit of the true probabilities, as opposed to the true probabilities themselves. To get back the probabilities, the logit transformation must be inverted.'' (Caruana, p. 28)
\par
\section{Annotations\\
(27/10/2022, 17:59:16)}

\par
``In a recent evaluation of learning algorithms (Caruana \& Niculescu-Mizil 2006), boosted decision trees had excellent performance on metrics such as accuracy, lift, area under the ROC curve, average precision, and precision/recall break even point. However, boosted decision trees had poor squared error and cross-entropy because AdaBoost does not produce good probability estimates.'' (Caruana, p. 28)
\par
``Friedman, Hastie, and Tibshirani (2000) provide an explanation for why boosting makes poorly calibrated predictions. They show that boosting can be viewed as an additive logistic regression model. A consequence of this is that the predictions made by boosting are trying to fit a logit of the true probabilities, as opposed to the true probabilities themselves. To get back the probabilities, the logit transformation must be inverted.'' (Caruana, p. 28)}
}

@article{cerdaEncodingHighcardinalityString2022,
  title = {Encoding High-Cardinality String Categorical Variables},
  author = {Cerda, Patricio and Varoquaux, Ga{\"e}l},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {3},
  eprint = {1907.01860},
  eprinttype = {arxiv},
  doi = {10.1109/TKDE.2020.2992529},
  archiveprefix = {arXiv}
}

@article{chakrabartyEvaluatingTradeClassification2015,
  title = {Evaluating Trade Classification Algorithms: Bulk Volume Classification versus the Tick Rule and the Lee-Ready Algorithm},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = {2015},
  journal = {Journal of Financial Markets},
  volume = {25},
  doi = {10.1016/j.finmar.2015.06.001}
}

@article{chakrabartyTradeClassificationAlgorithms2007,
  title = {Trade Classification Algorithms for Electronic Communications Network Trades},
  author = {Chakrabarty, Bidisha and Li, Bingguang and Nguyen, Vanthuan and Van Ness, Robert A.},
  year = {2007},
  journal = {Journal of Banking \& Finance},
  volume = {31},
  number = {12},
  doi = {10.1016/j.jbankfin.2007.03.003}
}

@article{chakrabartyTradeClassificationAlgorithms2012,
  title = {Trade Classification Algorithms: A Horse Race between the Bulk-Based and the Tick-Based Rules},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = {2012},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2182819}
}

@article{Chakraborti_2010,
  title = {Econophysics: Empirical Facts and Agent-Based Models},
  author = {Chakraborti, Anirban and Toke, Ioane Muni and Toke, Ioane Muni and Patriarca, Marco and Abergel, Fr{\'e}d{\'e}ric},
  year = {2010},
  journal = {arXiv: General Finance},
  doi = {null},
  mag_id = {1587470796},
  pmcid = {null},
  pmid = {null}
}

@article{Chakravarty_2004,
  title = {Informed Trading in Stock and Option Markets},
  author = {Chakravarty, Sugato and Gulen, Huseyin and Mayhew, Stewart},
  year = {2004},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.2004.00661.x},
  mag_id = {2155794029},
  pmcid = {null},
  pmid = {null}
}

@article{Chan_1995,
  title = {The Behavior of Stock Prices around Institutional Trades},
  author = {Chan, Louis K.C. and Lakonishok, Josef},
  year = {1995},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1995.tb04053.x},
  mag_id = {1992426919},
  pmcid = {null},
  pmid = {null}
}

@article{Chan_2002,
  title = {The Informational Role of Stock and Option Volume},
  author = {Chan, Kalok and Chung, Y. Peter and Fong, Wai-Ming},
  year = {2002},
  journal = {Review of Financial Studies},
  doi = {10.2139/ssrn.170356},
  mag_id = {2158240268},
  pmcid = {null},
  pmid = {null}
}

@article{chanAlgorithmicTrading,
  title = {Algorithmic Trading},
  author = {Chan, Ernie},
  doi = {10.1002/9781118676998}
}

@misc{chanTransformersGeneralizeDifferently2022,
  title = {Transformers Generalize Differently from Information Stored in Context vs in Weights},
  author = {Chan, Stephanie C. Y. and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K. and Hill, Felix},
  year = {2022},
  number = {arXiv:2210.05675},
  eprint = {2210.05675},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@inproceedings{chapelleSemiSupervisedClassificationLow2005,
  title = {Semi-Supervised Classification by Low Density Separation},
  booktitle = {Proceedings of the {{Tenth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chapelle, Olivier and Zien, Alexander},
  year = {2005},
  note = {\section{Annotations\\
(27/10/2022, 07:56:16)}

\par
``The goal ofsemi-sup ervised classi\O cation istouseunlabeleddata toimpro vethegeneralization. The cluster assumption states that thedecision boundary should notcross high densityregions, butinstead lieinlow densityregions.'' (Chapelle and Zien, 2005, p. 1)
\par
``Webelievethat virtually allsuccessful semi-supervised algorithms utilize thecluster assumption, though most ofthetime indirectly.'' (Chapelle and Zien, 2005, p. 1)
\par
``According tothe cluster assumption, the decision boundary should preferably notcutclusters.'' (Chapelle and Zien, 2005, p. 2)}
}

@book{chapelleSemisupervisedLearning2006,
  title = {Semi-Supervised Learning},
  editor = {Chapelle, Olivier and Sch{\"o}lkopf, Bernhard and Zien, Alexander},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}}
}

@article{chaumUntraceableElectronicMail1981,
  title = {Untraceable Electronic Mail, Return Addresses, and Digital Pseudonyms},
  author = {Chaum, David L.},
  year = {1981},
  journal = {Communications of the ACM},
  volume = {24},
  number = {2},
  doi = {10.1145/358549.358563}
}

@misc{cheferGenericAttentionmodelExplainability2021,
  title = {Generic Attention-Model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2021},
  number = {arXiv:2103.15679},
  eprint = {2103.15679},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(08/01/2023, 15:26:21)}

\par
``In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure selfattention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention.'' (Chefer et al., 2021, p. 1)
\par
``a residual connection, as shown in Fig. 1, we accumulate the relevancies by adding each layer's contribution to the aggregated relevancies, similar to [1] in which the identity matrix is added to account for residual connections.'' (Chefer et al., 2021, p. 3)
\par
``Our method uses the attention map A of each attention layer to update the relevancy maps. Since each such map is comprised of h heads, we follow [5] and use gradients to average across heads. Note that Voita et al. [41] show that attention heads differ in importance and relevance, thus a simple average across heads results in distorted relevancy maps. The final attention map{\= } A {$\in$} Rs\texttimes q of our method is then defined as follows:{\= } A = Eh(({$\nabla$}A A)+) (5) where is the Hadamard product, {$\nabla$}A := {$\partial$}yt {$\partial$}A for yt which is the model's output for the class we wish to visualize t, and Eh is the mean across the heads dimension. Following [5] we remove the negative contributions before averaging.'' (Chefer et al., 2021, p. 3)
\par
``For self-attention layers that satisfy{\= } A {$\in$} Rs\texttimes s the update rules for the affected aggregated relevancy scores are: Rss = Rss +{\= } A {$\cdot$} Rss (6) Rsq = Rsq +{\= } A {$\cdot$} Rsq (7)'' (Chefer et al., 2021, p. 3)
\par
``In Eq. 6 we account for the fact that the tokens were already contextualized in previous attention layers by applying matrix multiplication with the aggregated self-attention matrix Rss, as done in [1, 5].'' (Chefer et al., 2021, p. 3)
\par
``Since we initialized Rxx = Ix\texttimes x, and Eq. 6 accumulates the relevancy matrices at each layer, we can consider an aggregated self-attention matrix Rxx as a matrix comprised of two parts, the first is the identity matrix from the initialization, and the second, \textasciicircum{} Rxx = Rxx - Ix\texttimes x is the matrix created by the aggregation of self-attention across the layers'' (Chefer et al., 2021, p. 3)
\par
``Since Eq. 5 uses gradients to average across heads, the values of \textasciicircum{} Rxx are typically reduced. We wish to account equally both for the fact that each token influences itself and for the contextualization by the selfattention mechanism. Therefore, we normalize each row in \textasciicircum{} Rxx so that it sums to 1. Intuitively, row i in \textasciicircum{} Rxx disclosed the self-attention value of each token w.r.t. the i-th token, and the identity matrix Ix\texttimes x sets that value for each token'' (Chefer et al., 2021, p. 3)}
}

@inproceedings{cheferTransformerInterpretabilityAttention2021,
  title = {Transformer Interpretability beyond Attention Visualization},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2021},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00084},
  note = {\section{Annotations\\
(08/01/2023, 15:19:31)}

\par
``For comparison, using the same notation, the rollout [1] method is given by: \textasciicircum{} A(b) = I + EhA(b) (15) rollout = \textasciicircum{} A(1) {$\cdot$} \textasciicircum{} A(2) {$\cdot$} . . . {$\cdot$} \textasciicircum{} A(B) (16) We can observe that the result of rollout is fixed given an input sample, regardless of the target class to be visualized. In addition, it does not consider any signal, except for the pairwise attention scores.'' (Chefer et al., 2021, p. 786)}
}

@article{Chen_1998,
  title = {Atomic Decomposition by Basis Pursuit},
  author = {Chen, Scott and Donoho, David L. and Saunders, Michael A.},
  year = {1998},
  journal = {SIAM Journal on Scientific Computing},
  doi = {10.1137/s1064827596304010},
  mag_id = {1986931325},
  pmcid = {null},
  pmid = {null}
}

@misc{chenDeepLearningAsset2021,
  title = {Deep Learning in Asset Pricing},
  author = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
  year = {2021},
  number = {1904.00745},
  eprint = {1904.00745},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{chenExcelFormerNeuralNetwork2023,
  title = {{{ExcelFormer}}: A Neural Network Surpassing Gbdts on Tabular Data},
  author = {Chen, Jintai and Yan, Jiahuan and Chen, Danny Ziyi and Wu, Jian},
  year = {2023},
  number = {arXiv:2301.02819},
  eprint = {2301.02819},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(24/01/2023, 11:52:28)}

\par
``The investigation in (Grinsztajn et al., 2022) pointed out three inherent characteristics of tabular data that impeded known neural networks from top-tier performances, including irregular patterns of the target function, the negative effects of uninformative features, and the nonrotationally-invariant features. Based on this, we furthermore identify two points that highly promote the capabilities of neural networks on tabular data. (i) An appropriate feature embedding approach. Though it was demonstrated (Rahaman et al., 2019; Grinsztajn et al., 2022) that neural networks are likely to predict overly smooth solutions on tabular data, a deep learning model was also observed to be capable of memorizing random labels (Zhang et al., 2021). Since the target function patterns are irregular and spurious correlations between the targets and features exist, an appropriate feature embedding network should well fit the irregular patterns while maintaining generalizability. (ii) A careful feature interaction approach. Since features of tabular data are non-rotationally-variant and a considerable portion of them are uninformative, it harms the generalization when a model incorporates needless feature interactions.'' (Chen et al., 2023, p. 1)
\par
``Some previous approaches either designed feature embedding approaches (Gorishniy et al., 2022) to alleviate overly smooth solutions inspired by (Tancik et al., 2020) or employed regularization (Katzir et al., 2020) and shallow models (Cheng et al., 2016) to promote the model generalization, while some neural networks were equipped with sophisticated feature interaction approaches (Yan et al., 2023; Chen et al., 2022; Gorishniy et al., 2021) for better selectively feature interactions.'' (Chen et al., 2023, p. 1)
\par
``Apart from model designs, various data representation approaches, such as feature embedding (Gorishniy et al., 2022), discretization of continuous features (Guo et al., 2021; Wang et al., 2020), and rule search approaches (Wang et al., 2021), were proposed against the irregular target patterns (Tancik et al., 2020; Grinsztajn et al., 2022).'' (Chen et al., 2023, p. 2)}
}

@inproceedings{chengWideDeepLearning2016,
  title = {Wide \& {{Deep Learning}} for {{Recommender Systems}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Deep Learning}} for {{Recommender Systems}}},
  author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
  year = {2016},
  publisher = {{ACM}},
  address = {{Boston MA USA}},
  doi = {10.1145/2988450.2988454},
  note = {\section{Annotations\\
(26/01/2023, 06:24:43)}

\par
``The deep component is a feed-forward neural network, as shown in Figure 1 (right). For categorical features, the original inputs are feature strings (e.g., ``language=en''). Each of these sparse, high-dimensional categorical features are first converted into a low-dimensional and dense real-valued vector, often referred to as an embedding vector. The dimensionality of the embeddings are usually on the order of O(10) to O(100). The embedding vectors are initialized randomly and then the values are trained to minimize the final loss function during model training. These low-dimensional dense embedding vectors are then fed into the hidden layers of a neural network in the forward pass.'' (Cheng et al., 2016, p. 2)}
}

@misc{chenTrainingDeepNets2016,
  title = {Training Deep Nets with Sublinear Memory Cost},
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  year = {2016},
  number = {arXiv:1604.06174},
  eprint = {1604.06174},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: A Scalable Tree Boosting System},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  doi = {10.1145/2939672.2939785},
  archiveprefix = {arXiv}
}

@misc{cholakovGatedTabTransformerEnhancedDeep2022,
  title = {The {{GatedTabTransformer}}. {{An}} Enhanced Deep Learning Architecture for Tabular Modeling},
  author = {Cholakov, Radostin and Kolev, Todor},
  year = {2022},
  number = {arXiv:2201.00199},
  eprint = {2201.00199},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 10 pages, 6 figures}
}

@article{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = {2017},
  journal = {arXiv:1706.03741 [cs, stat]},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{chuanSuccessAdaBoostIts2021,
  title = {The Success of {{AdaBoost}} and Its Application in Portfolio Management},
  author = {Chuan, Yijian and Zhao, Chaoyi and He, Zhenrui and Wu, Lan},
  year = {2021},
  journal = {International Journal of Financial Engineering},
  volume = {08},
  number = {02},
  doi = {10.1142/S2424786321420019}
}

@misc{clarkELECTRAPretrainingText2020,
  title = {Electra: Pre-Training Text Encoders as Discriminators Rather than Generators},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  number = {arXiv:2003.10555},
  eprint = {2003.10555},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(24/10/2022, 16:54:02)}

\par
``Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out'' (Clark et al., 2020, p. 1)
\par
``As an alternative, we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. Instead of masking, our method corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language model. This corruption procedure solves a mismatch in BERT (although not in XLNet) where the network sees artificial [MASK] tokens during pre-training but not when being fine-tuned on downstream tasks. We then pre-train the network as a discriminator that predicts for every token whether it is an original or a replacement.'' (Clark et al., 2020, p. 1)
\par
``In contrast, MLM trains the network as a generator that predicts the original identities of the corrupted tokens.'' (Clark et al., 2020, p. 1)
\par
``A key advantage of our discriminative task is that the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient.'' (Clark et al., 2020, p. 1)
\par
``We call our approach ELECTRA1 for ``Efficiently Learning an Encoder that Classifies Token Replacements Accurately.'' (Clark et al., 2020, p. 2)
\par
``An overview of replaced token detection. The generator can be any model that produces an output distribution over tokens, but we usually use a small masked language model that is trained jointly with the discriminator. Although the models are structured like in a GAN, we train the generator with maximum likelihood rather than adversarially due to the difficulty of applying GANs to text. After pre-training, we throw out the generator and only fine-tune the discriminator (the ELECTRA model) on downstream tasks.'' (Clark et al., 2020, p. 3)
\par
``Our approach trains two neural networks, a generator G and a discriminator D. Each one primarily consists of an encoder (e.g., a Transformer network) that maps a sequence on input tokens x = [x1, ..., xn] into a sequence of contextualized vector representations h(x) = [h1, ..., hn]. For a given position t, (in our case only positions where xt = [MASK]), the generator outputs a probability for generating a particular token xt with a softmax layer: pG(xt|x) = exp (e(xt)T hG(x)t )/ {$\sum$} x{${'}$} exp (e(x{${'}$})T hG(x)t ) where e denotes token embeddings. For a given position t, the discriminator predicts whether the token xt is ``real,'' i.e., that it comes from the data rather than the generator distribution, with a sigmoid output layer: D(x, t) = sigmoid(wT hD(x)t) The generator is trained to perform masked language modeling (MLM). Given an input x = [x1, x2, ..., xn], MLM first select a random set of positions (integers between 1 and n) to mask out m = [m1, ..., mk].3 The tokens in the selected positions are replaced with a [MASK] token: we denote this as xmasked = REPLACE(x, m, [MASK]). The generator then learns to predict the original identities of the masked-out tokens. The discriminator is trained to distinguish tokens in the data from tokens that have been replaced by generator samples. More specifically, we create a corrupted example xcorrupt by replacing the masked-out tokens with generator samples and train the discriminator to predict which tokens in xcorrupt match the original input x. Formally, model inputs are constructed according to mi {$\sim$} unif\{1, n\} for i = 1 to k xmasked = REPLACE(x, m, [MASK]) \textasciicircum{} xi {$\sim$} pG(xi|xmasked) for i {$\in$} m xcorrupt = REPLACE(x, m, \textasciicircum{} x) and the loss functions are LMLM(x, \texttheta G) = E ( {$\sum$} i{$\in$}m - log pG(xi|xmasked) ) LDisc(x, \texttheta D) = E (n {$\sum$} t=1 -1(xcorrupt t = xt) log D(xcorrupt, t) - 1(xcorrupt t 6= xt) log(1 - D(xcorrupt, t)) ) Although similar to the training objective of a GAN, there are several key differences. First, if the generator happens to generate the correct token, that token is considered ``real'' instead of ``fake''; we found this formulation to moderately improve results on downstream tasks. More importantly, the generator is trained with maximum likelihood rather than being trained adversarially to fool the discriminator. Adversarially training the generator is challenging because it is impossible to backpropagate through sampling from the generator. Although we experimented circumventing this issue 3Typically k = d0.15ne, i.e., 15\% of the tokens are masked out.'' (Clark et al., 2020, p. 3)
\par
``Published as a conference paper at ICLR 2020 by using reinforcement learning to train the generator (see Appendix F), this performed worse than maximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input, as is typical with a GAN. We minimize the combined loss min \texttheta G ,\texttheta D {$\sum$} x{$\in$}X LMLM(x, \texttheta G) + {$\lambda$}LDisc(x, \texttheta D) over a large corpus X of raw text. We approximate the expectations in the losses with a single sample. We don't back-propagate the discriminator loss through the generator (indeed, we can't because of the sampling step). After pre-training, we throw out the generator and fine-tune the discriminator on downstream tasks.'' (Clark et al., 2020, p. 4)}
}

@article{congDEEPSEQUENCEMODELING,
  title = {Deep Sequence Modeling: Development and Applications in Asset Pricing},
  author = {Cong, Lin William and Tang, Ke and Wang, Jingyuan and Zhang, Yang}
}

@article{Cont_2010,
  title = {A Stochastic Model for Order Book Dynamics},
  author = {Cont, Rama and Stoikov, Sasha and Talreja, Rishi},
  year = {2010},
  journal = {Operations Research},
  doi = {10.1287/opre.1090.0780},
  mag_id = {2145533760},
  pmcid = {null},
  pmid = {null}
}

@article{Cont_2013,
  title = {Price Dynamics in a Markovian Limit Order Market},
  author = {Cont, Rama and {\noopsort{larrard}}{de Larrard}, Adrien},
  year = {2013},
  journal = {Siam Journal on Financial Mathematics},
  doi = {10.1137/110856605},
  mag_id = {3121639422},
  pmcid = {null},
  pmid = {null}
}

@article{Copeland_1983,
  title = {Information Effects on the Bid-ask Spread},
  author = {Copeland, Thomas E. and Galai, Dan},
  year = {1983},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1983.tb03834.x},
  mag_id = {1966977762},
  pmcid = {null},
  pmid = {null}
}

@article{cowgillAlgorithmicFairnessEconomics,
  title = {Algorithmic Fairness and Economics},
  author = {Cowgill, Bo and Tucker, Catherine}
}

@inbook{coxExploratoryDataAnalysis2017,
  title = {Exploratory {{Data Analysis}}: {{What Data Do I Have}}?},
  booktitle = {Translating {{Statistics}} to {{Make Decisions}}},
  author = {Cox, Victoria},
  year = {2017},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-2256-0_3},
  collaborator = {Cox, Victoria}
}

@article{coxOptionPricingSimplified1979,
  title = {Option Pricing: A Simplified Approach},
  author = {Cox, John C and Ross, A},
  year = {1979},
  doi = {10.1016/0304-405X(79)90015-1}
}

@article{coxRelationForwardPrices1981,
  title = {The Relation between Forward Prices and Futures Prices},
  author = {Cox, John C.},
  year = {1981},
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405X(81)90002-7}
}

@article{creamerAutomatedTradingBoosting2010,
  title = {Automated Trading with Boosting and Expert Weighting},
  author = {Creamer, Germ{\'a}n and Freund, Yoav},
  year = {2010},
  journal = {Quantitative Finance},
  volume = {10},
  number = {4},
  doi = {10.1080/14697680903104113}
}

@article{crspDATADESCRIPTIONSGUIDE,
  title = {Data Descriptions Guide Crsp Us Stock \& Us Index Databases},
  author = {{CRSP}}
}

@misc{culurcielloFallRNNLSTM2019,
  title = {The Fall of {{RNN}} / {{LSTM}}},
  author = {Culurciello, Eugenio},
  year = {2019},
  journal = {Medium},
  howpublished = {https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0}
}

@misc{culurcielloMemoryAttentionSequences2018,
  title = {Memory, Attention, Sequences},
  author = {Culurciello, Eugenio},
  year = {2018},
  journal = {Medium},
  howpublished = {https://towardsdatascience.com/memory-attention-sequences-37456d271992}
}

@misc{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: Attentive Language Models beyond a Fixed-Length Context},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  number = {arXiv:1901.02860},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: ACL 2019 long paper. Code and pretrained models are available at https://github.com/kimiyoung/transformer-xl}
}

@misc{darabiContrastiveMixupSelf2021,
  title = {Contrastive {{Mixup}}: {{Self-}} and {{Semi-Supervised}} Learning for {{Tabular Domain}}},
  author = {Darabi, Sajad and Fazeli, Shayan and Pazoki, Ali and Sankararaman, Sriram and Sarrafzadeh, Majid},
  year = {2021},
  number = {arXiv:2108.12296},
  eprint = {2108.12296},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@misc{dauphinLanguageModelingGated2017,
  title = {Language Modeling with Gated Convolutional Networks},
  author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
  year = {2017},
  number = {arXiv:1612.08083},
  eprint = {1612.08083},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{davisGradientBoostingQuantitative2019,
  title = {Gradient Boosting for Quantitative Finance},
  author = {Davis, Jesse and Devos, Laurens and Reyners, Sofie and Schoutens, Wim},
  year = {2019},
  journal = {The Journal of Computational Finance},
  volume = {24},
  number = {4},
  doi = {10.21314/JCF.2020.403}
}

@misc{dehghaniUniversalTransformers2019,
  title = {Universal Transformers},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  year = {2019},
  number = {arXiv:1807.03819},
  eprint = {1807.03819},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Published at ICLR2019}
}

@article{deisenrothMathematicsMachineLearning,
  title = {Mathematics for Machine Learning},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  doi = {10.1017/9781108679930}
}

@article{dengStrategicTradingManipulation,
  title = {Strategic Trading and Manipulation: Machine Learning in Limit Order Markets},
  author = {Deng, Xinyi and He, Xue-Zhong}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  volume = {1},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, MN}},
  doi = {10.18653/v1/N19-1423},
  note = {\section{Annotations\\
(24/10/2022, 16:03:05)}

\par
``e argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.'' (Devlin et al., 2019, p. 4171)
\par
``The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.'' (Devlin et al., 2019, p. 4172)
\par
``We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.'' (Devlin et al., 2019, p. 4172)
\par
``During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.'' (Devlin et al., 2019, p. 4173)
\par
``A distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture.'' (Devlin et al., 2019, p. 4173)
\par
``Task \#1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly ``see itself'', and the model could trivially predict the target word in a multi-layered context. former is often referred to as a ``Transformer encoder'' while the left-context-only version is referred to as a ``Transformer decoder'' since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a ``masked LM'' (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15\% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input. Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace ``masked'' words with the actual [MASK] token. The training data generator chooses 15\% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80\% of the time (2) a random token 10\% of the time (3) the unchanged i-th token 10\% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.'' (Devlin et al., 2019, p. 4174)
\par
``Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers.'' (Devlin et al., 2019, p. 4175)}
}

@article{dhondtArtificialIntelligenceAlter2020,
  title = {Artificial Intelligence Alter Egos: Who Might Benefit from Robo-Investing?},
  author = {D'Hondt, Catherine and De Winne, Rudy and Ghysels, Eric and Raymond, Steve},
  year = {2020},
  journal = {Journal of Empirical Finance},
  volume = {59},
  doi = {10.1016/j.jempfin.2020.10.002}
}

@article{dieboldComparingPredictiveAccuracy1995,
  title = {Comparing Predictive Accuracy},
  author = {Diebold, Francis X. and Mariano, Roberto S.},
  year = {1995},
  journal = {Journal of Business \& Economic Statistics},
  volume = {13},
  number = {3},
  doi = {10.1080/07350015.1995.10524599}
}

@inproceedings{dongTablePretrainingSurvey2022,
  title = {Table {{Pre-training}}: {{A Survey}} on {{Model Architectures}}, {{Pre-training Objectives}}, and {{Downstream Tasks}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Dong, Haoyu and Cheng, Zhoujun and He, Xinyi and Zhou, Mengyu and Zhou, Anda and Zhou, Fan and Liu, Ao and Han, Shi and Zhang, Dongmei},
  year = {2022},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Vienna, Austria}},
  doi = {10.24963/ijcai.2022/761}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(31/01/2023, 10:27:56)}

\par
``Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches (z00 = xclass), whose state at the output of the Transformer encoder (z0 L) serves as the image representation y (Eq. 4).'' (Dosovitskiy et al., 2021, p. 3)}
}

@article{Easley_1987,
  title = {{{PRICE}}, {{TRADE SIZE}}, {{AND INFORMATION IN SECURITIES MARKETS}}*},
  author = {Easley, David and O'Hara, Maureen},
  year = {1987},
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405x(87)90029-8},
  mag_id = {1979986072},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_1992,
  title = {Time and the Process of Security Price Adjustment},
  author = {Easley, David and O'Hara, Maureen},
  year = {1992},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1992.tb04402.x},
  mag_id = {2033096392},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_1996,
  title = {Liquidity, Information, and Infrequently Traded Stocks},
  author = {Easley, David and Kiefer, Nicholas M. and O'Hara, Maureen and Paperman, Joseph B.},
  year = {1996},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1996.tb04074.x},
  mag_id = {2100746131},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_1998,
  title = {Option Volume and Stock Prices: Evidence on Where Informed Traders Trade},
  author = {Easley, David and O'Hara, Maureen and Srinivas, P.S.},
  year = {1998},
  journal = {null},
  doi = {null},
  mag_id = {3122983259},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_2002,
  title = {Is Information Risk a Determinant of Asset Returns},
  author = {Easley, David and Hvidkjaer, Soeren and O'Hara, Maureen},
  year = {2002},
  journal = {Journal of Finance},
  doi = {10.1111/1540-6261.00493},
  mag_id = {2100821524},
  pmcid = {null},
  pmid = {null}
}

@article{Easley_2012,
  title = {Bulk Classification of Trading Activity},
  author = {Easley, David and {\noopsort{prado}}{de Prado}, Marcos Lopez and O'Hara, Maureen},
  year = {2012},
  journal = {null},
  doi = {null},
  mag_id = {1547818237},
  pmcid = {null},
  pmid = {null}
}

@article{easleyDiscerningInformationTrade2016,
  title = {Discerning Information from Trade Data},
  author = {Easley, David and {\noopsort{prado}}{de Prado}, Marcos Lopez and O'Hara, Maureen},
  year = {2016},
  journal = {Journal of Financial Economics},
  volume = {120},
  number = {2},
  doi = {10.1016/j.jfineco.2016.01.018}
}

@article{easleyFlowToxicityLiquidity2012,
  title = {Flow Toxicity and Liquidity in a High-Frequency World},
  author = {Easley, David and {L{\'o}pez de Prado}, Marcos M. and O'Hara, Maureen},
  year = {2012},
  journal = {Review of Financial Studies},
  volume = {25},
  number = {5},
  doi = {10.1093/rfs/hhs053}
}

@article{easleyOptionVolumeStock1998,
  title = {Option Volume and Stock Prices: Evidence on Where Informed Traders Trade},
  author = {Easley, David and O'Hara, Maureen and Srinivas, P.S.},
  year = {1998},
  journal = {The Journal of Finance},
  volume = {53},
  number = {2},
  doi = {10.1111/0022-1082.194060},
  note = {\section{Annotations\\
(23/10/2022, 10:43:53)}

\par
``The Berkeley Options Data Base does not classify trades as buyer-initiated or seller-initiated. This classification must be done using quote and trade information.'' (Easley et al., 1998, p. 453)
\par
``For researchers using transactions data, this classification problem is ubiquitous, and a cursory review of empirical papers using equity transaction data reveals many trade classification techniques. Lee and Ready (1991) present an excellent survey of techniques currently in use and evaluate their efficiency using NYSE transactions data.'' (Easley et al., 1998, p. 453)
\par
``The approach we pursue is as follows. For each trade, the active quote is identified. Then, we identify the trade as a buy or a sell by the following algorithm: 1. Trades occurring in the lower half of the spread, at the bid or below, are classified as sells. A similar scheme is used for trades in the upper half of the spread and these are classified as buys. Trades occurring below the bid or above the ask are classified similarly. 2. Trades occurring at the midpoint of the spread are first classified using the ``tick test'' applied to the previous trade. If the current trade price occurs at a price higher than the previous one, it is classified as a buy (trade on an uptick). A trade on a downtick is classified as a sell. Trades unclassifiable using the previous trade are classified using the ``zerouptick'' or the ``zero-downtick'' test, which identifies the last price change and then uses the tick test strategy.'' (Easley et al., 1998, p. 453)
\par
``On the CBOE, however, the frequency of quote revisions is less than five seconds, and so using the most recent quote does not cause a bias in classification. The classification scheme we use captures the natural concept that buys tend to go at higher prices, and sales at lower prices. We caution, however, that trading and reporting protocols on the CBOE may introduce timing difficulties in the data, and to the extent that these are large, our classification scheme will be affected.'' (Easley et al., 1998, p. 453)
\par
``First, the percentage of trades going off at the midpoint of the spread is far lower in CBOE trades than in NYSE trades. Vijh offers the explanation that the market design of the CBOE\textemdash a competitive dealer system\textemdash might be the cause of this phenomenon as marketmakers offer their lowest quotes, and hence are not willing to bargain on transactions prices. An alternative explanation is that if informed trading occurs on the CBOE, and, if it is harder to detect given the multiplicity of dealers, then marketmakers protect themselves by trading at quoted prices more often'' (Easley et al., 1998, p. 454)
\par
``A second observation from Table II is that, over time, the percentage of trades executed at the spread midpoint shows a strong downward trend. This should make trade data more easily classifiable using quote data alone. Also, although studies of NYSE transactions report a roughly even split between buys and sells, it is clear that trades on the CBOE are increasingly buys. Hence, options are actively bought, rather than sold. This strengthens the argument against using transactions prices in studies of option marketstock market interactions, as these prices are more likely to be at the ask than at the bid and, hence, would bias upward the implied stock price.'' (Easley et al., 1998, p. 454)}
}

@article{elhage2021mathematical,
  title = {A Mathematical Framework for Transformer Circuits},
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2021},
  journal = {Transformer Circuits Thread},
  note = {https://transformer-circuits.pub/2021/framework/index.html}
}

@article{ellisAccuracyTradeClassification2000,
  title = {The Accuracy of Trade Classification Rules: Evidence from Nasdaq},
  author = {Ellis, Katrina and Michaely, Roni and O'Hara, Maureen},
  year = {2000},
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {35},
  number = {4},
  doi = {10.2307/2676254}
}

@book{falkPracticalRecommenderSystems2019,
  title = {Practical Recommender Systems},
  author = {Falk, Kim},
  year = {2019},
  publisher = {{Manning}},
  address = {{Shelter Island, NY}}
}

@article{famaCAPMWantedDead1996,
  title = {The {{CAPM}} Is Wanted, Dead or Alive},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = {1996},
  journal = {The Journal of Finance},
  volume = {51},
  number = {5},
  doi = {10.1111/j.1540-6261.1996.tb05233.x},
  note = {The survivors are likely to have unexpectedly high returns in the turnaround years immediately preceding their inclusion on COMPUSTAT. Since COMPUSTAT typically includes some historical data when it adds firms, there can be positive survivor bias in the returns of high-BE/ME firms on COMPUSTAT.}
}

@article{famaCommonRiskFactors1993,
  title = {Common Risk Factors in the Returns on Stocks and Bonds},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = {1993},
  journal = {Journal of Financial Economics},
  volume = {33},
  number = {1},
  doi = {10.1016/0304-405X(93)90023-5}
}

@article{famaFivefactorAssetPricing2015,
  title = {A Five-Factor Asset Pricing Model},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = {2015},
  journal = {Journal of Financial Economics},
  volume = {116},
  number = {1},
  doi = {10.1016/j.jfineco.2014.10.010}
}

@article{fardRecommenderSystemBased2013,
  title = {Recommender System Based on Semantic Similarity},
  author = {Fard, Karamollah Bagheri and Nilashi, Mehrbakhsh and Rahmani, Mohsen and Ibrahim, Othman},
  year = {2013},
  volume = {3},
  number = {6}
}

@article{fawziDiscoveringFasterMatrix2022,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and {Romera-Paredes}, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  year = {2022},
  journal = {Nature},
  volume = {610},
  number = {7930},
  doi = {10.1038/s41586-022-05172-4}
}

@article{fedeniaMachineLearningCorporate2021,
  title = {Machine Learning in the Corporate Bond Market and beyond: A New Classifier},
  author = {Fedenia, Mark A. and Nam, Seunghan and Ronen, Tavy},
  year = {2021},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3848068}
}

@misc{federalreservebankofstlouisNBERBasedRecession2022,
  title = {{{NBER}} Based Recession Indicators for the United States from the Period Following the Peak through the Trough [{{USREC}}]},
  author = {{Federal Reserve Bank of St. Louis}},
  year = {2022},
  journal = {FRED, Federal Reserve Bank of St. Louis},
  howpublished = {https://fred.stlouisfed.org/series/USREC},
  note = {Last updated: 2022-07-01 18:01:02-05}
}

@article{feldhutterSameBondDifferent2012,
  title = {The Same Bond at Different Prices: Identifying Search Frictions and Selling Pressures},
  author = {Feldh{\"u}tter, Peter},
  year = {2012},
  journal = {The Review of Financial Studies},
  volume = {25},
  number = {4},
  doi = {10.1093/rfs/hhr093}
}

@article{fengDeepLearningPredicting2018,
  title = {Deep Learning for Predicting Asset Returns},
  author = {Feng, Guanhao and He, Jingyu and Polson, Nicholas G.},
  year = {2018},
  journal = {arXiv:1804.09314},
  eprint = {1804.09314},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{fengLogtransformationItsImplications2014,
  title = {Log-Transformation and Its Implications for Data Analysis},
  author = {Feng, Changyong and Wang, Hongyue and Lu, Naiji and Chen, Tian and He, Hua and Lu, Ying and Tu, Xin M},
  year = {2014},
  volume = {26},
  number = {2},
  note = {\section{Annotations\\
(04/12/2022, 12:09:23)}

\par
``Despite the common belief that the log transformation can decrease the variability of data and make data conform more closely to the normal distribution, this is usually not the case. Moreover, the results of standard statistical tests performed on log-transformed data are often not relevant for the original, non-transformed data'' (Feng et al., 2014, p. 105)
\par
``The log transformation is, arguably, the most popular among the different types of transformations used to transform skewed data to approximately conform to normality.'' (Feng et al., 2014, p. 106)
\par
``In general, for right-skewed data, the logtransformation may make it either right- or left-skewed. If the original data does follow a log-normal distribution,'' (Feng et al., 2014, p. 106)
\par
``the log-transformed data will follow or approximately follow the normal distribution. However, in general there is no guarantee that the log-transformation will reduce skewness and make the data a better approximation of the normal distribution.'' (Feng et al., 2014, p. 106)
\par
``Another popular use of the log transformation is to reduce the variability of data, especially in data sets that include outlying observations'' (Feng et al., 2014, p. 106)
\par
``Once the data is log-transformed, many statistical methods, including linear regression, can be applied to model the resulting transformed data. For example, the mean of the log-transformed observations (log yi), LT =(1/n)*{$\Sigma$}i=1 log yi is often used to estimate the population mean of the original data by applying the anti-log (i.e., exponential) function to obtain exp( LT). However, this inversion of the mean log value does not usually result in an appropriate estimate of the mean of the original data.'' (Feng et al., 2014, p. 107)}
}

@misc{fiedlerSimpleModificationsImprove2021,
  title = {Simple Modifications to Improve Tabular Neural Networks},
  author = {Fiedler, James},
  year = {2021},
  number = {arXiv:2108.03214},
  eprint = {2108.03214},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(27/10/2022, 08:38:22)}

\par
``Gradient boosted decision trees (GBDTs) are very good general-purpose models, and in fact are frequently used by tabular deep learning models as both inspiration and the standard by which to measure performance.'' (Fiedler, 2021, p. 1)
\par
``Possibly the biggest advantage is that neural networks have the potential to be end-to-end learners, removing the need for manual categorical encoding or other feature engineering. NN models also allow more training options than GBDTs. For example, they allow continual training on streaming data, and more generally allow adjustment of learned NN parameters by training on new data. Neural network models are also better suited to unsupervised pre-training; for example, see (Arik and Pfister 2020) and (Huang et al. 2020)'' (Fiedler, 2021, p. 1)
\par
``Several simple modifications that make MLP, PNN, and AutoInt models perform on par with, or better than, recent general-purpose tabular NNs and GBDTs 2. Model comparisons across a broad range of datasets showing the effectiveness of the proposed modifications 3. A demonstration of how one modification in particular contributes to model interpretability'' (Fiedler, 2021, p. 2)
\par
``GBN allows the use of large batch sizes, but with batch norm parameters calculated on smaller sub-batches. One big motivation for using GBN here is to speed up training, but (Hoffer, Hubara, and Soudry 2018) also showed that GBN improves generalization when using large batch sizes.'' (Fiedler, 2021, p. 2)
\par
``Leaky Gates will also be used in all of the models. These are a combination of two simple elements, an element-wise linear transformation followed by a LeakyReLU activation.'' (Fiedler, 2021, p. 2)
\par
``The results for MLP+, PNN, and AutoInt are compared against \textbullet{} Logistic regression (LR) \textbullet{} Gradient boosted decision trees, specifically, LightGBM (Ke et al. 2017) \textbullet{} A simple MLP model, created by removing layers from the TabTransformer model (see (Huang et al. 2020), \textsection 3.1, paragraph 1) \textbullet{} A sparse MLP, based on (Morcos et al. 2019) \textbullet{} TabTransformer (Huang et al. 2020) \textbullet{} TabNet (Arik and Pfister 2020) \textbullet{} Variational Information Bottleneck (VIB) (Alemi et al. 2017)'' (Fiedler, 2021, p. 5)
\par
``In particular, AutoInt, PNN, and MLP+ seem to outperform the recently-introduced TabTransformer and TabNet models.'' (Fiedler, 2021, p. 5)
\par
``The datasets and experiment setup were taken from (Huang et al. 2020) specifically to avoid choices that potentially favored the new models over, e.g., TabTransformer or TabNet.'' (Fiedler, 2021, p. 5)
\par
``A very recent article (Kadra et al. 2021) showed that MLPs with a ``cocktail'' of regularization strategies can obtain excellent performance. In that paper the MLP architecture was fixed and optimization focused on selecting regularization techniques from five categories: 1. Weight decay, e.g., `1, `2 regularization 2. Data augmentation, e.g., Cut-Out (DeVries and Taylor 2017) and Mix-Up (Zhang et al. 2018) 3. Model averaging, e.g., dropout and explicit average of models 4. Structural and linearization, e.g., skip layers 5. Implicit, e.g., batch normalization The approach was tested on a large collection of datasets and obtained better overall performance than GBDTs from XGBoost (Chen and Guestrin 2016) and models such as TabNet, Neural Oblivious Decision Ensembles (NODE), (Popov, Morozov, and Babenko 2019), and DNF-Net (Abutbul et al. 2020). There are a lot of similarities between those results and the results here. The MLP+, PNN, and AutoInt models have techniques from 3 of the 5 regularization categories: dropout, skip layers, batch normalization (via Ghost Batch Norm)7, and explicit averaging of sub-components. The results here showed that the modified MLP+ outperformed GBDTs (but using LightGBM instead of XGBoost). The results here go one step further and show that the modifications used for MLP+ can also improve other tabular neural network models'' (Fiedler, 2021, p. 8)
\par
``The results show that GBDTs generally outperform any single recent neural network. Another interesting finding is that the recent neural networks generally perform much better on datasets from their own papers. In other words, the recent tabular neural networks that they look at do not seem to generalize well. One lesson to take away is that the modified models introduced here should be tested on a variety of additional datasets.'' (Fiedler, 2021, p. 9)
\par
``Another recent paper, (Gorishniy et al. 2021), compares GBDTs and tabular neural networks, and argues that GBDTs and neural networks perform well on different problems. It uses GBDTs from XGBoost and CatBoost (Prokhorenkova et al. 2018), and TabNet, NODE, AutoInt (not the modified version used in the current paper) and other neural network models. The neural network models performed better when data was ``homogeneous'', when the concepts measured in the data were the same or very similar from feature to feature. For example, images with each pixel location as a different field would be homogeneous. GBDTs performed better when features were ``heterogeneous'' (Fiedler, 2021, p. 9)}
}

@article{finucaneDirectTestMethods2000,
  title = {A Direct Test of Methods for Inferring Trade Direction from Intra-Day Data},
  author = {Finucane, Thomas J.},
  year = {2000},
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {35},
  number = {4},
  doi = {10.2307/2676255},
  note = {\section{Annotations\\
(24/10/2022, 08:24:51)}

\par
``The most commonly used methods of inferring trade direction are the tick tests, which use changes in trade prices to infer direction; the quote method, which infers trade direction by comparing trade prices to quote'' (Finucane, 2000, p. 554)
\par
``Other researchers, including Hasbrouck ((1991), (1993)), Hausman, Lo, and MacKinlay (1992), Foster and Viswanathan (1993), Hasbrouck and Sofianos (1993), and Harris, Mclnish, and Chakravarty (1995), use the quote method to sign trades: trades above the quoted bid-ask midpoint are classified as buys, trades below the midpoint are classified as sells, and trades at the midpoint are either omitted or classified as trades with indeterminate directio'' (Finucane, 2000, p. 554)
\par
``The tick test classifies trades using previous trade prices to infer trade direction. If the trade occurs at a higher price than the previous trade (an uptick), the trade is classified as a buy. If the trade occurs at a lower price than the previous trade (a downtick) it is classified as a sell. When the price change between trades is zero (a zero tick), the trade is classified using the last price that differs from the current price. The reverse tick test is similar, but uses the next trade price to classify the current trade. If the next trade occurs on an uptick or zero uptick, the current trade is classified as a sell. If the next trade occurs on a downtick or zero downtick, the current trade is classified as a buy.'' (Finucane, 2000, p. 557)
\par
``A limitation of the quote method, which classifies trades above the midpoint of the spread as buys and trades below the midpoint as sells, is that trades that oc? cur at the midpoint of the quoted spread cannot be classifi'' (Finucane, 2000, p. 557)
\par
``Figure 1 illustrates how trades can be misclassified when quotes change. When quotes rise between trades, sales at the bid on upticks and zero upticks will be misclassified as buys by the tick test, but should be correctly classified using quote-based methods. Trade 6a in Panel A of Figure 1 illustrates the case of a sell (at the bid) being misclassified by the tick test on a zero uptick. If quotes are falling, as is the case for trade 6b in Panel B, buys at the ask on downticks and zero downticks will be misclassified as sells by the tick test, and should be correctly classified by quote-based methods. Figure 1 also illustrates the tendency for mid-spread trades to be misclassified by the tick test, even when quotes do not'' (Finucane, 2000, p. 557)
\par
``change. Trade 4a, a sale that occurs on an uptick, is misclassified as a buy, and trade 5b, a buy that occurs on a downtick with constant quotes, is misclassified as a sell. FIGURE 1 lllustrative Trade Sequences for Mid-Spread Trades Panel A - - - Ask Bid ?*? - ?X? - Sell Buy Sell Sell Cross Sell (1a) (2a) (3a) (4a) (5a) (6a) Panel B Bid ?*? ?*? - - - - Bid Sell Sell Buy Buy Buy Buy (1b) (2b) (3b) (4b) (5b) (6b) Solid lines represent ask and bid prices and Xs denote trades. Trade direction is indicated below each trade. Panel A illustrates the case where sells on upticks and zero upticks are misclassified by the tick test when quotes are increasing. Panel B illustrates the case where buys on downticks and zero downticks are misclassified when quotes are falling. Panel B also demonstrates how m'' (Finucane, 2000, p. 558)
\par
``LR assert that the simultaneous arrival of market buy and sell orders that are executed as a cross is an "extremely rare occurrence," but their data does not permit them to empirically verify this assertion. To the extent that such trades are present, the accuracy of all classification methods will be reduced; one side of the trade will always be incorrectly classified'' (Finucane, 2000, p. 558)
\par
``Additionally, trade direction may not always be unambiguously determined. While LR assume that trades generally occur only when a market buy or sell order arrives, trades that do not involve market orders also can occur, such as when two limit orders are crossed. Although the trade can be classified by the tick test or LR's algorithm, the true direction of the trade is ambiguous. Classifying such trades as buys or sells may lead to erroneous conclusions in empirical studies.'' (Finucane, 2000, p. 559)
\par
``The data for this study is extracted from the NYSE's TORQ (trades, orders, quotes) database. The TORQ database covers a representative sample of 144 firms for the three-month period November 1990 through January 1991'' (Finucane, 2000, p. 559)
\par
``Contrary to what is typically assumed, nearly one-fourth of all trades do not occur as the result of the arrival of market order'' (Finucane, 2000, p. 560)
\par
``An analysis of the 75.2\% of the trades that do contain market orders shows that the orders on the opposite side of the trade are nearly equally split between system side limit orders and crowd side market order'' (Finucane, 2000, p. 560)
\par
``hen the sample trades are classified by their location relative to the quoted spread, it becomes apparent that the accuracy of the tick test (and LR's algorithm that uses the tick test to classify mid-spread trades) for mid-spread trades is far lower than the 85\% predicted by LR's model. T'' (Finucane, 2000, p. 562)
\par
``Contrary to expectations, the performance of the tick test is only marginally worse than LR's algorithm for trades that occur at the quoted bid or ask.'' (Finucane, 2000, p. 562)
\par
``Further? more, LR's algorithm is less than 100\% accurate for trades at the bid or ask; both methods incorrectly classify at least 10\% of the trades at the bid or ask'' (Finucane, 2000, p. 562)
\par
``y to affect classification accuracy. To further explore the importance of zero ticks, the sample is divided into trades on zero and non-zero ticks. Table 4 shows that all three methods perform worse for trades on zero ticks than on non-zero ticks, but it also shows that trades on zero ticks are far more likely to be mid-spread trades, crosses, or trades that occur on quote changes, supporting the hypothesis that zero ticks proxy for other factors'' (Finucane, 2000, p. 563)
\par
``f zero ticks are causing the classification errors for the tick test, the same errors should not be found when LR's method is applied to trades at the quoted bid or ask on zero ticks; the trade direction predicted by LR's method is independent of previous trade price movements for trades at the bid or ask. The results of the tests in column 5 do not support the hypothesis that zero ticks per se are responsible for classification error'' (Finucane, 2000, p. 563)
\par
``Since it is clear that the factors affecting the accuracy of the classification algorithms are not independent and that each factor may affect the accuracy of LR's algorithm and the tick test differently, it is useful to consider the marginal'' (Finucane, 2000, p. 563)
\par
``impact of the factors on classification accuracy for the two algorithms. This is accomplished by estimating logit models for each of the algorithm'' (Finucane, 2000, p. 565)
\par
``Furthermore, price improvement will tend to lead to classification errors, since buys will occur away from the ask and sells will occur away from the bid. When trades receive price improvement, buys will also be more likely to occur on downticks and sells will be more likely to occur on upticks. I'' (Finucane, 2000, p. 565)
\par
``Table 5 contains the maximum likelihood coefficient estimates and asso? ciated x2-statistics for the two models, together with estimates of the marginal change in the probability of correctly classifying an observation for a one unit change in each independent variable.'' (Finucane, 2000, p. 566)
\par
``show that efforts to filter data in an attempt to increase classification accuracy may further exacerbate these biases. Somewhat surprisingly, although the classification error rates are slightly smaller for LR's method than for the tick test, the biases for estimated effective spreads and signed volume are smaller for the tick test than for LR's method. These findings sug? gest that researchers using the tick test to classify trades will achieve results that are close to the results that can be achieved using quote-based methods and, in at least some applications, the tick test may provide more accurate measures than quote-based methods.'' (Finucane, 2000, p. 574)}
}

@article{Fleming_1996,
  title = {Trading Costs and the Relative Rates of Price Discovery in Stock, Futures, and Option Markets},
  author = {Fleming, Jeff and Ostdiek, Barbara and Whaley, Robert E.},
  year = {1996},
  journal = {Journal of Futures Markets},
  doi = {10.1002/(sici)1096-9934(199606)16:4<353::aid-fut1>3.0.co;2-h},
  mag_id = {1988359375},
  pmcid = {null},
  pmid = {null}
}

@article{Fletcher_2010,
  title = {Multiple Kernel Learning on the Limit Order Book},
  author = {Fletcher, Tristan and Hussain, Zakria and {Shawe-Taylor}, John},
  year = {2010},
  journal = {null},
  doi = {null},
  mag_id = {2112124685},
  pmcid = {null},
  pmid = {null}
}

@article{Foucault_2006,
  title = {Competition for Order Flow and Smart Order Routing Systems},
  author = {Foucault, Thierry and Menkveld, Albert J.},
  year = {2006},
  journal = {null},
  doi = {null},
  mag_id = {2011591903},
  pmcid = {null},
  pmid = {null}
}

@article{Foucault_2010,
  title = {Limit Order Markets},
  author = {Foucault, Thierry},
  year = {2010},
  journal = {null},
  doi = {10.1002/9780470061602.eqf18017},
  mag_id = {1583967832},
  pmcid = {null},
  pmid = {null}
}

@article{frazziniBettingBeta2014,
  title = {Betting against Beta},
  author = {Frazzini, Andrea and Pedersen, Lasse Heje},
  year = {2014},
  journal = {Journal of Financial Economics},
  volume = {111},
  number = {1},
  doi = {10.1016/j.jfineco.2013.10.005}
}

@article{freybergerDissectingCharacteristicsNonparametrically,
  title = {Dissecting Characteristics Nonparametrically},
  author = {Freyberger, Joachim and Neuhierl, Andreas and Weber, Michael},
  year = {2020},
  journal = {The Review of Financial Studies},
  volume = {5},
  number = {33},
  doi = {10.1093/rfs/hhz123}
}

@article{friedmanAdditiveLogisticRegression2000,
  title = {Additive Logistic Regression: A Statistical View of Boosting (with Discussion and a Rejoinder by the Authors)},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2000},
  journal = {The Annals of Statistics},
  volume = {28},
  number = {2},
  doi = {10.1214/aos/1016218223}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy Function Approximation: A Gradient Boosting Machine.},
  author = {Friedman, Jerome H.},
  year = {2001},
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  doi = {10.1214/aos/1013203451}
}

@article{friedmanStochasticGradientBoosting2002,
  title = {Stochastic Gradient Boosting},
  author = {Friedman, Jerome H.},
  year = {2002},
  journal = {Computational Statistics \& Data Analysis},
  volume = {38},
  number = {4},
  doi = {10.1016/S0167-9473(01)00065-2}
}

@article{frommelAccuracyTradeClassification2021,
  title = {The {{Accuracy}} of {{Trade Classification Systems}} on the {{Foreign Exchange Market}}: {{Evidence}} from the {{RUB}}/{{USD Market}}},
  author = {Fr{\"o}mmel, Michael and D'Hoore, Dick and Lampaert, Kevin},
  year = {2021},
  journal = {Finance Research Letters},
  volume = {42},
  doi = {10.1016/j.frl.2020.101892}
}

@misc{gabrielDynamicPricingUsing2021,
  title = {Dynamic Pricing Using Reinforcement Learning and Neural Networks},
  author = {Gabriel, Reslley},
  year = {2021},
  journal = {Medium},
  howpublished = {https://towardsdatascience.com/dynamic-pricing-using-reinforcement-learning-and-neural-networks-cc3abe374bf5}
}

@book{gallianContemporaryAbstractAlgebra2021,
  title = {Contemporary Abstract Algebra},
  author = {Gallian, Joseph},
  editor = {Gallian, Joseph A.},
  year = {2021},
  edition = {Tenth},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781003142331}
}

@article{garleanuDemandBasedOptionPricing2009,
  title = {Demand-Based Option Pricing},
  author = {G{\^a}rleanu, Nicolae and Pedersen, Lasse Heje and Poteshman, Allen M.},
  year = {2009},
  journal = {Review of Financial Studies},
  volume = {22},
  number = {10},
  doi = {10.1093/rfs/hhp005},
  note = {\section{Annotations\\
(13/09/2022, 14:35:25)}

\par
``The model shows that demand pressure in one option contract increases its price by an amount proportional to the variance of the unhedgeable part of the option. Similarly, the demand pressure increases the price of any other option by an amount proportional to the covariance of their unhedgeable parts.'' (G\^arleanu et al., 2009, p. 1)
\par
``Empirically, we use a unique dataset to identify aggregate daily positions of dealers and end users. In particular, we define dealers as market makers and end users as proprietary traders and customers of brokers.2 We find that end users have a net long position in S\&P500 index options with large net positions in out-of-the-money puts. Hence, since options are in zero net supply, dealers are short index options.'' (G\^arleanu et al., 2009, p. 3)
\par
``We acquire the data from two different sources. Data for computing net option demand were obtained directly from the Chicago Board Options Exchange (CBOE). These data consist of a daily record of closing short and long open interest on all SPX and equity options for public customers and firm proprietary traders.'' (G\^arleanu et al., 2009, p. 16)
\par
``The other main source of data for this paper is the Ivy DB data set from OptionMetrics LLC. The OptionMetrics data include end-of-day volatilities implied from option prices, and we use the volatilities implied from SPX and CBOE listed equity options from the beginning of 1996 through the end of 2001. SPX options have European style exercise, and OptionMetrics computes implied volatilities by inverting the Black-Scholes formula.'' (G\^arleanu et al., 2009, p. 17)}
}

@article{Germain_2010,
  title = {Weak Convergence of the Regularization Path in Penalized {{M-estimation}}},
  author = {Germain, Jean-Francois and Roueff, Fran{\c c}ois},
  year = {2010},
  journal = {Scandinavian Journal of Statistics},
  doi = {10.1111/j.1467-9469.2009.00682.x},
  mag_id = {1592037586},
  pmcid = {null},
  pmid = {null}
}

@misc{gevaTransformerFeedForwardLayers2021,
  title = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  year = {2021},
  number = {arXiv:2012.14913},
  eprint = {2012.14913},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: EMNLP 2021}
}

@article{GivingSoftwareIts2019,
  title = {Giving Software Its Due},
  year = {2019},
  journal = {Nature Methods},
  volume = {16},
  number = {3},
  doi = {10.1038/s41592-019-0350-x}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep Sparse Rectifier Neural Networks},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  volume = {15},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  address = {{Fort Lauderdale, FL}}
}

@article{Glosten_1985,
  title = {Bid, Ask and Transaction Prices in a Specialist Market with Heterogeneously Informed Traders},
  author = {Glosten, Lawrence R. and Milgrom, Paul},
  year = {1985},
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405x(85)90044-3},
  mag_id = {1985808284},
  pmcid = {null},
  pmid = {null}
}

@article{Glosten_1988,
  title = {Estimating the Components of the Bid/Ask Spread},
  author = {Glosten, Lawrence R. and Harris, Lawrence},
  year = {1988},
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405x(88)90034-7},
  mag_id = {2035606631},
  pmcid = {null},
  pmid = {null}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  note = {http://www.deeplearningbook.org}
}

@book{goossensLaTeXGraphicsCompanion2008,
  title = {The Latex Graphics Companion},
  editor = {Goossens, Michel},
  year = {2008},
  series = {Addison-{{Wesley}} Series on Tools and Techniques for Computer Typesetting},
  edition = {2nd ed},
  publisher = {{Addison-Wesley}},
  address = {{Upper Saddle River, NJ}}
}

@misc{gorishniyEmbeddingsNumericalFeatures2022,
  title = {On Embeddings for Numerical Features in Tabular Deep Learning},
  author = {Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
  year = {2022},
  number = {arXiv:2203.05556},
  eprint = {2203.05556},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(25/01/2023, 14:59:42)}

\par
``Transformer-like architectures have a specific way to handle numerical features of the data. Namely, they map scalar values of numerical features to high-dimensional embedding vectors, which are then mixed by the self-attention modules.'' (Gorishniy et al., 2022, p. 1)
\par
``the existing architectures (Gorishniy et al., 2021; Somepalli et al., 2021; Kossen et al., 2021; Song et al., 2019; Guo et al., 2021) construct embeddings for numerical features using quite restrictive parametric mappings, e.g., linear functions, which can lead to suboptimal performance.'' (Gorishniy et al., 2022, p. 1)
\par
``As another important finding, we demonstrate that the step of embedding the numerical features is universally beneficial for different deep architectures, not only for arXiv:2203.05556v2 [cs.LG] 15 Mar 202'' (Gorishniy et al., 2022, p. 1)
\par
``On Embeddings for Numerical Features in Tabular Deep Learning Transformer-like ones.'' (Gorishniy et al., 2022, p. 2)
\par
``To sum up, our contributions are as follows: 1. We show that embedding schemes for numerical features are an underexplored research question in tabular DL. Namely, we show that more expressive embedding schemes can provide substantial performance improvements over prior models. 2. We demonstrate that the profit from embedding numerical features is not specific for Transformer-like architectures, and proper embedding schemes benefit traditional models as well. 3. On a number of public benchmarks, we achieve the new state-of-the-art of tabular DL.'' (Gorishniy et al., 2022, p. 2)
\par
``of tabular data requires mapping the scalar values of these features to high-dimensional embedding vectors. So far, the existing architectures perform this ``scalar'' \textrightarrow{} ``vector'' mapping by relatively simple computational blocks, which, in practice, can limit the model expressiveness.'' (Gorishniy et al., 2022, p. 2)
\par
``For instance, the recent FT-Transformer architecture (Gorishniy et al., 2021) employs only a single linear layer. In our experiments, we demonstrate that such embedding schemes can provide suboptimal performance, and more advanced schemes often lead to substantial profit.'' (Gorishniy et al., 2022, p. 2)
\par
``Feature binning. Binning is a discretization technique that converts numerical features to categorical features.'' (Gorishniy et al., 2022, p. 2)
\par
``Periodic activations. Recently, periodic activation functions have become a key component in processing coordinates-like inputs, which is required in many applications. Examples include NLP (Vaswani et al., 2017), vision (Li et al., 2021), implicit neural representations (Mildenhall et al., 2020; Tancik et al., 2020; Sitzmann et al., 2020). In our work, we show that periodic activations can be used to construct powerful embedding modules for numerical features in tabular data problems.'' (Gorishniy et al., 2022, p. 2)
\par
``Importantly, contrary to some of the aforementioned papers, where components of the multidimensional coordinates are mixed (e.g. with linear layers) before passing them to periodic functions (Sitzmann et al., 2020; Tancik et al., 2020), we find it crucial to embed each feature separately before mixing them in the main backbone.'' (Gorishniy et al., 2022, p. 2)
\par
``We formalize the notion of ''embeddings for numerical features'' in Equation 1: zi = fi ( x(num) i ) {$\in$} Rdi (1) where fi(x) is the embedding function for the i-th numerical feature, zi is the embedding of the i-th numerical feature and di is the dimensionality of the embedding'' (Gorishniy et al., 2022, p. 3)
\par
``While vanilla MLP is known to be a universal approximator (Cybenko, 1989; Hornik, 1991), in practice, due to optimization peculiarities, it has limitations in its learning capabilities (Rahaman et al., 2019). However, the recent work by Tancik et al. (2020) uncovers the case where changing the input space alleviates the above issue.'' (Gorishniy et al., 2022, p. 3)
\par
``Namely, it allows existing DL backbones to achieve noticeably better results and significantly reduce the gap with Gradient Boosted Decision Trees.'' (Gorishniy et al., 2022, p. 10)
\par
``We have also shown that traditional MLP-like models coupled with embeddings for numerical features can perform on par with attention-based models.'' (Gorishniy et al., 2022, p. 10)
\par
Comment: Code: https://github.com/Yura52/tabular-dl-num-embeddings (v2: minor fixes)}
}

@inproceedings{gorishniyRevisitingDeepLearning2021,
  title = {Revisiting Deep Learning Models for Tabular Data},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = {2021},
  volume = {34},
  publisher = {{Curran Associates, Inc.}},
  address = {{Red Hook, NY}},
  note = {\section{Annotations\\
(06/10/2022, 19:42:44)}

\par
``In these problems, data points are represented as vectors of heterogeneous features, which is typical for industrial applications and ML competitions, where neural networks have a strong non-deep competitor in the form of GBDT (Chen and Guestrin, 2016; Ke et al., 2017; Prokhorenkova et al., 2018).'' (Gorishniy et al., 2021, p. 1)
\par
``Along with potentially higher performance, using deep learning for tabular data is appealing as it would allow constructing multi-modal pipelines for problems, where only one part of the input is tabular, and other parts include images, audio and other DL-friendly data. Such pipelines can then be trained end-to-end by gradient optimization for all modalities.'' (Gorishniy et al., 2021, p. 1)
\par
``Additionally, despite the large number of novel architectures, the field still lacks simple and reliable solutions that allow achieving competitive performance with moderate effort and provide stable performance across many tasks.'' (Gorishniy et al., 2021, p. 1)
\par
``our simple adaptation of the Transformer architecture (Vaswani et al., 2017) for tabular data.'' (Gorishniy et al., 2021, p. 2)
\par
``Second, FT-Transformer demonstrates the best performance on most tasks and becomes a new powerful solution for the field. Interestingly, FT-Transformer turns out to be a more universal architecture for tabular data:'' (Gorishniy et al., 2021, p. 2)
\par
``Finally, we compare the best DL models to GBDT and conclude that there is still no universally superior solution.'' (Gorishniy et al., 2021, p. 2)
\par
``Attention-based models. Due to the ubiquitous success of attention-based architectures for different domains (Dosovitskiy et al., 2021; Vaswani et al., 2017), several authors propose to employ attentionlike modules for tabular DL as well (Arik and Pfister, 2020; Huang et al., 2020; Song et al., 2019)'' (Gorishniy et al., 2021, p. 2)
\par
``Notation. In this work, we consider supervised learning problems. D=\{(xi, yi)\}in=1 denotes a dataset, where xi=(x(num) i , x(cat) i ) {$\in$} X represents numerical x(num) ij and categorical x(cat) ij features of an object and yi {$\in$} Y denotes the corresponding object label. The total number of features is denoted as k. The dataset is split into three disjoint subsets: D = Dtrain {$\cup$} Dval {$\cup$} Dtest, where Dtrain is used for training, Dval is used for early stopping and hyperparameter tuning, and Dtest is used for the final evaluation. We consider three types of tasks: binary classification Y = \{0, 1\}, multiclass classification Y = \{1, . . . , C\} and regression Y = R.'' (Gorishniy et al., 2021, p. 3)}
}

@misc{GradientBoostPart,
  title = {Gradient Boost Part 1 (of 4): Regression Main Ideas - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=3CC4N4z3GJc}
}

@article{grammigDivergingRoadsTheoryBased2020,
  title = {Diverging Roads: Theory-Based vs. Machine Learning-Implied Stock Risk Premia},
  author = {Grammig, Joachim and Hanenberg, Constantin and Schlag, Christian and S{\"o}nksen, Jantje},
  year = {2020},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3536835}
}

@article{grauerOptionTradeClassification2022,
  title = {Option Trade Classification},
  author = {Grauer, Caroline and Schuster, Philipp and {Uhrig-Homburg}, Marliese},
  year = {2022},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.4098475},
  note = {\section{Annotations\\
(19/10/2022, 12:42:54)}

\par
``We evaluate the performance of common stock trade classification algorithms including the quote, tick, Lee and Ready (1991), and Ellis, Michaely, and O'Hara (2000) rule to infer the trade direction of option trades.'' (Grauer et al., 2022, p. 0)
\par
``We evaluate the performance of common stock trade classification algorithms including the quote, tick, Lee and Ready (1991), and Ellis, Michaely, and O'Hara (2000) rule to infer the trade direction of option trades.'' (Grauer et al., 2022, p. 0)
\par
``the prevailing Lee and Ready algorithm is only able to correctly sign between 60\% to 64\% of option trades, which is a similar magnitude as using the quote rule alone.'' (Grauer et al., 2022, p. 0)
\par
``Particularly, the trade direction is required to determine the information content of trades, the order imbalance and inventory accumulation of liquidity providers, the price impact of transactions, and to calculate many liquidity measures.'' (Grauer et al., 2022, p. 1)
\par
``First, options are much more illiquid than stocks with many series not recording a trade for days or weeks. For that reason, tick rules that depend on the information from preceding or succeeding trades might be problematic.'' (Grauer et al., 2022, p. 1)
\par
``Against this backdrop, it is surprising that there is just one study comparing trade classification rules in option markets, which is conducted on a small and more than twenty-five year old dataset (Savickas and Wilson (2003)).'' (Grauer et al., 2022, p. 1)
\par
``The document is available via the following link: https://osf.io/kj86r/ ?view\_only=388a89b23254425a8271402e2b11fc4e.'' (Grauer et al., 2022, p. 2)
\par
``Generally, quote rules outperform tick rules by far.'' (Grauer et al., 2022, p. 3)
\par
``The highest success rate of 63.92\% can be achieved by applying the quote rule first to NBBO and then to ISE quotes, and classifying all remaining trades using the reverse tick rule'' (Grauer et al., 2022, p. 3)
\par
``Overall, the accuracy of existing classification methods is considerably lower for option trades than for stocks, which is mostly between 70\% and 90\%'' (Grauer et al., 2022, p. 3)
\par
``main idea of our new ``trade size rule'' is that when the trade size matches exactly either the bid or ask quote size, it is likely that the quote came from a customer, the market maker found it attractive and, therefore, decided to fill it completely.'' (Grauer et al., 2022, p. 4)
\par
``The hypothesis of market makers filling the limit orders of customers seems most plausible for relatively small orders and trades that are not outside of the bid ask spread (for them, it is likely that customers submitted a market order exceeding the prevailing bid or ask quote size)'' (Grauer et al., 2022, p. 4)
\par
``Our second improvement addresses the fact that midspread trades are particularly difficult to classify, which leads to the poor performance of the LR and EMO rule compared to the quote rule.'' (Grauer et al., 2022, p. 4)
\par
``The general finding from this literature is that the overall success of classification rules for stock markets is relatively high, but varies widely across security markets and time periods.'' (Grauer et al., 2022, p. 5)
\par
``To the best of our knowledge, Savickas and Wilson (2003) provide the only study that examines the trade classification accuracy for option trades.'' (Grauer et al., 2022, p. 6)
\par
``Based on this mechanism and the poor performance of the tick test, we propose two simple rules that can be used in combination with existing classification algorithms'' (Grauer et al., 2022, p. 6)
\par
``LiveVol provides intraday transaction-level option data for all option trades on all U.S. exchanges.'' (Grauer et al., 2022, p. 7)
\par
``We filter out option trades with a trading price less than or equal to zero. We also remove trades with negative or zero volume and those whose trading volume exceeds 10 million contracts. Furthermore, we delete entries with multiple underlying symbols for the same root and other duplicates along with any cancelled trades.'' (Grauer et al., 2022, p. 7)
\par
``Because evaluating the performance of trade classification algorithms requires information on the true side of the trade, we combine information from intraday transaction data and daily Open/Close data to arrive at such a benchmark. Our two Open/Close datasets are available on a daily level and cover trading volume at the ISE and the CBOE, respectively.'' (Grauer et al., 2022, p. 8)
\par
``We take advantage of the fact that if there were only customer buy (sell) orders on a specific day for a given option series at one particular exchange, Open/Close data allows to classify all transactions in the LiveVol dataset on that day at the respective exchange as buy (sell) orders.'' (Grauer et al., 2022, p. 8)
\par
``We use the unique key specified by trade date, expiration date, strike price, option type, and root symbol of the underlying to match the samples.'' (Grauer et al., 2022, p. 8)
\par
``with the OSF (see footnote 2)'' (Grauer et al., 2022, p. 9)
\par
``As the probability of observing only buy trades or only sell trades decreases with an increasing number of trades, the number of trades per option day is lower and the time between two trades is higher in our matched samples compared to their full sample equivalents.'' (Grauer et al., 2022, p. 9)
\par
``Because most classification rules have a lower performance for illiquid securities, our results can be interpreted as a lower boundary on their overall performance.'' (Grauer et al., 2022, p. 9)
\par
``If the trade occurs above the midpoint of the bid-ask spread, it is classified as buyer-initiated. Conversely, if the trade price is below the midspread, the trade is classified as seller-initiated. Trades that occur exactly at the midpoint cannot be classified.'' (Grauer et al., 2022, p. 10)
\par
``Second, tick tests use changes in trade prices and look at previous trade prices to infer trade direction. If the trade occurs at a higher price than the previous one, it is classified as buyer-initiated. Conversely, if the trade price is below the previous one, it is classified as seller-initiated. If there is no price change between successive trades, the trade direction is inferred using the last price that differs from the current price.'' (Grauer et al., 2022, p. 10)
\par
``Conversely, if the next distinguishable price is above the current price, the current trade is classified as seller-initiated. The tick test and reverse tick test can be applied using trade prices on all option exchanges or one specific exchange only'' (Grauer et al., 2022, p. 10)
\par
``To make the performance of algorithms that are unable to completely classify all trades comparable, we assume unclassified trades to be correctly classified with a random probability of 50\%.'' (Grauer et al., 2022, p. 11)
\par
``This affects quote rules only, as they are unable to classify midspread trades.'' (Grauer et al., 2022, p. 11)
\par
``Moreover, we find that the LR algorithm outperforms the EMO rule as, in addition to midspread trades, the latter uses the tick test to a greater extent. However, the commonly used LR rule using the tick test to classify midspread trades is only able to classify 63.53\% of trades correctly, which is worse than using the quote rule alone'' (Grauer et al., 2022, p. 12)
\par
``The last two columns of Table 3 show that the weak performance is mainly driven by trades with trade sizes equal to either the bid quote size or the ask quote size at the ISE at the time of the trade.'' (Grauer et al., 2022, p. 12)
\par
``We start with the hypothesis that the weak performance of existing trade classification methods for trades with a trade size equal to either the size of the ask or the bid quote is due to limit orders placed by sophisticated customers.'' (Grauer et al., 2022, p. 13)
\par
``After applying this ``trade size rule'', the existing trade classification algorithms are applied to all other trades for which the trade size is not equal to one of the quote sizes (or for which it is equal to both the bid and the ask size). Panel A of Table 4 shows that this modification leads to a substantial improvement between 10.7\% and 11.3\% in the performance of the quote rule and combined methods and an improvement of 5.6\% to 7.3\% for the tick tests.'' (Grauer et al., 2022, p. 13)
\par
``We hypothesize that a larger bid or ask quoted size, i.e., a higher depth at the best bid or ask, indicates a higher liquidity similar to a tighter bid or ask quote'' (Grauer et al., 2022, p. 14)
\par
``We coin this combination ``depth rule + reverse LR'''' (Grauer et al., 2022, p. 14)
\par
``We show the overall success rates of the classification algorithms using our trade size rule and also calculate the change in the success rates compared to the same algorithms not using the trade size rule in parentheses. The results show that our new rule works best for small to medium-sized trades and even leads to a slight deterioration of the performance for the largest trade sizes.'' (Grauer et al., 2022, p. 15)
\par
``Namely, tick tests perform best when using most current price information across all exchanges and reverse tick tests based on subsequent prices dominate their counterparts based on preceding ones.'' (Grauer et al., 2022, p. 16)
\par
``For this reason, the LR algorithm outperforms the EMO rule as the former uses the tick tests to a smaller extent.'' (Grauer et al., 2022, p. 16)
\par
``The last two columns of Table 6 show that the weak performance is once again mainly driven by trades with trade sizes equal to either the bid quote size or the ask quote size at the CBOE at the time of the trade. For them, average success rates of quote rules are only about 19\%. Because our trade size rule addresses exactly these trades, this is a promising first indication that our new rule also works for the CBOE sample.'' (Grauer et al., 2022, p. 17)
\par
``The highest success rate of 72.40\% is achieved by the trade size rule in combination with the quote rule first applied to CBOE quotes and then to the NBBO. In combination with the tick test or reverse tick test for the LR algorithm, we are able to correctly classify 72.12\% and 72.39\% of the option trades, respectively.'' (Grauer et al., 2022, p. 18)
\par
``Applying our depth rule after using the trade size rule and the quote rule and classifying the very small number of midspread trades that cannot be signed by our depth rule using the reverse tick test yields an additional improvement of 1.2\% on average. It improves the success rate of the quote rule applied to CBOE quotes first and then to the NBBO to 73.37\%. These results confirm that our depth rule outperforms the standard and reverse tick test in classifying midspread trades.'' (Grauer et al., 2022, p. 18)
\par
``Finally, to compare the performance of the algorithms over time, we look at the individual years of our sample period. To conserve space, we compute average success rates for the different specifications of the quote, tick, LR, reverse LR, EMO, and depth rules.'' (Grauer et al., 2022, p. 19)
\par
``Comparing the classification precision of options written on common stocks, index options, and options written on other underlyings (mainly ETFs), we find lower success rates for index options, which is consistent with Savickas and Wilson (2003). Interestingly, the improvements due to our trade size rule are particularly high for index options at the CBOE.'' (Grauer et al., 2022, p. 20)
\par
``Summarizing the results from our robustness checks, we find that in all subsamples and for all existing trade classification algorithms, improvements due to the application of our new trade size rule are positive and range between 1\% and 23\%.'' (Grauer et al., 2022, p. 21)
\par
``Most importantly, in contrast to standard and reverse tick tests, our newly proposed depth rule leads to a significant improvement compared to using the quote rule alone, pointing to its superior performance to sign midspread trades that quote rules cannot classify.'' (Grauer et al., 2022, p. 21)
\par
``Using our new methodology allows to correctly classify between 73\% and 75\% of option trades in our sample, which is more than 10\% higher compared to the rules that are currently used in the literature.'' (Grauer et al., 2022, p. 22)
\par
\section{Annotations\\
(19/10/2022, 12:58:51)}

\par
``We evaluate the performance of common stock trade classification algorithms including the quote, tick, Lee and Ready (1991), and Ellis, Michaely, and O'Hara (2000) rule to infer the trade direction of option trades.'' (Grauer et al., 2022, p. 0)
\par
``We evaluate the performance of common stock trade classification algorithms including the quote, tick, Lee and Ready (1991), and Ellis, Michaely, and O'Hara (2000) rule to infer the trade direction of option trades.'' (Grauer et al., 2022, p. 0)
\par
``the prevailing Lee and Ready algorithm is only able to correctly sign between 60\% to 64\% of option trades, which is a similar magnitude as using the quote rule alone.'' (Grauer et al., 2022, p. 0)
\par
``Particularly, the trade direction is required to determine the information content of trades, the order imbalance and inventory accumulation of liquidity providers, the price impact of transactions, and to calculate many liquidity measures.'' (Grauer et al., 2022, p. 1)
\par
``First, options are much more illiquid than stocks with many series not recording a trade for days or weeks. For that reason, tick rules that depend on the information from preceding or succeeding trades might be problematic.'' (Grauer et al., 2022, p. 1)
\par
``Against this backdrop, it is surprising that there is just one study comparing trade classification rules in option markets, which is conducted on a small and more than twenty-five year old dataset (Savickas and Wilson (2003)).'' (Grauer et al., 2022, p. 1)
\par
``The document is available via the following link: https://osf.io/kj86r/ ?view\_only=388a89b23254425a8271402e2b11fc4e.'' (Grauer et al., 2022, p. 2)
\par
``Generally, quote rules outperform tick rules by far.'' (Grauer et al., 2022, p. 3)
\par
``The highest success rate of 63.92\% can be achieved by applying the quote rule first to NBBO and then to ISE quotes, and classifying all remaining trades using the reverse tick rule'' (Grauer et al., 2022, p. 3)
\par
``Overall, the accuracy of existing classification methods is considerably lower for option trades than for stocks, which is mostly between 70\% and 90\%'' (Grauer et al., 2022, p. 3)
\par
``main idea of our new ``trade size rule'' is that when the trade size matches exactly either the bid or ask quote size, it is likely that the quote came from a customer, the market maker found it attractive and, therefore, decided to fill it completely.'' (Grauer et al., 2022, p. 4)
\par
``The hypothesis of market makers filling the limit orders of customers seems most plausible for relatively small orders and trades that are not outside of the bid ask spread (for them, it is likely that customers submitted a market order exceeding the prevailing bid or ask quote size)'' (Grauer et al., 2022, p. 4)
\par
``Our second improvement addresses the fact that midspread trades are particularly difficult to classify, which leads to the poor performance of the LR and EMO rule compared to the quote rule.'' (Grauer et al., 2022, p. 4)
\par
``The general finding from this literature is that the overall success of classification rules for stock markets is relatively high, but varies widely across security markets and time periods.'' (Grauer et al., 2022, p. 5)
\par
``To the best of our knowledge, Savickas and Wilson (2003) provide the only study that examines the trade classification accuracy for option trades.'' (Grauer et al., 2022, p. 6)
\par
``Based on this mechanism and the poor performance of the tick test, we propose two simple rules that can be used in combination with existing classification algorithms'' (Grauer et al., 2022, p. 6)
\par
``LiveVol provides intraday transaction-level option data for all option trades on all U.S. exchanges.'' (Grauer et al., 2022, p. 7)
\par
``We filter out option trades with a trading price less than or equal to zero. We also remove trades with negative or zero volume and those whose trading volume exceeds 10 million contracts. Furthermore, we delete entries with multiple underlying symbols for the same root and other duplicates along with any cancelled trades.'' (Grauer et al., 2022, p. 7)
\par
``Because evaluating the performance of trade classification algorithms requires information on the true side of the trade, we combine information from intraday transaction data and daily Open/Close data to arrive at such a benchmark. Our two Open/Close datasets are available on a daily level and cover trading volume at the ISE and the CBOE, respectively.'' (Grauer et al., 2022, p. 8)
\par
``We take advantage of the fact that if there were only customer buy (sell) orders on a specific day for a given option series at one particular exchange, Open/Close data allows to classify all transactions in the LiveVol dataset on that day at the respective exchange as buy (sell) orders.'' (Grauer et al., 2022, p. 8)
\par
``We use the unique key specified by trade date, expiration date, strike price, option type, and root symbol of the underlying to match the samples.'' (Grauer et al., 2022, p. 8)
\par
``with the OSF (see footnote 2)'' (Grauer et al., 2022, p. 9)
\par
``As the probability of observing only buy trades or only sell trades decreases with an increasing number of trades, the number of trades per option day is lower and the time between two trades is higher in our matched samples compared to their full sample equivalents.'' (Grauer et al., 2022, p. 9)
\par
``Because most classification rules have a lower performance for illiquid securities, our results can be interpreted as a lower boundary on their overall performance.'' (Grauer et al., 2022, p. 9)
\par
``If the trade occurs above the midpoint of the bid-ask spread, it is classified as buyer-initiated. Conversely, if the trade price is below the midspread, the trade is classified as seller-initiated. Trades that occur exactly at the midpoint cannot be classified.'' (Grauer et al., 2022, p. 10)
\par
``Second, tick tests use changes in trade prices and look at previous trade prices to infer trade direction. If the trade occurs at a higher price than the previous one, it is classified as buyer-initiated. Conversely, if the trade price is below the previous one, it is classified as seller-initiated. If there is no price change between successive trades, the trade direction is inferred using the last price that differs from the current price.'' (Grauer et al., 2022, p. 10)
\par
``Conversely, if the next distinguishable price is above the current price, the current trade is classified as seller-initiated. The tick test and reverse tick test can be applied using trade prices on all option exchanges or one specific exchange only'' (Grauer et al., 2022, p. 10)
\par
``To make the performance of algorithms that are unable to completely classify all trades comparable, we assume unclassified trades to be correctly classified with a random probability of 50\%.'' (Grauer et al., 2022, p. 11)
\par
``This affects quote rules only, as they are unable to classify midspread trades.'' (Grauer et al., 2022, p. 11)
\par
``Moreover, we find that the LR algorithm outperforms the EMO rule as, in addition to midspread trades, the latter uses the tick test to a greater extent. However, the commonly used LR rule using the tick test to classify midspread trades is only able to classify 63.53\% of trades correctly, which is worse than using the quote rule alone'' (Grauer et al., 2022, p. 12)
\par
``The last two columns of Table 3 show that the weak performance is mainly driven by trades with trade sizes equal to either the bid quote size or the ask quote size at the ISE at the time of the trade.'' (Grauer et al., 2022, p. 12)
\par
``We start with the hypothesis that the weak performance of existing trade classification methods for trades with a trade size equal to either the size of the ask or the bid quote is due to limit orders placed by sophisticated customers.'' (Grauer et al., 2022, p. 13)
\par
``After applying this ``trade size rule'', the existing trade classification algorithms are applied to all other trades for which the trade size is not equal to one of the quote sizes (or for which it is equal to both the bid and the ask size). Panel A of Table 4 shows that this modification leads to a substantial improvement between 10.7\% and 11.3\% in the performance of the quote rule and combined methods and an improvement of 5.6\% to 7.3\% for the tick tests.'' (Grauer et al., 2022, p. 13)
\par
``We hypothesize that a larger bid or ask quoted size, i.e., a higher depth at the best bid or ask, indicates a higher liquidity similar to a tighter bid or ask quote'' (Grauer et al., 2022, p. 14)
\par
``We coin this combination ``depth rule + reverse LR'''' (Grauer et al., 2022, p. 14)
\par
``We show the overall success rates of the classification algorithms using our trade size rule and also calculate the change in the success rates compared to the same algorithms not using the trade size rule in parentheses. The results show that our new rule works best for small to medium-sized trades and even leads to a slight deterioration of the performance for the largest trade sizes.'' (Grauer et al., 2022, p. 15)
\par
``Namely, tick tests perform best when using most current price information across all exchanges and reverse tick tests based on subsequent prices dominate their counterparts based on preceding ones.'' (Grauer et al., 2022, p. 16)
\par
``For this reason, the LR algorithm outperforms the EMO rule as the former uses the tick tests to a smaller extent.'' (Grauer et al., 2022, p. 16)
\par
``The last two columns of Table 6 show that the weak performance is once again mainly driven by trades with trade sizes equal to either the bid quote size or the ask quote size at the CBOE at the time of the trade. For them, average success rates of quote rules are only about 19\%. Because our trade size rule addresses exactly these trades, this is a promising first indication that our new rule also works for the CBOE sample.'' (Grauer et al., 2022, p. 17)
\par
``The highest success rate of 72.40\% is achieved by the trade size rule in combination with the quote rule first applied to CBOE quotes and then to the NBBO. In combination with the tick test or reverse tick test for the LR algorithm, we are able to correctly classify 72.12\% and 72.39\% of the option trades, respectively.'' (Grauer et al., 2022, p. 18)
\par
``Applying our depth rule after using the trade size rule and the quote rule and classifying the very small number of midspread trades that cannot be signed by our depth rule using the reverse tick test yields an additional improvement of 1.2\% on average. It improves the success rate of the quote rule applied to CBOE quotes first and then to the NBBO to 73.37\%. These results confirm that our depth rule outperforms the standard and reverse tick test in classifying midspread trades.'' (Grauer et al., 2022, p. 18)
\par
``Finally, to compare the performance of the algorithms over time, we look at the individual years of our sample period. To conserve space, we compute average success rates for the different specifications of the quote, tick, LR, reverse LR, EMO, and depth rules.'' (Grauer et al., 2022, p. 19)
\par
``Comparing the classification precision of options written on common stocks, index options, and options written on other underlyings (mainly ETFs), we find lower success rates for index options, which is consistent with Savickas and Wilson (2003). Interestingly, the improvements due to our trade size rule are particularly high for index options at the CBOE.'' (Grauer et al., 2022, p. 20)
\par
``Summarizing the results from our robustness checks, we find that in all subsamples and for all existing trade classification algorithms, improvements due to the application of our new trade size rule are positive and range between 1\% and 23\%.'' (Grauer et al., 2022, p. 21)
\par
``Most importantly, in contrast to standard and reverse tick tests, our newly proposed depth rule leads to a significant improvement compared to using the quote rule alone, pointing to its superior performance to sign midspread trades that quote rules cannot classify.'' (Grauer et al., 2022, p. 21)
\par
``Using our new methodology allows to correctly classify between 73\% and 75\% of option trades in our sample, which is more than 10\% higher compared to the rules that are currently used in the literature.'' (Grauer et al., 2022, p. 22)}
}

@inproceedings{grinsztajnWhyTreebasedModels2022,
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular Data?},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = {2022},
  series = {{{NeurIPS}} 2022},
  volume = {36},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY}},
  note = {\section{Annotations\\
(24/01/2023, 15:44:15)}

\par
``This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural network: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions.'' (Grinsztajn et al., 2022, p. 1)
\par
``Our contributions are as follow: 1. We create a new benchmark for tabular data, with a precise methodology for choosing and preprocessing a large number of representative datasets.'' (Grinsztajn et al., 2022, p. 2)
\par
``The paper closest to our work is Gorishniy et al. [2021], benchmarking novel algorithms, on 11 tabular datasets. We provide a more comprehensive benchmark, with 45 datasets, split across different settings (medium-sized / large-size, with/without categorical features), accounting for the hyperparameter tuning cost, to establish a standard benchmark.'' (Grinsztajn et al., 2022, p. 2)
\par
``FT\_Transformer : a simple Transformer model combined with a module embedding categorical and numerical features, created in Gorishniy et al. [2021]. We choose this model because it was benchmarked in a convincing way against tree-based models and other tabular-specific models. It can thus be considered a ``best case'' for Deep learning models on tabular data.'' (Grinsztajn et al., 2022, p. 5)
\par
``Tuning hyperparameters does not make neural networks state-of-the-art Tree-based models are superior for every random search budget, and the performance gap stays wide even after a large number of random search iterations. This does not take into account that each random search iteration is generally slower for neural networks than for tree-based models (see A.2).'' (Grinsztajn et al., 2022, p. 6)
\par
``Such results suggest that the target functions in our datasets are not smooth, and that neural networks struggle to fit these irregular functions compared to tree-based models. This is in line with Rahaman et al. [2019], which finds that neural networks are biased toward low-frequency functions. Models based on decision trees, which learn piece-wise constant functions, do not exhibit such a bias.'' (Grinsztajn et al., 2022, p. 6)
\par
``MLP-like architectures are not robust to uninformative features In the two experiments shown in Fig. 4, we can see that removing uninformative features (4a) reduces the performance gap between MLPs (Resnet) and the other models (FT Transformers and tree-based models), while adding uninformative features widens the gap. This shows that MLPs are less robust to uninformative features, and, given the frequency of such features in tabular datasets, partly explain the results from Sec. 4.2.'' (Grinsztajn et al., 2022, p. 7)
\par
``Why are MLPs much more hindered by uninformative features, compared to other models? One answer is that this learner is rotationally invariant in the sense of Ng [2004]: the learning procedure which learns an MLP on a training set and evaluate it on a testing set is unchanged when applying a rotation (unitary matrix) to the features on both the training and testing set. On the contrary, tree-based models are not rotationally invariant, as they attend to each feature separately, and neither are FT Transformers, because of the initial FT Tokenizer, which implements a pointwise operation.'' (Grinsztajn et al., 2022, p. 7)
\par
``theoretical link between this concept and uninformative features is provided by Ng [2004], which shows that any rotationallly invariant learning procedure has a worst-case sample complexity that grows at least linearly in the number of irrelevant features. Intuitively, to remove uninformative features, a rotationaly invariant algorithm has to first find the original orientation of the features, and then select the least informative ones: the information contained in the orientation of the data is lost.'' (Grinsztajn et al., 2022, p. 8)
\par
``Our findings shed light on the results of Somepalli et al. [2021] and Gorishniy et al. [2022], which add an embedding layer, even for numerical features, before MLP or Transformer models.'' (Grinsztajn et al., 2022, p. 8)
\par
``this layer breaks rotation invariance. The fact that very different types of embedding seem to improve performance suggests that the sheer presence of an embedding which breaks the invariance is a key part of these improvements. We note that a promising avenue for further research would be to find other ways to break rotation invariance which might be less computationally costly than embeddings.'' (Grinsztajn et al., 2022, p. 9)
\par
``Study how both tree-based models and neural networks cope with specific challenges such as missing data or high-cardinality categorical features, thus extending to neural networks prior empirical work [Cerda et al., 2018, Cerda and Varoquaux, 2020, Perez-Lebel et al., 2022].'' (Grinsztajn et al., 2022, p. 9)}
}

@article{guAutoencoderAssetPricing2021,
  title = {Autoencoder Asset Pricing Models},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  year = {2021},
  journal = {Journal of Econometrics},
  volume = {222},
  number = {1},
  doi = {10.1016/j.jeconom.2020.07.009}
}

@article{guEmpiricalAssetPricing2020,
  title = {Empirical Asset Pricing via Machine Learning},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  year = {2020},
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {5},
  doi = {10.1093/rfs/hhaa009}
}

@article{gunnarssonDeepLearningCredit2021,
  title = {Deep Learning for Credit Scoring: Do or Don't?},
  author = {Gunnarsson, Bj{\"o}rn Rafn and {\noopsort{broucke}}{vanden Broucke}, Seppe and Baesens, Bart and {\'O}skarsd{\'o}ttir, Mar{\'i}a and Lemahieu, Wilfried},
  year = {2021},
  journal = {European Journal of Operational Research},
  volume = {295},
  number = {1},
  doi = {10.1016/j.ejor.2021.03.006}
}

@inproceedings{guoEmbeddingLearningFramework2021,
  title = {An Embedding Learning Framework for Numerical Features in {{CTR}} Prediction},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Guo, Huifeng and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Li, Zhenguo and He, Xiuqiang},
  year = {2021},
  eprint = {2012.08986},
  eprinttype = {arxiv},
  doi = {10.1145/3447548.3467077},
  archiveprefix = {arXiv},
  note = {Comment: 9 pages}
}

@misc{guoEntityEmbeddingsCategorical2016,
  title = {Entity {{Embeddings}} of {{Categorical Variables}}},
  author = {Guo, Cheng and Berkhahn, Felix},
  year = {2016},
  number = {arXiv:1604.06737},
  eprint = {1604.06737},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(26/01/2023, 06:40:19)}

\par
``We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables.'' (Guo and Berkhahn, 2016, p. 1)
\par
``As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.'' (Guo and Berkhahn, 2016, p. 1)
\par
``Therefore, naively applying neural networks on structured data with integer representation for category variables does not work well. A common way to circumvent this problem is to use onehot encoding, but it has two shortcomings: First when we have many high cardinality features one-hot encoding often results in an unrealistic amount of computational resource requirement. Second, it treats different values of categorical variables completely independent of each other and often ignores the informative relations between them.'' (Guo and Berkhahn, 2016, p. 1)
\par
``The most common variable types in structured data are continuous variables and discrete variables. Continuous variables such as temperature, price, weight can be represented by real numbers. Discrete variables such as age, color, bus line number can be represented by integers. Often the integers are just used for convenience to label the different states and have no information in themselves. For example if we use 1, 2, 3 to represent red, blue and yellow, one can not assume that ''blue is bigger than red'' or ''the average of red and yellow are blue'' or anything that introduces additional information based on the properties of integers. These integers are called nominal numbers. Other times there is an intrinsic ordering in the integer index such as age or month of the year. These integers are called cardinal number or ordinal numbers. Note that the meaning or order may not be more useful for the problem than only considering the'' (Guo and Berkhahn, 2016, p. 3)
\par
``With entity embedding we want to put similar values of a categorical variable closer to each other in the embedding space. If we use a real number to define similarity of the values then entity embedding is closely related to the embedding of finite metric space problem in topology'' (Guo and Berkhahn, 2016, p. 4)
\par
``Neural networks with one-hot encoding give slightly better results than entity embedding for the shuffled data while entity embedding is clearly better than one-hot encoding for the non-shuffled data. The explanation is that entity embedding, by restricting the network in a much smaller parameter space in a meaningful way, reduces the chance that the network converges to local minimums far from the global minimum. More intuitively, entity embeddings force the network to learn the intrinsic properties of each of the feature as well as the sales distribution in the feature space'' (Guo and Berkhahn, 2016, p. 6)}
}

@inproceedings{gyamerahStockMarketMovement2019,
  title = {On Stock Market Movement Prediction via Stacking Ensemble Learning Method},
  booktitle = {2019 {{IEEE Conference}} on {{Computational Intelligence}} for {{Financial Engineering}} \& {{Economics}} ({{CIFEr}})},
  author = {Gyamerah, Samuel Asante and Ngare, Philip and Ikpe, Dennis},
  year = {2019},
  publisher = {{IEEE}},
  address = {{Shenzhen, China}},
  doi = {10.1109/CIFEr.2019.8759062}
}

@inproceedings{hanAutoEncoderInspiredUnsupervised2018,
  title = {{{AutoEncoder}} Inspired Unsupervised Feature Selection},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Han, Kai and Wang, Yunhe and Zhang, Chao and Li, Chao and Xu, Chao},
  year = {2018},
  eprint = {1710.08310},
  eprinttype = {arxiv},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8462261},
  archiveprefix = {arXiv},
  note = {Comment: accepted by ICASSP 2018}
}

@article{hancockSurveyCategoricalData2020,
  title = {Survey on Categorical Data for Neural Networks},
  author = {Hancock, John T. and Khoshgoftaar, Taghi M.},
  year = {2020},
  journal = {Journal of Big Data},
  volume = {7},
  number = {1},
  doi = {10.1186/s40537-020-00305-w},
  note = {\section{Annotations\\
(26/01/2023, 07:37:58)}

\par
``Entity embedding [3] is an example of an automatic technique'' (Hancock and Khoshgoftaar, 2020, p. 3)
\par
``We refer to these techniques as entity embedding algorithms. The output of an entity embedding algorithm is a mapping from some set of categorical values S to a space of n-dimensional vectors in Rn'' (Hancock and Khoshgoftaar, 2020, p. 5)
\par
``Let S be the set of distinct values of some variable that is characterized as categorical data. Then an entity embedding e is a mapping of the elements of S to vectors of real numbers. We define the range of an entity embedding as Rd to allow us the luxury of employing any theorems that hold for real numbers. In practice, the range of our embedding is a finite subset of vectors of rational numbers Qd because computers are currently only capable of storing rational approximations of real numbers. We refer to this as ``entity embedding'', ``embedding categorical data'', ``embedding categorical values'', or simply as ``embedding'' when the context is clear.'' (Hancock and Khoshgoftaar, 2020, p. 5)
\par
``The example we work through here is similar to the functioning of a Keras embedding layer [20]. To compute the embedded value e of the input we compute e = W v, where, W is the edge weight matrix for nodes in the neural network's embedding layer, and v is the input value. For embedding categorical variables, v must be some encoded value of a categorical variable. Typically, v is a One-hot encoded value. Please see "One-hot encoding" section for a definition of One-hot encoding. Equations~2, and 3 give an example of how one may compute the embedded value of a One-hot encoded categorical variable. Assuming the entry equal to 1 in the column vector on the right-hand side of Eq.~2 is the encoding vector v , and the value equal to 1 is on the jth row of v , the product on the right-hand side of Eq.~2 will be An easy way to think of the embedding process we illustrate in Eqs.~2 and 3 is that W is a look-up table for One-hot encoded values. A neural network's optimizer applies some procedure such as Stochastic Gradient Descent (SGD ) to update its weight values, including those of the weight matrix W. Over time the optimizer finds a value of W that minimizes the value of some loss function, J. As the neural network changes the value of W, the components of the embedded vectors e change as well. Since the values of e are updated as a result of the neural network's weight update function, we call the embedding automatic embedding.'' (Hancock and Khoshgoftaar, 2020, p. 8)}
}

@article{hansenApplicationsMachineLearning,
  title = {Applications of Machine Learning in High-Frequency Trade Direction Classification},
  author = {Hansen, Jared E},
  note = {\section{Annotations\\
(08/10/2022, 10:30:16)}

\par
``The correct assignment of trades as buyer-initiated or seller-initiated is paramount in many quantitative finance studies. Simple decision rule methods have been used for signing trades since many data sets available to researchers do not include the sign of each trade executed.'' (Hansen, p. 5)
\par
``Rosenthal moved from the simplicity of decision rules to a statistical modeling approach, employing logistic regression to predict trade sign [4].'' (Hansen, p. 18)
\par
``Rosenthal's model [4] encoded prices and quotes into original and once-lagged metrics for use as predictors, and was influential in inspiring our feature engineering.'' (Hansen, p. 18)
\par
``As detailed by Rosenthal, a modest improvement in trade signing accuracy, even just one or two percent, would affect estimates of price impact and could potentially results in savings of hundreds of millions of dollars over the course of a year for a large bank [4].'' (Hansen, p. 19)
\par
``Drawing from the work of LR, EMO, and CLNV, we also examine the position of trades' prices relative to quotes.'' (Hansen, p. 46)}
}

@article{harveyMultivariateStochasticVariance1994,
  title = {Multivariate Stochastic Variance Models},
  author = {Harvey, A. and Ruiz, E. and Shephard, N.},
  year = {1994},
  journal = {The Review of Economic Studies},
  volume = {61},
  number = {2},
  doi = {10.2307/2297980}
}

@article{harveyTestingEqualityPrediction1997,
  title = {Testing the Equality of Prediction Mean Squared Errors},
  author = {Harvey, David and Leybourne, Stephen and Newbold, Paul},
  year = {1997},
  journal = {International Journal of Forecasting},
  volume = {13},
  number = {2},
  doi = {10.1016/S0169-2070(96)00719-4}
}

@article{Hasbrouck_1991,
  title = {Measuring the Information Content of Stock Trades},
  author = {Hasbrouck, Joel},
  year = {1991},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1991.tb03749.x},
  mag_id = {2108032253},
  pmcid = {null},
  pmid = {null}
}

@article{Hasbrouck_1995,
  title = {One Security, Many Markets: Determining the Contributions to Price Discovery},
  author = {Hasbrouck, Joel},
  year = {1995},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1995.tb04054.x},
  mag_id = {2153900362},
  pmcid = {null},
  pmid = {null}
}

@article{Hasbrouck_2007,
  title = {Empirical Market Microstructure: The Institutions, Economics, and Econometrics of Securities Trading},
  author = {Hasbrouck, Joel},
  year = {2007},
  journal = {null},
  doi = {null},
  mag_id = {653452102},
  pmcid = {null},
  pmid = {null}
}

@article{hasbrouckTradesQuotesInventories1988,
  title = {Trades, Quotes, Inventories, and Information},
  author = {Hasbrouck, Joel},
  year = {1988},
  journal = {Journal of Financial Economics},
  volume = {22},
  number = {2},
  doi = {10.1016/0304-405X(88)90070-0},
  note = {\section{Annotations\\
(26/10/2022, 07:35:44)}

\par
``The reported values are the proportion of transactions and proportion of trade volume classified as buy or sell using the indicated rule. Classifications made by the preceding quote are those in which the transaction price is at or outside the prevailing bid or ask. The remaining rules deal with cases in which the reported transaction price is between the prevailing quotes. Using the contemporaneous transaction rule, when a midpoint transaction is found to be contemporaneous with a transaction at a quote, the buy/sell classification of the latter is applied to the former. Using the subseque.it transaction role, when a transaction at a posted quote occurs subsequent to a midpoint transaction, but prior to a quote revision, the midpoint is taken as the de fcto quote. Using the subsequent quote revision rule, when quote revision immediately follows a midpoint transaction, the order type is inferred from the direction of the quote revision.'' (Hasbrouck, 1988, p. 12)
\par
``The model described here is one of a market-maker who sets take-it-orleave-it quotes and traders who trade against these quotes. In this framework, a trade can easily be classified as a buy or sell order hy reference to the current quotes. The proportion of trades and trading volumes that can be classified in this fashion for the current data set is reported in table 1 . Roughly \$85 \textbackslash\%\$ of the transactions can be so classified.

The substantial remainder consists of transactions occurring at the midpoint of the current quotes. (The most common situation involves a bid-ask spread of \$\textbackslash frac\{1\}\{4\}\$ and a transaction on the middle \$\textbackslash frac\{1\}\{8\}\$ th.) Although the present model does not allow for midpoint transactions, they are clearly an important consideration.

Midpoint transactions can arise in several situations. The most benign occurs when market buy and sell orders exist simultaneously. A Market orders may not be exccuted instantly. . . Lour broker acting as agent for a public customer may have some leeway in presenting the order to the specialist and other floor brokers. This leads to the possibility that market buy and sell orders will exist at the same time. Under exchange rules, the buy order must be presented at one minimum fractior (generally \$\textbackslash frac\{1\}\{8\}\$ ) below the current ask and the sell order must be presented at one minimum fraction above the current bid. When these prices coincide, the orders are crossed at the quose midpoint. For the present purposes, the crossing of public orders is essentially immaterial: there is no effect on market-maker inventory and no information can be inferred from two ofisetting orders. Unfortunately, at no time did I observe such simultaneous existence of orders and discussions with a specialist confirmed that such events are relatively rare.

More often, the midpoint transaction arises when the specialist or floor trader responds to a broker's inquiry by bettering the current quote. Suppose, for example, that the quotes currently stand at \$\textbackslash left(20,20 \textbackslash frac\{1\}\{4\}\textbackslash right)\$, but when a trader presents a buy order at \$20 \textbackslash frac\{1\}\{8\}\$, the market-maker accepts it. Microstructure models typically assume that the posted qu\'otes accurately reflect the marketmaker's demand function, and do not permit this behavior. In these situations, it appears that the market-maker benefits from concealing his actual propensity to trade.

A though no clear-cut rules help the researchers classify midpoint transactiuns in these cases some considerations offer partial relief. These admittedly crude ruies are described below, and the proportions of transactions classified are reported in table 1.

1. Contemporanevus Transaction. 1 occasionally observed that when a midpoint transaction occurs it represents only a portion of the total order size, with the remainder being filled at the quoted price. Within this data set, therefore, when a mickoint transaeticn is found to be contemporaneous with a transaction at a quote price, the buy/sell classification of the latter is applied to the former.\\
2. Subsequent Transaction. When a transaction at a posted quote occurs after a midpoint transaction, but before a quote revision, the midpoint is taken as the de facto quote. For example, if the current quotes are (bid, ask) \$\textbackslash left(20,20 \textbackslash frac\{1\}\{4\}\textbackslash right.\$ ) and a midpoint transaction occurs (at \$20 \textbackslash frac\{1\}\{8\}\$ ), and if a later transaction occurs at \$20 \textbackslash frac\{1\}\{4\}\$ (a buy), then \$20 \textbackslash frac\{1\}\{8\}\$ is takeri as the de facto bid quote and the midpoint transaction is considered a saie.\\
3. Subsequent Quote Revision. When quote revision immediately follows a midpoint transaction the order type is inferred from the direction of the revision. In this view, a midpoint transaction indicates the de facto quote by marking the side of the market opposite the direction of the movement. Take, for example, current quotes at \$\textbackslash left(20,20 \textbackslash frac\{1\}\{4\}\textbackslash right)\$, a midpoint transaction at \$20 \textbackslash frac\{1\}\{8\}\$, followed by new quotes \$?\^2\textbackslash left(20 \textbackslash frac\{1\}\{8\}, 20 \textbackslash frac\{3\}\{8\}\textbackslash right)\$. The midpoint transaction is classed as a sell, with the interpretation that the seller is unwilling to meet the currently posted bid quote, necessitating an upward revision.

Since this classification scheme uses subsequent quotes to determine trade sign, it causes a spurious relation between trades and subsequent quote revisions, which will be confounded with the actual effect. Accordingly, \$z\_z\$ based on the above classification scheme were used only in the univariate time-series analysis of trade behavior. In the bivariate trade/quote revision analyses, trades were classified solely on the basis of preceding quotes.'' (Hasbrouck, 1988, p. 12)
\par
``AhhO no clear-cut rules help the tiWs in cases some considerations offer p'' (Hasbrouck, 1988, p. 13)}
}

@book{hastietrevorElementsStatisticalLearning2009,
  title = {The Elements of Statistical Learning},
  author = {Hastie, Trevor, Sami and Friedman, Harry and Tibshirani, Robert},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer}},
  address = {{New York, NY}},
  note = {\section{Annotations\\
(14/12/2022, 06:57:13)}

\par
``It is difficult to give a general rule on how to choose the number of observations in each of the three parts, as this depends on the signal-tonoise ratio in the data and the training sample size. A typical split might be 50\% for training, and 25\% each for validation and testing:'' (Hastie, Trevor et al., 2009, p. 241)
\par
``Of course, the main caveat here is ``independent,'' and bagged trees are not. Figure 8.11 illustrates the power of a consensus vote in a simulated example, where only 30\% of the voters have some knowledge. In Chapter 15 we see how random forests improve on bagging by reducing the correlation between the sampled trees. Note that when we bag a model, any simple structure in the model is lost. As an example, a bagged tree is no longer a tree. For interpretation of the model this is clearly a drawback. More stable procedures like nearest neighbors are typically not affected much by bagging. Unfortunately, the unstable models most helped by bagging are unstable because of the emphasis on interpretability, and this is lost in the bagging process.'' (Hastie, Trevor et al., 2009, p. 305)
\par
``Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one.'' (Hastie, Trevor et al., 2009, p. 324)
\par
``Let's consider a regression problem with continuous response Y and inputs X1 and X2,'' (Hastie, Trevor et al., 2009, p. 324)
\par
``To simplify matters, we restrict attention to recursive binary partitions like that in the top right panel'' (Hastie, Trevor et al., 2009, p. 324)
\par
``We first split the space into two regions, and model the response by the mean of Y in each region.'' (Hastie, Trevor et al., 2009, p. 324)
\par
``We now turn to the question of how to grow a regression tree. Our data consists of p inputs and a response, for each of N observations: that is, (xi, yi) for i = 1, 2, . . . , N , with xi = (xi1, xi2, . . . , xip). The algorithm needs to automatically decide on the splitting variables and split points, and also what topology (shape) the tree should have. Suppose first that we have a partition into M regions R1, R2, . . . , RM , and we model the response as a constant cm in each region: f (x) = M {$\sum$} m=1'' (Hastie, Trevor et al., 2009, p. 326)
\par
``{$\sum$}(yi - f (xi))2, it is easy to see that the best \textasciicircum{} cm is just the average of yi in region Rm: \textasciicircum{} cm = ave(yi|xi {$\in$}'' (Hastie, Trevor et al., 2009, p. 326)
\par
``Now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible. Hence we proceed with a greedy algorithm. Starting with all of the data, consider a splitting variable j and split point s,'' (Hastie, Trevor et al., 2009, p. 326)
\par
``For each splitting variable, the determination of the split point s can be done very quickly and hence by scanning through all of the inputs, determination of the best pair (j, s) is feasible.'' (Hastie, Trevor et al., 2009, p. 326)
\par
``Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions.'' (Hastie, Trevor et al., 2009, p. 326)
\par
``How large should we grow the tree? Clearly a very large tree might overfit the data, while a small tree might not capture the important structure.'' (Hastie, Trevor et al., 2009, p. 326)
\par
``Then this large tree is pruned using cost-complexity pruning, which we now describe.'' (Hastie, Trevor et al., 2009, p. 327)
\par
``Besides the size of the constituent trees, J, the other meta-parameter of gradient boosting is the number of boosting iterations M . Each iteration usually reduces the training risk L(fM ), so that for M large enough this risk can be made arbitrarily small. However, fitting the training data too well can lead to overfitting, which degrades the risk on future predictions. Thus, there is an optimal number M {${_\ast}$} minimizing future risk that is application dependent. A convenient way to estimate M {${_\ast}$} is to monitor prediction risk as a function of M on a validation sample. The value of M that minimizes this risk is taken to be an estimate of M {${_\ast}$}. This is analogous to the early stopping strategy often used with neural networks (Section 11.4)'' (Hastie, Trevor et al., 2009, p. 383)
\par
``10.12.1 Shrinkage Controlling the value of M is not the only possible regularization strategy. As with ridge regression and neural networks, shrinkage techniques can be employed as well (see Sections 3.4.1 and 11.5). The simplest implementation of shrinkage in the context of boosting is to scale the contribution of each tree by a factor 0 {$<$} {$\nu$} {$<$} 1 when it is added to the current approximation. That is, line 2(d) of Algorithm 10.3 is replaced by fm(x) = fm-1(x) + {$\nu$} {$\cdot$} J {$\sum$} j=1 {$\gamma$}jmI(x {$\in$} Rjm). (10.41) The parameter {$\nu$} can be regarded as controlling the learning rate of the boosting procedure. Smaller values of {$\nu$} (more shrinkage) result in larger training risk for the same number of iterations M . Thus, both {$\nu$} and M control prediction risk on the training data. However, these parameters d'' (Hastie, Trevor et al., 2009, p. 383)
\par
``10.12 Regularization 365 not operate independently. Smaller values of {$\nu$} lead to larger values of M for the same training risk, so that there is a tradeoff between them.'' (Hastie, Trevor et al., 2009, p. 384)
\par
``Empirically it has been found (Friedman, 2001) that smaller values of {$\nu$} favor better test error, and require correspondingly larger values of M . In fact, the best strategy appears to be to set {$\nu$} to be very small ({$\nu$} {$<$} 0.1) and then choose M by early stopping.'' (Hastie, Trevor et al., 2009, p. 384)
\par
``We saw in Section 8.7 that bootstrap averaging (bagging) improves the performance of a noisy classifier through averaging. Chapter 15 discusses in some detail the variance-reduction mechanism of this sampling followed by averaging. We can exploit the same device in gradient boosting, both to improve performance and computational efficiency. With stochastic gradient boosting (Friedman, 1999), at each iteration we sample a fraction {$\eta$} of the training observations (without replacement), and grow the next tree using that subsample. The rest of the algorithm is identical. A typical value for {$\eta$} can be 1 2 , although for large N , {$\eta$} can be substantially smaller than 1 2.'' (Hastie, Trevor et al., 2009, p. 384)}
}

@article{Hausman_1992,
  title = {An Ordered Probit Analysis of Transaction Stock Prices},
  author = {Hausman, Jerry A. and Lo, Andrew W. and MacKinlay, A. Craig},
  year = {1992},
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405x(92)90038-y},
  mag_id = {3124399064},
  pmcid = {null},
  pmid = {null}
}

@inproceedings{heatonEmpiricalAnalysisFeature2016,
  title = {An Empirical Analysis of Feature Engineering for Predictive Modeling},
  booktitle = {{{SoutheastCon}} 2016},
  author = {Heaton, Jeff},
  year = {2016},
  eprint = {1701.07852},
  eprinttype = {arxiv},
  doi = {10.1109/SECON.2016.7506650},
  archiveprefix = {arXiv}
}

@misc{heBagTricksImage2018,
  title = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = {2018},
  number = {arXiv:1812.01187},
  eprint = {1812.01187},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(27/10/2022, 19:02:06)}

\par
``Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study.'' (He et al., 2018, p. 1)
\par
``Mini-batch SGD groups multiple samples to a minibatch to increase parallelism and decrease communication costs. Using large batch size, however, may slow down the training progress. For convex problems, convergence rate decreases as batch size increases'' (He et al., 2018, p. 3)
\par
``Linear scaling learning rate. In mini-batch SGD, gradient descending is a random process because the examples are randomly selected in each batch. Increasing the batch size does not change the expectation of the stochastic gradient but reduces its variance. In other words, a large batch size reduces the noise in the gradient, so we may increase the learning rate to make a larger progress along the opposite of the gradient direction.'' (He et al., 2018, p. 3)
\par
``Learning rate warmup. At the beginning of the training, all parameters are typically random values and therefore far away from the final solution. Using a too large learning rate may result in numerical instability. In the warmup heuristic, we use a small learning rate at the beginning and then switch back to the initial learning rate when the training process is stable [9]'' (He et al., 2018, p. 3)
\par
``o bias decay. The weight decay is often applied to all learnable parameters including both weights and bias. It's equivalent to applying an L2 regularization to all parameters to drive their values towards 0'' (He et al., 2018, p. 3)
\par
``Other parameters, including the biases and {$\gamma$} and {$\beta$} in BN layers, are left unregularized.'' (He et al., 2018, p. 3)
\par
``Despite the performance benefit, a reduced precision has a narrower range that makes results more likely to be out-ofrange and then disturb the training progress.'' (He et al., 2018, p. 3)
\par
``Learning rate adjustment is crucial to the training. After the learning rate warmup described in Section 3.1, we typically steadily decrease the value from the initial learning rate. The widely used strategy is exponentially decaying the learning rate.'' (He et al., 2018, p. 5)
\par
``In contrast to it, Loshchilov et al. [18] propose a cosine annealing strategy. An simplified version is decreasing the learning rate from the initial value to 0 by following the cosine function.'' (He et al., 2018, p. 5)
\par
``These scores can be normalized by the softmax operator to obtain predicted probabilities.'' (He et al., 2018, p. 6)
\par
``The idea of label smoothing was first proposed to train Inception-v2 [26]. It changes the construction of the true probability to qi = \{ 1 - {$\epsilon$} if i = y, {$\epsilon$}/(K - 1) otherwise, (4) where {$\epsilon$} is a small constant. Now the optimal solution becomes z{${_\ast}$} i= \{ log((K - 1)(1 - {$\epsilon$})/{$\epsilon$}) + {$\alpha$} if i = y, {$\alpha$} otherwise, (5) where {$\alpha$} can be an arbitrary real number. This encourages a finite output from the fully-connected layer and can generalize better.''\vphantom{\}\}} (He et al., 2018, p. 6)
\par
``In knowledge distillation [10], we use a teacher model to help train the current model, which is called the student model. The teacher model is often a pre-trained model with higher accuracy, so by imitation, the student model is able to improve its own accuracy while keeping the model complexity the same.'' (He et al., 2018, p. 6)
\par
``Here we consider another augmentation method called mixup [29]. In mixup, each time we randomly sample two examples (xi, yi) and (xj, yj).'' (He et al., 2018, p. 7)
\par
``In this paper, we survey a dozen tricks to train deep convolutional neural networks to improve model accuracy.'' (He et al., 2018, p. 8)
\par
``More excitingly, stacking all of them together leads to a significantly higher accuracy. In addition, these improved pre-trained models sho'' (He et al., 2018, p. 8)
\par
``strong advantages in transfer learning, which improve both object detection and semantic segmentation. We believe the benefits can extend to broader domains where classification base models are favored.'' (He et al., 2018, p. 9)
\par
Comment: 10 pages, 9 tables, 4 figures}
}

@misc{heDeepResidualLearning2015,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Tech report}
}

@article{heegaardPhDResearchPlan,
  title = {{{PhD}} Research Plan},
  author = {Heegaard, Poul}
}

@misc{hegselmannTabLLMFewshotClassification2022,
  title = {{{TabLLM}}: Few-Shot Classification of Tabular Data with Large Language Models},
  author = {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  year = {2022},
  number = {arXiv:2210.10723},
  eprint = {2210.10723},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@misc{hendrycksGaussianErrorLinear2020,
  title = {Gaussian Error Linear Units (Gelus)},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2020},
  number = {arXiv:1606.08415},
  eprint = {1606.08415},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Trimmed version of 2016 draft; add exact formula}
}

@misc{hertzPrompttoPromptImageEditing2022,
  title = {Prompt-to-Prompt Image Editing with Cross Attention Control},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and {Cohen-Or}, Daniel},
  year = {2022},
  number = {arXiv:2208.01626},
  eprint = {2208.01626},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{hidasiRecurrentNeuralNetworks2018,
  title = {Recurrent Neural Networks with Top-k Gains for Session-Based Recommendations},
  author = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros},
  year = {2018},
  journal = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  eprint = {1706.03847},
  eprinttype = {arxiv},
  doi = {10.1145/3269206.3271761},
  archiveprefix = {arXiv}
}

@article{hintonForwardForwardAlgorithmPreliminary,
  title = {The Forward-Forward Algorithm: Some Preliminary Investigations},
  author = {Hinton, Geoffrey}
}

@article{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{hirtEndtoendProcessModel,
  title = {An End-to-End Process Model for Supervised Machine Learning Classification: From Problem to Deployment in Information Systems},
  author = {Hirt, Robin and Kuhl, Niklas and Satzger, Gerhard}
}

@article{hoangMachineLearningMethods,
  title = {Machine Learning Methods in Finance: Recent Applications and Prospects},
  author = {Hoang, Daniel and Wiegratz, Kevin}
}

@article{hoAxialAttentionMultidimensional2019,
  title = {Axial Attention in Multidimensional Transformers},
  author = {Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  year = {2019},
  journal = {arXiv:1912.12180 [cs]},
  eprint = {1912.12180},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  note = {Comment: 10 pages}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  doi = {10.1162/neco.1997.9.8.1735}
}

@article{holthausenEffectLargeBlock1987,
  title = {The Effect of Large Block Transactions on Security Prices: A Cross-Sectional Analysis},
  author = {Holthausen, Robert W. and Leftwich, Richard W. and Mayers, David},
  year = {1987},
  journal = {Journal of Financial Economics},
  volume = {19},
  number = {2},
  doi = {10.1016/0304-405X(87)90004-3},
  note = {\section{Annotations\\
(08/10/2022, 20:50:33)}

\par
``The tick classification rule The signs of the expected price effects differ for buyer- and seller-initiated transactions. but data sources do not identify the party initiating the transaction. We rely on a tick classification rule to classify transactions as buyer- or seller-initiated, as does previous research on block trades. Transactions are assumed to be seller-initiated if they trade on a downtick and buyer-initiated if they trade on an uptick (i.e., if the change in price from the prior price to the block price is negative or positive). The sensitivity of the results to this classification rule is explored in section 5 of the paper.'' (Holthausen et al., 1987, p. 8)}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  doi = {10.1016/0893-6080(89)90020-8}
}

@misc{huangSnapshotEnsemblesTrain2017,
  title = {Snapshot Ensembles: Train 1, Get {{M}} for Free},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  year = {2017},
  number = {arXiv:1704.00109},
  eprint = {1704.00109},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/10/2022, 09:21:26)}

\par
``Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules'' (Huang et al., 2017, p. 1)
\par
``Although deep networks typically never converge to a global minimum, there is a notion of ``good'' and ``bad'' local minima with respect to generalization. Keskar et al. (2016) argue that local minima with flat basins tend to generalize better. SGD tends to avoid sharper local minima because gradients are computed from small mini-batches and are therefore inexact (Keskar et al., 2016). If the learningrate is sufficiently large, the intrinsic random motion across gradient steps prevents the optimizer from reaching any of the sharp basins along its optimization path. However, if the learning rate is small, the model tends to converge into the closest local minimum.'' (Huang et al., 2017, p. 1)
\par
``Although different local minima often have very similar error rates, the corresponding neural networks tend to make different mistakes.'' (Huang et al., 2017, p. 1)
\par
``diversity can be exploited through ensembling, in which multiple neural networks are trained from different initializations and then combined with majority voting or averaging (Caruana et al., 2004)'' (Huang et al., 2017, p. 2)
\par
``Our approach leverages the non-convex nature of neural networks and the ability of SGD to converge to and escape from local minima on demand. Instead of training M neural networks independently from scratch, we let SGD converge M times to local minima along its optimization path. Each time the model converges, we save the weights and add the corresponding network to our ensemble. We then restart the optimization with a large learning rate to escape the current local minimum. More specifically, we adopt the cycling procedure suggested by Loshchilov \& Hutter (2016), in which the learning rate is abruptly raised and then quickly lowered to follow a cosine function. Because our final ensemble consists of snapshots of the optimization path, we refer to our approach as Snapshot Ensembling.'' (Huang et al., 2017, p. 2)
\par
``During testing time, one can evaluate and average the last (and therefore most accurate) m out of M models.'' (Huang et al., 2017, p. 2)
\par
``The Dropout (Srivastava et al., 2014) technique creates an ensemble out of a single model by ``dropping'' \textemdash{} or zeroing \textemdash{} random sets of hidden nodes during each mini-batch. At test time, no nodes are dropped, and each node is scaled by the probability of surviving during training. Srivastava et al. claim that Dropout reduces overfitting by preventing the co-adaptation of nodes. An alternative explanation is that this mechanism creates an exponential number of networks with shared weights during training, which are then implicitly ensembled at test time.'' (Huang et al., 2017, p. 3)
\par
``At the heart of Snapshot Ensembling is an optimization process which visits several local minima before converging to a final solution. We take model snapshots at these various minima, and average their predictions at test time.'' (Huang et al., 2017, p. 3)
\par
``Ensembles work best if the individual models (1) have low test error and (2) do not overlap in the set of examples they misclassify. Along most of the optimization path, the weight assignments of a neural network tend not to correspond to low test error. In fact, it is commonly observed that the validation error drops significantly only after the learning rate has been reduced, which is typically done after several hundred epochs.'' (Huang et al., 2017, p. 4)
\par
``We lower the learning rate at a very fast pace, encouraging the model to converge towards its first local minimum after as few as 50 epochs. The optimization is then continued at a larger learning rate, which perturbs the model and dislodges it from the minimum. We repeat this process several times to obtain multiple convergences. Formally, the learning rate {$\alpha$} has the form: {$\alpha$}(t) = f (mod (t - 1, dT /M e)) , (1) where t is the iteration number, T is the total number of training iterations, and f is a monotonically decreasing function. In other words, we split the training process into M cycles, each of which starts with a large learning rate, which is annealed to a smaller learning rate.'' (Huang et al., 2017, p. 4)
\par
``At the end of each training cycle, it is apparent that the model reaches a local minimum with respect to the training loss. Thus, before raising the learning rate, we take a ``snapshot'' of the model weights (indicated as vertical dashed black lines). After training M cycles, we have M model snapshots, f1 . . . fM , each of which will be used in the final ensemble.'' (Huang et al., 2017, p. 4)
\par
``The ensemble prediction at test time is the average of the last m (m {$\leq$} M ) model's softmax outputs. Let x be a test sample and let hi (x) be the softmax score of snapshot i. The output of the ensemble is a simple average of the last m models: hEnsemble = 1 m {$\sum$}m-1 0 hM-i (x) . We always ensemble the last m models, as these models tend to have the lowest test error.'' (Huang et al., 2017, p. 4)}
}

@misc{huangTabTransformerTabularData2020,
  title = {{{TabTransformer}}: Tabular Data Modeling Using Contextual Embeddings},
  author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  year = {2020},
  number = {arXiv:2012.06678},
  eprint = {2012.06678},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 15:01:26)}

\par
``The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy.'' (Huang et al., 2020, p. 1)
\par
``Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-theart deep learning methods for tabular data by at least 1.0\% on mean AUC, and matches the performance of tree-based ensemble models.'' (Huang et al., 2020, p. 1)
\par
``Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability.'' (Huang et al., 2020, p. 1)
\par
``The state-of-the-art for modeling tabular data is treebased ensemble methods such as the gradient boosted decision trees (GBDT)'' (Huang et al., 2020, p. 1)
\par
``The tree-based ensemble models can achieve competitive prediction accuracy, are fast to train and easy to interpret. These benefits make them highly favourable among machine learning practitioners. However, the tree-based models have several limitations in comparison to deep learning models. (a) They are not suitable for continual training from streaming data, and do not allow efficient end-to-end learning of image/text encoders in presence of multi-modality along with tabular data. (b) In their basic form they are not suitable for state-of-the-art'' (Huang et al., 2020, p. 1)
\par
``semi-supervised learning methods.'' (Huang et al., 2020, p. 1)
\par
``Although these deep learning models achieve comparable prediction accuracy, they do not address all the limitations of GBDT and MLP. Furthermore, their comparisons are done in a limited setting of a handful of datasets. In particular, in Section 3.3 we show that when compared to standard GBDT on a large collection of datasets, GBDT perform significantly better than these recent models.'' (Huang et al., 2020, p. 1)
\par
``The TabTransformer is built upon Transformers (Vaswani et al. 2017) to learn efficient contextual embeddings of categorical features.'' (Huang et al., 2020, p. 1)
\par
``In particular, TabTransformer applies a sequence of multi-head attention-based Transformer layers on parametric embeddings to transform them into contextual embeddings,'' (Huang et al., 2020, p. 2)
\par
``We find that highly correlated features (including feature pairs in the same column and cross column) result in embedding vectors that are close together in Euclidean distance, whereas no such pattern exists in contextfree embeddings learned in a baseline MLP model.'' (Huang et al., 2020, p. 2)
\par
``We also study the robustness of the TabTransformer against random missing and noisy data. The contextual embeddings make them highly robust in comparison to MLPs.'' (Huang et al., 2020, p. 2)
\par
``One of the key benefits of our proposed method for semi-supervised learning is the two independent training phases: a costly pre-training phase on unlabeled data and a lightweight fine-tuning phase on labeled data.'' (Huang et al., 2020, p. 2)
\par
``that require a single training job including both the labeled and unlabeled data.'' (Huang et al., 2020, p. 2)
\par
``The TabTransformer architecture comprises a column embedding layer, a stack of N Transformer layers, and a multilayer perceptron. Each Transformer layer (Vaswani et al. 2017) consists of a multi-head self-attention layer followed by a position-wise feed-forward layer.'' (Huang et al., 2020, p. 2)
\par
``Let (x, y) denote a feature-target pair, where x {$\equiv$} \{xcat, xcont\}. The xcat denotes all the categorical features and xcont {$\in$} Rc denotes all of the c continuous features. Let xcat {$\equiv$} \{x1, x2, {$\cdot$} {$\cdot$} {$\cdot$} , xm\} with each xi being a categorical feature, for i {$\in$} \{1, {$\cdot$} {$\cdot$} {$\cdot$} , m\}. We embed each of the xi categorical features into a parametric embedding of dimension d using Column embedding, which is explained below in detail. Let e{$\varphi$}i (xi) {$\in$} Rd for i {$\in$} \{1, {$\cdot$} {$\cdot$} {$\cdot$} , m\} be the embedding of the xi feature, and E{$\varphi$}(xcat) = \{e{$\varphi$}1 (x1), {$\cdot$} {$\cdot$} {$\cdot$} , e{$\varphi$}m (xm)\} be the set of embeddings for all the categorical features. Next, these parametric embeddings E{$\varphi$}(xcat) are inputted to the first Transformer layer. The output of the'' (Huang et al., 2020, p. 2)
\par
``first Transformer layer is inputted to the second layer Transformer, and so forth. Each parametric embedding is transformed into contextual embedding when outputted from the top layer Transformer, through successive aggregation of context from other embeddings. We denote the sequence of Transformer layers as a function f\texttheta. The function f\texttheta{} operates on parametric embeddings \{e{$\varphi$}1 (x1), {$\cdot$} {$\cdot$} {$\cdot$} , e{$\varphi$}m (xm)\} and returns the corresponding contextual embeddings \{h1, {$\cdot$} {$\cdot$} {$\cdot$} , hm\} where hi {$\in$} Rd for i {$\in$} \{1, {$\cdot$} {$\cdot$} {$\cdot$} , m\}. The contextual embeddings \{h1, {$\cdot$} {$\cdot$} {$\cdot$} , hm\} are concatenated along with the continuous features xcont to form a vector of dimension (d \texttimes{} m + c). This vector is inputted to an MLP, denoted by g{$\psi$}, to predict the target y. Let H be the cross-entropy for classification tasks and mean square error for regression tasks. We minimize the following loss function L(x, y) to learn all the TabTransformer parameters in an end-to-end learning by the first-order gradient methods. The TabTransformer parameters include {$\varphi$} for column embedding, \texttheta{} for Transformer layers, and {$\psi$} for the top MLP layer. L(x, y) {$\equiv$} H(g{$\psi$}(f\texttheta (E{$\varphi$}(xcat)), xcont), y) . (1) Below, we explain the Transformer layers and column embedding.'' (Huang et al., 2020, p. 3)
\par
``-to-end learning by the first-order gradient methods. The TabTransformer parameters include {$\varphi$} for column embedding, \texttheta{} for Transformer layers, and {$\psi$} for the top MLP layer. L(x, y) {$\equiv$} H(g{$\psi$}(f\texttheta (E{$\varphi$}(xcat)), xcont), y) . (1) Below, we'' (Huang et al., 2020, p. 3)
\par
``The first layer expands the embedding to four times its size and the second layer projects it back to its original size.'' (Huang et al., 2020, p. 3)
\par
``Since, in tabular data, there is no ordering of the features, we do not use positional encodings.'' (Huang et al., 2020, p. 3)
\par
``We explore two different types of pre-training procedures, the masked language modeling (MLM) (Devlin et al. 2019) and the replaced token detection (RTD) (Clark et al. 2020).'' (Huang et al., 2020, p. 3)
\par
``Given an input xcat = \{x1, x2, ..., xm\}, MLM randomly selects k\% features from index 1 to m and masks them as missing. The Transformer layers along with the column embeddings are trained by minimizing cross-entropy loss of a multi-class classifier that tries to predict the original features of the masked features, from the contextual embedding outputted from the top-layer Transformer.'' (Huang et al., 2020, p. 3)
\par
``Instead of masking features, RTD replaces the original feature by a random value of that feature. Here, the loss is minimized for a binary classifier that tries to predict whether or not the feature has been replaced. The RTD procedure as proposed in (Clark et al. 2020) uses auxiliary generator for sampling a subset of features that a feature should be replaced with. The reason they used an auxiliary encoder network as the generator is that there are tens of thousands of tokens in language data and a uniformly random token is too easy to detect. In contrast, (a) the number of classes within each categorical feature is typically limited; (b) a different binary classifier is defined for each column rather than a shared one, as each column has its own embedding lookup table.'' (Huang et al., 2020, p. 3)
\par
``We evaluate TabTransformer and baseline models on 15 publicly available binary classification datasets'' (Huang et al., 2020, p. 3)
\par
``Each dataset is divided into five cross-validation splits.'' (Huang et al., 2020, p. 3)
\par
``For the TabTransformer, the hidden (embedding) dimension, the number of layers and the number of attention heads are fixed to 32, 6, and 8 respectively. The MLP layer sizes are set to \{4 \texttimes{} l, 2 \texttimes{} l\}, where l is the size of its input. For hyperparameter optimization (HPO), each model is given 20 HPO rounds for each cross-validation split.'' (Huang et al., 2020, p. 4)
\par
``Note, the pre-training is only applied in semi-supervised scenario. We do not find much benefit in using it when the entire data is labeled. Its benefit is evident when there is a large number of unlabeled examples and a few labeled examples.'' (Huang et al., 2020, p. 4)
\par
``Next, we take contextual embeddings from different layers of the Transformer and compute a t-SNE plot (Maaten and Hinton 2008) to visualize their similarity in function space. More precisely, for each dataset we take its test data, pass their categorical features into a trained TabTransformer, and extract all contextual embeddings (across all columns) from a certain layer of the Transformer.'' (Huang et al., 2020, p. 4)
\par
``We can see that semantically similar classes are close Table 1: Comparison between TabTransfomers and the baseline MLP. The evaluation metric is AUC in percentage. Dataset Baseline MLP TabTransformer Gain (\%) albert 74.0 75.7 1.7 1995 income 90.5 90.6 0.1 dota2games 63.1 63.3 0.2 hcdr main 74.3 75.1 0.8 adult 72.5 73.7 1.2 bank marketing 92.9 93.4 0.5 blastchar 83.9 83.5 -0.4 insurance co 69.7 74.4 4.7 jasmine 85.1 85.3 0.2 online shoppers 91.9 92.7 0.8 philippine 82.1 83.4 1.3 qsar bio 91.0 91.8 0.8 seismicbumps 73.5 75.1 1.6 shrutime 84.6 85.6 1.0 spambase 98.4 98.5 0.1 with each other and form clusters in the embedding space. Each cluster is annotated by a set of labels.'' (Huang et al., 2020, p. 4)
\par
``n addition to prove the effectiveness of Transformer layers, on the test data we take all of the contextual embeddings from each Transformer layer of a trained TabTransformer, use the embeddings from each layer along with the continuous variables as features, and separately fit a linear model with target y. Since all of the experimental datasets are for binary classification, the linear model is logistic regression. The motivation for this evaluation is defining the success of a simple linear model as a measure of quality for the learned embeddings.'' (Huang et al., 2020, p. 4)
\par
``We further demonstrate the robustness of TabTransformer on the noisy data and data with missing values, against the baseline MLP. We consider these two scenarios only on categorical features to specifically prove the robustness of contextual embeddings from the Transformer layers.'' (Huang et al., 2020, p. 5)
\par
``On the test examples, we firstly contaminate the data by replacing a certain number of values by randomly generated ones from the corresponding columns (features)'' (Huang et al., 2020, p. 5)
\par
``As the noisy rate increases, TabTransformer performs better in prediction accuracy and thus is more robust than MLP.'' (Huang et al., 2020, p. 5)
\par
``We conjecture that the robustness comes from the contextual property of the embeddings. Despite a feature being noisy, it draws information from the correct features allowing for a certain amount of correction'' (Huang et al., 2020, p. 5)
\par
``Similarly, on the test data we artificially select a number of values to be missing and send the data with missing values to a trained TabTransformer to compute the prediction score.'' (Huang et al., 2020, p. 5)
\par
``There are two options to handle the embeddings of missing values: (1) Use the average learned embeddings over all classes in the corresponding column; (2) the embedding for the class of missing value, the additional embedding for each column mentioned in Section 2. Since the benchmark datasets do not contain enough missing values to effectively train the embedding in option (2), we use the average embedding in (1) for imputation.'' (Huang et al., 2020, p. 5)
\par
``Specifically, we compare our pretrained and then fine-tuned TabTransformer-RTD/MLM against following semi-supervised models: (a) Entropy Regularization (ER) (Grandvalet and Bengio 2006) combined with MLP and TabTransformer (b) Pseudo Labeling (PL) (Lee 2013) combined with MLP, TabTransformer, and GBDT (Jain 2017) (c) MLP (DAE): an unsupervised pre-training method designed for deep models on tabular data: the swap noise Denoising AutoEncoder (Jahrer 2018).'' (Huang et al., 2020, p. 6)
\par
``When the number of unlabeled data is large, Table 3 shows that our TabTransformer-RTD and TabTransformer-MLM significantly outperform all the other competitors. Particularly, TabTransformer-RTD/MLM improves over all the other competitors by at least 1.2\%, 2.0\% and 2.1\% on mean AUC for the scenario of 50, 200, and 500 labeled data points respectively.'' (Huang et al., 2020, p. 6)
\par
``Furthermore, we observe that when the number of unlabeled data is small as shown in Table 4, TabTransformerRTD performs better than TabTransformer-MLM, thanks to its easier pre-training task (a binary classification) than that of MLM (a multi-class classification). This is consistent with the finding of the ELECTRA paper (Clark et al. 2020).'' (Huang et al., 2020, p. 7)
\par
``Both evaluation results, Table 3 and Table 4, show that our TabTransformer-RTD and Transformers-MLM models are promising in extracting useful information from unlabeled data to help supervised training, and are particularly useful when the size of unlabeled data is large.'' (Huang et al., 2020, p. 7)
\par
``The Pseudo labeling uses the current network to infer pseudo-labels of unlabeled examples, by choosing the most confident class. These pseudo-labels are treated like human-provided labels in the cross entropy loss'' (Huang et al., 2020, p. 7)
\par
``The goal of having column embedding is to enable the model to distinguish the classes in one column from those in the other columns.'' (Huang et al., 2020, p. 10)
\par
``aining and testing conditions. As all the datasets are for binary classification, the cross entropy loss was used for both supervised and semisupervised training (for pre-training, the problem is binary classification in RTD and multi-class classification in MLM). For all deep models, the AdamW optimizer (Loshchilov and Hutter 2017) was used to update the model parameters, and a constant learning rate was applied throughout each training job. All models used early stopping based on the performance on the validation set and the early stopping patience (the number of epochs) is set as 15.'' (Huang et al., 2020, p. 10)
\par
``Table 5: Performance of TabTransformer with no column embedding, concatenation column embedding, and addition column embedding'' (Huang et al., 2020, p. 11)
\par
``For TabTransformer, the hidden (embedding) dimension, the number of layers and the number of attention heads in the Transformer were fixed to 32, 6, and 8 respectively during the experiments. The MLP layer sizes were fixed to \{4 \texttimes{} l, 2 \texttimes{} l\}, where l was the size of its input. However, these parameters were optimally selected based on 50 rounds of HPO run on 5 datasets. The search spaces were the number of attention heads \{2, 4, 8\}, the hidden dimension \{32, 64, 128, 256\}, and the number of layers \{1, 2, 3, 6, 12\}. The search spaces of the first and second hidden layer in MLP are exactly the same as those in MLP model setting. The dimension of c{$\varphi$}i , ` was chosen as d/8 based on the ablation study in Appendix A.'' (Huang et al., 2020, p. 12)
\par
``or categorical variables, the processing options include whether to one-hot encode versus learn a parametric embedding, what embedding dimension to use, and how to apply dropout regularization (whether to drop vector elements or whole embeddings). In our experiments we found that learned embeddings nearly always improved performance as long as the cardinality of the categorical variable is significantly less than the number of data points, otherwise the feature is merely a means for the model to overfit'' (Huang et al., 2020, p. 13)
\par
``For scalar variables, the processing options include how to re-scale the variable (via quantiles, normalization, or log scaling) or whether to quantize the feature and treat it like a categorical variable. While we have not explored this idea fully, the best strategy is likely to use all the different types of encoding in parallel, turning each scalar feature into three re-scaled features and one categorical feature. Unlike learning embeddings for high-cardinality categorical features, adding potentially-redundant encodings for scalar variables should not lead to overfitting, but can make the difference between a feature being useful or not.'' (Huang et al., 2020, p. 13)}
}

@article{huDoesOptionTrading2014,
  title = {Does Option Trading Convey Stock Price Information?},
  author = {Hu, Jianfeng},
  year = {2014},
  journal = {Journal of Financial Economics},
  volume = {111},
  number = {3},
  doi = {10.1016/j.jfineco.2013.12.004},
  note = {\section{Annotations\\
(25/10/2022, 07:06:18)}

\par
``This paper decomposes the total stock order imbalance into an imbalance induced by option transactions and an imbalance independent of options. The analysis shows that the option-induced imbalance significantly predicts future stock returns in the cross section controlling for the past stock and options returns, but the imbalance independent of options has only a transitory price impact.'' (Hu, 2014, p. 625)
\par
``The direction of each option transaction is determined according to the Lee and Ready (1991) algorithm. Over a fixed time interval (say, a day), the volume of the signed option transactions can be aggregated to generate an order flow measure for each option contract. Weighting the order flow of each option contract by its delta exposure, the paper computes a measure for aggregated delta exposure over all of the option transactions for each stock.'' (Hu, 2014, p. 626)
\par
``Trade Alert matches each option transaction record with the underlying stock price and computes the implied volatility from a binomial tree. Following the quote rule, Trade Alert also classifies an option trade as buyer-initiated (seller-initiated) if the transaction price is above (below) the most recent mid quote price. Such comprehensive trade data enable a cross-sectional study over a relatively long period.'' (Hu, 2014, p. 631)}
}

@book{hullOptionsFuturesOther2012,
  title = {Options, Futures, and Other Derivatives},
  author = {Hull, John},
  year = {2012},
  edition = {8th ed},
  publisher = {{Prentice Hall}},
  address = {{Boston}}
}

@book{huyenDesigningMachineLearning,
  title = {Designing Machine Learning Systems},
  author = {Huyen, Chip},
  note = {\section{Annotations\\
(09/11/2022, 16:47:20)}

\par
``Nonprobability sampling is when the selection of data isn't based on any probability criteria. Here are some of the criteria for nonprobability sampling:'' (Huyen, p. 83)
\par
``The samples selected by nonprobability criteria are not representative of the realworld data and therefore are riddled with selection biases.'' (Huyen, p. 83)
\par
``Nonprobability sampling can be a quick and easy way to gather your initial data to get your project off the ground. However, for reliable models, you might want to use probability-based sampling, which we will cover next.'' (Huyen, p. 84)
\par
``Simple Random Sampling In the simplest form of random sampling, you give all samples in the population equal probabilities of being selected.4 For example, you randomly select 10\% of the population, giving all members of this population an equal 10\% chance of being selected.'' (Huyen, p. 84)
\par
``The advantage of this method is that it's easy to implement. The drawback is that rare categories of data might not appear in your selection.'' (Huyen, p. 84)
\par
``To avoid the drawback of simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately.'' (Huyen, p. 84)
\par
``This method allows you to leverage domain expertise. For example, if you know that a certain subpopulation of data, such as more recent data, is more valuable to your model and want it to have a higher chance of being selected, you can give it a higher weight'' (Huyen, p. 85)
\par
``Importance Sampling Importance sampling is one of the most important sampling methods, not just in ML. It allows us to sample from a distribution when we only have access to another distribution'' (Huyen, p. 87)
\par
``Weak supervision Leverages (often noisy) heuristics to generate labels No, but a small number of labels are recommended to guide the development of heuristics Semisupervision Leverages structural assumptions to generate labels Yes, a small number of initial labels as seeds to generate more labels Transfer learning Leverages models pretrained on another task for your new task No for zero-shot learning Yes for fine-tuning, though the number of ground truths required is often much smaller than what would be needed if you train the model from scratch'' (Huyen, p. 94)
\par
``If weak supervision leverages heuristics to obtain noisy labels, semi-supervision leverages structural assumptions to generate new labels based on a small set of initial labels. Unlike weak supervision, semi-supervision requires an initial set of labels'' (Huyen, p. 98)
\par
``For a comprehensive review, I recommend ``Semi-Supervised Learning Literature Survey'' (Xiaojin Zhu, 2008) and ``A Survey on Semi-Supervised Learning'' (Engelen and Hoos, 2018)'' (Huyen, p. 98)
\par
``A classic semi-supervision method is self-training. You start by training a model on your existing set of labeled data and use this model to make predictions for unlabeled samples. Assuming that predictions with high raw probability scores are correct, you add the labels predicted with high probability to your training set and train a new model on this expanded training set. This goes on until you're happy with your model performance'' (Huyen, p. 98)
\par
``Another semi-supervision method assumes that data samples that share similar characteristics share the same labels.'' (Huyen, p. 98)
\par
``In some cases, semi-supervision approaches have reached the performance of purely supervised learning, even when a substantial portion of the labels in a given dataset has been discarded.17'' (Huyen, p. 99)
\par
``Semi-supervision is the most useful when the number of training labels is limited. One thing to consider when doing semi-supervision with limited data is how much of this limited data should be used to evaluate multiple candidate models and select the best one. If you use a small amount, the best performing model on this small evaluation set might be the one that overfits the most to this set. On the other hand, if you use a large amount of data for evaluation, the performance boost gained by selecting the best model based on this evaluation set might be less than the boost gained by adding the evaluation set to the limited training set.'' (Huyen, p. 99)
\par
``Active learning is a method for improving the efficiency of data labels. The hope here is that ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.'' (Huyen, p. 101)
\par
``The first reason is that class imbalance often means there's insufficient signal for your model to learn to detect the minority classes. In the case where there is a small number of instances in the minority class, the problem becomes a few-shot learning problem where your model only gets to see the minority class a few times before having to make a decision on it.'' (Huyen, p. 103)
\par
``The second reason is that class imbalance makes it easier for your model to get stuck in a nonoptimal solution by exploiting a simple heuristic instead of learning anything useful about the underlying pattern of the data.'' (Huyen, p. 103)
\par
``The third reason is that class imbalance leads to asymmetric costs of error\textemdash the cost of a wrong prediction on a sample of the rare class might be much higher than a wrong prediction on a sample of the majority class.'' (Huyen, p. 104)
\par
``Data-level methods: Resampling Data-level methods modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn. A common family of techniques is resampling. Resampling includes oversampling, adding more instances from the minority classes, and undersampling, removing instances of the majority classes. The simplest way to undersample is to randomly remove instances from the majority class, whereas the simplest way to oversample is to randomly make copies of the minority class until you have a ratio that you're happy with.'' (Huyen, p. 109)
\par
``A popular method of oversampling low-dimensional data is SMOTE (synthetic minority oversampling technique).39'' (Huyen, p. 109)
\par
``Undersampling runs the risk of losing important data from removing data. Oversampling runs the risk of overfitting on training data, especially if the added copies of the minority class are replicas of existing data.'' (Huyen, p. 110)
\par
``Algorithm-level methods If data-level methods mitigate the challenge of class imbalance by altering the distribution of your training data, algorithm-level methods keep the training data distribution intact but alter the algorithm to make it more robust to class imbalance.'' (Huyen, p. 110)
\par
``Cost-sensitive learning. Back in 2001, based on the insight that misclassification of different classes incurs different costs, Elkan proposed cost-sensitive learning in which the individual loss function is modified to take into account this varying cost'' (Huyen, p. 111)
\par
``Class-balanced loss. What might happen with a model trained on an imbalanced dataset is that it'll bias toward majority classes and make wrong predictions on minority classes.'' (Huyen, p. 112)
\par
``Missing not at random (MNAR) This is when the reason a value is missing is because of the true value itself. In this example, we might notice that some respondents didn't disclose their income. Upon investigation it may turn out that the income of respondents who failed to report tends to be higher than that of those who did disclose. The income values are missing for reasons related to the values themselves. Missing at random (MAR) This is when the reason a value is missing is not due to the value itself, but due to another observed variable. In this example, we might notice that age values are often missing for respondents of the gender ``A,'' which might be because the people of gender A in this survey don't like disclosing their age. Missing completely at random (MCAR) This is when there's no pattern in when the value is missing. In this example, we might think that the missing values for the column ``Job'' might be completely random, not because of the job itself and not because of any other variable. People just forget to fill in that value sometimes for no particular reason. However, this type of missing is very rare. There are usually reasons why certain values are missing, and you should investigate.'' (Huyen, p. 124)
\par
``Even though deletion is tempting because it's easy to do, deleting data can lead to losing important information and introduce biases into your model. If you don't want to delete missing values, you will have to impute them, which means ``fill them with certain values.'''' (Huyen, p. 125)
\par
``Before inputting features into models, it's important to scale them to be similar ranges. This process is called feature scaling. This is one of the simplest things you can do that often results in a performance boost for your model.'' (Huyen, p. 126)
\par
``While this technique can yield performance gain in many cases, it doesn't work for all cases, and you should be wary of the analysis performed on log-transformed data instead of the original data.5'' (Huyen, p. 127)
\par
``One is that it's a common source of data leakage (this will be covered in greater detail in the section ``Data Leakage'' on page 135).'' (Huyen, p. 128)
\par
``Another is that it often requires global statistics\textemdash you have to look at the entire or a subset of training data to calculate its min, max, or mean'' (Huyen, p. 128)
\par
``The downside is that this categorization introduces discontinuities at the category boundaries\textemdash\$34,999 is now treated as completely different from \$35,000, which is treated the same as \$100,000.'' (Huyen, p. 129)
\par
``One solution to this problem is the hashing trick, popularized by the package Vowpal Wabbit developed at Microsoft.7 The gist of this trick is that you use a hash function to generate a hashed value of each category. The hashed value will become the index of that category.'' (Huyen, p. 130)
\par
``Because you can specify the hash space, you can fix the number of encoded values for a feature in advance, without having to know how many 130 | Chapter 5: Feature Engineerin'' (Huyen, p. 130)
\par
``8 Lucas Bernardi, ``Don't Be Tricked by the Hashing Trick,'' Booking.com, January 10, 2018, https://oreil.ly/VZmaY. categories there will be. For example, if you choose a hash space of 18 bits, which corresponds to 218 = 262,144 possible hashed values, all the categories, even the ones that your model has never seen before, will be encoded by an index between 0 and 262,143'' (Huyen, p. 131)
\par
``Fixed positional embedding is a special case of what is known as Fourier features. If positions in positional embeddings are discrete, Fourier features can also be continuous. Consider the task involving representations of 3D objects, such as a teapot. Each position on the surface of the teapot is represented by a three-dimensional coordinate, which is continuous. When positions are continuous, it'd be very hard to build an embedding matrix with continuous column indices, but fixed position embeddings using sine and cosine functions still work.'' (Huyen, p. 135)
\par
``The following is the generalized format for the embedding vector at coordinate v, also called the Fourier features of coordinate v. Fourier features have been shown to improve models' performance for tasks that take in coordinates (or positions) as inputs. If interested, you might want to read more about it in ``Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains'' (Tancik et al. 2020). {$\gamma$} v = a1 cos 2{$\pi$}b1Tv , a1 sin 2{$\pi$}b1Tv , ..., am cos 2{$\pi$}bmTv , am sin 2{$\pi$}bmTv T'' (Huyen, p. 135)
\par
``To prevent future information from leaking into the training process and allowing models to cheat during evaluation, split your data by time, instead of splitting randomly, whenever possible. For example, if you have data from five weeks, use the first four weeks for the train split, then randomly split week 5 into validation and test splits as shown in Figure 5-7.'' (Huyen, p. 137)
\par
``To avoid this type of leakage, always split your data first before scaling, then use the statistics from the train split to scale all the splits. Some even suggest that we split our data before any exploratory data analysis and data processing, so that we don't accidentally gain information about the test split.'' (Huyen, p. 138)
\par
``Leakage might occur if the mean or median is calculated using entire data instead of just the train split'' (Huyen, p. 138)
\par
``Poor handling of data duplication before splitting If you have duplicates or near-duplicates in your data, failing to remove them before splitting your data might cause the same samples to appear in both train and validation/test splits.'' (Huyen, p. 139)
\par
``To avoid this, always check for duplicates before splitting and also after splitting just to make sure. If you oversample your data, do it after splitting'' (Huyen, p. 139)
\par
``Group leakage A group of examples have strongly correlated labels but are divided into different splits.'' (Huyen, p. 139)
\par
``The example earlier about how information on whether a CT scan shows signs of lung cancer is leaked via the scan machine is an example of this type of leakage. Detecting this type of data leakage requires a deep understanding of the way data is collected.'' (Huyen, p. 140)
\par
``Measure the predictive power of each feature or a set of features with respect to the target variable (label). If a feature has unusually high correlation, investigate how this feature is generated and whether the correlation makes sense.'' (Huyen, p. 140)
\par
``Do ablation studies to measure how important a feature or a set of features is to your model. If removing a feature causes the model's performance to deteriorate significantly, investigate why that feature is so important.'' (Huyen, p. 141)
\par
``SHAP is great because it not only measures a feature's importance to an entire model, it also measures each feature's contribution to a model's specific prediction.'' (Huyen, p. 142)
\par
``Not only good for choosing the right features, feature importance techniques are also great for interpretability as they help you understand how your models work under the hood.'' (Huyen, p. 144)
\par
``Split data by time into train/valid/test splits instead of doing it randomly. \textbullet{} \textbullet{} If you oversample your data, do it after splitting. \textbullet{} Scale and normalize your data after splitting to avoid data leakage. \textbullet{} \textbullet{} Use statistics from only the train split, instead of the entire data, to scale your \textbullet{} features and handle missing values.'' (Huyen, p. 146)
\par
``Understand how your data is generated, collected, and processed. Involve \textbullet{} domain experts if possible. \textbullet{} Keep track of your data's lineage. \textbullet{} \textbullet{} Understand feature importance to your model. \textbullet{} \textbullet{} Use features that generalize well. \textbullet{} Remove no longer useful features from your models.'' (Huyen, p. 147)
\par
``A simple way to estimate how your model's performance might change with more data is to use learning curves. A learning curve of a model is a plot of its performance\textemdash e.g., training loss, training accuracy, validation accuracy\textemdash against the number of training samples it uses, as shown in Figure 6-1.'' (Huyen, p. 153)
\par
``With different sets of hyperparameters, the same model can give drastically different performances on the same dataset. Melis et al. showed in their 2018 paper ``On the State of the Art of Evaluation in Neural Language Models'' that weaker models with well-tuned hyperparameters can outperform stronger, fancier models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters for a given model within a search space\textemdash the performance of each set evaluated on a validation set.'' (Huyen, p. 173)
\par
``Simple heuristic Forget ML. If you just make predictions based on simple heuristics, what performance would you expect?'' (Huyen, p. 180)
\par
``ero rule baseline The zero rule baseline is a special case of the simple heuristic baseline when your baseline model always predicts the most common class.'' (Huyen, p. 180)
\par
``In academic settings, when evaluating ML models, people tend to fixate on their performance metrics. However, in production, we also want our models to be robust, fair, calibrated, and overall make sense. We'll introduce some evaluation methods that help with measuring these characteristics of a model.'' (Huyen, p. 181)
\par
``lice-based evaluation Slicing means to separate your data into subsets and look at your model's performance on each subset separately. A common mistake that I've seen in many companies is that they are focused too much on coarse-grained metrics like overall F1 or accuracy on the entire data and not enough on sliced-based metrics. This can lead to two problems.'' (Huyen, p. 185)
\par
``A fascinating and seemingly counterintuitive reason why slice-based evaluation is crucial is Simpson's paradox, a phenomenon in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This means that model B can perform better than model A on all data together, but model A performs better than model B on each subgroup separately.'' (Huyen, p. 186)
\par
``Heuristics-based Slice your data using domain knowledge you have of the data and the task at hand. For example, when working with web traffic, you might want to slice your data along dimensions like mobile versus desktop, browser type, and locations. Mobile users might behave very differently from desktop users. Similarly, internet users in different geographic locations might have different expectations on what a website should look like.47 Error analysis Manually go through misclassified examples and find patterns among them. We discovered our model's problem with mobile users when we saw that most of the misclassified examples were from mobile users. Slice finder There has been research to systemize the process of finding slices, including Chung et al.'s ``Slice Finder: Automated Data Slicing for Model Validation'' in 2019 and covered in Sumyea Helal's ``Subgroup Discovery Algorithms: A Survey and Empirical Evaluation'' (2016). The process generally starts with generating slice candidates with algorithms such as beam search, clustering, or decision, then pruning out clearly bad candidates for slices, and then ranking the candidates that are left.'' (Huyen, p. 188)}
}

@book{hyndmanForecastingPrinciplesPractice2021,
  title = {Forecasting: Principles and Practice},
  author = {Hyndman, Rob J and Athanasopoulos, George},
  year = {2021},
  edition = {Third}
}

@article{inceINDIVIDUALEQUITYRETURN2006,
  title = {Individual Equity Return Data from Thomson Datastream: Handle with Care!},
  author = {Ince, Ozgur S. and Porter, R. Burt},
  year = {2006},
  journal = {Journal of Financial Research},
  volume = {29},
  number = {4},
  doi = {10.1111/j.1475-6803.2006.00189.x}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{ismailfawazDeepLearningTime2019,
  title = {Deep Learning for Time Series Classification: A Review},
  author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2019},
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {4},
  doi = {10.1007/s10618-019-00619-1}
}

@misc{ivanovDataMovementAll2021,
  title = {Data Movement Is All You Need: A Case Study on Optimizing Transformers},
  author = {Ivanov, Andrei and Dryden, Nikoli and {Ben-Nun}, Tal and Li, Shigang and Hoefler, Torsten},
  year = {2021},
  number = {arXiv:2007.00072},
  eprint = {2007.00072},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 22 pages, 8 figures; MLSys 2021 camera ready}
}

@misc{izmailovAveragingWeightsLeads2019,
  title = {Averaging Weights Leads to Wider Optima and Better Generalization},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  number = {arXiv:1803.05407},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/10/2022, 09:52:46)}

\par
``We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training.'' (Izmailov et al., 2019, p. 1)
\par
``we show that an equally weighted average of the points traversed by SGD with a cyclical or high constant learning rate, which we refer to as Stochastic Weight Averaging (SWA), has many surprising and promising features for training deep neural networks, leading to a better understanding of the geometry of their loss surfaces.'' (Izmailov et al., 2019, p. 1)
\par
``WA achieves notable improvement for training a broad range of architectures over several consequential benchmarks.'' (Izmailov et al., 2019, p. 2)
\par
``SWA is extremely easy to implement and has virtually no computational overhead compared to the conventional training schemes.'' (Izmailov et al., 2019, p. 2)
\par
``the name SWA has two meanings: on the one hand, it is an average of SGD weights. On the other, with a cyclical or constant learning rate, SGD proposals are approximately sampling from the loss surface of the DNN, leading to stochastic weights.'' (Izmailov et al., 2019, p. 3)
\par
``We run SGD with cyclical and constant learning rate schedules starting from a pretrained point for a Preactivation ResNet-164 on CIFAR-100. We then use the first, middle and last point of each of the trajectories to define a 2-dimensional plane in the weight space containing all affine combinations of these points'' (Izmailov et al., 2019, p. 4)
\par
``The main difference between the two approaches is that the individual proposals of SGD with a cyclical learning rate schedule are in general much more accurate than the proposals of a fixed-learning rate SGD. After making a large step, SGD with a cyclical learning rate spends several epochs fine-tuning the resulting point with a decreasing learning rate. SGD with a fixed learning rate on the other hand is always making steps of relatively large sizes, exploring more efficiently than with a cyclical learning rate, but the individual proposals are worse.'' (Izmailov et al., 2019, p. 4)
\par
``if mod(i, c) = 0 then nmodels {$\leftarrow$} i/c \{Number of models\} wSWA {$\leftarrow$} wSWA{$\cdot$}nmodels+w nmodels+1 \{Update average\}'' (Izmailov et al., 2019, p. 5)
\par
Comment: Appears at the Conference on Uncertainty in Artificial Intelligence (UAI), 2018}
}

@inproceedings{jacobGroupLassoOverlap2009,
  title = {Group Lasso with Overlap and Graph Lasso},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}} - {{ICML}} '09},
  author = {Jacob, Laurent and Obozinski, Guillaume and Vert, Jean-Philippe},
  year = {2009},
  publisher = {{ACM Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1145/1553374.1553431}
}

@misc{jainAttentionNotExplanation2019,
  title = {Attention Is Not Explanation},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  number = {arXiv:1902.10186},
  eprint = {1902.10186},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Accepted as NAACL 2019 Long Paper}
}

@article{japkowiczClassImbalanceProblem2002,
  title = {The Class Imbalance Problem: A Systematic Study},
  author = {Japkowicz, Nathalie and Stephen, Shaju},
  year = {2002},
  journal = {Intelligent Data Analysis},
  volume = {6},
  number = {5},
  doi = {10.3233/IDA-2002-6504}
}

@inproceedings{jawaharWhatDoesBERT2019,
  title = {What {{Does BERT Learn}} about the {{Structure}} of {{Language}}?},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1356},
  note = {\section{Annotations\\
(29/01/2023, 18:56:48)}

\par
``We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top.'' (Jawahar et al., 2019, p. 3651)
\par
``BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.'' (Jawahar et al., 2019, p. 3651)
\par
``We have shown that phrasal representations learned by BERT reflect phraselevel information and that BERT composes a hierarchy of linguistic signals ranging from surface to semantic features. We have also shown that BERT requires deeper layers to model long-range dependency information. Finally, we have shown that BERT's internal representations reflect a compositional modelling that shares parallels with traditional syntactic analysis. It would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and with higher word order flexibility as experienced in some morphologically-rich languages.'' (Jawahar et al., 2019, p. 3655)}
}

@inproceedings{jinDatadrivenApproachPredict2015,
  title = {A Data-Driven Approach to Predict Default Risk of Loan for Online Peer-to-Peer ({{P2P}}) Lending},
  booktitle = {2015 {{Fifth International Conference}} on {{Communication Systems}} and {{Network Technologies}}},
  author = {Jin, Yu and Zhu, Yudan},
  year = {2015},
  doi = {10.1109/CSNT.2015.25}
}

@misc{jinPruningEffectGeneralization2022,
  title = {Pruning's Effect on Generalization through the Lens of Training and Regularization},
  author = {Jin, Tian and Carbin, Michael and Roy, Daniel M. and Frankle, Jonathan and Dziugaite, Gintare Karolina},
  year = {2022},
  number = {arXiv:2210.13738},
  eprint = {2210.13738},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 49 pages, 20 figures}
}

@article{johnsonSurveyDeepLearning2019,
  title = {Survey on Deep Learning with Class Imbalance},
  author = {Johnson, Justin M. and Khoshgoftaar, Taghi M.},
  year = {2019},
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  doi = {10.1186/s40537-019-0192-5}
}

@article{Jondeau_2008,
  title = {Optimal Liquidation Strategies in Illiquid Markets},
  author = {Jondeau, Eric and Perilla, Augusto and Rockinger, Michael},
  year = {2008},
  journal = {null},
  doi = {10.2139/ssrn.1431869},
  mag_id = {1795368712},
  pmcid = {null},
  pmid = {null}
}

@misc{josseConsistencySupervisedLearning2020,
  title = {On the Consistency of Supervised Learning with Missing Values},
  author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = {2020},
  number = {arXiv:1902.06931},
  eprint = {1902.06931},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(04/12/2022, 14:40:09)}

\par
``A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative.'' (Josse et al., 2020, p. 1)
\par
``This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data, through multiple imputation.'' (Josse et al., 2020, p. 1)
\par
``These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the ``missing incorporated in attribute'' method as it can handle both non-informative and informative missing values'' (Josse et al., 2020, p. 1)
\par
``The classical literature on missing values, led by Rubin (1976), defines missing-values mechanisms based on the relationship between missingness and observed values: if they are independent, the mechanism is said to be Missing Completely At Random (MCAR); if the missingness only depends on the observed values, then it is Missing At Random (MAR); otherwise it is Missing Not At Random (MNAR). However, this nomenclature has seldom been discussed in the context of supervised learning, accounting for the target variable of the prediction.'' (Josse et al., 2020, p. 2)
\par
``Listwise deletion, i.e. removing incomplete observations, may allow to train the model on complete data. Yet it does not suffice for supervised learning, as the test set may also contain incomplete data. Hence the prediction procedure should handle missing values. A popular solution is to impute missing values, that is to replace them with plausible values to produce a completed data set.'' (Josse et al., 2020, p. 2)
\par
``The benefit of imputation is that it adapts existing pipelines and software to the presence of missing values. The widespread practice of imputing with the mean of the variable on the observed entries is known to have serious drawbacks as it distorts the joint and marginal distributions of the data which induces bias in estimators (Little and Rubin, 2019).'' (Josse et al., 2020, p. 2)
\par
``In Section 5, to compare imputation to learning directly with missing values, we analyze further decision trees. Indeed, their greedy and discrete natures allow adapting them to handle missing values directly.'' (Josse et al., 2020, p. 3)
\par
``We also show the benefits for prediction of an approach often used in practice, which consists in ``adding the mask'', i.e. adding binary variables that code for the missingness of each variables as new covariates, even though this method is not recommended for parameter estimation (Jones, 1996).'' (Josse et al., 2020, p. 3)
\par
``Imputation prior to analysis'' (Josse et al., 2020, p. 6)
\par
``Most statistical models and machine-learning procedures are not designed for incomplete data. To use existing pipelines in the presence on missing values, imputing the data is commonly used to form a completed data set'' (Josse et al., 2020, p. 6)
\par
``One important issue with ``single'' imputation, i.e. predicting only one value for each missing entries, is that it forgets that some values were missing and considers imputed values and observed values in the same way. It leads to underestimation of the variance of the parameters (Little and Rubin, 2019) estimated on the completed data.'' (Josse et al., 2020, p. 7)
\par
``Supervised learning typically assumes that the data are i.i.d. In particular, an out-of-sample observation (test set) is supposed to be drawn from the same distribution as the original sample (train set). Hence it has the same missing-values mechanism. An appropriate method should be able to predict on new data with missing values.'' (Josse et al., 2020, p. 8)
\par
``This highlights the fact that the imputation method should be chosen in accordance with the learning algorithm that will be applied later on.'' (Josse et al., 2020, p. 13)
\par
``A simple option to extend CART methodology in presence of missing values is to select the best split only on the available cases for each variable. More precisely, for any node A, the best split in presence of missing values is a solution of the new optimization problem'' (Josse et al., 2020, p. 17)
\par
``This splitting strategy is described in Algorithm 1. As the missing values were not used to calculate the criterion, it is still necessary to specify to which cell they are sent. The solution consisting in discarding missing data at each step would lead to a drastic reduction of the data set and is therefore not viable.'' (Josse et al., 2020, p. 17)
\par
``Surrogate splits Once the best split is chosen via Algorithm 1, surrogate splits search for a split on another variable that induces a data partition close to the original one.'' (Josse et al., 2020, p. 17)
\par
``Probabilistic splits Another option is to propagate missing observations according to a Bernoulli distribution B( \#L \#L+\#R ), where \#L (resp. \#R) is the number of points already on the left (resp. right) (see Algorithm 3 in Appendix A.3)'' (Josse et al., 2020, p. 18)
\par
``Block propagation A third option to choose the split on the observed values, and then send all incomplete observations as a block, to a side chosen by minimizing the error (see Algorithm 4 in Appendix A.3). This is the method used in LightGBM (Ke et al., 2017).'' (Josse et al., 2020, p. 18)
\par
``A second class of methods uses missing values to compute the splitting criterion itself. Consequently, the splitting location depends on the missing values, contrary to all methods presented in Section 5.2. Its most common instance is ``missing incorporated in attribute'' (MIA, Twala et al. 2008, )'' (Josse et al., 2020, p. 19)
\par
``More specifically, MIA considers the following splits, for all splits (j, z): \textbullet{} \{{\~ } Xj {$\leq$} z or{\~ } Xj = NA\} vs \{{\~ } Xj {$>$} z\}, \textbullet{} \{{\~ } Xj {$\leq$} z\} vs \{{\~ } Xj {$>$} z or{\~ } Xj = NA\}, \textbullet{} \{{\~ } Xj 6= NA\} vs \{{\~ } Xj = NA\}. In a nutshell, for each possible split, MIA tries to send all missing values to the left or to the right, and compute for each choice the corresponding error (right-hand side in 9, as well as the error associated to separating the observed values from the missing ones.'' (Josse et al., 2020, p. 19)
\par
``Finally, it chooses the split among the previous ones with the lowest error (see Algorithm 5 in Appendix A.3).'' (Josse et al., 2020, p. 19)
\par
``MIA is thought to be a good method to apply when missing pattern is informative, as this procedure allows to cut with respect to missing/ non missing and uses missing data to compute the best splits. Note this latter property implies that the MIA approach does not require a different method to propagate missing data down the tree. Notably, MIA is implemented in the R packages partykit (Hothorn and Zeileis, 2015) and grf (Tibshirani et al., 2020), as well as in XGBoost (Chen and Guestrin, 2016) and for the HistGradientBoosting models in scikit-learn (Pedregosa et al., 2011).'' (Josse et al., 2020, p. 19)
\par
``Remark 5 Implementation: A simple way to implement MIA consists in duplicating the incomplete columns, and replacing the missing entries once by +{$\infty$} and once by -{$\infty$} (or an extreme out-of-range value). This creates two dummy variables for each original one containing missing values. Splitting along a variable and sending all missing data to the left (for example) is the same as splitting along the corresponding dummy variable where missing entries have been completed by -{$\infty$}. Alternatively, MIA can be with two scans on a feature's values in ascending and descending orders (Chen and Guestrin, 2016, Alg 3).'' (Josse et al., 2020, p. 19)
\par
``Remark 6 Implicit imputation: Whether it is in the case where the missing values are propagated in the available case method (Section 5.2), or incorporated in the split choice in MIA, missing values are assigned either to the left or the right interval. Consequently, handling missing values in a tree can be seen as implicit imputation by an interval value.'' (Josse et al., 2020, p. 20)
\par
``We have studied procedures for supervised learning with missing data. Unlike in the classic missing data literature, the goal of the procedures is to yield the best possible prediction on test data with missing values. Focusing on simple ways of adapting existing procedures, our theoretical and empirical results outline simple practical recommendations: \textbullet{} Given a model suitable for the fully observed data, good prediction can be achieved on a test set by multiple imputation of its missing values with a conditional imputation model fit on the train set (Theorem 3). \textbullet{} To train and test on data with missing values, the same imputation model should be used. Single mean imputation is consistent, provided a powerful, non-linear model (Theorem 4). \textbullet{} For tree-based models, a good solution for missing values is Missing Incorporated in Attribute (MIA, Twala et al. 2008, see implementation Remark 5): optimizing jointly the split and the handling of the missing values (Proposition 8 and experimental results). Empirically, this approach also performs well outside of MAR settings. \textbullet{} Empirically, good imputation methods applied at train and test time reduce the number of samples required to reach good prediction (Figure 5). \textbullet{} When missingness is related to the prediction target, imputation does not suffice and it is useful to add indicator variables of missing entries as features (Example 3 and Figure 3). These recommendations hold to minimize the prediction error in an asymptotic regime. More work is needed to establish theoretical results in the finite sample regime. In addition, different practices may be needed to control for the uncertainty associated to a prediction.'' (Josse et al., 2020, p. 27)}
}

@article{jurkatisInferringTradeDirections2022,
  title = {Inferring Trade Directions in Fast Markets},
  author = {Jurkatis, Simon},
  year = {2022},
  journal = {Journal of Financial Markets},
  volume = {58},
  doi = {10.1016/j.finmar.2021.100635},
  note = {\section{Annotations\\
(24/10/2022, 11:40:56)}

\par
``The established methods, most notably the algorithms of Lee and Ready (1991) (LR), Ellis et al. (2000) (EMO), and Chakrabarty et al. (2007) (CLNV), classify trades based on the proximity of the transaction price to the quotes in effect at the time of the trade. This is problematic due to the increased frequency of order submission and cancellation. With several quote changes taking place at the time of the trade, it is not clear which quotes to select for the decision rule of the algorithm.'' (Jurkatis, 2022, p. 6)
\par
``Panayides et al. (2019) use data from Euronext timestamped to seconds in 2007\textendash 2008. For data from the LSE in 2017 that is timestamped to the microsecond, the authors find an even worse performance of only 46\% accuracy.'' (Jurkatis, 2022, p. 6)
\par
``Older studies analyzing the accuracy of the LR algorithm, as well as the alternative EMO and CLNV algorithms, find classification accuracies ranging from 75\% to 93\% (e.g., Ellis et al., 2000; Finucane, 2000; Lee and Radhakrishna, 2000; Odders-White, 2000; Theissen, 2001; Chakrabarty et al., 2007).'' (Jurkatis, 2022, p. 6)
\par
``Recent proposals to counter the problem of modern fast markets for trade classification include Easley et al.'s (2012) bulk volume classification (BVC) algorithm and Holden and Jacobsen (2014) interpolation method. The latter interpolates trade and quote times of imprecisely timestamped data before applying one of the traditional algorithms.'' (Jurkatis, 2022, p. 6)
\par
``To give the improvement in classification accuracy more economic meaning, I apply the trade classification methods to the estimation of transaction costs. The transaction costs in turn are used in a portfolio optimization exercise. The results show that an investor with a mean-variance utility function would be willing to forgo up to 33 bps on yearly returns to use the proposed algorithm to estimate transaction costs instead of the LR algorithm.'' (Jurkatis, 2022, p. 7)
\par
``With several quote changes occurring at the same time as the trade, however, the choice is less clear. For example, with one trade and three quote changes recorded at the same millisecond, the quotes corresponding to the trade could be the last quotes from before the millisecond or one of the first two recorded at the millisecond. The convention in such a case is to take the last ask and bid from before the time of the trade.'' (Jurkatis, 2022, p. 7)
\par
``An alternative suggested by Holden and Jacobsen (2014) to circumvent the problem of imprecise timestamps is to transform timestamps to a higher precision. This is done by interpolating the recorded times according to: t = s + 2i - 1 2I , i = 1, ... , I, where t is the interpolated timestamp and s is the originally recorded time. I isthenumberoftradesorthenumberofchanges at the ask or bid at time s depending on which timestamp to interpolate. The algorithm then proceeds as described above using the last ask and bid price from before the time of the trade according to the interpolated time.'' (Jurkatis, 2022, p. 7)
\par
``To provide an example of how differences in trade classification can influence the results in an application, I use the competing algorithms (excluding the interpolation method) to estimate transaction costs. These estimates in turn are used in a'' (Jurkatis, 2022, p. 15)
\par
``portfolio optimization exercise assuming an investor with mean-variance utility function.15 The differences in the investor's utility obtained under the different transaction cost estimates represent the return that the investor would be willing to give up to use one estimate rather than the other. A comparison of the different utilities therefore allows me to put a ``price tag'' in return units on the different classification algorithms.'' (Jurkatis, 2022, p. 16)}
}

@article{kadanBoundExpectedStock2020,
  title = {A Bound on Expected Stock Returns},
  author = {Kadan, Ohad and Tang, Xiaoxiao},
  editor = {Van Nieuwerburgh, Stijn},
  year = {2020},
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {4},
  doi = {10.1093/rfs/hhz075}
}

@inproceedings{kadraWelltunedSimpleNets2021,
  title = {Well-Tuned Simple Nets Excel on Tabular Datasets},
  booktitle = {{{NeurIPS}} 2021},
  author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  year = {2021},
  volume = {34},
  publisher = {{Curran Associates, Inc.}},
  note = {\section{Annotations\\
(26/10/2022, 10:17:45)}

\par
``we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques'' (Kadra et al., 2021, p. 1)
\par
``The extensive experiments on 40 datasets we report indeed confirm that recent neural networks [1, 46, 11] do not outperform GBDT when the hyperparameters of all methods are thoroughly tuned'' (Kadra et al., 2021, p. 1)
\par
``lies in exploiting the recent DL advances on regularization techniques (reviewed in Section 3), such as data augmentation, decoupled weight decay, residual blocks and model averaging (e.g., dropout or snapshot ensembles), or on learning dynamics (e.g., look-ahead optimizer or stochastic weight averaging)'' (Kadra et al., 2021, p. 1)
\par
``In fact, the performance improvements are quite pronounced and highly significant.1'' (Kadra et al., 2021, p. 2)
\par
``In contrast, we do not propose a new kind of neural architecture, but a novel paradigm for learning a combination of regularization methods'' (Kadra et al., 2021, p. 2)
\par
``Weight decay: The most classical approaches of regularization focused on minimizing the norms of the parameter values, e.g., either the L1 [51], the L2 [52], or a combination of L1 and L2 known as the Elastic Net [63].'' (Kadra et al., 2021, p. 2)
\par
``Data Augmentation: Among the augmentation regularizers, Cut-Out [10] proposes to mask a subset of input features (e.g., pixel patches for images) for ensuring that the predictions remain invariant to distortions in the input space'' (Kadra et al., 2021, p. 3)
\par
``Ensemble methods: Ensembled machine learning models have been shown to reduce variance and act as regularizers [45].'' (Kadra et al., 2021, p. 3)
\par
``Structural and Linearization: In terms of structural regularization, ResNet adds skip connections across layers [18], while the Inception model computes latent representations by aggregating diverse convolutional filter sizes [50].'' (Kadra et al., 2021, p. 3)
\par
``Implicit: The last family of regularizers broadly encapsulates methods that do not directly propose novel regularization techniques but have an implicit regularization effect as a virtue of their `modus operandi' [2]. The simplest such implicit regularization is Early Stopping [57], which limits overfitting by tracking validation performance over time and stopping training when validation performance no longer improves. Another implicit regularization method is Batch Normalization, which improves generalization by reducing internal covariate shift [24]. The scaled exponential linear units (SELU) represent an alternative to batch-normalization through self-normalizing activation functions [30]. On the other hand, stabilizing the convergence of the training routine is another implicit regularization, for instance by introducing learning rate scheduling schemes [35]. The recent strategy of stochastic weight averaging relies on averaging parameter values from the local optima encountered along the sequence of optimization steps [25], while another approach conducts updates in the direction of a few `lookahead' steps [61].'' (Kadra et al., 2021, p. 3)
\par
``While we can in principle use any hyperparameter optimization method, we decided to use the multi-fidelity Bayesian optimization method BOHB [12] since it achieves strong performance across a wide range of computing budgets by combining Hyperband [33] and Bayesian Optimization [40], and since BOHB can deal with the categorical hyperparameters we use for enabling or disabling regularization techniques and the corresponding conditional structures.'' (Kadra et al., 2021, p. 4)
\par
``The datasets are retrieved from the OpenML repository [54] using the OpenML-Python connector [14] and split as 60\% training, 20\% validation, and 20\% testing sets. The data is standardized to have zero mean and unit variance where the statistics for the standardization are calculated on the training split.'' (Kadra et al., 2021, p. 5)
\par
``Taking into account the dimensions D of the considered configuration spaces, we ran BOHB for at most 4 days, or at most 40 \texttimes{} D hyperparameter configurations, whichever came first. During the training phase, each configuration was run for 105 epochs, in accordance with the cosine learning rate annealing with restarts (described in the following subsection).'' (Kadra et al., 2021, p. 5)
\par
``For the sake of studying the effect on more datasets, we only evaluated a single train-val-test split. After the training phase is completed, we report the results of the best hyperparameter configuration found, retrained on the joint train and validation set.'' (Kadra et al., 2021, p. 5)
\par
``We use a 9-layer feed-forward neural network with 512 units for each layer, a choice motivated by previous work [42]'' (Kadra et al., 2021, p. 6)
\par
``Moreover, we set a low learning rate of 10-3 after performing a grid search for finding the best value across datasets. We use AdamW [36], which implements decoupled weight decay, and cosine annealing with restarts [35] as a learning rate scheduler. Using a learning rate scheduler with restarts helps in our case because we keep a fixed initial learning rate.'' (Kadra et al., 2021, p. 6)
\par
``For the restarts, we use an initial budget of 15 epochs, with a budget multiplier of 2, following published practices [62]'' (Kadra et al., 2021, p. 6)
\par
``We use the Critical Difference (CD) diagram of the ranks based on the Wilcoxon significance test, a standard metric for comparing classifiers across multiple datasets [9]'' (Kadra et al., 2021, p. 8)
\par
``well-regularized simple deep MLPs outperform specialized neural architectures.'' (Kadra et al., 2021, p. 8)
\par
``We conclude that well-regularized simple deep MLPs outperform GBDT, which validates Hypothesis 2 in Section 5.3.'' (Kadra et al., 2021, p. 9)
\par
``The final cumulative comparison in Figure 2c provides a further result: none of the specialized previous deep learning methods (TabNet, NODE, AutoGluon Tabular) outperforms GBDT significantly. To the best of our awareness, this paper is therefore the first to demonstrate that neural networks beat GBDT with a statistically significant margin over a large-scale experimental protocol that conducts a thorough hyperparameter optimization for all methods'' (Kadra et al., 2021, p. 9)
\par
``The grouping reveals that a cocktail for each dataset often has at least one ingredient from every regularization family (detailed in Section 3), highlighting the need for jointly applying diverse regularization methods'' (Kadra et al., 2021, p. 9)
\par
``Based on these results, we conclude that regularization cocktails are time-efficient and achieve strong anytime results, which validates Hypothesis 3 in Section 5.3'' (Kadra et al., 2021, p. 9)
\par
``Focusing on the important domain of tabular datasets, this paper studied improvements to deep learning (DL) by better regularization techniques. We presented regularization cocktails, per-dataset-optimized combinations of many regularization techniques, and demonstrated that these improve the performance of even simple neural networks enough to substantially and significantly'' (Kadra et al., 2021, p. 9)
\par
``surpass XGBoost, the current state-of-the-art method for tabular datasets.'' (Kadra et al., 2021, p. 10)
\par
``empirically showed that (i) modern DL regularization methods developed in the context of raw data (e.g., vision, speech, text) substantially improve the performance of deep neural networks on tabular data; (ii) regularization cocktails significantly outperform recent neural networks architectures, and most importantly iii) regularization cocktails outperform GBDT on tabular datasets.'' (Kadra et al., 2021, p. 10)
\par
``We also did not study datasets with extreme outliers, missing labels, semi-supervised data, streaming data, and many more modalities in which tabular data arises.'' (Kadra et al., 2021, p. 10)
\par
``An important point worth noticing is that the recent neural network architectures (Section 5.4) could also benefit from our regularization cocktails, but integrating the regularizers into these baseline libraries requires considerable coding efforts.'' (Kadra et al., 2021, p. 10)}
}

@article{Keim_1996,
  title = {The Upstairs Market for Large-Block Transactions: Analysis and Measurement of Price Effects},
  author = {Keim, Donald B. and Madhavan, Ananth},
  year = {1996},
  journal = {Review of Financial Studies},
  doi = {10.1093/rfs/9.1.1},
  mag_id = {2168038988},
  pmcid = {null},
  pmid = {null}
}

@inproceedings{keLightGBMHighlyEfficient2017,
  title = {{{LightGBM}}: A Highly Efficient Gradient Boosting Decision Tree},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@article{kellyCharacteristicsAreCovariances2019,
  title = {Characteristics Are Covariances: A Unified Model of Risk and Return},
  author = {Kelly, Bryan T. and Pruitt, Seth and Su, Yinan},
  year = {2019},
  journal = {Journal of Financial Economics},
  volume = {134},
  number = {3},
  doi = {10.1016/j.jfineco.2019.05.001}
}

@article{keskarLargeBatchTrainingDeep2017,
  title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  journal = {arXiv: 1609.04836},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  note = {Comment: Accepted as a conference paper at ICLR 2017}
}

@article{khorramEndtoendCNNLSTM2021,
  title = {End-to-End {{CNN}} + {{LSTM}} Deep Learning Approach for Bearing Fault Diagnosis},
  author = {Khorram, Amin and Khalooei, Mohammad and Rezghi, Mansoor},
  year = {2021},
  journal = {Applied Intelligence},
  volume = {51},
  number = {2},
  doi = {10.1007/s10489-020-01859-1}
}

@article{kichererSeamlesslyPortableApplications2012,
  title = {Seamlessly Portable Applications: Managing the Diversity of Modern Heterogeneous Systems},
  author = {Kicherer, Mario and Nowak, Fabian and Buchty, Rainer and Karl, Wolfgang},
  year = {2012},
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {8},
  number = {4},
  doi = {10.1145/2086696.2086721}
}

@misc{kitaevReformerEfficientTransformer2020,
  title = {Reformer: The Efficient Transformer},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = {2020},
  number = {arXiv:2001.04451},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(29/01/2023, 10:06:14)}

\par
``The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \textbullet{} Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \textbullet{} Since the depth dff of intermediate feed-forward layers is often much larger than the depth dmodel of attention activations, it accounts for a large fraction of memory use. \textbullet{} Attention on sequences of length L is O(L2) in both computational and memory complexity, so even for a single sequence of 64K tokens can exhaust accelerator memory.'' (Kitaev et al., 2020, p. 1)
\par
``Dot-product attention. The standard attention used in the Transformer is the scaled dot-product attention (Vaswani et al., 2017). The input consists of queries and keys of dimension dk, and values of dimension dv. The dot products of the query with all keys are computed, scaled by {$\surd$}dk, and a softmax function is applied to obtain the weights on the values. In practice, the attention function on a set of queries is computed simultaneously, packed together into a matrix Q. Assuming the keys and values are also packed together into matrices K and V , the matrix of outputs is defined as: Attention(Q, K, V ) = softmax( QKT {$\surd$}dk )V'' (Kitaev et al., 2020, p. 2)
\par
``Multi-head attention. In the Transformer, instead of performing a single attention function with dmodel-dimensional keys, values and queries, one linearly projects the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. Attention is applied to each of these projected versions of queries, keys and values in parallel, yielding dvdimensional output values. These are concatenated and once again projected, resulting in the final values. This mechanism is known as multi-head attention.'' (Kitaev et al., 2020, p. 2)
\par
Comment: ICLR 2020}
}

@misc{klingenbrunnTransformerImplementationTimeseries2021,
  title = {Transformer Implementation for Time-Series Forecasting},
  author = {Klingenbrunn, Natasha},
  year = {2021},
  journal = {MLearning.ai}
}

@inproceedings{kossenSelfAttentionDatapointsGoing2021,
  title = {Self-Attention between Datapoints: Going beyond Individual Input-Output Pairs in Deep Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
  year = {2021},
  volume = {34},
  publisher = {{Curran Associates, Inc.}},
  note = {\section{Annotations\\
(25/01/2023, 15:10:25)}

\par
``We stack these and apply an identical linear embedding to each of n datapoints, obtaining an input representation H(0) {$\in$} Rn\texttimes d\texttimes e (Fig. 2b).'' (Kossen et al., 2021, p. 3)}
}

@article{kraussDeepNeuralNetworks2017,
  title = {Deep Neural Networks, Gradient-Boosted Trees, Random Forests: Statistical Arbitrage on the {{S}}\&{{P}} 500},
  author = {Krauss, Christopher and Do, Xuan Anh and Huck, Nicolas},
  year = {2017},
  journal = {European Journal of Operational Research},
  volume = {259},
  number = {2},
  doi = {10.1016/j.ejor.2016.10.031},
  note = {\section{Annotations\\
(12/07/2022, 15:58:37)}

\par
``This article implements and analyses the effectiveness of deep neural networks (DNN), gradient-boosted-trees (GBT), random forests (RAF), and several ensembles of these methods in the context of statistical arbitrage. Each model is trained on lagged returns of all stocks in the S\&P 500, after elimination of survivor bias. From 1992 to 2015, daily one-day-ahead trading signals are generated based on the probability forecast of a stock to outperform the general market. The highest k probabilities are converted into long and the lowest k probabilities into short positions, thus censoring the less certain middle part of the ranking. Empirical findings are promising. A simple, equal-weighted ensemble (ENS1) consisting of one deep neural network, one gradient-boosted tree, and one random forest produces out-of-sample returns exceeding 0.45 percent per day for k = 10, prior to transaction costs'' (Krauss et al., 2017, p. 2)}
}

@article{krogerKapitelOutlierDetection,
  title = {{Kapitel 6: outlier detection}},
  author = {Kr{\"o}ger, Peer and Zimek, Arthur}
}

@article{kuhlHumanVsSupervised2020,
  title = {Human vs. Supervised Machine Learning: Who Learns Patterns Faster?},
  author = {K{\"u}hl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik},
  year = {2020},
  journal = {arXiv:2012.03661 [cs]},
  eprint = {2012.03661},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@book{kuhnFeatureEngineeringSelection2020,
  title = {Feature Engineering and Selection: A Practical Approach for Predictive Models},
  author = {Kuhn, Max and Johnson, Kjell},
  year = {2020},
  volume = {74},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, FL}},
  note = {\section{Annotations\\
(20/12/2022, 08:36:27)}

\par
``This lack of performance may be due to a simple to explain, but difficult to pinpoint, cause: relevant predictors that were collected are represented in a way that models have trouble achieving good performance.'' (Kuhn and Johnson, 2020, p. xi)
\par
``Key relationships that are not directly available as predictors may be between the response and: \textbullet{} a transformation of a predictor, \textbullet{} an interaction of two or more predictors such as a product or ratio, \textbullet{} a functional relationship among predictors, or \textbullet{} an equivalent re-representation of a predictor. Adjusting and reworking the predictors to enable models to better uncover predictorresponse relationships has been termed feature engineering.'' (Kuhn and Johnson, 2020, p. xi)
\par
``A simple logtransformation, or more complex Box-Cox or Yeo-Johnson transformation (Section 6.1), can be used to place the data on a scale where the distribution is approximately symmetric, thus removing the appearance of outliers in the data (Figure 2.2b). This kind of transformation makes sense for measurements that increase exponentially.'' (Kuhn and Johnson, 2020, p. 24)
\par
``Does this replace the test set (or, analogously, the assessment set)? No. Since the validation data are guiding the training process, they can't be used for a fair assessment for how well the modeling process is working'' (Kuhn and Johnson, 2020, p. 53)
\par
``A Box-Cox transformation (Box and Cox, 1964) was used to estimate this transformation. The Box-Cox procedure, originally intended as a transformation of a model's outcome, uses maximum likelihood estimation to estimate a transformation parameter {$\lambda$} in the equation x{${_\ast}$} = \{ x{$\lambda-$}1 {$\lambda$}{\~ } x{$\lambda-$}1 , {$\lambda$} 6= 0{\~ } x log x, {$\lambda$} = 0 where{\~ } x is the geometric mean of the predictor data. In this procedure, {$\lambda$} is estimated from the data. Because the parameter of interest is in the exponent, this type of transformation is called a power transformation. Some values of {$\lambda$} map to common transformations, such as {$\lambda$} = 1 (no transformation), {$\lambda$} = 0 (log), {$\lambda$} = 0.5 (square root), and {$\lambda$} = -1 (inverse). As you can see, the Box-Cox transformation is quite flexible in its ability to address many different data distributions.''\vphantom\} (Kuhn and Johnson, 2020, p. 122)
\par
``Also, note that both transformations are unsupervised since, in this application, the outcome is not used in the computations. While the transformation might improve the predictor distribution, it has no guarantee of improving the model. However, there are a variety of parametric models that utilize polynomial calculations on the predictor data, such as most linear models, neural networks, and support vector machines. In these situations, a skewed predictor distribution can have a harmful effect on these models since the tails of the distribution can dominate the underlying calculations.'' (Kuhn and Johnson, 2020, p. 123)
\par
``It should be noted that the Box-Cox transformation was originally used as a supervised transformation of the outcome. A simple linear model would be fit to the data and the transformation would be estimated from the model residuals. The outcome variable would be transformed using the results of the Box-Cox method. Here, th'' (Kuhn and Johnson, 2020, p. 123)
\par
``124 Chapter 6. Engineering Numeric Predictors method has been appropriated to be independently applied to each predictor and uses their data, instead of the residuals, to determine an appropriate transformation.'' (Kuhn and Johnson, 2020, p. 124)
\par
``Another common technique for modifying the scale of a predictor is to standardize its value in order to have specific properties. Centering a predictor is a common technique. The predictor's training set average is subtracted from the predictor's individual values. When this is applied separately to each variable, the collection of variables would have a common mean value (i.e., zero). Similarly, scaling is the process of dividing a variable by the corresponding training set's standard deviation. This ensures that that variables have a standard deviation of one. Alternatively, range scaling uses the training set minimum and maximum values to translate the data to be within an arbitrary range (usually zero and one). Again, it is emphasized that the statistics required for the transformation (e.g., the mean) are estimated from the training set and are applied to all data sets (e.g., the test set or new samples).'' (Kuhn and Johnson, 2020, p. 124)
\par
``These transformations are mostly innocuous and are typically needed when the model requires the predictors to be in common units. For example, when the distance or dot products between predictors are used (such as K -nearest neighbors or support vector machines) or when the variables are required to be a common scale in order to apply a penalty (e.g., the lasso or ridge regression described in Section 7.3), a standardization procedure is essential.'' (Kuhn and Johnson, 2020, p. 124)
\par
``There are a few apparent reasons for subjecting the data to such a transformation: \textbullet{} Some feel that it simplifies the analysis and/or interpretation of the results. Suppose that a person's age was a predictor and this was binned by whether someone was above 40 years old or not. One might be able to make a statement that there is a 25\% increase in the probability of the event between younger and older people. There is no discussion of per-unit increases in the response. \textbullet{} Binning may avoid the problem of having to specify the relationship between the predictor and outcome. A set of bins can be perceived as being able t'' (Kuhn and Johnson, 2020, p. 130)
\par
``6.2. 1:Many Transformations 131 model more patterns without having to visualize or think about the underlying pattern. \textbullet{} Using qualitative versions of the predictors may give the perception that it reduces the variation in the data. This is discussed at length below.'' (Kuhn and Johnson, 2020, p. 131)
\par
``There are a number of problematic issues with turning continuous data categorical. First, it is extremely unlikely that the underlying trend is consistent with the new model. Secondly, when a real trend exists, discretizing the data is most likely making it harder for the model to do an effective job since all of the nuance in the data has been removed. Third, there is probably no objective rationale for a specific cut-point. Fourth, when there is no relationship between the outcome and the predictor, there is a substantial increase in the probability that an erroneous trend will be ``discovered''. This has been widely researched and verified. See Altman (1991), Altman et al. (1994), and the references therein.'' (Kuhn and Johnson, 2020, p. 131)
\par
``Another approach to handling missing values is to impute or estimate them. Missing value imputation has a long history in statistics and has been thoroughly researched. Good places to start are Little and Rubin (2014), Van Buuren (2012) and Allison (2001). In essence, imputation uses information and relationships among the nonmissing predictors to provide an estimate to fill in the missing value.'' (Kuhn and Johnson, 2020, p. 198)
\par
``Historically, statistical methods for missing data have been concerned with the impact on inferential models. In this situation, the characteristics and quality of the imputation strategy have focused on the test statistics that are produced by the model. The goal of these techniques is to ensure that the statistical distributions are tractable and of good enough quality to support subsequent hypothesis testing. The primary approach in this scenario is to use multiple imputations; several variations of the data set are created with different estimates of the missing values. The variations of the data sets are then used as inputs to models and the test statistic replicates are computed for each imputed data set. From these replicate statistics, appropriate hypothesis tests can be constructed and used for decision making.'' (Kuhn and Johnson, 2020, p. 198)
\par
``In many predictive models, there is no notion of distributional assumptions (or they are often intractable). For example, when constructing most treebased models, the algorithm does not require any specification of a probability distribution to the predictors. As such, many predictive models are incapable of producing inferential results even if that were a primary goal.73 Given this, traditional multiple imputation methods may not have relevance for these models'' (Kuhn and Johnson, 2020, p. 198)
\par
``Since predictive models are judged on their ability to accurately predict yet-tobe-seen samples (including the test set and new unknown samples), as opposed 72A perhaps more difficult situation would be explaining to the consumers of the model that ``We know that this is important but we don't know why!'' 73Obviously, there are exceptions such as linear and logistic regression, Naive Bayes models, etc'' (Kuhn and Johnson, 2020, p. 198)
\par
``8.5. Imputation Methods 199 to statistical appropriateness, it is critical that the imputed values be as close as possible to their true (unobserved) values.'' (Kuhn and Johnson, 2020, p. 199)
\par
``The last point underscores the main objective of imputation with machine learning models: produce the most accurate prediction of the missing data point. Some other important characteristics that a predictive imputation method should have are: \textbullet{} Within a sample data point, other variables may also be missing. For this reason, an imputation method should be tolerant of other missing data. \textbullet{} Imputation creates a model embedded within another model. There is a prediction equation associated with every predictor in the training set that might have missing data. It is desirable for the imputation method to be fast and have a compact prediction equation. \textbullet{} Many data sets often contain both numeric and qualitative predictors. Rather than generating dummy variables for qualitative predictors, a useful imputation method would be able to use predictors of various types as inputs. \textbullet{} The model for predicting missing values should be relatively (numerically) stable and not be overly influenced by outlying data points.'' (Kuhn and Johnson, 2020, p. 199)
\par
``It is also important to consider that imputation is probably the first step in any preprocessing sequence. Imputing qualitative predictors prior to creating indicator variables so that the binary nature of the resulting imputations can be preserved is a good idea. Also, imputation should usually occur prior to other steps that involve parameter estimation. For example, if centering and scaling is performed on data prior to imputation, the resulting means and standard deviations will inherit the potential biases and issues incurred from data deletion.'' (Kuhn and Johnson, 2020, p. 199)}
}

@article{Kyle_1985,
  title = {Continuous Auctions and Insider Trading},
  author = {Kyle, Albert S.},
  year = {1985},
  journal = {Econometrica : journal of the Econometric Society},
  doi = {10.2307/1913210},
  mag_id = {2050031929},
  pmcid = {null},
  pmid = {null}
}

@article{lambertonIntroductionStochasticCalculus,
  title = {Introduction to Stochastic Calculus Applied to Finance, Second Edition},
  author = {Lamberton, Damien and Lapeyre, Bernard}
}

@misc{lampleLargeMemoryLayers2019,
  title = {Large Memory Layers with Product Keys},
  author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  year = {2019},
  number = {arXiv:1907.05242},
  eprint = {1907.05242},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Advances in Neural Information Processing Systems, 2019}
}

@incollection{lecunEfficientBackProp2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_3}
}

@article{leeInferringInvestorBehavior2000,
  title = {Inferring Investor Behavior: Evidence from {{TORQ}} Data},
  author = {Lee, Charles M.C. and Radhakrishna, Balkrishna},
  year = {2000},
  journal = {Journal of Financial Markets},
  volume = {3},
  number = {2},
  doi = {10.1016/S1386-4181(00)00002-1}
}

@article{leeInferringTradeDirection1991,
  title = {Inferring Trade Direction from Intraday Data},
  author = {Lee, Charles M. C. and Ready, Mark J.},
  year = {1991},
  journal = {The Journal of Finance},
  volume = {46},
  number = {2},
  doi = {10.1111/j.1540-6261.1991.tb02683.x},
  note = {\section{Annotations\\
(05/11/2022, 10:05:46)}

\par
``This paper evaluates alternative methods for classifying individual trades as market buy or market sell orders using intraday trade and quote data.'' (Lee and Ready, 1991, p. 1)
\par
``We then propose and test relatively simple procedures for improving trade classifications'' (Lee and Ready, 1991, p. 1)
\par
``In this paper, we identify two serious potential problems with this method, namely, that quotes are often recorded ahead of the trade that triggered them, and that'' (Lee and Ready, 1991, p. 1)
\par
``trades are often (30\%of the time) inside the spread'' (Lee and Ready, 1991, p. 2)
\par
``We show that misclassificationscan be greatly reduced by comparing the trade to the quote in effect 5 seconds earlier. For trades inside of the spread, we provide evidence that the effective spread is often on one side of the quoted spread due to standing orders held by floor brokers. In these cases, we suggest that the ``tick test'' provides the best way to classify the trades as buys or sells. When only price data is available, we show that the ``tick test'' also performs remarkably well'' (Lee and Ready, 1991, p. 2)
\par
``The tick test is a technique which infers the direction of a trade by comparing its price to the price of the preceding trade@'' (Lee and Ready, 1991, p. 3)
\par
``The test classifies each trade into four categories: an uptick, a downtick, a zero-uptick, and a zero-downtick. A trade is an uptick (downtick) if the price is higher (lower) than the price of the previous trade. When the price is the same as the previous trade (a zero tick), if the last price change was an uptick, then the trade is a zero-uptic'' (Lee and Ready, 1991, p. 3)
\par
``A trade is classified as a buy if it occurs on an uptick or a zero-uptick;otherwise it is classified as a sell'' (Lee and Ready, 1991, p. 3)
\par
``In theory, all trades can be classified as either a buy or a sell order by using a tick test.5In practice, certain trades are not classifiable because they are either reported out of sequence or are sold with special conditions attached'' (Lee and Ready, 1991, p. 3)
\par
``The primary limitation of the tick test is its relative imprecision when compared to a quote-based approach, particularly if the prevailing quote has changed or it has been a long time since the last trade.'' (Lee and Ready, 1991, p. 3)
\par
``A possible alternative to the tick test is the ``reverse tick test,'' which classifies trades by comparing the trade price to prices of trades immediately'' (Lee and Ready, 1991, p. 3)
\par
``after the current trade. If the current trade is followed by a trade with a higher (lower)price, the reverse tick test classifies the current trade as a sell (buy). This method was used by Hasbrouck (1988) to classify trades at the midpoint of the bid-ask spread.'' (Lee and Ready, 1991, p. 4)
\par
``The tick test and the reverse tick test yield the same classification when the current trade is bracketed by a price reversal (i.e., when the price change before the trade is the opposite of the price change after the trade).'' (Lee and Ready, 1991, p. 4)
\par
``Thus, there is a high degree of agreement between the tick test and quote-based classifications when the identity ofthe prevailing quote is unambiguous.'' (Lee and Ready, 1991, p. 5)
\par
``The quote revisions are clearly clustered near the trade, with a substantial portion (59.3percent) of the quotes recorded ahead of the trade'' (Lee and Ready, 1991, p. 6)
\par
``The shape of the distribution suggests that these quote revisions were attributable to the trade in question.'' (Lee and Ready, 1991, p. 6)
\par
``These findings point to a data problem which will need to be addressed in future studies. The sharp drop in quotes between 5 and 6 seconds before the trade indicates that a simple procedure could mitigate this problem'' (Lee and Ready, 1991, p. 6)
\par
``If the current quote is less than 5 seconds old, it was probably caused by the trade under consideration, so the previous quote should be used for classification'' (Lee and Ready, 1991, p. 6)
\par
``When a trade causes a quote revision, the new quote tends to straddle the trade that triggered it. If new quotes are recorded ahead of the trade, then naively using the current quote should cause a larger number of trades to appear inside the spread.'' (Lee and Ready, 1991, p. 6)
\par
``A different delay may be appropriate for other time periods. For example, tests we conducted using AMEX and NYSE data from September and October 1987 showed that during that period, most of the pretrade quotes occurred within 2 seconds of the trade'' (Lee and Ready, 1991, p. 6)
\par
``Although some of the apparent trading inside the spread is actually due to the timing issue identified earlier, Table I1 indicates that 30 percent of all trades are inside the spread even after correcting the timing problem'' (Lee and Ready, 1991, p. 7)
\par
``An alternative way to classify trades inside the spread, used by Harris (1989), is to call them buys (sells) if they are closer to the ask (bid). However, when the spread is an even number of eighths, trades at the midpoint of the spread will be unclassified'' (Lee and Ready, 1991, p. 8)
\par
``In Subsection B a simple model is used to demonstrate the effectiveness of the tick test in classifying midpoint trades.'' (Lee and Ready, 1991, p. 8)
\par
``price change will be an increase or a decrease with equal probability. On the other hand, if there is an effective spread on one side of the quoted spread, we should observe reversals on midpoint trades.'' (Lee and Ready, 1991, p. 9)
\par
``In other words, given a midpoint trade on a down (up) tick the next price change would likely be an up (down) tick'' (Lee and Ready, 1991, p. 9)
\par
``The evidence presented thus far implies trades inside the spread often arise as a result of standing orders'' (Lee and Ready, 1991, p. 10)
\par
``Moreover, the pattern of price reversals reported in Figure 3 suggests it is possible to infer the direction of these standing orders by using the tick test. Empirically, it is difficult to quantif'' (Lee and Ready, 1991, p. 10)
\par
``Inferring TradeDirection from Intraday Data 743 the expected improvement from using the tick test without knowing the actual direction of each trade inside the spread. However, the expected improvement can be evaluated analytically by means of a simple model'' (Lee and Ready, 1991, p. 11)
\par
``The tick test will only misclassify the second midpoint trade after the arrival of the standing order if it misclassifies the first midpoint trade and the second trade is in the same direction as the first trade (i.e., another buy).'' (Lee and Ready, 1991, p. 11)
\par
``In this paper, we show that the price-based trade classification method commonly known as the ``tick test'' provides remarkably accurate directional inferences.'' (Lee and Ready, 1991, p. 14)
\par
``We also identify two potential problems with classifying trades as buys or sells using quoted spreads'' (Lee and Ready, 1991, p. 14)
\par
``n a sample of trades on the NYSE during 1988, more than half of the quote changes resulting from trades are recorded ahead of the trade'' (Lee and Ready, 1991, p. 14)
\par
``We show that the problem of quote identification can be mitigated by using a time-delayed quote which, in the case of 1988 data, is the quote in effect 5 seconds before the trade time stamp'' (Lee and Ready, 1991, p. 14)
\par
``We present evidence that trading inside the spread is due largely to ``standing orders'' that cause the effective spread to be narrower than the quoted spread'' (Lee and Ready, 1991, p. 14)
\par
``For trades closer to the bid or ask we show that the tick test continues to perform well, although a simple assignment of trades as buys (sells),if they are closer to the bid (ask), will also perform well.'' (Lee and Ready, 1991, p. 14)}
}

@article{leePseudolabelSimpleEfficient,
  title = {Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
  author = {Lee, Dong-Hyun},
  year = {2013},
  note = {\section{Annotations\\
(25/10/2022, 14:51:18)}

\par
``Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels.'' (Lee, 2013, p. 2)
\par
``This is in effect equivalent to Entropy Regularization.'' (Lee, 2013, p. 2)
\par
``In a first phase, unsupervised pre-training, the weights of all layers are initialized by this layer-wise unsupervised training. In a second phase, fine-tuning, the weights are trained globally with labels using backpropagation algorithm in a supervised fashion. All of these methods also work in a semi-supervised fashion. We have only to use extra unlabeled data for unsupervised pretraining'' (Lee, 2013, p. 2)
\par
``In this article we propose the simpler way of training neural network in a semi-supervised fashion. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability every weights update, are used as if they were true labels.'' (Lee, 2013, p. 2)
\par
``In principle, this method can combine almost all neural network models and training methods. In our experiments, Denoising Auto-Encoder (Vincent et al., 2008) and Dropout (Hinton et al., 2012) boost up the performance'' (Lee, 2013, p. 2)
\par
``This method is in effect equivalent to Entropy Regularization (Grandvalet et al., 2006). The conditional entropy of the class probabilities can be used for a measure of class overlap. By minimizing the entropy for unlabeled data, the overlap of class probability distribution can be reduced.'' (Lee, 2013, p. 2)
\par
``Because in each weights update we train a different sub-model by omitting a half of hidden units, this training procedure is similar to bagging (Breiman, 1996), where many different networks are trained on different subsets of th \ldots{} But dropout is different from bagging in that all of the sub-models share same weights.'' (Lee, 2013, p. 4)
\par
``Pseudo-Label are target classes for unlabeled data as if they were true labels. We just pick up the class which has maximum predicted probability for each unlabeled sample y{${'}$} i= \{ 1 if i = argmaxi{${'}$} fi{${'}$} (x) 0 otherwise We use Pseudo-Label in a fine-tuning phase with Dropout. The pre-trained network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Label s recalculated every weights update are used for the same loss function of supervised learning task''\vphantom\} (Lee, 2013, p. 4)
\par
``Because the total number of labeled data and unlabeled data is quite different and the training balance between them is quite important for the network performance, the overall loss function is L= 1 n n {$\sum$} m=1 C {$\sum$} i=1 L(ym i ,fm i )+{$\alpha$}(t) 1 n{${'}$} n{${'}$} {$\sum$} m=1 C {$\sum$} i=1 L(y{${'}$}m i , f {${'}$}m i ), (15) where n is the number of mini-batch in labeled data for SGD, n{${'}$} for unlabeled data, f m i is the output units of m's sample in labeled data, ym i is the label of that, f {${'}$}m i for unlabeled data, y{${'}$}m i is the pseudo-label of that for unlabeled data, {$\alpha$}(t) is a coefficient balancing them. The proper scheduling of {$\alpha$}(t) is very important for the network performance. If {$\alpha$}(t) is too high, it disturbs training even for labeled data. Whereas if {$\alpha$}(t) is too small, we cannot use benefit from unlabeled data. Furthermore, the deterministic annealing process, by which {$\alpha$}(t) is slowly increased, is expected to help the optimization process to avoid poor local minima (Grandvalet et al., 2006) so that the pseudo-labels of unlabeled data are similar to true labels as much as possible.'' (Lee, 2013, p. 4)
\par
``The goal of semi-supervised learning is to improve generalization performance using unlabeled data. The cluster assumption states that the decision boundary should lie in low-density regions to improve generalization performance (Chapelle et al., 2005).'' (Lee, 2013, p. 4)
\par
``Figure 1 shows t-SNE (Van der Maaten et al., 2008) 2D embedding results of the network output of MNIST test data (not included in unlabeled data). The neural network was trained with 600 labeled data and with or without 60000 unlabeled data and Pseudo-Labels. Though the train error is zero in the two cases, the network outputs of test data is more condensed near 1-ofK code by training with unlabeled data and PseudoLabels, in other words, the entropy of (17) is minimized'' (Lee, 2013, p. 5)
\par
``Without complex training scheme and computationally expensive similarity matrix, the proposed method shows the state-of-the-art performance.'' (Lee, 2013, p. 6)}
}

@article{leitchEconomicForecastEvaluation1991,
  title = {Economic Forecast Evaluation: Profits versus the Conventional Error Measures},
  author = {Leitch, Gordon and Tanner, J. Ernest},
  year = {1991},
  journal = {The American Economic Review},
  volume = {81},
  number = {3},
  publisher = {{American Economic Association}}
}

@inproceedings{lemorvanWhatGoodImputation2021,
  title = {What's a Good Imputation to Predict with Missing Values?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Le Morvan, Marine and Josse, Julie and Scornet, Erwan and Varoquaux, Gael},
  year = {2021},
  volume = {34},
  publisher = {{Curran Associates, Inc.}},
  note = {\section{Annotations\\
(28/11/2022, 12:21:16)}

\par
``Yet, this widespread practice has no theoretical grounding. Here we show that for almost all imputation functions, an impute-then-regress procedure with a powerful learner is Bayes optimal.'' (Le Morvan et al., 2021, p. 1)
\par
``ession function will generally be discontinuous, which makes it hard to learn. Crafting instead the imputation so as to leave the regression function unchanged simply shifts the problem to learning discontinuous imputations.'' (Le Morvan et al., 2021, p. 1)
\par
``en simple data-generating mechanisms lead to complex decision rules [Le Morvan et al., 2020b]. To date, there are few supervised-learning models natively suited for partially-observed data. A notable 35th Conference on Neural Information Processing Systems (NeurIPS 2021)'' (Le Morvan et al., 2021, p. 1)
\par
``exception is found with tree-based models [Twala et al., 2008, Chen and Guestrin, 2016], widely used in data-science practice.'' (Le Morvan et al., 2021, p. 2)
\par
``The most common practice however remains by far to use off-the-shelf methods first for imputation of missing values and second for supervised-learning on the resulting completed data.'' (Le Morvan et al., 2021, p. 2)
\par
``We contribute a systematic analysis of Impute-the-Regress procedures in a general setting: non-linear response function and any missingness mechanism (no MAR assumptions). We show that:'' (Le Morvan et al., 2021, p. 2)
\par
``Impute-then-Regress procedures are Bayes optimal for all missing data mechanisms and for almost all imputation functions, whatever the number of variables that may be missing. This very general result gives theoretical grounding to such widespread procedures.'' (Le Morvan et al., 2021, p. 2)
\par
``GBRT: Gradient boosted regression trees (Scikit-learn's HistGradientBoostingRegressor with default parameters). This predictor readily supports missing values: during training, missing values on the decision variable for a given split are sent to the left or right child depending on which provides the largest gain. This is know as the Missing Incorporated Attribute strategy [Twala et al., 2008]'' (Le Morvan et al., 2021, p. 9)
\par
``Impute-then-regress procedures assemble standard statistical routines to build predictors suited for data with missing values. However, we have shown that seeking the best prediction of the outcome leads to different tradeoffs compared to inferential purposes. Given a powerful learner, almost all imputations lead asymptotically to the optimal prediction, whatever the missingness mechanism.'' (Le Morvan et al., 2021, p. 10)}
}

@misc{levinTransferLearningDeep2022,
  title = {Transfer Learning with Deep Tabular Models},
  author = {Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C. Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  year = {2022},
  number = {arXiv:2206.15306},
  eprint = {2206.15306},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(24/01/2023, 09:00:34)}

\par
``Leading methods in tabular deep learning [26, 27, 64, 44] now perform on par with the traditionally dominant gradient boosted decision trees (GBDT) [24, 56, 16, 41].'' (Levin et al., 2022, p. 1)
\par
``ransfer learning plays a central role in industrial computer vision and natural language processing pipelines, where models learn generic features that are useful across many tasks'' (Levin et al., 2022, p. 1)
\par
``For example, feature extractors pre-trained on the ImageNet dataset can enhance object detectors [59], and large transformer models trained on vast text corpora develop conceptual understandings which can be readily fine-tuned for question answering or language inference [20]. One might wonder if deep neural networks for tabular data, which are typically shallow and whose hierarchical feature extraction is unexplored, can also build representations that are transferable beyond their pre-training tasks. In fact, a recent survey paper on deep learning with tabular data suggested that efficient knowledge transfer in tabular data is an open research question [11].'' (Levin et al., 2022, p. 1)
\par
``Additionally, we compare supervised pre-training and self-supervised pre-training strategies and find that supervised pre-training leads to more transferable features in the tabular domain, contrary to findings in vision where a mature progression of self-supervised methods exhibit strong performance [30]'' (Levin et al., 2022, p. 2)
\par
``As tabular data is highly heterogeneous, the problem of downstream tasks whose formats and features differ from those of upstream data is common and has been reported to complicate knowledge transfer [47]. Nonetheless, if our upstream data is missing columns present in downstream data, we still want to leverage pre-training'' (Levin et al., 2022, p. 2)
\par
``An extensive line of work on tabular deep learning aims to challenge the dominance of GBDT models. Numerous tabular neural architectures have been introduced, based on the ideas of creating differentiable learner ensembles [55, 29, 77, 43, 8], incorporating attention mechanisms and transformer architectures [64, 26, 6, 34, 65, 44], as well as a variety of other approaches [70, 71, 10, 42, 23, 61]. However, recent systematic benchmarking of deep tabular models [26, 63] shows that while these models are competitive with GBDT on some tasks, there is still no universal best method. Gorishniy et al. [26] show that transformer-based models are the strongest alternative to GBDT and that ResNet and MLP models coupled with a strong hyperparameter tuning routine [2] offer competitive baselines. Similarly, Kadra et al. [40] find that carefully regularized MLPs are competitive. In a follow-up work, Gorishniy et al. [27] show that transformer architectures equipped with advanced embedding schemes for numerical features bridge the performance gap between deep tabular models and GBDT'' (Levin et al., 2022, p. 3)
\par
``Transfer learning [54, 72, 83] has been incredibly successful in domains of computer vision and natural language processing (NLP). Large fine-tuned models excel on a variety of image classification [21, 18] and NLP benchmarks [20, 33].'' (Levin et al., 2022, p. 3)
\par
``Multiple works mention that transfer learning in the tabular domain is challenging due to the highly heterogeneous nature of tabular data [36, 47]. Several papers focus on converting tabular data to images instead [62, 82, 66] and leveraging transfer learning with vision models [66]. Other studies explore designing CNN-like inductive biases for tabular models [37], transferring XGBoost hyperparameters [76, 28], and transferring whole models [22, 3, 49] in the limited setting of shared label and feature space between the upstream and downstream tasks. Stacking could also be seen as a form of leveraging upstream knowledge in classical methods [75, 67].'' (Levin et al., 2022, p. 3)
\par
``Self-supervised learning. Self-supervised learning (SSL) aimed at harnessing unlabelled data through learning its structure and invariances has accumulated a large body of works over the last few years. Prominent SSL methods, such as Masked Language Modeling (MLM) [20] in NLP and contrastive pre-training in computer vision [17] have revolutionized their fields making SSL the pre-training approach of choice [20, 45, 50, 48, 17, 30, 12, 9, 53]. In fact, SSL pre-training in vision has been shown to produce more transferable features than supervised pre-training on ImageNet [30]. Recently, SSL has been adopted in the tabular domain for semi-supervised learning [78, 79, 69, 64, 34]. Contrastive pre-training on auxilary unlabelled data [64] and MLM-like approaches [34] have been shown to provide gains over training from scratch for transformer tabular architectures in cases of limited labelled data.'' (Levin et al., 2022, p. 3)
\par
``For deep models with transfer learning, we tune the hyperparameters on the full upstream data using the available large upstream validation set with the goal to obtain the best performing feature extractor for the pre-training multi-target task. We then fine-tune this feature extractor with a small learning rate on the downstream data. As this strategy offers considerable performance gains over default hyperparameters, we highlight the importance of tuning the feature extractor and present the comparison with default hyperparameters in Appendix B as well as the details on hyperparameter search spaces for each model.'' (Levin et al., 2022, p. 6)
\par
``In domains where established SSL methods are increasingly dominant, such as computer vision, self-supervised learners are known to extract more transferable features than models trained on labelled data [30, 31]. In this section, we compare supervised pre-training with unsupervised pre-training and find that the opposite is true in the tabular domain. We use the Masked Language Model (MLM) pre-training recently adapted to tabular data [34] and the tabular version of contrastive learning [64]. Since both methods were proposed for tabular transformer architectures, we conduct the experiments with the FT-Transformer model. The inferior performance of self-supervised pre-training might be a consequence of the fact that SSL is significantly less explored and tuned in the tabular domain than in vision or NLP.'' (Levin et al., 2022, p. 7)
\par
``Masked Language Modeling (MLM) was first proposed for language models by Devlin et al. [20] as a powerful unsupervised learning strategy. MLM involves training a model to predict tokens in text masked at random so that its learned representations contain information useful for reconstructing these masked tokens. In the tabular domain, instead of masking tokens, a random subset of features is masked for each sample, and the masked values are predicted in a multi-target classification manner [34]. In our experiments, we mask one randomly selected feature for each sample, asking the network to learn the structure of the data and form representations from n - 1 features that are useful in producing the value in the n-th feature. For more detail, see Appendix A.'' (Levin et al., 2022, p. 8)
\par
``Contrastive pre-training uses data augmentations to generate positive pairs, or two different augmented views of a given example, and the loss function encourages a feature extractor to map positive pairs to similar features. Meanwhile, the network is also trained to map negative pairs, or augmented views of different base examples, far apart in feature space. We use the implementation of contrastive learning from Somepalli et al. [64]. In particular, we generate positive pairs by applying two data augmentations: CutMix [80] in the input space and Mixup [81] in the embedding space. For more details, see Appendix A'' (Levin et al., 2022, p. 8)
\par
``In Figure 3, we compare supervised pre-training with contrastive and MLM pre-training strategies and show that supervised pre-training always attains the best average rank. Contrastive pre-training produces better results than training from scratch on the downstream data when using a linear head, but it is still inferior to supervised pre-training. Tabular MLM pretraining also falls behind the supervised strategy and performs comparably to training from scratch in the lower data regimes but leads to a weaker downstream model in the higher data regimes.'' (Levin et al., 2022, p. 8)}
}

@inproceedings{liangFactorizationMeetsItem2016,
  title = {Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-Occurrence},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Liang, Dawen and Altosaar, Jaan and Charlin, Laurent and Blei, David M.},
  year = {2016},
  publisher = {{ACM}},
  address = {{Boston Massachusetts USA}},
  doi = {10.1145/2959100.2959182}
}

@article{liINVESTABLEINTERPRETABLEMACHINE,
  title = {Investable and Interpretable Machine Learning for Equities},
  author = {Li, Yimou and Simon, Zachary and Turkington, David}
}

@article{Lillo_2003,
  title = {Econophysics: Master Curve for Price-Impact Function.},
  author = {Lillo, Fabrizio and Farmer, J. Doyne and Mantegna, Rosario N.},
  year = {2003},
  journal = {Nature},
  doi = {10.1038/421129a},
  mag_id = {2078237894},
  pmcid = {null},
  pmid = {12520292}
}

@article{linnainmaaHistoryCrossSection,
  title = {The History of the Cross Section of Stock Returns},
  author = {Linnainmaa, Juhani T and Roberts, Michael}
}

@article{linnainmaaWeatherTimeSeries2009,
  title = {Weather and Time Series Determinants of Liquidity in a Limit Order Market},
  author = {Linnainmaa, Juhani T. and Rosu, Ioanid},
  year = {2009},
  journal = {null},
  doi = {10.2139/ssrn.1108862},
  mag_id = {1595367183},
  pmcid = {null},
  pmid = {null}
}

@misc{linSurveyTransformers2021,
  title = {A Survey of Transformers},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  year = {2021},
  number = {arXiv:2106.04554},
  eprint = {2106.04554},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{linWhyOptionsPrices2015,
  title = {Why Do Options Prices Predict Stock Returns? {{Evidence}} from Analyst Tipping},
  author = {Lin, Tse-Chun and Lu, Xiaolong},
  year = {2015},
  journal = {Journal of Banking \& Finance},
  volume = {52},
  doi = {10.1016/j.jbankfin.2014.11.008}
}

@article{littleStatisticalAnalysisMissing,
  title = {Statistical Analysis with Missing Data},
  author = {Little, Roderick J A and {Rubin, Donald}},
  doi = {10.1002/9781119013563}
}

@article{littlestoneWeightedMajorityAlgorithm,
  title = {The Weighted Majority Algorithm},
  author = {Littlestone, Nick and Warmuth, Manfred K},
  journal = {. Introduction}
}

@article{liuDataQualityProblems2020,
  title = {Data Quality Problems Troubling Business and Financial Researchers: A Literature Review and Synthetic Analysis},
  author = {Liu, Grace},
  year = {2020},
  journal = {Journal of Business \& Finance Librarianship},
  volume = {25},
  number = {3-4},
  doi = {10.1080/08963568.2020.1847555}
}

@misc{liuMonolithRealTime2022,
  title = {Monolith: Real Time Recommendation System with Collisionless Embedding Table},
  author = {Liu, Zhuoran and Zou, Leqi and Zou, Xuan and Wang, Caihua and Zhang, Biao and Tang, Da and Zhu, Bolin and Zhu, Yijie and Wu, Peng and Wang, Ke and Cheng, Youlong},
  year = {2022},
  number = {arXiv:2209.07663},
  eprint = {2209.07663},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: ORSUM@ACM RecSys 2022}
}

@misc{liuPayAttentionMLPs2021,
  title = {Pay Attention to Mlps},
  author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
  year = {2021},
  number = {arXiv:2105.08050},
  eprint = {2105.08050},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@inproceedings{liuRelatedPinsPinterest2017,
  title = {Related Pins at Pinterest: The Evolution of a Real-World Recommender System},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web Companion}} - {{WWW}} '17 {{Companion}}},
  author = {Liu, David C. and Rogers, Stephanie and Shiau, Raymond and Kislyuk, Dmitry and Ma, Kevin C. and Zhong, Zhigang and Liu, Jenny and Jing, Yushi},
  year = {2017},
  publisher = {{ACM Press}},
  address = {{Perth, Australia}},
  doi = {10.1145/3041021.3054202}
}

@inproceedings{liuRethinkingSkipConnection2020,
  title = {Rethinking Skip Connection with Layer Normalization},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Liu, Fenglin and Ren, Xuancheng and Zhang, Zhiyuan and Sun, Xu and Zou, Yuexian},
  year = {2020},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.320}
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: A Robustly Optimized {{BERT}} Pretraining Approach},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@inproceedings{liuSTAMPShorttermAttention2018,
  title = {{{STAMP}}: Short-Term Attention/Memory Priority Model for Session-Based Recommendation},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Liu, Qiao and Zeng, Yifu and Mokhosi, Refuoe and Zhang, Haibin},
  year = {2018},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3219950}
}

@misc{liuUnderstandingDifficultyTraining2020,
  title = {Understanding the Difficulty of Training Transformers},
  author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  year = {2020},
  number = {arXiv:2004.08249},
  eprint = {2004.08249},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: EMNLP 2020}
}

@misc{liuVarianceAdaptiveLearning2021,
  title = {On the Variance of the Adaptive Learning Rate and Beyond},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  year = {2021},
  number = {arXiv:1908.03265},
  eprint = {1908.03265},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: ICLR 2020. Fix several typos in the previous version}
}

@article{Lo_2002,
  title = {Econometric Models of Limit-Order Executions},
  author = {Lo, Andrew W. and MacKinlay, A. Craig and Zhang, June},
  year = {2002},
  journal = {Journal of Financial Economics},
  doi = {10.1016/s0304-405x(02)00134-4},
  mag_id = {2041147733},
  pmcid = {null},
  pmid = {null}
}

@misc{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  author = {Lones, Michael A.},
  year = {2022},
  number = {arXiv:2108.02497},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 17 pages}
}

@book{lopezdepradoAdvancesFinancialMachine2018,
  title = {Advances in Financial Machine Learning},
  author = {{\{L{\'o}pez de Prado\}}, Marcos},
  year = {2018},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  note = {Includes index
\par
Machine generated contents note: About the Author Preamble 1. Financial Machine Learning as a Distinct Subject Part 1: Data Analysis 2. Financial Data Structures 3. Labeling 4. Sample Weights 5. Fractionally Differentiated Features Part 2: Modelling 6. Ensemble Methods 7. Cross-validation in Finance 8. Feature Importance 9. Hyper-parameter Tuning with Cross-Validation Part 3: Backtesting 10. Bet Sizing 11. The Dangers of Backtesting 12. Backtesting through Cross-Validation 13. Backtesting on Synthetic Data 14. Backtest Statistics 15. Understanding Strategy Risk 16. Machine Learning Asset Allocation Part 4: Useful Financial Features 17. Structural Breaks 18. Entropy Features 19. Microstructural Features Part 5: High-Performance Computing Recipes 20. Multiprocessing and Vectorization 21. Brute Force and Quantum Computers 22. High-Performance Computational Intelligence and Forecasting Technologies Dr. Kesheng Wu and Dr. Horst Simon Index
\par
"Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. Readers will learn how to structure Big data in a way that is amenable to ML algorithms; how to conduct research with ML algorithms on that data; how to use supercomputing methods; how to backtest your discoveries while avoiding false positives. The book addresses real-life problems faced by practitioners on a daily basis, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their particular setting. Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance"-- "This book begins by structuring financial data in a way that is amenable to machine learning (ML) algorithms. Then, the author discusses how to conduct research with ML algorithms on that data and how to backtest your discoveries. Most of the problems and solutions are explained using math, supported by code. This makes the book very practical and hands-on. Readers become active users who can test the solutions proposed in their work. Readers will learn how to structure, label, weight, and backtest data. Machine learning is the future, and this book will equip investment professionals with the tools to utilize it moving forward"--}
}

@misc{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled Weight Decay Regularization},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Published as a conference paper at ICLR 2019}
}

@misc{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: Stochastic Gradient Descent with Warm Restarts},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  number = {arXiv:1608.03983},
  eprint = {1608.03983},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(25/10/2022, 07:49:56)}

\par
``The commonly used procedure to optimize f is to iteratively adjust xt {$\in$} IRn (the parameter vector at time step t) using gradient information {$\nabla$}ft(xt) obtained on a relatively small t-th batch of b datapoints. The Stochastic Gradient Descent (SGD) procedure then becomes an extension of the Gradient Descent (GD) to stochastic optimization of f as follows: xt+1 = xt - {$\eta$}t{$\nabla$}ft(xt), (1) where {$\eta$}t is a learning rate. One would like to consider second-order information xt+1 = xt - {$\eta$}tH-1 t {$\nabla$}ft(xt), (2) but this is often infeasible since the computation and storage of the inverse Hessian H-1 t is intractable for large n.'' (Loshchilov and Hutter, 2017, p. 1)
\par
``In this paper, we propose to periodically simulate warm restarts of SGD, where in each restart the learning rate is initialized to some value and is scheduled to decrease.'' (Loshchilov and Hutter, 2017, p. 2)
\par
``Our empirical results suggest that SGD with warm restarts requires 2\texttimes{} to 4\texttimes{} fewer epochs than the currently-used learning rate schedule schemes to achieve comparable or even better results.'' (Loshchilov and Hutter, 2017, p. 2)
\par
``Warm restarts are usually employed to improve the convergence rate rather than to deal with multimodality: often it is sufficient to approach any local optimum to a given precision and in many cases the problem at hand is unimodal.'' (Loshchilov and Hutter, 2017, p. 3)
\par
``Since the condition number is an unknown parameter and its value may vary during the search, they proposed two adaptive warm restart techniques (O'Donoghue \& Candes, 2012): \textbullet{} The function scheme restarts whenever the objective function increases. \textbullet{} The gradient scheme restarts whenever the angle between the momentum term and the negative gradient is obtuse, i.e, when the momentum seems to be taking us in a bad direction, as measured by the negative gradient at that point. This scheme resembles the one of Powell (1977) for the conjugate gradient method.'' (Loshchilov and Hutter, 2017, p. 3)
\par
``Smith (2015; 2016) recently introduced cyclical learning rates for deep learning, his approach is closely-related to our approach in its spirit and formulation but does not focus on restarts.'' (Loshchilov and Hutter, 2017, p. 3)
\par
``The existing restart techniques can also be used for stochastic gradient descent if the stochasticity is taken into account. Since gradients and loss values can vary widely from one batch of the data to another, one should denoise the incoming information: by considering averaged gradients and losses, e.g., once per epoch, the above-mentioned restart techniques can be used again.'' (Loshchilov and Hutter, 2017, p. 4)
\par
``n this work, we consider one of the simplest warm restart approaches. We simulate a new warmstarted run / restart of SGD once Ti epochs are performed, where i is the index of the run. Importantly, the restarts are not performed from scratch but emulated by increasing the learning rate {$\eta$}t while the old value of xt is used as an initial solution. The amount of this increase controls to which extent the previously acquired information (e.g., momentum) is used. Within the i-th run, we decay the learning rate with a cosine annealing for each batch as follows: {$\eta$}t = {$\eta$}i min + 1 2 ({$\eta$}i max - {$\eta$}i min)(1 + cos( Tcur Ti {$\pi$})), (5) where {$\eta$}i min and {$\eta$}i max are ranges for the learning rate, and Tcur accounts for how many epochs have been performed since the last restart. Since Tcur is updated at each batch iteration t, it can take discredited values such as 0.1, 0.2, etc. Thus, {$\eta$}t = {$\eta$}i max when t = 0 and Tcur = 0. Once Tcur = Ti, the cos function will output -1 and thus {$\eta$}t = {$\eta$}i min. The decrease of the learning rate is shown in Figure 1 for fixed Ti = 50, Ti = 100 and Ti = 200; note that the logarithmic axis obfuscates the typical shape of the cosine function. In order to improve anytime performance, we suggest an option to start with an initially small Ti and increase it by a factor of Tmult at every restart (see, e.g., Figure 1 for T0 = 1, Tmult = 2 and T0 = 10, Tmult = 2). It might be of great interest to decrease {$\eta$}i max and {$\eta$}i min at every new restart. However, for the sake of simplicity, here, we keep {$\eta$}i max and {$\eta$}i min the same for every i to reduce the number of hyperparameters involved.'' (Loshchilov and Hutter, 2017, p. 4)
\par
``Since SGDR achieves good performance faster, it may allow us to train larger networks.'' (Loshchilov and Hutter, 2017, p. 7)
\par
``Thus, naively building ensembles from models obtained at last epochs only (i.e., M = 3 snapshots at epochs 148, 149, 150) did not improve the results (i.e., the baseline of M = 1 snapshot at 150) thereby confirming the conclusion of Huang et al. (2016a) that snapshots of SGDR provide a useful diversity of predictions for ensembles.'' (Loshchilov and Hutter, 2017, p. 8)}
}

@article{ludewigEvaluationSessionbasedRecommendation2018,
  title = {Evaluation of Session-Based Recommendation Algorithms},
  author = {Ludewig, Malte and Jannach, Dietmar},
  year = {2018},
  journal = {User Modeling and User-Adapted Interaction},
  volume = {28},
  number = {4-5},
  eprint = {1803.09587},
  eprinttype = {arxiv},
  doi = {10.1007/s11257-018-9209-6},
  archiveprefix = {arXiv}
}

@misc{lundbergConsistentIndividualizedFeature2019,
  title = {Consistent Individualized Feature Attribution for Tree Ensembles},
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  year = {2019},
  number = {arXiv:1802.03888},
  eprint = {1802.03888},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(18/10/2022, 19:08:38)}

\par
``Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction'' (Lundberg et al., 2019, p. 1)
\par
Comment: Follow-up to 2017 ICML Workshop arXiv:1706.06060}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A Unified Approach to Interpreting Model Predictions},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M. and Lee, Su-In},
  year = {2017},
  series = {{{NeurIPS}} 2017},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  address = {{Long Beach, CA}}
}

@inproceedings{luoCollaborativeSelfattentionNetwork2020,
  title = {Collaborative Self-Attention Network for Session-Based Recommendation},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Luo, Anjing and Zhao, Pengpeng and Liu, Yanchi and Zhuang, Fuzhen and Wang, Deqing and Xu, Jiajie and Fang, Junhua and Sheng, Victor S.},
  year = {2020},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/359}
}

@misc{MachineLearningHow,
  title = {Machine Learning - How to Intuitively Explain What a Kernel Is?},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is}
}

@misc{malininUncertaintyGradientBoosting2021,
  title = {Uncertainty in Gradient Boosting via Ensembles},
  author = {Malinin, Andrey and Prokhorenkova, Liudmila and Ustimenko, Aleksei},
  year = {2021},
  number = {arXiv:2006.10562},
  eprint = {2006.10562},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{Manaster_1982,
  title = {Option Prices as Predictors of Equilibrium Stock Prices},
  author = {Manaster, Steven and Rendleman, Richard J.},
  year = {1982},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1982.tb03597.x},
  mag_id = {2096808000},
  pmcid = {null},
  pmid = {null}
}

@article{maraisDeepLearningTabular,
  title = {Deep Learning for Tabular Data: An Exploratory Study},
  author = {Marais, Jan Andr{\'e}}
}

@book{martinEconometricModellingTime2012,
  title = {Econometric Modelling with Time Series: Specification, Estimation and Testing},
  author = {Martin, Vance and Hurn, Stan and Harris, David},
  year = {2012},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139043205}
}

@techreport{martinMarketEfficiencyAge2019,
  title = {Market Efficiency in the Age of Big Data},
  author = {Martin, Ian and Nagel, Stefan},
  year = {2019},
  number = {w26586},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w26586}
}

@article{martinWhatExpectedReturn2016,
  title = {What Is the Expected Return on the Market?},
  author = {Martin, Ian},
  year = {2016},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2772101}
}

@article{martinWhatExpectedReturn2019,
  title = {What Is the Expected Return on a Stock?},
  author = {Martin, Ian and Wagner, Christian},
  year = {2019},
  journal = {The Journal of Finance},
  volume = {74},
  number = {4},
  doi = {10.1111/jofi.12778}
}

@book{mccarthyApplyingPredictiveAnalytics2019,
  title = {Applying Predictive Analytics: Finding Value in Data},
  author = {McCarthy, Richard V. and McCarthy, Mary M. and Ceccucci, Wendy and Halawi, Leila},
  year = {2019},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-14038-0}
}

@article{measeBoostedClassificationTrees,
  title = {Boosted Classification Trees and Class Probability/Quantile Estimation},
  author = {Mease, David and Wyner, Abraham J and Buja, Andreas}
}

@misc{melisStateArtEvaluation2017,
  title = {On the State of the Art of Evaluation in Neural Language Models},
  author = {Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  year = {2017},
  number = {arXiv:1707.05589},
  eprint = {1707.05589},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(27/10/2022, 09:07:05)}

\par
``nce hyperparameters have been properly controlled for, we find that LSTMs outperform the more recent models, contra the published claims. Our result is therefore a demonstration that replication failures can happen due to poorly controlled hyperparameter variation, and this paper joins other recent papers in warning of the under-acknowledged existence of replication failure in deep learning (Henderson et al., 2017; Reimers \& Gurevych, 2017)'' (Melis et al., 2017, p. 1)
\par
``However, we do show that careful controls are possible, albeit at considerable computational cost.'' (Melis et al., 2017, p. 1)
\par
``Hyperparameters are optimised by Google Vizier (Golovin et al., 2017), a black-box hyperparameter tuner based on batched GP bandits using the expected improvement acquisition function (Desautels et al., 2014). Tuners of this nature are generally more efficient than grid search when the number of hyperparameters is small.'' (Melis et al., 2017, p. 3)
\par
``On two of the three datasets, we improved previous results substantially by careful model specification and hyperparameter optimisation, but the improvement for RHNs is much smaller compared to that for LSTMs. While it cannot be ruled out that our particular setup somehow favours LSTMs, we believe it is more likely that this effect arises due to the original RHN experimental condition having been tuned more extensively (this is nearly unavoidable during model development).'' (Melis et al., 2017, p. 5)
\par
``With a large number of hyperparameter combinations evaluated, the question of how much the tuner overfits arises. There are multiple sources of noise in play, (a) non-deterministic ordering of floating-point operations in optimised linear algebra routines, (b) different initialisation seeds, (c) the validation and test sets being finite samples from a infinite population. To assess the severity of these issues, we conducted the following experiment: models with the best hyperparameter settings for Penn Treebank and Wikitext-2 were retrained from scratch with various initialisation seeds and the validation and test scores were recorded. If during tuning, a model just got a lucky run due to a combination of (a) and (b), then retraining with the same hyperparameters but with different seeds would fail to reproduce the same good results.'' (Melis et al., 2017, p. 6)
\par
``Third, the validation perplexities of the best checkpoints are about one standard deviation lower than the sample mean of the reruns, so the tuner could fit the noise only to a limited degree'' (Melis et al., 2017, p. 7)
\par
``We have not explicitly dealt with the unknown uncertainty remaining in the Gaussian Process that may affect model comparisons, apart from running it until apparent convergence. All in all, our findings suggest that a gap in perplexity of 1.0 is a statistically robust difference between models trained in this way on these datasets. The distribution of results was approximately normal with roughly the same variance for all models, so we still report numbers in a tabular form instead of plotting the distribution of results, for example in a violin plot (Hintze \& Nelson, 1998).'' (Melis et al., 2017, p. 7)
\par
``To further verify that the best hyperparameter setting found by the tuner is not a fluke, we plotted the validation loss against the hyperparameter settings. Fig. 2 shows one such typical plot, for a 4-layer LSTM. We manually restricted the ranges around the best hyperparameter values to around 15\textendash 25\% of the entire tuneable range, and observed that the vast majority of settings in that neighbourhood produced perplexities within 3.0 of the best value.'' (Melis et al., 2017, p. 7)
\par
``Still, we demonstrate how, with a huge amount of computation, noise levels of various origins can be carefully estimated and models meaningfully compared. This apparent tradeoff between the amount of computation and the reliability of results seems to lie at the heart of the matter. Solutions to the methodological challenges must therefore make model evaluation cheaper by, for instance, reducing the number of hyperparameters and the sensitivity of models to them, employing better hyperparameter optimisation strategies, or by defining ``leagues'' with predefined computational budgets for a single model representing different points on the tradeoff curve.'' (Melis et al., 2017, p. 8)}
}

@misc{meyesAblationStudiesArtificial2019,
  title = {Ablation Studies in Artificial Neural Networks},
  author = {Meyes, Richard and Lu, Melanie and {\noopsort{puiseau}}{de Puiseau}, Constantin Waubert and Meisen, Tobias},
  year = {2019},
  number = {arXiv:1901.08644},
  eprint = {1901.08644},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 19 pages, 23 figures, intention to submit as a conference publication}
}

@inproceedings{michelAreSixteenHeads2019,
  title = {Are Sixteen Heads Really Better than One?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Michel, Paul and Levy, Omer and Neubig, Graham},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{mikolovLinguisticRegularitiesContinuous2013,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  publisher = {{Association for Computational Linguistics}},
  address = {{Atlanta, Georgia}}
}

@misc{mirzaeiHowUseDeepLearning2019,
  title = {How to Use Deep-Learning for Feature-Selection, Python, Keras},
  author = {Mirzaei, Ali},
  year = {2019},
  journal = {Medium}
}

@article{mockusApplicationBayesianApproach1994,
  title = {Application of Bayesian Approach to Numerical Methods of Global and Stochastic Optimization},
  author = {Mockus, Jonas},
  year = {1994},
  journal = {Journal of Global Optimization},
  volume = {4},
  number = {4},
  doi = {10.1007/BF01099263},
  note = {\section{Annotations\\
(02/11/2022, 06:10:05)}

\par
``Let us to consider for example the global optimization of the family of Lifschitzian functions with unknown constant. Then the best method in the minimax sense is a uniform grid on a compact feasible set, see Sukharev (1975). It means that this global optimization algorithm is of exponential complexity. The number of observations is increasing as exponent of the dimension of problem. Here "observation" means an evaluation of objective function f(x) at some fixed point X. If the Lifschitzian constant is known, then some nonuniform grid technique is preferable, see Evtushenko (1985). However, even here the complexity of algorithm apparently remains exponential, perhaps with a better constant. In global optimization of continuous functions on a compact set we cannot apply a minimax approach at all. It is well known that maximum does not exist on a set of all continuous functions. It means that for any fixed continuous function and a fixed method of search there exists some other continuous function with a larger deviation from a global minimum. So the strong condition of uniform convergence does not apply here.'' (Mockus, 1994, p. 348)
\par
``We assume that a Bayesian method should converge to a global minimum of any continuous function, if an a priori distribution is chosen correctly. It means that the asymptotic of Bayesian method is at least as good as that of any classical one for a family of continuous functions. In fact it is even better. The asymptotic density of observations of Bayesian methods is considerably higher near global minimum'' (Mockus, 1994, p. 350)
\par
``However, the main advantage of Bayesian methods is that they minimize an expected deviation from the global minimum for any fixed number of observations'' (Mockus, 1994, p. 350)}
}

@article{mogharStockMarketPrediction2020,
  title = {Stock Market Prediction Using {{LSTM}} Recurrent Neural Network},
  author = {Moghar, Adil and Hamiche, Mhamed},
  year = {2020},
  journal = {Procedia Computer Science},
  volume = {170},
  doi = {10.1016/j.procs.2020.03.049}
}

@misc{monnierCrossvalidationToolsTime2018,
  title = {Cross-Validation Tools for Time Series},
  author = {Monnier, Samuel},
  year = {2018},
  journal = {Medium}
}

@article{muravyevOrderFlowExpected2016,
  title = {Order Flow and Expected Option Returns: Order Flow and Expected Option Returns},
  author = {Muravyev, Dmitriy},
  year = {2016},
  journal = {The Journal of Finance},
  volume = {71},
  number = {2},
  doi = {10.1111/jofi.12380},
  note = {\section{Annotations\\
(25/10/2022, 06:55:50)}

\par
``OPRA does not report option trade direction, and thus I infer this by applying the quote rule to the NBBO. If the trade is at the midpoint of the NBBO, the quote rule is applied to the best bid offer (BBO) from the exchange at which the trade occurs. In the Internet Appendix, I argue that this algorithm has small estimation error.'' (Muravyev, 2016, p. 688)
\par
``The data only include transactions that were executed at ISE; however, throughout most of the sample period, ISE was the largest equity options exchange with a market share of about 30\%. OptionMetrics is a common source of price information on equity options. For each option contract, it contains end-of-day best bid and ask prices as well as other information such as volume, open interest, implied volatility, and option Greeks. Returns and volume for the underlying stocks are also taken from OptionMetrics to avoid data loss from merging with CRSP.'' (Muravyev, 2016, p. 688)}
}

@article{murdochInterpretableMachineLearning2019,
  title = {Interpretable Machine Learning: Definitions, Methods, and Applications},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and {Abbasi-Asl}, Reza and Yu, Bin},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {44},
  eprint = {1901.04592},
  eprinttype = {arxiv},
  doi = {10.1073/pnas.1900654116},
  archiveprefix = {arXiv},
  note = {Comment: 11 pages}
}

@article{nabiNovelApproachStock2020,
  title = {A Novel Approach for Stock Price Prediction Using Gradient Boosting Machine with Feature Engineering ({{GBM-wFE}})},
  author = {Nabi, Rebwar M. and Ab. M. Saeed, Soran and Harron, Habibollah},
  year = {2020},
  journal = {Kurdistan Journal of Applied Research},
  volume = {5},
  number = {1},
  doi = {10.24017/science.2020.1.3}
}

@book{nagelMachineLearningAsset2021,
  title = {Machine Learning in Asset Pricing: Stefan Nagel.},
  author = {Nagel, Stefan},
  year = {2021}
}

@misc{narangTransformerModificationsTransfer2021,
  title = {Do Transformer Modifications Transfer across Implementations and Applications?},
  author = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and Zhou, Yanqi and Li, Wei and Ding, Nan and Marcus, Jake and Roberts, Adam and Raffel, Colin},
  year = {2021},
  number = {arXiv:2102.11972},
  eprint = {2102.11972},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 13:47:45)}

\par
``First, we apply layer normalization before the selfattention and feedforward blocks instead of after. This small change has been unanimously adopted by all current Transformer implementations because it leads to more effective training (Baevski and Auli, 2019; Xiong et al., 2020).'' (Narang et al., 2021, p. 4)
\par
``Overall, the decoder is structured similarly to the encoder, with the following changes: First, the self-attention mechanisms are ``causal'' which prevents the decoder from looking at future items from the target sequence when it is fed in during training'' (Narang et al., 2021, p. 15)
\par
Comment: To appear at EMNLP 2021 as a conference paper}
}

@article{Neal_1992,
  title = {A {{Comparison}} of {{Transaction Costs Between Competitive Market Maker}} and {{Specialist Market Structures}}},
  author = {Neal, Robert},
  year = {1992},
  journal = {The Journal of Business},
  doi = {10.1086/296573},
  mag_id = {2007440627},
  pmcid = {null},
  pmid = {null}
}

@incollection{nelsonMachineLearningStrategic2023,
  title = {Machine Learning for Strategic Trade Analysis},
  booktitle = {Methods of {{Strategic Trade Analysis}}: {{Data-Driven Approaches}} to {{Detect Illicit Dual-Use Trade}}},
  author = {Nelson, Christopher},
  editor = {Nelson, Christopher},
  year = {2023},
  series = {Advanced {{Sciences}} and {{Technologies}} for {{Security Applications}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20036-6_10}
}

@misc{NetflixUpdateTry,
  title = {Netflix Update: Try This at Home},
  howpublished = {https://sifter.org/\textasciitilde simon/journal/20061211.html}
}

@incollection{neumannMotivatingSupportingUser2007,
  title = {Motivating and Supporting User Interaction with Recommender Systems},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Neumann, Andreas W.},
  editor = {Kov{\'a}cs, L{\'a}szl{\'o} and Fuhr, Norbert and Meghini, Carlo},
  year = {2007},
  volume = {4675},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74851-9_36}
}

@article{Newey_1987,
  title = {A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix},
  author = {Newey, Whitney K. and West, Ken},
  year = {1987},
  journal = {Econometrica : journal of the Econometric Society},
  doi = {10.2307/1913610},
  mag_id = {3023906401},
  pmcid = {null},
  pmid = {null}
}

@inproceedings{ngFeatureSelectionVs2004,
  title = {Feature Selection, \${{L}}\_1\$ vs. \${{L}}\_2\$ Regularization, and Rotational Invariance},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Ng, Andrew Y.},
  year = {2004},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015435}
}

@article{nguyenTransformersTearsImproving2019,
  title = {Transformers without Tears: Improving the Normalization of Self-Attention},
  author = {Nguyen, Toan Q. and Salazar, Julian},
  year = {2019},
  eprint = {1910.05895},
  eprinttype = {arxiv},
  doi = {10.5281/zenodo.3525484},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 13:54:31)}

\par
``Our work demonstrates additional consequences in the base ({$\leq$}6-layer encoder) Transformer regime. We show that PRENORM enables warmup-free, validation-based training with large learning rates even for small batches, in contrast to past work on scaling NMT (Ott et al., 2018). We also partly reclaim POSTNORM's stability via smaller initializations, although PRENORM is less sensitive to this magnitude and can improve performance. However, despite PRENORM's recent adoption in many NMT frameworks, we find it degrades base Transformer performance on WMT '14 English-German.'' (Nguyen and Salazar, 2019, p. 1)
\par
``Residual connections (He et al., 2016a) were first introduced to facilitate the training of deep convolutional networks, where the output of the `-th layer F` is summed with its input: x`+1 = x` + F`(x`). (1) The identity term x` is crucial to greatly extending the depth of such networks (He et al., 2016b). If one were to scale x` by a scalar {$\lambda$}`, then the contribution of x` to the final layer FL is ({$\prod$}L-1 i=` {$\lambda$}i)x`. For deep networks with dozens or even hundreds of layers L, the term {$\prod$}L-1 i=` {$\lambda$}i becomes very large if {$\lambda$}i {$>$} 1 or very small if {$\lambda$}i {$<$} 1, for enough i. When backpropagating from the last layer L back to `, these multiplicative terms can cause exploding or vanishing gradients, respectively. Therefore they fix {$\lambda$}i = 1, keeping the total residual path an identity map.'' (Nguyen and Salazar, 2019, p. 2)
\par
``We conjecture this has caused past convergence failures (Popel and Bojar, 2018; Shazeer and Stern, 2018), with LAYERNORMs in the residual path acting similarly to {$\lambda$}i 6= 1; furthermore, warmup was needed to let LAYERNORM safely adjust scale during early parts of training.'' (Nguyen and Salazar, 2019, p. 2)
\par
``Inspired by He et al. (2016b), we apply LAYERNORM immediately before each sublayer (PRENORM): x`+1 = x` + F`(LAYERNORM(x`)). (3) This is cited as a stabilizer for Transformer training (Chen et al., 2018; Wang et al., 2019) and is already implemented in popular toolkits (Vaswani et al., 2018; Ott et al., 2019; Hieber et al., 2018), though not necessarily used by their default recipes. Wang et al. (2019) make a similar argument to motivate the success of PRENORM in training very deep Transformers. Note that one must append an additional normalization after both encoder and decoder so their outputs are appropriately scaled.'' (Nguyen and Salazar, 2019, p. 2)}
}

@article{Ni_2007,
  title = {Does Option Trading Have a Pervasive Impact on Underlying Stock Prices},
  author = {Ni, Sophie Xiaoyan and Pearson, Neil D. and Poteshman, Allen M. and White, Joshua S.},
  year = {2007},
  journal = {null},
  doi = {10.2139/ssrn.2867461},
  mag_id = {1549278275},
  pmcid = {null},
  pmid = {null}
}

@misc{noriAccuracyInterpretabilityDifferential2021,
  title = {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},
  author = {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},
  year = {2021},
  number = {arXiv:2106.09680},
  eprint = {2106.09680},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: To be published in ICML 2021. 12 pages, 6 figures}
}

@inproceedings{nothmanStopWordLists2018,
  title = {Stop Word Lists in Free Open-Source Software Packages},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  year = {2018},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-2502}
}

@incollection{nowakAccuracyTradeClassification2020,
  title = {The {{Accuracy}} of {{Trade Classification Rules}} for the {{Selected CEE Stock Exchanges}}},
  booktitle = {Contemporary {{Trends}} and {{Challenges}} in {{Finance}}},
  author = {Nowak, Sabina},
  editor = {Jajuga, Krzysztof and {Locarek-Junge}, Hermann and Orlowski, Lucjan T. and Staehr, Karsten},
  year = {2020},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-43078-8_6}
}

@misc{ntakourisTimeSeriesTransformer2021,
  title = {The Time Series Transformer},
  author = {Ntakouris, Theodoros},
  year = {2021},
  journal = {Medium},
  howpublished = {https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3},
  note = {fooo}
}

@article{O'Hara_2001,
  title = {The Accuracy of Trade Classification Rules: Evidence from {{NASDAQ}}},
  author = {O'Hara, Maureen and Ellis, Katrina and Michaely, Roni},
  year = {2001},
  journal = {null},
  doi = {null},
  mag_id = {3121731372},
  pmcid = {null},
  pmid = {null}
}

@inproceedings{obthongSurveyMachineLearning2020,
  title = {A Survey on Machine Learning for Stock Price Prediction: Algorithms and Techniques:},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Finance}}, {{Economics}}, {{Management}} and {{IT Business}}},
  author = {Obthong, Mehtabhorn and Tantisantiwong, Nongnuch and Jeamwatthanachai, Watthanasak and Wills, Gary},
  year = {2020},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Prague, Czech Republic}},
  doi = {10.5220/0009340700630071}
}

@article{odders-whiteOccurrenceConsequencesInaccurate2000,
  title = {On the Occurrence and Consequences of Inaccurate Trade Classification},
  author = {{Odders-White}, Elizabeth R},
  year = {2000},
  journal = {Journal of Financial Markets},
  volume = {3},
  number = {3},
  doi = {10.1016/S1386-4181(00)00006-9},
  note = {\section{Annotations\\
(06/11/2022, 09:18:51)}

\par
``This study uses the TORQ data to investigate the performance of the Lee and Ready (1991, Journal of Finance 46, 733\vphantom\{\}746.) trade classi"cation algorithm.'' (Odders-White, 2000, p. 259)
\par
``I "nd that the algorithm correctly classi"es 85\% of the transactions in my sample, but systematically misclassi"es transactions at the midpoint of the bid\vphantom\{\}ask spread, small transactions, and transactions in large or frequently traded stocks.'' (Odders-White, 2000, p. 259)
\par
``The validity of many economic studies hinges on the ability to accurately classify trades as buyer or seller-initiated.'' (Odders-White, 2000, p. 259)
\par
``Lee and Ready (1991) examined a pair of commonly used algorithms, namely the quote method and the tick method, which classify transactions based on execution prices and quotes. Lee and Ready then recommended that a combination of the two algorithms be used in practice (hereafter referred to as the Lee and Ready method).'' (Odders-White, 2000, p. 260)
\par
``If the probability of misclassi"cation is the same for all types of trades (e.g. large buys occurring the in the morning are as likely to be misclassi"ed as small sells occurring in the afternoon), then trade misclassi"cation will simply add random error to the data. If instead, particular types of transactions are more likely than others to be misclassi"ed, then trade misclassi"cation will add systematic error to the data and may ultimately bias the results.'' (Odders-White, 2000, p. 260)
\par
``Using the TORQ (Trades, Orders, Reports, and Quotes) database from the NYSE, which makes the direct determination of the initiator of a transaction possible, I evaluate the overall performance of the Lee and Ready algorithms and examine the consequences of misclassi"cation.'' (Odders-White, 2000, p. 260)
\par
``I "nd that the quote method misclassi"es 9.1\% of the transactions in my sample and fails to classify 15.9\% of the transactions. The tick method misclassi"es 21.4\% of the transactions, and the combination recommended by Lee and Ready misclassi"es 15.0\%. Moreover, transactions inside the bid\vphantom\{\}ask spread, small transactions, and transactions in large or frequently traded stocks are especially problematic'' (Odders-White, 2000, p. 260)
\par
``Second, although they also use the TORQ database, they focus on a smaller subset of the data'' (Odders-White, 2000, p. 261)
\par
``One way to describe initiators is as traders who demand immediate execution (hereafter, the immediacy de"nition). A natural consequence of this de"nition is that traders placing market orders (or limit orders at the opposite quote) are labeled the initiators, and traders placing limit orders are viewed as non-initiators or passive suppliers of liquidity'' (Odders-White, 2000, p. 261)
\par
``Problems with this de"nition arise, however, when market orders cross, when limit orders are matched with other limit orders, and when market orders are FINMAR=38=KGM=VVC=BG E.R. Odders-White / Journal of Financial Markets 3 (2000) 259\vphantom\{\}286 26'' (Odders-White, 2000, p. 261)
\par
``Fig. 1. Sample transaction. stopped, all of which can occur frequently.'' (Odders-White, 2000, p. 262)
\par
``De5nition. The initiator of a transaction is the investor (buyer or seller) who placed his or her order last, chronologically.'' (Odders-White, 2000, p. 262)
\par
``The advantage of the chronological de"nition is that it can be applied when the immediacy de"nition cannot.'' (Odders-White, 2000, p. 262)
\par
``First, they demonstrated that because updated quotes are often reported before the transactions that triggered them, a comparison of the execution price to the quotes in e!ect at the time of the transaction is inappropriate'' (Odders-White, 2000, p. 263)
\par
``The disadvantage is that the tick method incorporates less information than the quote method since it does not use the posted quotes.'' (Odders-White, 2000, p. 264)
\par
``First, they noted that \&the primary limitation of the tick test is its relative imprecision when compared to a quotebased approach'. This implies that the quote method should be employed whenever possible.'' (Odders-White, 2000, p. 264)
\par
``Lee and Ready recognized that these algorithms were imperfect, however, and emphasized the di\$culty in truly evaluating their performance without data on the true trade classi"cation'' (Odders-White, 2000, p. 264)
\par
``he sample for this study comes from the TORQ database, which contains data on 144 NYSE stocks for the period from November 1, 1990 to January 31, FINMAR=38=KGM=VVC 264 E.R. Odders-White / Journal of Financial Markets 3 (2000) 259\vphantom\{\}28'' (Odders-White, 2000, p. 264)
\par
``2 For a description of the TORQ database, see Hasbrouck (1992). 1991. The TORQ data consist of transaction, quote, and order records for all orders placed through one of the automated routing systems, as well as audit trail data, providing information on the parties involved and other detailed information about the trades.'' (Odders-White, 2000, p. 265)
\par
``Recall that Lee and Radhakrishna (1996) found a 93\% accuracy rate for the Lee and Ready method. Their accuracy rate exceeds the 85\% rate found here because the trades that they eliminate are more likely to be misclassi"ed by the algorithm.'' (Odders-White, 2000, p. 267)
\par
``For example, if the 50,000 transactions misclassi"ed by the Lee and Ready method constitute a representative cross-section of the entire sample, then the misclassi"cation will simply add noise to the data. In this case, the 85\% accuracy rate is quite good.'' (Odders-White, 2000, p. 268)
\par
``If, on the other hand, the Lee and Ready method systematically misclassi"es certain types of transactions, a bias could result'' (Odders-White, 2000, p. 268)
\par
``I divide the sample into three groups: transactions that occurred at or outside the quotes, transactions that occurred at the spread midpoint, and transactions that occurred elsewhere inside the spread (not at the midpoint).'' (Odders-White, 2000, p. 268)
\par
``The Chi-square statistic tests the hypothesis that the frequency of misclassi"cation is independent of price.'' (Odders-White, 2000, p. 269)
\par
``In fact, zero-tick trades are problematic in general because the prior trade is often an inappropriate benchmark. For example, if the prior trade took place long ago, it is \&stale' and does not re\#ect current market information.'' (Odders-White, 2000, p. 275)
\par
``If the "ndings are consistent across partitions, then researchers can be reasonably con"dent that their results are robust to misclassi"cation bias. On the other hand, if the results change along these dimensions without any clear explanation given the focus of the research, this suggests that misclassi"cation may be a problem. In this case, choices should clearly be guided by the goal of the study in question and the nature of the data.'' (Odders-White, 2000, p. 276)
\par
``Because these data are used to determine the true initiator of each transaction, I am implicitly assuming that the data accurately represent the truth. While no data set is error free, the TORQ data are quite clean and I have no reason to suspect that any non-random errors that could bias my results exist.'' (Odders-White, 2000, p. 276)
\par
``Evidence of the impact of inaccurate trade classi"cation on economic research is provided.'' (Odders-White, 2000, p. 280)
\par
``In light of this evidence, I recommend that researchers partition their transaction samples along the dimensions investigated in the paper and examine the impact on the results of their studies. If the "ndings are consistent across partitions, then researchers can be reasonably con"dent that their results are robust to misclassi"cation bias. On the other hand, if the results change along these dimensions without any clear explanation given the focus of the research, this suggests that misclassi"cation may be a problem. In this case, at a minimum, di!erences across partitions should be discussed along with the overall results.'' (Odders-White, 2000, p. 280)}
}

@article{olbrysEvaluatingTradeSide2018,
  title = {Evaluating Trade Side Classification Algorithms Using Intraday Data from the Warsaw Stock Exchange},
  author = {Olbrys, Joanna and Mursztyn, Micha{\l}},
  year = {2018},
  publisher = {{Karlsruhe}},
  doi = {10.5445/KSP/1000085951/20},
  copyright = {Closed Access, Creative Commons Namensnennung \textendash{} Weitergabe unter gleichen Bedingungen 4.0 International},
  note = {\section{Annotations\\
(03/10/2022, 11:34:08)}

\par
``Trade side classification algorithms enable us to assign the side that initiates a transaction and to distinguish between the so-called buyer- and seller-initiated trades.'' (Olbrys and Mursztyn, 2018, p. 1)
\par
``The main motivation for this study is growing interest in market liquidity, dimensions of liquidity, and commonality in liquidity that has emerged in the literature over the recent years.'' (Olbrys and Mursztyn, 2018, p. 2)
\par
``Moreover, to calculate several liquidity/illiquidity proxies using intraday data, it is essential to recognize the side initiating the transaction and to distinguish between the so-called buyer- and seller-initiated trades.'' (Olbrys and Mursztyn, 2018, p. 2)
\par
``The literature provides many alternative measures of stock market liquidity/illiquidity based on intraday data (e.g. Chordia et al, 2002, 2005; Goyenko et al, 2009; Ranaldo, 2001; Stoll, 2000; von Wyss, 2004; Nowak, 2017; Olbry\'s and Mursztyn, 2017a).'' (Olbrys and Mursztyn, 2018, p. 2)
\par
``The Warsaw Stock Exchange (WSE) is an order-driven market with an electronic order book, but information of the best bid and ask price is not made public.'' (Olbrys and Mursztyn, 2018, p. 2)
\par
``As a consequence, the researchers rely on indirect trade classification rules to infer trade sides.'' (Olbrys and Mursztyn, 2018, p. 3)
\par
``To verify the robustness of the obtained results, the comparison of trade side classification rules is provided both in the whole sample from January 3, 2005 to December 30, 2016 (3005 trading days) and over three adjacent sub-periods, each of equal size (436 trading days; see Olbry\'s and Mursztyn, 2015)'' (Olbrys and Mursztyn, 2018, p. 3)
\par
``For example, the so-called ``immediacy'' definition describes initiators as traders who demand immediate execution (e.g. Lee and Radhakrishna, 2000). According to Odders-White (2000), the initiator of a transaction is the investor (buyer or seller) who placed his/her order last, chronologically (the so called ``chronological'' definition)'' (Olbrys and Mursztyn, 2018, p. 4)
\par
``There are quite many trade side classification procedures proposed in the literature: the quote rule, the at the quote rule, the revised quote rule, the tick rule, the reverse tick rule, the LR algorithm, the revised LR algorithm, the EMO algorithm, and the Bulk Volume Classification methodology (BVC). We will describe the content of each classification rule as follows:'' (Olbrys and Mursztyn, 2018, p. 4)
\par
``Andersen and Bondarenko (2015)'' (Olbrys and Mursztyn, 2018, p. 5)
\par
``Trade classification algorithms are commonly used to generate estimates of market liquidity measures. Among others, Chan and Fong (2000) examine the significance of the number of trades, size of trades, and order imbalance (buyer- versus seller-initiated trades) in explaining the volatility-volume relation for a sample of NYSE and NASDAQ stocks.'' (Olbrys and Mursztyn, 2018, p. 7)
\par
``The second strand concerns various versions of bid-ask spread and other transaction costs.'' (Olbrys and Mursztyn, 2018, p. 7)
\par
``The next strand of the literature regards short sales.'' (Olbrys and Mursztyn, 2018, p. 8)
\par
``The empirical experiments show that the Lee and Ready algorithm performs better then other procedures on the WSE (see Table 5), which is consistent with the literature. Moreover, the robustness analysis reveals that the empirical findings turn out to be robust to the choice of the sample and rather do not depend on firm size.'' (Olbrys and Mursztyn, 2018, p. 14)}
}

@misc{oliverRealisticEvaluationDeep2019,
  title = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  year = {2019},
  number = {arXiv:1804.09170},
  eprint = {1804.09170},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@misc{OptionTrades,
  title = {Option Trades},
  journal = {Cboe DataShop},
  howpublished = {https://datashop.cboe.com/option-trades}
}

@book{owenHyperparameterTuningPython2022,
  title = {Hyperparameter Tuning with Python},
  author = {Owen, Louis},
  year = {2022},
  publisher = {{PACKT PUBLISHING LIMITED}},
  address = {{S.l.}}
}

@article{panayidesBulkVolumeClassification2019,
  title = {Bulk Volume Classification and Information Detection},
  author = {Panayides, Marios A. and Shohfi, Thomas D. and Smith, Jared D.},
  year = {2019},
  journal = {Journal of Banking \& Finance},
  volume = {103},
  doi = {10.1016/j.jbankfin.2019.04.001},
  note = {\section{Annotations\\
(07/11/2022, 07:35:48)}

\par
``uropean stock data'' (Panayides et al., 2019, p. 113)
\par
``BVC is data efficient, but may identify trade aggressors less accurately than ``bulk'' versions of traditional trade-level algorithms.'' (Panayides et al., 2019, p. 113)
\par
``BVC-estimated trade flow is the only algorithm related to proxies of informed trading, however.'' (Panayides et al., 2019, p. 113)
\par
``Finally, we find that after calibrating BVC to trading characteristics in out-of-sample data, it is better able to detect information and to identify trade aggressors.'' (Panayides et al., 2019, p. 113)
\par
``hough information detection is of primary importance, aggressor-signing can be useful as well, for characterizing investor clientele behavior and assessing trading costs, and an ideal algorithm should do both.'' (Panayides et al., 2019, p. 113)
\par
``This paper helps address these issues by examining the newly developed bulk volume classification algorithm (Easley et al., 2016); hereafter BVC) in modern, low-latency equity markets. BVC uses total volume and price changes within a block of trades to classify order flow into buying and selling volume. We assess BVC performance in terms of the accuracy in finding trade aggressors and ability to capture informative trade flow'' (Panayides et al., 2019, p. 113)
\par
``To help us calibrate the algorithm, we compare BVC's performance to bulk versions of traditional trade-level algorithms, bulk tick test (Smidt, 1985; Holthausen et al., 1987), and the Lee and Ready (1991) algorithm (hereafter LR)'' (Panayides et al., 2019, p. 113)
\par
``We find that BVC can identify trade aggressors as https://doi.org/10.1016/j.jbankfin.2019.04.001 0378-4266/\textcopyright{} 2019 Elsevier B.V. All rights reserved'' (Panayides et al., 2019, p. 113)
\par
``114 M.A. Panayides, T.D. Shohfi and J.D. Smith / Journal of Banking and Finance 103 (2019) 113 \textendash{} 12 9 well as traditional algorithms, and its signed order flow is the only measure that is reliably related to different illiquidity measures shown to capture the trading intensions of informed traders (e.g., Easley et al., 2016).'' (Panayides et al., 2019, p. 114)
\par
``We use equities data from NYSE Euronext for 2007 and 2008 and from the London Stock Exchange for 2017 to perform our analyses.'' (Panayides et al., 2019, p. 114)
\par
``We begin our analysis by investigating how well BVC classifies trade aggressors in our samples. BVC involves putting trades into blocks, or bars, by either volume or time. A percentage of the block is then classified as buys (the remainder as sells) based upon the movement of prices around the bars. By construction, the BVC algorithm is highly data efficient as it uses aggregate bar-size trading volume and prices, which translates to less than 1\% of the trade data points.'' (Panayides et al., 2019, p. 114)
\par
``The Euronext sample results on finding trade aggressors comport with those in Chakrabarty et al. (2015) and Easley et al. (2016); BVC is not as accurate as bulk versions of traditional trade-level algorithms. This reverses, however, in our more recent 2017 sample. Indeed, even though all three algorithms perform worse in the LSE sample, BVC is the most accurate.'' (Panayides et al., 2019, p. 114)
\par
``In our next set of analyses, we examine whether BVC can effectively uncover underlying trading intentions in order flow. In today's markets, researchers and practitioners are increasingly interested in identifying buying or selling pressure that can be destabilizing and/or toxic. Information-related order flow will unavoidably disadvantage other traders (retail traders and some institutional traders; O'Hara, 2015).'' (Panayides et al., 2019, p. 114)
\par
``We measure spreads in two ways, using the Corwin and Schultz (2012) high-low spread and calculating intraday effective spreads.'' (Panayides et al., 2019, p. 114)
\par
``These results indicate that trade aggressor identification does not convey underlying information in today's fast markets, where sophisticated traders use smart algorithmic trading to hide their trading intentions and minimize market impact.'' (Panayides et al., 2019, p. 114)
\par
``For example, informed traders are increasingly relying on passive orders, i.e., limit orders, to disguise themselves in the market (Bouchaud et al., 2009; Menkhoff et al., 2010; Zhang, 2013). Therefore, any traditional trade-level algorithm designed to find a trade's aggressor will not tell us much about information, no matter how accurate.'' (Panayides et al., 2019, p. 114)
\par
``This paper contributes to the nascent literature on trade classification algorithms (including BVC) and low-latency trading, as well as adding to the long list of papers investigating the performance of the LR and tick test algorithms'' (Panayides et al., 2019, p. 114)
\par
``In our data from less fragmented, European equity markets, we find that an out-of-sample calibrated BVC successfully captures the aggressor side of trades and information, and it does so without requiring costly, real-time analysis of low latency individual trade and quote data. This suggests that researchers can use BVC to both classify aggressors and measure information, while capturing the data efficiency gains inherent in BVC.'' (Panayides et al., 2019, p. 115)
\par
``The data are time-stamped at the second-level. The second set, from the London Stock Exchange (LSE), is built from the ``Tick Data'' and ``Rebuild Order Book'' data.'' (Panayides et al., 2019, p. 116)
\par
``First, we choose sample periods. From Euronext, we use April 2007, February 2008, and April 2008, because these months represent different periods of volatility, stable-low, stable-high, and dropping periods of volatility, respectively. From LSE, we choose a much more recent sample, February and April 2017. These data are characterized by lower, more stable volatility and much greater trade volume.'' (Panayides et al., 2019, p. 116)
\par
``We also impose standard trade and quote filters on the data, such as positive price, volume, and quote size, and the bid must be weakly lower than the ask. These filters result in approximately 210 and 335 gigabytes of trade, quote, and order data for Euronext and LSE respectively'' (Panayides et al., 2019, p. 116)
\par
``o identify whether each trade in our sample is a buy or a sell, we follow the definition of trade aggressor/initiator used in Odders-White (2000), which Ellis et al. (2000) note is preferred when a researcher has access to the order book. She defines the trade initiator based on chronological order arrival, that is, the order that arrives second is the order that actually ``initiates'' the trade. For example, if a market buy order comes in at 11:15AM and hits a limit sell order that had been standing in the book since 11:00AM, that trade would be classified as a buy for our purposes. To determine the trade aggressor in our sample, we first classify fully-executed orders into active and passive categories. An active order is executed at the same date and time as it is submitted to the marketplace, and is, essentially, a market order. A passive order is a non-market order whose execution time is always later than its submission time. In this case, the initiator of a trade will be the opposite buy or sell direction of a matching passive order. Active orders account for approximately 98\% of trade aggressors identified across our sample.'' (Panayides et al., 2019, p. 116)
\par
``The bulk volume classification procedure was developed in ELO for use in the Easley et al. (2012) volume-synchronized probability of informed trading (VPIN) calculation. It is designed to classify bars of trades (i.e., trades put in blocks either by time or'' (Panayides et al., 2019, p. 116)
\par
``volume)5 as a percentage of buys and sells, rather than classifying each individual trade.'' (Panayides et al., 2019, p. 117)
\par
``By putting the trades into volume blocks, the algorithm mitigates any impact from order splitting and economizes on the number of data points used for classification.'' (Panayides et al., 2019, p. 117)
\par
``Next, for each stock-month combination, we calculate the volume-weighted standard deviation of price changes between consecutive bars as shown in formula (1). {$\sigma$}Pi = {$\surd\sum$} {$\tau$}n =1 Vi,{$\tau$} (Pi,{$\tau$} - Pi )2 {$\sum$} {$\tau$}n =1 Vi,{$\tau$} (1) where Vi, {$\tau$} is the actual volume of shares traded of stock-month i during the time or volume bar {$\tau$} which is decomposed into the buy ( \textasciicircum{} V Buy i,{$\tau$} ) and sell ( \textasciicircum{} V Sell i,{$\tau$} ) volume estimate components. Pi,{$\tau$} = Pi,{$\tau$} - Pi,{$\tau$} -1 is the price change between two consecutive bars. With these available data points, we can then use formula (2) of ELO to calculate BVC's buy volume for each bar: \textasciicircum{} V Buy i,{$\tau$} = Vi,{$\tau$} {$\cdot$} t ( Pi,{$\tau$} - Pi,{$\tau$} -1 {$\sigma$}Pi , df ) \textasciicircum{} V Sell i,{$\tau$} = Vi, {$\tau$} - \textasciicircum{} V Buy i,{$\tau$} = Vi, {$\tau$} {$\cdot$} [ 1-t ( Pi,{$\tau$} - Pi,{$\tau$} -1 {$\sigma$}Pi , df )] (2)'' (Panayides et al., 2019, p. 117)
\par
``Because BVC puts trades into bars and offsets misclassifications, we compute bulk versions of LR and the tick test to better compare across algorithms (Chakrabarty et al., (2015); ELO, 2016).'' (Panayides et al., 2019, p. 118)
\par
``We first note that the accuracy rates are lower than those reported in ELO, who use futures data; their accuracy rates top 94\% versus 89\% in our analysis.'' (Panayides et al., 2019, p. 118)
\par
``Despite this challenge, BVC performs well in our two samples. Regarding the Euronext sample (columns 1\textendash 3 of Table 2) BVC accuracy ranges from 62.62\% to 87.90\% in Panel A with volume bars, and from 58.66\% to 89.68\% in Panel B with time bars.'' (Panayides et al., 2019, p. 118)
\par
``n the LSE sample (columns 4\textendash 6) we see some important differences from the Euronext results. First, accuracy rates are lower for BVC, as well as bulk tick test and bulk LR.'' (Panayides et al., 2019, p. 118)
\par
``To further investigate the effect of bar size choice on BVC accuracy, we consider scenarios in which bar size can either be ``too small'' or ``too large,'' given the distribution of trade sizes. In particular, with respect to time bars, if the bar size is too small, the 15 Bakshi et al. (2003) identify that the returns distribution kurtosis across stocks increases with market capitalization so, we follow their return distribution analysis by reducing the degrees of freedom in the large and mid-cap group Student's tdistributions to 0.05 and 0.1, respectively. bar will not contain enough trades to benefit from netting misclassified trades. This reduced netting will impact both bulk tick test/LR and BVC algorithms.'' (Panayides et al., 2019, p. 122)
\par
``A potential source of bias in the volume bar BVC applied to equities arises not from the choice of bar size but how that bar size is applied to the data. BVC proposed in ELO does not specify whether volume bars should contain volume equal to bar size or if that size is a minimum amount of volume for each bar.'' (Panayides et al., 2019, p. 124)
\par
``Researchers commonly use the Lee and Ready trade classification algorithm if quotes are available or the tick test if not. The recently introduced bulk volume classification (BVC) has an alternative design that makes it much more data efficient.'' (Panayides et al., 2019, p. 128)
\par
``Importantly, we find\textemdash using both spread regressions and a returns event study\textemdash that BVC has a consistent advantage in capturing information rather than just trade aggressors, which suggests that BVC offers real advantages over methods built on signing individual trades.'' (Panayides et al., 2019, p. 128)}
}

@article{panayidesComparingTradeFlow2014,
  title = {Comparing Trade Flow Classification Algorithms in the Electronic Era: The Good, the Bad, and the Uninformative},
  author = {Panayides, Marios A. and Shohfi, Thomas and Smith, Jared D.},
  year = {2014},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2503628}
}

@article{panInformationOptionVolume2006,
  title = {The Information in Option Volume for Future Stock Prices},
  author = {Pan, Jun and Poteshman, Allen M.},
  year = {2006},
  journal = {Review of Financial Studies},
  volume = {19},
  number = {3},
  doi = {10.1093/rfs/hhj024}
}

@article{parkEffectiveHashbasedAlgorithm1995,
  title = {An Effective Hash-Based Algorithm for Mining Association Rules},
  author = {Park, Jong Soo and Chen, Ming-Syan and Yu, Philip S.},
  year = {1995},
  journal = {ACM SIGMOD Record},
  volume = {24},
  number = {2},
  doi = {10.1145/568271.223813}
}

@article{Parkinson_1980,
  title = {The Extreme Value Method for Estimating the Variance of the Rate of Return},
  author = {Parkinson, Michael},
  year = {1980},
  journal = {The Journal of Business},
  doi = {10.1086/296071},
  mag_id = {1970694313},
  pmcid = {null},
  pmid = {null}
}

@article{Parlour_2008,
  title = {Limit Order Markets: A Survey 1},
  author = {Parlour, Christine A. and Seppi, Duane J.},
  year = {2008},
  journal = {null},
  doi = {10.1016/b978-044451558-2.50007-6},
  mag_id = {2166890233},
  pmcid = {null},
  pmid = {null}
}

@inproceedings{parmarImageTransformer2018,
  title = {Image Transformer},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  publisher = {{PMLR}}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  series = {{{NeurIPS}} 2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  address = {{Red Hook, NY}}
}

@misc{patrignaniWhyShouldAnyone2021,
  title = {Why Should Anyone Use Colours? Or, Syntax Highlighting beyond Code Snippets},
  author = {Patrignani, Marco},
  year = {2021},
  number = {arXiv:2001.11334},
  eprint = {2001.11334},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@misc{pedregosaScikitlearnMachineLearning2018,
  title = {Scikit-Learn: Machine Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and M{\"u}ller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2018},
  number = {arXiv:1201.0490},
  eprint = {1201.0490},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Update authors list and URLs}
}

@article{perez-lebelBenchmarkingMissingvaluesApproaches2022,
  title = {Benchmarking Missing-Values Approaches for Predictive Models on Health Databases},
  author = {{Perez-Lebel}, Alexandre and Varoquaux, Ga{\"e}l and Le~Morvan, Marine and Josse, Julie and Poline, Jean-Baptiste},
  year = {2022},
  journal = {GigaScience},
  volume = {11},
  doi = {10.1093/gigascience/giac013},
  note = {\section{Annotations\\
(04/11/2022, 14:58:25)}

\par
``For prediction after imputation, we find that adding an indicator to express which values have been imputed is important, suggesting that the data are missing not at random. Elaborate missing-values imputation can improve prediction compared to simple strategies but requires longer computational time on large data. Learning trees that model missing values\textemdash with missing incorporated attribute\textemdash leads to robust, fast, and well-performing predictive modeling'' (Perez-Lebel et al., 2022, p. 1)
\par
``Conclusions: Native support for missing values in supervised machine learning predicts better than state-of-the-art imputation with much less computational cost. When using imputation, it is important to add indicator columns expressing which values have been imputed.'' (Perez-Lebel et al., 2022, p. 1)
\par
``For such a problem, an important distinction between missing data mechanisms was introduced by Rubin [3]: missing completely at random (MCAR), where the probability of having missing data does not depend on the covariates; missing at random (MAR), where the probability of a missing value only depends on the observed values of other variables; and missing not at random (MNAR), which covers all other cases. MNAR corresponds to cases where the missingness carries information.'' (Perez-Lebel et al., 2022, p. 1)
\par
``The simplest one is to delete all observations containing missing values. However, leaving aside the possible biases that this practice may induce, it often leads to considerable loss of information in high and even moderate dimensions'' (Perez-Lebel et al., 2022, p. 2)
\par
``To deal with arbitrary subsets of input features, the most common practice currently consists in first imputing the missing values and then learning a predictive model (e.g., regression or classification) on the completed data.'' (Perez-Lebel et al., 2022, p. 2)
\par
``Recent theoretical results show that applying a supervised-learning regression on imputed data can asymptotically recover the optimal prediction function; however most imputation strategies, including the common imputation by the conditional expectation, create discontinuities in the regression function to learn [16].'' (Perez-Lebel et al., 2022, p. 2)
\par
``A small number of machine learning models can natively handle missing values, in particular popular tree-based methods. Trees greedily partition the input space into subspaces in order to minimize a risk. This non-smooth optimization scheme enables them to be easily adapted to directly learn from incomplete data.'' (Perez-Lebel et al., 2022, p. 2)
\par
``High-quality conditional imputation gives good prediction provided that a variable indicating which entries were imputed is added to the completed data. However, its algorithmic complexity makes it prohibitively costly on large data'' (Perez-Lebel et al., 2022, p. 2)
\par
``Rather, tree-based methods with integrated support for missing values (MIA) perform as well or better, at a fraction of the computational cost'' (Perez-Lebel et al., 2022, p. 2)
\par
``Our experiments compare 2-step procedures based on imputation followed by regression or classification, as well as tree-based models with an intrinsic support for missing values thanks to MIA.'' (Perez-Lebel et al., 2022, p. 2)
\par
``Constant imputation: mean and median.The simplest approach to imputation is to replace missing values by a constant such as the mean, the median, or the mode of the corresponding feature. This is frowned upon in classical statistical practice because the resulting data distribution is severely distorted compared to that of fully observed data. Yet, in a supervised setting, the goal is different from that of inferential tasks. Recent theoretical results have established that powerful learners such as those based on trees can learn to recognize such imputed values and give the best possible predictions [7].'' (Perez-Lebel et al., 2022, p. 2)
\par
``Conditional imputation: MICE and KNN. Powerful imputation approaches rely on conditional dependencies between features to fill in the missing values. Adapting machine learning techniques gives flexible estimators of these dependencies.'' (Perez-Lebel et al., 2022, p. 2)
\par
``For these reasons, it can be useful after imputation to add new binary features that encode whether a value was originally missing or not: the ``mask'' or ``missingness indicator'' [6, 7, 21]'' (Perez-Lebel et al., 2022, p. 3)
\par
``When estimating model parameters, it is important to reflect the uncertainty due to the missing values.'' (Perez-Lebel et al., 2022, p. 3)
\par
``Indeed, it has been shown recently that a sufficiently flexible learner reaches optimal performance asymptotically with single imputation, whatever the missing data mechanism and whatever the choice of imputation function [16].'' (Perez-Lebel et al., 2022, p. 3)
\par
``Because these methods all come with a substantial computing cost, we focus on the most promising approach: bagging single imputation'' (Perez-Lebel et al., 2022, p. 3)
\par
``More precisely, for each task we draw 100 bootstrap replicates. We then fit the single imputation and the predictive model on each of these replicates to obtain 100 predictors. Final predictions are made either by voting or by averaging (see Table A4)'' (Perez-Lebel et al., 2022, p. 3)
\par
``It has the benefit of using all samples, including incomplete ones, to produce the splits of the input space. More precisely for each split based on variable j, all samples with a missing value in variable j are sent either to the left or to the right child node, depending on which option leads to the lowest risk.'' (Perez-Lebel et al., 2022, p. 3)
\par
``That makes MIA particularly suited to MNAR settings because it can harness the missingness information.'' (Perez-Lebel et al., 2022, p. 3)
\par
``Moreover, because trees with MIA directly learn with missing values, they provide a straightforward way of dealing with missing values in the test set'' (Perez-Lebel et al., 2022, p. 3)
\par
``We applied supervised learning to the imputed data for the imputation-based methods. We also used the tree models with their support of MIA for a direct handling of missing values'' (Perez-Lebel et al., 2022, p. 3)
\par
``Gradient-boosted trees are state-of-the art predictors for tabular data [24\textendash 26] and thus constitute a strong baseline'' (Perez-Lebel et al., 2022, p. 3)
\par
``Iterative+mask+bagging obtains the best overall mean rank (2.6) across all tasks and sizes in terms of prediction score, closely followed by MIA+bagging (2.8) as shown on Fig. 1A and Table A7B.'' (Perez-Lebel et al., 2022, p. 4)
\par
``MIA makes it possible to navigate a trade-off between prediction performance and computational tractability: with bagging it comes close to iterative+mask with one-half the computational cost on large databases'' (Perez-Lebel et al., 2022, p. 4)
\par
``In terms of computing time, beyond the fact that bagging multiplies the cost of every method by 100, MIA is almost always the fastest (Fig. 1B), although it gives excellent prediction performance.'' (Perez-Lebel et al., 2022, p. 4)
\par
``performance\textemdash doubles their computing times. At the other end of the spectrum, iterative+mask and KNN+mask are the slowest non-bagged methods. The gaps between training times of the methods increase with the size of the database, revealing the difference in algorithmic scalability.'' (Perez-Lebel et al., 2022, p. 4)
\par
``The Friedman test compares the mean ranks of several algorithms run on several datasets. The null hypothesis assumes that all algorithms are equivalent, i.e., their rank should be equal. Table A2 shows that the null hypothesis is rejected, with P-values much less than the 0.05 level for the sizes 2,500, 10,000, and 25,000. This indicates that {$\geq$}1 algorithm has significantly different performances from 1 other on these sizes.'' (Perez-Lebel et al., 2022, p. 4)
\par
``We run a complementary analysis with a 1-sided Wilcoxon signed-rank test, used for non-parametric tests comparing algorithms pairwise. We compare MIA with every other method. The null hypothesis claims that the median of the score differences between the 2 methods is positive (respectively, negative) for the 1-sided right (1-sided left) test'' (Perez-Lebel et al., 2022, p. 4)
\par
``mputations with the additional variable representing the mask perform systematically better in terms of mean prediction score than their counterpart without mask (Fig. 1A, Table A7b)'' (Perez-Lebel et al., 2022, p. 4)
\par
``Figure 1 shows that conditional imputation using iterative or KNN imputers does not perform consistently better than constant imputation. The overall mean rank of iterative and KNN are 9.0 and 11.5 versus 7.5 and 9.2 for mean and median, respectively (Fig. 1A and Table A7), and a similar delta is visible on the masked version.'' (Perez-Lebel et al., 2022, p. 4)
\par
``Indeed, bagging in itself is known to improve generalization. To answer whether the good performance of multiple imputation can be attributed to ensembling (averaging multiple predictors) or capturing the conditional distribution, we performed an additional experiment with mean+mask+bagging (see Fig. A8)'' (Perez-Lebel et al., 2022, p. 5)
\par
``It may be surprising at first that a sophisticated conditional imputation does not outperform constant imputation. Indeed, it contradicts the intuition that better imputation should lead to better prediction. Theoretical work shows that this intuition is not always true [16]: even in MAR settings, it may not hold for strongly non-linear mechanisms and little dependency across features.'' (Perez-Lebel et al., 2022, p. 5)
\par
``MIA, the missing-values support inside gradient-boosted trees, appears as a method of choice to deal with missing values. Once put aside the prohibitively costly bagged methods, MIA was on average the best in terms of performance in our extensive benchmark while having a low computational cost.'' (Perez-Lebel et al., 2022, p. 6)
\par
``For imputation-based pipelines, prediction significantly improves with the missingness mask added as input features'' (Perez-Lebel et al., 2022, p. 6)
\par
``We checked that features' missing rates and predictive importance were not associated. For this, we measured permutation features: the drop in a model score after shuffling a feature, thereby canceling its contribution to the model performance.'' (Perez-Lebel et al., 2022, p. 6)
\par
``We ran this experiment for each task and each feature using scikit-learn's implementation (see Table A4). We found no association between a feature's missing rate and its importance (Fig. A7). Predictions do not only rely on features with few missing values. Moreover, even features with a very high level of missing values (e.g., {$>$}80\%) seem to be as important as the others.'' (Perez-Lebel et al., 2022, p. 6)
\par
``irst, directly incorporating missing values in treebased models with MIA gives a small but systematic improvement in prediction performance over prior imputation. Second, the computational cost of imputation using MICE or KNN becomes intractable for large datasets. Third, gradient-boosted trees give better predictions than linear models. Fourth, bagging increases predictive performance but with a severe computational cost. Fifth, good imputation does not imply good prediction because both have different trade-offs. Finally, the experiments reveal that the missingness is informative.'' (Perez-Lebel et al., 2022, p. 7)}
}

@article{perlinPerformanceTickTest2014,
  title = {On the Performance of the Tick Test},
  author = {Perlin, Marcelo and Brooks, Chris and Dufour, Alfonso},
  year = {2014},
  journal = {The Quarterly Review of Economics and Finance},
  volume = {54},
  number = {1},
  doi = {10.1016/j.qref.2013.07.009},
  note = {\section{Annotations\\
(24/10/2022, 12:23:59)}

\par
``By analyzing the derived equation, we provide formal arguments for the use of the tick test by proving that it is bounded to perform better than chance (50/50) and that the set of rules from the tick test provides an unbiased estimator of the trade signs.'' (Perlin et al., 2014, p. 42)
\par
``This paper is a seminal work in the literature as it was the first to formally study the inference of trade direction based on incomplete data.'' (Perlin et al., 2014, p. 43)
\par
``Notice that the work of Finucane (2000) already shows some of the findings presented in this paper, specifically that the spread will contribute in a positive way to the performance of the tick test (i.e., the higher the spread, the higher the accuracy of this particular method of prediction of the sign of a trade)'' (Perlin et al., 2014, p. 43)}
}

@article{Petersen_1994,
  title = {Posted versus Effective Spreads: Good Prices or Bad Quotes?},
  author = {Petersen, Mitchell A. and Fialkowski, David},
  year = {1994},
  journal = {Journal of Financial Economics},
  doi = {10.1016/0304-405x(94)90034-5},
  mag_id = {1590007075},
  pmcid = {null},
  pmid = {null}
}

@article{petersenMatrixCookbook,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind}
}

@article{petersonEvaluationBiasesExecution2003,
  title = {Evaluation of the Biases in Execution Cost Estimation Using Trade and Quote Data},
  author = {Peterson, Mark and Sirri, Erik},
  year = {2003},
  journal = {Journal of Financial Markets},
  volume = {6},
  number = {3},
  doi = {10.1016/S1386-4181(02)00065-4}
}

@incollection{petitUNetTransformerSelf2021,
  title = {U-Net Transformer: Self and Cross Attention for Medical Image Segmentation},
  booktitle = {Machine {{Learning}} in {{Medical Imaging}}},
  author = {Petit, Olivier and Thome, Nicolas and Rambour, Clement and Themyr, Loic and Collins, Toby and Soler, Luc},
  editor = {Lian, Chunfeng and Cao, Xiaohuan and Rekik, Islem and Xu, Xuanang and Yan, Pingkun},
  year = {2021},
  volume = {12966},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-87589-3_28}
}

@article{phuongFormalAlgorithmsTransformers2022,
  title = {Formal Algorithms for Transformers},
  author = {Phuong, Mary and Hutter, Marcus},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2207.09238},
  note = {\section{Annotations\\
(04/12/2022, 11:23:27)}

\par
``It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.'' (Phuong and Hutter, 2022, p. 1)
\par
``It aims to be a self-contained, complete, precise and compact overview of transformer architectures and formal algorithms (but not results).'' (Phuong and Hutter, 2022, p. 1)
\par
``n practice, the distribution estimate is often decomposed via the chain rule as \textasciicircum{} {$\mathsl{P}^{1}\mathbit{x}$}\textordmasculine{} = \textasciicircum{} {$\mathsl{P}\mathbit{\theta}$} {$^{1}\mathsl{x}$} \guillemotright 1{$\frac{1}{4}$}\textordmasculine{} \textasciicircum{} {$\mathsl{P}\mathbit{\theta}^{1}\mathsl{x}$} \guillemotright 2{$\frac{1}{4}$} j {$\mathsl{x}$} \guillemotright 1{$\frac{1}{4}$}\textordmasculine{} \textasciicircum{} {$\mathsl{P}\mathbit{\theta}$} {$^{1}\mathsl{x}$} \guillemotright{$\frac{1}{4}$} j {$\mathbit{x}$} \guillemotright 1 : 1{$\frac{1}{4}$}\textordmasculine, where {$\mathbit{\theta}$} consists of all neural network parameters to be learned. The goal is to learn a distribution over a single token {$\mathsl{x}$} \guillemotright{$\mathsl{t}\frac{1}{4}$} given its preceding tokens {$\mathsl{x}$} \guillemotright 1 : {$\mathsl{t}$} 1{$\frac{1}{4}$} as context.'' (Phuong and Hutter, 2022, p. 3)
\par
``Given a vocabulary {$\mathsl{V}$} and a set of classes \guillemotright{$\mathsl{N}$}C{$\frac{1}{4}$}, let {$^{1}\mathbit{x}\mathsl{n}$}Â {$\mathsl{c}\mathsl{n}$}\textordmasculine{} 2 {$\mathsl{V}$} \guillemotright{$\mathsl{N}$}C{$\frac{1}{4}$} for {$\mathsl{n}$} 2 \guillemotright{$\mathsl{N}$}data{$\frac{1}{4}$} be an i.i.d. dataset of sequence-class pairs sampled from {$\mathsl{P}^{1}\mathbit{x}$}Â {$\mathsl{c}$}\textordmasculine. The goal in classification is to learn an estimate of the conditional distribution {$\mathsl{P}^{1}\mathsl{c}$}j{$\mathbit{x}$}\textordmasculine.'' (Phuong and Hutter, 2022, p. 4)
\par
``here are in fact many ways to do subword tokenization. One of the simplest and most successful ones is Byte Pair Encoding [Gag94, SHB16] used in GPT-2 [RWC{\c  }19].'' (Phuong and Hutter, 2022, p. 4)
\par
``Token embedding. The token embedding learns to represent each vocabulary element as a vector in {$\mathbb{R}\mathsl{d}$}e; see Algorithm 1'' (Phuong and Hutter, 2022, p. 4)
\par
``The positional embedding learns to represent a token's position in a sequence as a vector in {$\mathbb{R}\mathsl{d}$}e. For example, the position of the first token in a sequence is represented by a (learned) vector {$\mathbit{W}\mathbit{p}$} \guillemotright :Â 1{$\frac{1}{4}$}, the position of the second token is represented by another (learned) vector {$\mathbit{W}\mathbit{p}$} \guillemotright :Â 2{$\frac{1}{4}$}, etc. The purpose of the positional embedding is to allow a Transformer to make sense of word ordering; in its absence the representation would be permutation invariant and the model would perceive sequences as ``bags of words'' instead.'' (Phuong and Hutter, 2022, p. 5)
\par
``The positional embedding of a token is usually added to the token embedding to form a token's initial embedding. For the {$\mathsl{t}$}-th token of a sequence {$\mathbit{x}$}, the embedding is {$\mathbit{e}$} = {$\mathbit{W}\mathbit{e}$} \guillemotright :Â {$\mathsl{x}$} \guillemotright{$\mathsl{t}\frac{1}{4}\frac{1}{4}$}{\c  } {$\mathbit{W}\mathbit{p}$} \guillemotright :Â {$\mathsl{t}\frac{1}{4}$}Â'' (Phuong and Hutter, 2022, p. 5)
\par
``Attention. Attention is the main architectural component of transformers. It enables a neural Algorithm 2: Positional embedding. Input: 2 \guillemotright max{$\frac{1}{4}$}, position of a token in the sequence. Output: {$\mathbit{e}\mathbit{p}$} 2 {$\mathbb{R}\mathsl{d}$}e, the vector representation of the position. Parameters: {$\mathbit{W}\mathbit{p}$} 2 {$\mathbb{R}\mathsl{d}$}emax , the positional embedding matrix. 1 return {$\mathbit{e}\mathbit{p}$} = {$\mathbit{W}\mathbit{p}$} \guillemotright :Â {$\frac{1}{4}$} network to make use of contextual information (e.g. preceding text or the surrounding text) for predicting the current token.'' (Phuong and Hutter, 2022, p. 5)
\par
``On a high level, attention works as follows: the token currently being predicted is mapped to a query vector {$\mathbit{q}$} 2 {$\mathbb{R}\mathsl{d}$}attn, and the tokens in the context are mapped to key vectors {$\mathbit{k}\mathsl{t}$} 2 {$\mathbb{R}\mathsl{d}$}attn and value vectors {$\mathbit{v}\mathsl{t}$} 2 {$\mathbb{R}\mathsl{d}$}value . The inner products {$\mathbit{q}$}áµ{$\mathbit{k}\mathsl{t}$} are interpreted as the degree to which token {$\mathsl{t}$} 2 {$\mathsl{V}$} is important for predicting the current token {$\mathsl{q}$} \textendash{} they are used to derive a distribution over the context tokens, which is then used to combine the value vectors. An intuitive explanation how this achieves attention can be found at [Ala18, Ala19]. The precise algorithm is given in Algorithm 3'' (Phuong and Hutter, 2022, p. 5)
\par
``he attention algorithm presented so far (Algorithm 4) describes the operation of a single attention head. In practice, transformers run multiple attention heads (with separate learnable parameters) in parallel and combine their outputs; this is called multi-head attention; see Algorithm 5'' (Phuong and Hutter, 2022, p. 6)
\par
``Layer normalisation explicitly controls the mean and variance of individual neural network activations; the pseudocode is given in Algorithm 6'' (Phuong and Hutter, 2022, p. 7)
\par
``It uses the GELU nonlinearity instead of ReLU: GELU{$^{1}\mathsl{x}$}\textordmasculine{} = {$\mathsl{x}$} {$\mathbb{P}\mathsl{X}$}N {$^1$}0Â1\textordmasculine{} \guillemotright{$\mathsl{X}$} Â {$\mathsl{x}\frac{1}{4}$}Â (5) (When called with vector or matrix arguments, GELU is applied element-wise.)'' (Phuong and Hutter, 2022, p. 8)
\par
``EDTraining() Algorithm 11 shows how to train a sequence-to-sequence transformer (the original Transformer [VSP{\c  }17]).'' (Phuong and Hutter, 2022, p. 8)
\par
``Gradient descent. The described training Algorithms 11 to 13 use Stochastic Gradient Descent'' (Phuong and Hutter, 2022, p. 8)
\par
``Formal Algorithms for Transformers (SGD) {$\mathbit{\theta}$} {$\mathbit{\theta}$} {$\mathsl{H}$} rloss{$^{1}\mathbit{\theta}$}\textordmasculine{} to minimize the log loss (aka cross entropy) as the update rule. Computation of the gradient is done via automatic differentiation tools; see [BPRS18, Table 5]. In practice, vanilla SGD is usually replaced by some more refined variation such as RMSProp or AdaGrad or others [Rud16]. Adam [KB15] is used most often these days.'' (Phuong and Hutter, 2022, p. 9)}
}

@article{Piwowar_2006,
  title = {The Sensitivity of Effective Spread Estimates to Trade\textendash Quote Matching Algorithms},
  author = {Piwowar, Michael S. and Wei, Li},
  year = {2006},
  journal = {Electronic Markets},
  doi = {10.1080/10196780600643803},
  mag_id = {2013480621},
  pmcid = {null},
  pmid = {null}
}

@article{Pomponio_2010,
  title = {Trade-Throughs: Empirical Facts and Application to Lead-Lag Measures},
  author = {Pomponio, Fabrizio and Abergel, Fr{\'e}d{\'e}ric},
  year = {2010},
  journal = {null},
  doi = {10.2139/ssrn.1694103},
  mag_id = {1774313365},
  pmcid = {null},
  pmid = {null}
}

@article{popelTrainingTipsTransformer2018,
  title = {Training Tips for the Transformer Model},
  author = {Popel, Martin and Bojar, Ond{\v r}ej},
  year = {2018},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  volume = {110},
  number = {1},
  eprint = {1804.00247},
  eprinttype = {arxiv},
  doi = {10.2478/pralin-2018-0002},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(09/01/2023, 14:04:11)}

\par
``The situation in NMT is further complicated by the fact that the training of NMT systems is usually non-deterministic,4 and (esp. with the most recent models) hardly ever converges or starts overfitting5 on reasonably big datasets. This leads to learning curves that never fully flatten let alone start decreasing (see Section 4.2). The common practice of machine learning to evaluate the model on a final test set when it started overfitting (or a bit sooner) is thus not applicable in practice.'' (Popel and Bojar, 2018, p. 45)
\par
``Most probably, the training was run until no further improvements were clearly apparent on the development test set, and the model was evaluated at that point. Such an approximate stopping criterion is rather risky: it is conceivable that different setups were stopped at different stages of training and their comparison is not fair'' (Popel and Bojar, 2018, p. 45)
\par
``When we tried the standard technique of early stopping, when N subsequent evaluations on the development test set do not give improvements larger than a given delta, we saw a big variance in the training time and final BLEU, even for experiments with the same hyper-parameters and just a different random seed. Moreover to get the best results, we would have had to use a very large N and a very small delta'' (Popel and Bojar, 2018, p. 45)
\par
``By overfitting we mean here that the translation quality (test-set BLEU) begins to worsen, while the training loss keeps improving'' (Popel and Bojar, 2018, p. 45)
\par
``Batch Size is the number of training examples used by one GPU in one training step. In sequence-to-sequence models, batch size is usually specified as the number of sentence pairs. However, the parameter batch\_size in T2T translation specifies the approximate number of tokens (subwords) in one batch.6 This allows to use a higher number of short sentences in one batch or a smaller number of long sentences. Effective Batch Size is the number of training examples consumed in one training step. When training on multiple GPUs, the parameter batch\_size is interpreted per GPU. That is, with batch\_size=1500 and 8 GPUs, the system actually digests 12k subwords of each language in one step.'' (Popel and Bojar, 2018, p. 46)
\par
``In this section, we focus however only on the computation speed and training throughput. Both are affected by three important factors: batch size, number of used GPUs and model size. The speed is usually almost constant for a given experiment'' (Popel and Bojar, 2018, p. 50)
\par
``The training throughput grows sub-linearly with increasing batch size, so based on these experiments only, there is just a small advantage when setting the batch size to the maximum value.'' (Popel and Bojar, 2018, p. 51)
\par
``able 3 uses the BIG model and batch\_size=1500, while varying the number of GPUs. The overhead in GPU synchronization is apparent from the decreasing computation speed. Nevertheless, the training throughput still grows with more GPUs, so e.g. with 6 GPUs we process 3.2 times more training data per hour relative to a single GPU (while without any overhead we would hypothetically expect 6 times more data).'' (Popel and Bojar, 2018, p. 51)
\par
``Training on the bigger dataset gives slightly worse results in the first eight hours of training (not shown in the graph) but clearly better results after two days of training, reaching over 26.5 BLEU after eight days.'' (Popel and Bojar, 2018, p. 52)
\par
``This means about 10 epochs in the smaller dataset were needed for reaching the convergence and this is also the moment when the bigger'' (Popel and Bojar, 2018, p. 52)
\par
``For comparing different datasets (e.g. smaller and cleaner vs. bigger and noisier), we need to train long enough because results after first hours (or days if training on a single GPU) may be misleading. \textbullet{} For large training data (as CzEng 1.7 which has over half a gigaword), BLEU improves even after one week of training on eight GPUs (or after 20 days of training on two GPUs in another experiment). \textbullet{} We cannot easily interpolate one dataset results to another dataset. While the smaller training data (with CzEng 1.0) converged after 2 days, the main training data (with CzEng 1.7), which is 2.5 times bigger, continues improving even after 2.52 days.'' (Popel and Bojar, 2018, p. 53)
\par
``For example, to get over BLEU of 18 with batch\_size=3000, we need 7 hours (260M examples), and with batch\_size=1500, we need about 3 days (2260M examples) i.e. 10 times longer (9 time more examples). From Table 2a we know that bigger batches have slower computation speed, so when re-plotting Figure 5 with steps instead of time on the x-axis, the difference between the curves would be even bigger. From Table 2b we know that bigger batches have slightly higher training throughput, so when re-plotting with number of examples processed on the x-axis, the difference will be smaller, but still visible.'' (Popel and Bojar, 2018, p. 57)
\par
``Batch size should be set as high as possible while keeping a reserve for not hitting the out-of-memory errors. It is advisable to establish the largest possible batch size before starting the main and long training.'' (Popel and Bojar, 2018, p. 59)
\par
``The default learning rate in T2T translation models is 0.20. Figure 7 shows that varying the value within range 0.05\textendash 0.25 makes almost no difference. Setting the learning rate too low (0.01) results in notably slower convergence. Setting the learning rate too high (0.30, not shown in the figure) results in diverged training, which means in this case that the learning curve starts growing as usual, but at one moment drops down almost to zero and stays there forever. A common solution to prevent diverged training is to decrease the learning\_rate parameter or increase learning\_rate\_warmup\_steps or introduce gradient clipping.'' (Popel and Bojar, 2018, p. 59)
\par
``Figure 8 shows the effect of different warmup steps with a fixed learning rate (the default 0.20). Setting warmup steps too low (12k) results in diverged training. Setting them too high (48k, green curve) results in a slightly slower convergence at first, but matching the baseline after a few hours of training.'' (Popel and Bojar, 2018, p. 60)
\par
``In case of diverged training, try gradient clipping and/or more warmup steps.'' (Popel and Bojar, 2018, p. 60)
\par
``If that does not help (or if the warmup steps are too high relative to the expected total training steps), try decreasing the learning rate.'' (Popel and Bojar, 2018, p. 61)
\par
``For the fastest BLEU convergence use as many GPUs as available (in our experiments up to 8). \textbullet{} This holds even when there are more experiments to be done. For example, it is better to run one 8-GPUs experiment after another, rather than running two 4-GPUs experiments in parallel or eight single-GPU experiments in parallel.'' (Popel and Bojar, 2018, p. 62)
\par
``There is a growing number of papers on scaling deep learning to multiple machines with synchronous SGD (or its variants) by increasing the effective batch size. We will focus mostly on the question how to adapt the learning rate schedule, when scaling from one GPU (or any device, in general) to k GPUs. Krizhevsky (2014) says ``Theory suggests that when multiplying the batch size by k, one should multiply the learning rate by pk to keep the variance in the gradient expectation constant'', without actually explaining which theory suggests so. However, in the experimental part he reports that what worked the best, was a linear scaling heuristics, i.e. multiplying the learning rate by k, again without any explanation nor details on the difference between pk scaling and k scaling.'' (Popel and Bojar, 2018, p. 63)
\par
``We can see that large-batch training is still an open research question. Most of the papers cited above have experimental support only from the image recognition tasks (usually ImageNet) and convolutional networks (e.g. ResNet), so it is not clear whether their suggestions can be applied also on sequence-to-sequence tasks (NMT) with self-attentional networks (Transformer). There are several other differences as well: Modern convolutional networks are usually trained with batch normalization 29 To close the gap between small-batch training and large-batch training, Hoffer et al. (2017) introduce (in addition to pk scaling) so-called ghost batch normalization and adapted training regime, which means decaying the learning rate after a given number of steps instead of epochs. 63 Unauthenticated Download Date | 11/13/18 8:36 A'' (Popel and Bojar, 2018, p. 63)
\par
``PBML 110 APRIL 2018 (Ioffe and Szegedy, 2015), which seems to be important for the scaling, while Transformer uses layer normalization (Lei Ba et al., 2016).30 Also, Transformer uses Adam together with an inverse-square-root learning-rate decay, while most ImageNet papers use SGD with momentum and piecewise-constant learning-rate decay.'' (Popel and Bojar, 2018, p. 64)
\par
``Jastrzebski et al. (2017) shows that ``the invariance under simultaneous rescaling of learning rate and batch size breaks down if the learning rate gets too large or the batch size gets too small''. A similar observation was reported e.g. by Bottou et al. (2016). Thus our initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for stable training in our experiments even when we scale from a single GPU to 8 GPUs. Considering this initial hypothesis, we were surprised that we were able to achieve so good Time Till Score with 8 GPUs (more than 8 times smaller relative to a single GPU, as reported in Table 6).'' (Popel and Bojar, 2018, p. 64)
\par
``In deed, many researchers (e.g. Goyal et al., 2017) suggest to use a warmup when scaling to more GPUs in order to prevent divergence. Transformer uses learning rate warmup by default even for single-GPU training (cf. Section 4.6), but it makes sense to use more warmup training examples in multi-GPU setting.'' (Popel and Bojar, 2018, p. 65)
\par
``Tips on Learning Rate and Warmup Steps on Multiple GPUs \textbullet{} Keep the learning\_rate parameter at its optimal value found in single-GPU experiments. \textbullet{} You can try decreasing the warmup steps, but less than linearly and you should not expect to improve the final BLEU this way.'' (Popel and Bojar, 2018, p. 65)
\par
``For example, Smith et al. (2017) suggest to increase the effective batch size (and number of GPUs) during training, instead of decaying the learning rate.'' (Popel and Bojar, 2018, p. 66)
\par
``Among other practical observations, we've seen that for the Transformer model, larger batch sizes lead not only to faster training but more importantly better translation quality. Given at least a day and a 11GB GPU for training, the larger setup (BIG) should be always preferred. The Transformer model and its implementation in Tensor2Tensor is also best fit for ``intense training'': using as many GPUs as possible and running experiments one after another should be preferred over running several single-GPU experiments concurrently.'' (Popel and Bojar, 2018, p. 68)}
}

@article{poppeSensitivityVPINChoice2016,
  title = {The Sensitivity of Vpin to the Choice of Trade Classification Algorithm},
  author = {P{\"o}ppe, Thomas and Moos, Sebastian and Schiereck, Dirk},
  year = {2016},
  journal = {Journal of Banking \& Finance},
  volume = {73},
  doi = {10.1016/j.jbankfin.2016.08.006}
}

@article{Porter_1992,
  title = {The Probability of a Trade at the Ask: An Examination of Interday and Intraday Behavior},
  author = {Porter, David and Porter, David C.},
  year = {1992},
  journal = {Journal of Financial and Quantitative Analysis},
  doi = {10.2307/2331368},
  mag_id = {2116311701},
  pmcid = {null},
  pmid = {null}
}

@article{Potters_2003,
  title = {More Statistical Properties of Order Books and Price Impact},
  author = {Potters, Marc and Bouchaud, Jean-Philippe and Bouchaud, Jean-Philippe and Bouchaud, Jean-Philippe},
  year = {2003},
  journal = {Physica A-statistical Mechanics and Its Applications},
  doi = {10.1016/s0378-4371(02)01896-4},
  mag_id = {3122247321},
  pmcid = {null},
  pmid = {null}
}

@misc{pressImprovingTransformerModels2020,
  title = {Improving Transformer Models by Reordering Their Sublayers},
  author = {Press, Ofir and Smith, Noah A. and Levy, Omer},
  year = {2020},
  number = {arXiv:1911.03864},
  eprint = {1911.03864},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: To appear at ACL 2020}
}

@inproceedings{prokhorenkovaCatBoostUnbiasedBoosting2018,
  title = {{{CatBoost}}: Unbiased Boosting with Categorical Features},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  year = {2018},
  series = {{{NeurIPS}} 2018},
  volume = {32},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY}}
}

@article{provostTreeInductionProbabilityBased,
  title = {Tree Induction for Probability-Based Ranking},
  author = {Provost, Foster}
}

@inproceedings{puigMultidimensionalShrinkagethresholdingOperator2009,
  title = {A Multidimensional Shrinkage-Thresholding Operator},
  booktitle = {2009 {{IEEE}}/{{SP}} 15th {{Workshop}} on {{Statistical Signal Processing}}},
  author = {Puig, Arnau Tibau and Wiesel, Ami and Hero, Alfred O.},
  year = {2009},
  publisher = {{IEEE}},
  address = {{Cardiff, United Kingdom}},
  doi = {10.1109/SSP.2009.5278625}
}

@misc{pulugundlaAttentionbasedNeuralBeamforming2021,
  title = {Attention-Based Neural Beamforming Layers for Multi-Channel Speech Recognition},
  author = {Pulugundla, Bhargav and Gao, Yang and King, Brian and Keskin, Gokce and Mallidi, Harish and Wu, Minhua and Droppo, Jasha and Maas, Roland},
  year = {2021},
  number = {arXiv:2105.05920},
  eprint = {2105.05920},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@misc{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Final version as published in JMLR}
}

@inproceedings{rahmanIntegratingMultimodalInformation2020,
  title = {Integrating {{Multimodal Information}} in {{Large Pretrained Transformers}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Rahman, Wasifur and Hasan, Md Kamrul and Lee, Sangwu and Bagher Zadeh, AmirAli and Mao, Chengfeng and Morency, Louis-Philippe and Hoque, Ehsan},
  year = {2020},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.214}
}

@article{raschkaIntroductionLatestTechniques2021,
  title = {An Introduction to the Latest Techniques},
  author = {Raschka, Sebastian},
  year = {2021}
}

@book{raschkaMachineLearningPyTorch2022,
  title = {Machine Learning with {{PyTorch}} and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python},
  author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid and Dzhulgakov, Dmytro},
  year = {2022},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}}
}

@misc{raschkaModelEvaluationModel2020,
  title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
  author = {Raschka, Sebastian},
  year = {2020},
  number = {arXiv:1811.12808},
  eprint = {1811.12808},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: v3 (Nov 2020): Fixes SD from pooled proportions in Sec 4.2 Fixes exact binomial p-value in Sec 4.4 by using max(B, C) instead of B in the sum. Fixes typo using wrong symbols in Looney's F-test's SSA in Sec 4.7}
}

@article{raschkaRecentTrendsTechnologies2021,
  title = {Recent Trends, Technologies, and Challenges},
  author = {Raschka, Sebastian},
  year = {2021}
}

@misc{RecipeTrainingNeural,
  title = {A Recipe for Training Neural Networks},
  howpublished = {http://karpathy.github.io/2019/04/25/recipe/}
}

@article{ribeiroEnsembleApproachBased2020,
  title = {Ensemble Approach Based on Bagging, Boosting and Stacking for Short-Term Prediction in Agribusiness Time Series},
  author = {Ribeiro, Matheus Henrique Dal Molin and {\noopsort{santos coelho}}{dos Santos Coelho}, Leandro},
  year = {2020},
  journal = {Applied Soft Computing},
  volume = {86},
  doi = {10.1016/j.asoc.2019.105837}
}

@article{ronenMachineLearningTrade2022,
  title = {Machine Learning and Trade Direction Classification: Insights from the Corporate Bond Market},
  author = {Ronen, Tavy and Fedenia, Mark A. and Nam, Seunghan},
  year = {2022},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.4213313},
  note = {\section{Annotations\\
(04/11/2022, 07:56:25)}

\par
``As trade initiation information is generally not provided in most intraday transaction databases, trade direction is often inferred from local price and/or quote behavior,'' (Ronen et al., 2022, p. 2)
\par
``The efficacy of these rules has been hotly debated, and their relative accuracy in different markets has been studied extensively.'' (Ronen et al., 2022, p. 2)
\par
``Our paper contributes to the market microstructure literature by examining the applicability of machine learning methods to better understand and improve trade direction classification. In particular, we introduce a machine learning model that outperforms traditional classifiers. Moreover, we illustrate the importance of optimizing the feature set in correctly classifying trade direction and provide new insights on the efficacy of trading rules in different market conditions.'' (Ronen et al., 2022, p. 3)
\par
``but also allows us to discover market characteristics that may affect our understanding of existing trade classification rules in general which may extend to other markets'' (Ronen et al., 2022, p. 4)
\par
``We find that machine learning models offer trade classification accuracy that at least matches and often exceeds the accuracy of the most commonly used models in the bond and stock markets.'' (Ronen et al., 2022, p. 4)
\par
``The Random Forest (RF) algorithm is found to be superior to a number of other machine learning choices as well as to traditional classifiers. The combination of additional features and a more flexible form of the classification function improves classification accuracy. For example, improvements over the tick rule (TR) are roughly 8.3\%.'' (Ronen et al., 2022, p. 4)
\par
``The accuracy of trade direction predictors depends on the trading and information environment in the market at the time of the trade. Consistent with Ellis et al. (2000), Finucane (2000), Chakrabarty et al. (2007), and Chakrabarty et al. (2012) who find that trade direction classifier accuracy is generally lower for larger trades in the equity markets, we find that the accuracy for smaller trades seem to be higher than for larger ones.'' (Ronen et al., 2022, p. 4)
\par
``Further, as bond liquidity increases, the gap between RF and TR accuracy widens'' (Ronen et al., 2022, p. 4)
\par
``Using the inferences we glean from the bond market regarding model and feature choice, we examine the efficacy of an equity-trained machine learning model in our TotalView-ITCH sample of Nasdaq stocks trading in 2013 and find that it is on a par with both TR and LR. When we supplement our feature set with quotes for comparability with classifiers that employ order information, RF outperforms TR by 3.3\% and LR by 3.6\%.'' (Ronen et al., 2022, p. 5)
\par
``Indeed, the positive correlation between the RF algorithm and trade frequency across all ranges renders it more suitable for today's faster markets, particularly in light of recent papers on new trade classification algorithms (Easley et al. (2016), Panayides et al. (2019), Chakrabarty et al. (2015), and Carrion and Kolay (2021)), which call in to question the ability of traditional trade direction classifiers to achieve high accuracy rates as the number of trades per unit time increases.'' (Ronen et al., 2022, p. 6)
\par
``BVC constitutes a robust alternative to traditional trade-level classification methods, particularly useful in low latency fragmented markets, where it is challenging to ensure that the exact sequencing of orders and trades are correctly reported'' (Ronen et al., 2022, p. 7)
\par
``Since BVC aggregates trading activity across volume, time or trade intervals (bars), it is not directly comparable to trade-based rules, and studies examining relative efficacy generally compute bulk versions of TR and LR.'' (Ronen et al., 2022, p. 8)
\par
``The classifiers described above rely entirely on the intuition of the researcher to define an estimator that captures the trading dynamics.'' (Ronen et al., 2022, p. 9)
\par
``By the same token, we admit that a myriad of models and combinations of models (ensemble models) are not explored. Our purpose here is not to identify the best prediction model possible. Instead, our goal is to see if an inductive approach can deliver insights into a classic problem in market microstructure'' (Ronen et al., 2022, p. 10)
\par
``Can machine learning inductively discover a very simple but useful trade classification rule, namely TR?'' (Ronen et al., 2022, p. 10)
\par
``Additionally, we can test whether machine learning improves the TR by adding the tick rule prediction as a feature to see if the resulting ensemble forecast accuracy is more accurat'' (Ronen et al., 2022, p. 10)
\par
``For this inquiry, we train a model in the corporate bond market and use the model to predict trade direction in the stock market.'' (Ronen et al., 2022, p. 10)
\par
``We examine all corporate bonds disseminated on the Enhanced Trade Reporting and Compliance Engine (TRACE) between July 1, 2002 and December 31, 2019.'' (Ronen et al., 2022, p. 11)
\par
``We employ five days of trade data from the TotalView-ITCH database provided by Nasdaq (December 9, 2013-December 13, 2013) for 1,984 NASDAQ stocks (1,493,298 non-cross trades). Our ITCH data include signed orders and trades, time-stamped to the nanosecond. For each trade, we capture ticker, price, size, timestamp, and the buy/sell trade flag.'' (Ronen et al., 2022, p. 13)
\par
``We consider several algorithms including Decision Trees, Discriminant Analysis, Logistic Regression, Support Vector Machines, k-Nearest Neighbor and a Neural Network (NN)'' (Ronen et al., 2022, p. 13)
\par
``Some 10The imbalance in the number of buys versus sells in our sample is likely not a concern, since we do not train the model on this sub-sample. 11Since we aggregate ITCH trades to the millisecond, approximately 14\% of ITCH trades are discarded in this matching procedure. An alternative matching procedure is considered, in which we compare each ITCH trade with the next or last available millisecond and retain an additional 13.5\% of trades, which yields stronger results regarding the dominance of our RF model, rendering our choice the conservative one. 13 Electronic copy available at: https://ssrn.com/abstract=421331'' (Ronen et al., 2022, p. 13)
\par
``of these algorithms were not well suited to the quantity of data we employed or they did not improve forecasts enough to be useful.'' (Ronen et al., 2022, p. 14)
\par
``We explicitly report results for RF, NN, and Logistic Regression. However, we focus our attention on RF as it is the dominant performer'' (Ronen et al., 2022, p. 14)
\par
``RF was implemented with 100 trees and increasing that number did little to improve out-of-sample accuracy. We increased the number of leaves in a tree and varied the pruning parameters. These changes did not affect the accuracy significantly. Additionally, we employed 10-Fold cross-validation to check if our 70/30 random partition is appropriate. In 10-Fold cross-validation, we divide the sample into ten equal sizes and ten iterations of Random Forest using 90/10 partition are performed.'' (Ronen et al., 2022, p. 14)
\par
``The algorithms we employ do not efficiently incorporate lagged information. Therefore we include relevant lagged data in each observation.'' (Ronen et al., 2022, p. 15)
\par
``We apply the classification algorithms to seven feature sets labeled \{1\} - \{7\} described below and shown in Table 4.'' (Ronen et al., 2022, p. 15)
\par
``Our first task is to invoke a number of popular classification algorithms and evaluate their efficacy in fifteen rolling windows of three years each. In each window we use 70\% of the data to train the classifier and then measure the out-of-bag prediction accuracy on the remaining 30\% of the observations.14'' (Ronen et al., 2022, p. 16)
\par
``We find that a Neural Network performs on par with most other decision tree models (not reported) except for the Random Forest. Logistic Regression performs about as well as TR (the most widely used classifier in markets without pre-trade transparency) but not as well as either a Neural Network or Random Forest. As mentioned earlier, the Random Forest approach (Breiman (2001) has an advantage over other algorithms we tried partly because it is an ensemble method.'' (Ronen et al., 2022, p. 16)
\par
``Across all random forest models we consider, adding more inclusive feature sets improve classification accuracy'' (Ronen et al., 2022, p. 16)
\par
``Choices regarding the rolling window periods and percentages of training and testing batches are somewhat ad-hoc by nature. Our results are robust to all other choices made regarding the testing-to-training ratio (85\%/15\%, and 60\%/40\%), as well as to fixing the ratios to the first and last data points (as opposed to using random batches that maintain these ratios). In addition, we consider one and two year rolling windows and obtain similar results.'' (Ronen et al., 2022, p. 16)
\par
``On average, RF\{4\} (RF\{5\}) accuracy level is 81.1\% (80.6\%) compared with accuracy rates of 77.1\%, 77.2\%, and 78.1\% for RF\{1\}, RF\{2\}, and RF\{3\}, respectively.'' (Ronen et al., 2022, p. 17)
\par
``We now examine how well the models shown in Table 5 hold up out-of-sample'' (Ronen et al., 2022, p. 17)
\par
``Compared to out-of-bag accuracy levels shown in Table 5, the out of sample prediction rates are mildly lower, although the ordinal properties across model accuracy rates remains similar. RF\{4\} still outperforms NN\{4\}, LG\{4\}, and TR. Since RF\{4\} and RF\{5\} are effectively equally accurate, and since RF\{5\} is more universally applicable in both the bond and stock markets, we focus on RF\{5\} and de-emphasize RF\{4\} going forward.'' (Ronen et al., 2022, p. 17)
\par
``Table 7 reports the incremental out-of-sample prediction accuracy provided by each of the feature sets described in Table 4.'' (Ronen et al., 2022, p. 18)
\par
``We graph the change in RF\{5\} and TR accuracy as a function of the number of transactions per day, for the entire sample. The figure shows improved accuracy rates for both models as trading activity increases, with RF\{5\} uniformly outperforming TR across all trading frequency ranges, even in the extreme case of one trade per day.'' (Ronen et al., 2022, p. 19)
\par
``As the time elapsed since the last trade decreases, accuracy rates for model RF\{5\} and TR improve. Prediction rates for RF\{5\} (TR) range from 71.57\% (68.05\%) in decile 1 to 81.43\% (72.33\%) in decile 10.'' (Ronen et al., 2022, p. 19)
\par
``Figure 2a shows that trade direction accuracy is lowest for both RF\{5\} and TR in the morning hours after long periods of low or no trading information. Conversely, trade accuracy levels peak for both RF\{5\} and TR in the middle of the trading day and plummet at 6PM.'' (Ronen et al., 2022, p. 20)
\par
``Bulk RF\{5\}.25 Table 11 reports that across all bars (volume, time, and trade), Bulk RF\{5\} outperforms Bulk TR, which in turn outperforms the BVC.26'' (Ronen et al., 2022, p. 25)
\par
``In previous sections we found that in the bond market RF\{5\} and its bulk counterpart outperform the TR and BVC'' (Ronen et al., 2022, p. 28)
\par
``We train RF\{5\} and RF\{7\} over the first three days (using 842,494 observations) in the sample, test on the remaining days (434,910 transactions), and compute the variables required by RF\{5\} and RF\{7\}.'' (Ronen et al., 2022, p. 29)
\par
``The corresponding accuracy rates for RF\{5\}, LR, and TR are 74.6\%, 74.0\%, and 74.3\%, respectively.31'' (Ronen et al., 2022, p. 30)
\par
``The combined results from our study, which spans a large range of trading frequencies, including high latency trades in the corporate bond markets (when interpreted alongside those in earlier papers) reveal that most classifiers' accuracy is maximized when the number of daily trades is highest (with several local maxima in the equity market), when trade is fast, and when trade sizes are smaller.'' (Ronen et al., 2022, p. 31)
\par
``Our finding that trade frequency is positively related to RF\{7\} accuracy across all ranges is notable in light of recent literature on new trade classification algorithms (Easley et al. (2016), Panayides et al. (2019), Chakrabarty et al. (2015), and Carrion and Kolay (2021)), which generally suggest that as markets become increasingly faster (more trades per unit time), existing algorithms perform worse, exacerbating the need for new, more effective ones, that are more compatible with these faster markets. Thus, one clear advantage of the machine learning algorithm we propose is that it performs better as the number of trades increases'' (Ronen et al., 2022, p. 31)
\par
``Overall, the results in Table 13 as well as those in Section 4 reveal that RF\{5\} and RF\{7\} outperform LR and TR across all samples and for different types of trades'' (Ronen et al., 2022, p. 32)
\par
``As an alternative to the ITCH trained RF\{7\}, we also test the ability of the TRACEtrained RF\{5\} to classify equity market trades and find that its accuracy in the ITCH data is an unimpressive 61.2\%, trailing that of TR (71.7\%). While it is disappointing that we could not train the model in one market and predict well in another, we harvested useful insights from the TRACE dataset regarding the relative efficacy of different machine learning and traditional trade direction classifiers. The TRACE trained models led us to RF\{5\} and ultimately RF\{7\} in the equity market. Despite the lackluster transfer efficacy of cross-market trained models, we can report that machine learning does improve accuracy when trained on in-market data.'' (Ronen et al., 2022, p. 32)
\par
``One caveat is that machine learning and other data driven methods achieve limited success with incomplete data. Since equity markets are fragmented, even if researchers obtain full sets of data, the inability to ensure correct sequencing of trades due to fragmentation and high frequency trading in the current equity markets presents problems for constructing machine learning models.'' (Ronen et al., 2022, p. 32)
\par
``We fit models to a variety of algorithms and determined that a Random Forest model is the superior machine learning algorithm for this problem.'' (Ronen et al., 2022, p. 33)
\par
``Looking at the specific features that improve forecasting accuracy we found the biggest improvements came from including the Tick Rule forecast and a variety of rolling price statistics. These features were important in both the stock and bond markets.'' (Ronen et al., 2022, p. 33)
\par
``Accuracy for smaller trades is higher than for larger trades. Our model is also sensitive to information as it performs best when price uncertainty is lower.'' (Ronen et al., 2022, p. 33)
\par
``We find that we cannot train a model only on the data used to compute existing classification rules without human intervention. At least for the algorithms we employ, it appears that machine learning must be paired with intuition provided by the financial economist to improve the accuracy of traditional estimation methods.'' (Ronen et al., 2022, p. 34)
\par
``Future work might include many other features that we did not investigate. Although we employed a number of machine learning algorithms for our models, methods that handle the time-series component of panel data in a more sophisticated way may yield even better results'' (Ronen et al., 2022, p. 34)}
}

@article{rosenthalModelingTradeDirection2012,
  title = {Modeling {{Trade Direction}}},
  author = {Rosenthal, D. W. R.},
  year = {2012},
  journal = {Journal of Financial Econometrics},
  volume = {10},
  number = {2},
  doi = {10.1093/jjfinec/nbr014},
  note = {\section{Annotations\\
(08/10/2022, 13:07:12)}

\par
``Trade classification is used in many different areas and is thus important to many researchers. For example, some event studies seek to determine the balance of buying and selling in response to a release of information or a governmental action.'' (Rosenthal, 2012, p. 1)
\par
``A 1\%\textendash 2\% accuracy improvement would increase estimates of price impact by 2\%\textendash 4\% and result in more careful and inexpensive trading of illiquid orders. Given that a large investment bank may trade \$2 billion per day, a cost savings of 0.01\% would be worth \$100 million annually to one such bank.'' (Rosenthal, 2012, p. 2)
\par
``A few methods exist for classifying trades, most comparing trade prices to prevailing price quotes.'' (Rosenthal, 2012, p. 2)
\par
``A further complication is the delays between publishing times (i.e. timestamps) of trades and quotes; however, classification methods without rigorous delay assumptions are often compared.'' (Rosenthal, 2012, p. 2)
\par
``To improve classification accuracy, I incorporate different methods into a model for the likelihood a trade was buyer-initiated. I also allow for joint estimation of a (latent) delay model. Modeling trade classifications is a new approach and one of the unique contributions of this work.'' (Rosenthal, 2012, p. 2)
\par
``Trade classification infers which trade participant initiated a trade by being the aggressor, consistent with Odders-White (2000) defining the laterarriving order as the trade initiator. Three approaches to trade classification dominate the literature: tick tests, midpoint tests, and bid/ask tests. Finucane (2000) recommends a tick test: comparing a trade price to the previous (differing) trade price for that stock. A lower previous trade price is taken as evidence the current trade was buy-initiated. Lee and Ready (1991) suggested a midpoint test: comparing a trade price to the lagged midpoint (i.e. average of best bid and ask quote). Trades at prices above (below) the midpoint are classified as buy- (sell-) initiated. Trades at the midpoint are resolved with a tick test. Ellis et al. (2000) suggested a bid/ask test for Nasdaq stocks; Peterson and Sirri (2003) then suggested it for NYSE stocks. Trades at the lagged ask (bid) are classified as buy- (sell-) initiated; other trades are resolved with a tick test. Trades are published with delay relative to quotes. Therefore, midpoint and bid/ask methods require delay assumptions. For midpoint methods, Lee and Ready (1991) use a delay of five seconds; Vergote (2005) suggests'' (Rosenthal, 2012, p. 3)
\par
``two seconds; and, Henker and Wang (2006) suggest a one-second delay. For bid/ask methods, all previous analyses used unlagged quotes.'' (Rosenthal, 2012, p. 4)
\par
``Many studies note that trades are published with non-ignorable delays. Lee and Ready (1991) first suggested a five-second delay (now commonly used) for 1988 data, two seconds for 1987 data, and ``a different delay . . . for other time periods''. Ellis et al. (2000) note (Section IV.C) that quotes are updated almost immediately while trades are published with delay2. Therefore, determining the quote prevailing at trade time requires finding quotes preceding the trade by some (unknown) delay'' (Rosenthal, 2012, p. 4)
\par
``3.3. Partial Likelihood. The classification model is formally valid when formulated as a partial likelihood as in Cox (1975) and Wong (1986). Since we are classifying a sequence of trades and conditioning on Ft, t is not random. The randomness in the (conditional) classification model is due only to (i) the unknown amount of time to look backwards for a quote; and, (ii) the unknown trade classification. Were this not so, we would need to condition on the likelihood of each trade happening at its observed time. If ti is the i-th trade time and Gi-1 is a sigma-field encapsulating trade classifications 1, . . . , i - 1, the full likelihood ratio can be decomposed as: (5) L(all data) = n {$\prod$} i=1 L(Bti |Fti , Gi-1) \texttimes{} n {$\prod$} i=1 L(Fti , Gi-1|Fti-1 , Gi-1). 7Error assumptions for GLMs are detailed in McCullagh and Nelder (1989)'' (Rosenthal, 2012, p. 8)
\par
``MODELING TRADE DIRECTION 9 For inference we only use the first factor, making this a partial likelihood. We assume Bti is conditionally independent of Gi-1 given Fti, yielding L(Bti |Fti , Gi-1) = L(Bti |Fti ). 3.4. Model Statement. Thus we can now state the (conditional) model: P (Bjt = Buy|Ft, ck, dk`; \texttheta o, {$\kappa$}o) = {$\pi$}jt; {$\pi$}jt = logit({$\eta$}jt); and, {$\eta$}jt = {$\beta$}0 ï¸¸ï¸·ï¸·ï¸¸ bias =0? + {$\beta$}o1g(pjt, \textasciicircum{} mjt) ï¸¸ ï¸·ï¸· ï¸¸ midpoint metric + {$\beta$}o2g(pjt, p{${'}$} jt-) ï¸¸ ï¸·ï¸· ï¸¸ tick metric + {$\beta$}o3J (pjt, b \textasciicircum jt, \textasciicircum{} ajt; {$\tau$} ) ï¸¸ ï¸·ï¸· ï¸¸ bid/ask metric + {$\beta$}o4g(pjt-, \textasciicircum{} mjt-) ï¸¸ ï¸·ï¸· ï¸¸ lag-1 midpoint metric + {$\beta$}o5g(pjt-, p{${'}$} jt-) ï¸¸ ï¸·ï¸· ï¸¸ lag-1 tick metric + {$\beta$}o6J (pjt-, \textasciicircum bjt-, \textasciicircum{} ajt-; {$\tau$} ) ï¸¸ ï¸·ï¸· ï¸¸ lag-1 bid/ask metric + ck ï¸¸ï¸·ï¸·ï¸¸ overall effect + dk` ï¸¸ï¸·ï¸·ï¸¸ withinsector effect (6) where j indexes stocks, k indexes time bins; ` indexes sectors; and, o indexes primary exchanges (e.g. NYSE, Nasdaq). A stock j thus implies a sector ` and primary exchange o. The parameters of fY are estimated jointly with model coefficients. The random effects are assumed to be ck iid {$\sim$} N (0, {$\sigma$}2 c) for all bins k and dk` iid {$\sim$} N (0, {$\sigma$}2 d) for all bins k and sectors `. Further, ck and dk` are assumed to be independent of the sigma-field Ft.'' (Rosenthal, 2012, p. 9)}
}

@article{rossiMachineLearning,
  title = {Predicting Stock Market Returns with Machine Learning},
  author = {Rossi, Alberto},
  year = {2018},
  journal = {Unpublished Working Paper},
  note = {\section{Annotations\\
(12/07/2022, 16:25:33)}

\par
``We employ a semi-parametric method known as Boosted Regression Trees (BRT) to forecast stock returns and volatility at the monthly frequency'' (Rossi, 2018, p. 0)
\par
``as Boosted Regression Trees (BRT)'' (Rossi, 2018, p. 0)
\par
``g information without imposing strong parametric assumptions such as linearity or monotonicity. Our forecasts outperform those generated by benchmark models in terms of both mean squared error and directional accuracy. They also generate profitable portfolio allocations for mean-variance investors even when market frictions are accounted for.'' (Rossi, 2018, p. 28)}
}

@article{Rosu_2009,
  title = {A Dynamic Model of the Limit Order Book},
  author = {Rosu, Ioanid},
  year = {2009},
  journal = {Review of Financial Studies},
  doi = {10.1093/rfs/hhp011},
  mag_id = {3125906678},
  pmcid = {null},
  pmid = {null}
}

@book{rothmanTransformersNaturalLanguage2021,
  title = {Transformers for Natural Language Processing},
  author = {Rothman, Denis},
  year = {2021},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  note = {\section{Annotations\\
(09/01/2023, 08:12:24)}

\par
``model = 512'' (Rothman, 2021, p. 11)
\par
``For each word embedding vector, we need to find a way to provide information to i in the range(0,512) dimensions of the word embedding vector of black and brown.'' (Rothman, 2021, p. 11)
\par
``unit sphere to represent positional encoding with sine and cosine values that will thus remain small but very useful'' (Rothman, 2021, p. 11)
\par
``The authors of the Transformer found a simple way by merely adding the positional encoding vector to the word embedding vector:'' (Rothman, 2021, p. 15)
\par
``f we go back and take the word embedding of black, for example, and name it y1=black, we are ready to add it to the positional vector pe(2) we obtained with positional encoding functions. We will obtain the positional encoding pc(black) of the input word black: pc(black)=y1+pe(2) The solution is straightforward. However, if we apply it as shown, we might lose the information of the word embedding, which will be minimized by the positional encoding vector.'' (Rothman, 2021, p. 15)
\par
``One of the many possibilities is to add an arbitrary value to y1, the word embedding of black: y1*math.sqrt(dmodel) We can now add the positional vector to the embedding vector of the word black, both of which are the same size (512):'' (Rothman, 2021, p. 16)}
}

@misc{rozemberczkiShapleyValueMachine2022,
  title = {The Shapley Value in Machine Learning},
  author = {Rozemberczki, Benedek and Watson, Lauren and Bayer, P{\'e}ter and Yang, Hao-Tsung and Kiss, Oliv{\'e}r and Nilsson, Sebastian and Sarkar, Rik},
  year = {2022},
  number = {arXiv:2202.05594},
  eprint = {2202.05594},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: https://github.com/AstraZeneca/awesome-shapley-value}
}

@misc{rubachevRevisitingPretrainingObjectives2022,
  title = {Revisiting Pretraining Objectives for Tabular Deep Learning},
  author = {Rubachev, Ivan and Alekberov, Artem and Gorishniy, Yury and Babenko, Artem},
  year = {2022},
  number = {arXiv:2207.03208},
  publisher = {{arXiv}},
  note = {\section{Annotations\\
(23/01/2023, 14:54:48)}

\par
``Pretraining in deep learning. For domains with structured data, like natural images or texts, pretraining is currently an established stage in the typical pipelines, which leads to higher general performance and better model robustness [18, 10].'' (Rubachev et al., 2022, p. 2)
\par
``Pretraining for the tabular domain. Numerous pretraining methods were recently proposed in several recent works on tabular DL [2, 42, 9, 39, 37, 27]. However, most of these works do not focus on the pretraining objective per se and typically introduce it as a component of their tabular DL pipeline. Moreover, the experimental setup varies significantly between methods. Therefore,'' (Rubachev et al., 2022, p. 2)
\par
``it is difficult to extract conclusive evidence about pretraining effectiveness from the literature. To the best of our knowledge, there is only one systematic study on the tabular pretraining [4], but its experimental evaluation is performed only with the simplest MLP models, and we found that the superiority of the contrastive pretraining, reported in [4], does not hold for tuned models in our setup, where contrastive objective is comparable to the simpler self-prediction objectives.'' (Rubachev et al., 2022, p. 3)
\par
``We use MLP as a simple deep baseline to compare and ablate the methods. Our implementation of MLP exactly follows [16], the model is regularized by dropout and weight decay. As more advanced deep models, we evaluate MLP equipped with numerical feature embeddings, specifically, target-aware piecewise linear encoding (MLP-T-LR) and embeddings with periodic activations (MLPPLR) from [14]. These models represent the current state-of-the-art solution for tabular DL [14], and are of interest as most prior work on pretraining in tabular DL focus on pretraining with the simplest'' (Rubachev et al., 2022, p. 3)
\par
``MLP models in evaluation.'' (Rubachev et al., 2022, p. 4)
\par
``retraining. Pretraining is always performed directly on the target dataset and does not exploit additional data. The learning process thus comprises two stages. On the first stage, the model parameters are optimized w.r.t. the pretraining objective. On the second stage, the model is initialized with the pretrained weights and finetuned on the downstream classification or regression task. We focus on the fully-supervised setup, i.e., assume that target labels are provided for all dataset objects. Typically, pretraining stage involves the input corruption: for instance, to generate positive pairs in contrastive-like objectives or to corrupt the input for reconstruction in self-prediction based objectives. We use random feature resampling as a proven simple baseline for input corruption in tabular data [4, 42].'' (Rubachev et al., 2022, p. 4)
\par
``Pretraining is beneficial for the state-of-the-art models. Models with the numerical feature embeddings also benefit from pretraining with either reconstruction or mask prediction demonstrating the top performance on the downstream task. However, the improvement is typically less noticeable compared to the vanilla MLPs.'' (Rubachev et al., 2022, p. 4)
\par
``There is no universal solution between self-prediction objectives. We observe that for some datasets the reconstruction objective outperforms the mask prediction (OT, WE, CO, MI), while on others the mask prediction is better (GE, CH, HI, AD). We also note that the mask prediction objective sometimes leads to unexpected performance drops for models with numerical embeddings (WE, MI), we do not observe significant performance drops for the reconstruction objective.'' (Rubachev et al., 2022, p. 4)
\par
``The main takeaway: simple pretraining strategies based on self-prediction lead to significant improvements in the downstream accuracy compared to the tuned supervised baselines learned from scratch across different tabular DL models and datasets. In practice, we recommend trying both'' (Rubachev et al., 2022, p. 4)
\par
``reconstruction and mask prediction as tabular pretraining baselines, as either one might show superior performance depending on the dataset being used.'' (Rubachev et al., 2022, p. 5)
\par
``Here we demonstrate that early stopping the pretraining by the value of the pretraining objective on the hold-out validation set is comparable to the early stopping by the downstream metric on the hold-out validation set after finetuning. See Table 12 for the results. This is an important practical observation, as computing pretraining objective is much faster than the full finetuning of the model, especially on large scale datasets.'' (Rubachev et al., 2022, p. 15)}
}

@article{rubinInferenceMissingData1976,
  title = {Inference and Missing Data},
  author = {Rubin, Donald B.},
  year = {1976},
  journal = {Biometrika},
  volume = {63},
  number = {3},
  doi = {10.1093/biomet/63.3.581},
  note = {\section{Annotations\\
(28/11/2022, 13:03:56)}

\par
``The missing data are mxissing at random if for each possible value of the parameter ?i, the conditional probability of the observed pattern of missing data, given the missing data and the value of the observed data, is the samne for all possible values of the missing data.'' (Rubin, 1976, p. 582)
\par
``The observed data are observed at random if for each possible value of the missing data and the parameter qS, the conditional probability of the observed pattern of missing data, given the massing data and the observed data, is the same for all possible values of the observed data. The parameter ?i is distinct from 0 if there are no a priori ties, via parameter space restrictions or prior distributions, between ?i and 0'' (Rubin, 1976, p. 582)
\par
``Definition 1. The missing data are missing at random if for each value of q, g4 Aiiji) takes the same value for all U(o). Definition 2. The observed data are observed at random if for each value of . and U(0)gq5(nu) takes the same value for all U(D. This content downloaded from 46.223.:ffff:ffff:ffff:ffff on Thu, 01 Jan 1976 12:34:56 UTC All use subject to https://about.jstor.org/term'' (Rubin, 1976, p. 584)
\par
``Inference and missing data 585 Definition 3. The parameter 0 is distinct from 0 if their joint parameter space factorizes into a 0-space and a 0-space, and when prior distributions are specified for 0 and 0, if these are independent.'' (Rubin, 1976, p. 585)}
}

@article{rubinsteinRelationBinomialTrinomial2000,
  title = {On the Relation between Binomial and Trinomial Option Pricing Models},
  author = {Rubinstein, Mark},
  year = {2000},
  journal = {The Journal of Derivatives},
  volume = {8},
  number = {2},
  doi = {10.3905/jod.2000.319149}
}

@article{sametFoundationsMultidimensionalMetric,
  title = {Foundations of Multidimensional and Metric Data Structures},
  author = {Samet, Hanan}
}

@book{sandersSequentialParallelAlgorithms2019,
  title = {Sequential and Parallel Algorithms and Data Structures: The Basic Toolbox},
  author = {Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and Dementiev, Roman},
  year = {2019},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-25209-0}
}

@article{savickasInferringDirectionOption2003,
  title = {On Inferring the Direction of Option Trades},
  author = {Savickas, Robert and Wilson, Arthur J},
  year = {2003},
  journal = {Journal of Financial and Quantitative Analysis},
  volume = {38},
  doi = {10.2307/4126747},
  note = {\section{Annotations\\
(30/08/2022, 12:07:28)}

\par
``To sign option trades as buys and sells, researchers often employ stock trade classification rules including the quote, the Lee and Ready (1991), the Ellis, Michaely, and O'Hara (2000), and the tick methods'' (Savickas and Wilson, 2003, p. 881)
\par
``We find that the components of index option complex trades not executed on the Retail Automated Execution System are misclassified almost 50\% of the time by any method. The elimination of these trades (15\% of the sample) results in a success rate of over 87\% for the quote rule.'' (Savickas and Wilson, 2003, p. 881)
\par
``The correct classification rate varies inversely with a rule's reliance on the past transaction price: 59\%, 78\%, 80\%, and 83\% for the tick, EMO, LR, and quote rules, respectively'' (Savickas and Wilson, 2003, p. 882)
\par
``the four rules exhibit lower (up to 24 percentage points) classification precision with options data than with stock data'' (Savickas and Wilson, 2003, p. 882)
\par
``We use two datasets covering the period July 3, 1995December 31, 1995; both are provided by CBOE.'' (Savickas and Wilson, 2003, p. 882)
\par
``Each record in the matched trade dataset reports the following information: date and execution time of a trade, trade size, price, the option's parameters (put/call, expiration month and year, and strike price), the underlying security,'' (Savickas and Wilson, 2003, p. 883)
\par
``the reporting trader's type code, and a dummy variable that indicates whether the reporting trader bought or sold the option. Thus, there is one record for each trading party for all trades on CBOE.'' (Savickas and Wilson, 2003, p. 884)
\par
``We group all records in the matched trade dataset into pairs, one pair per trade. The two records in each pair have the same trade date, underlying security, option's parameters, trade size, and trade price. The buy/sell indicators are different for the two records in each pair (a buy and a sell).'' (Savickas and Wilson, 2003, p. 884)
\par
``onsistent with previous literature, we assume that all MC and BC trades are initiated by the customer, all MB trades are initiated by the broker, all MF and BF trades are initiated by the firm, and all MN and BN trades are initiated by the non-member.4'' (Savickas and Wilson, 2003, p. 885)
\par
``All trades and quotes for which the underlying asset price is reported as zero and all cancelled trades are excluded from the sample.'' (Savickas and Wilson, 2003, p. 885)
\par
``According to the quote rule, a trade is buyer- (seller-) initiated if it occurs above (below) the midpoint of the bid-ask spread. Trades that occur exactly at the midspread cannot be classified. The tick rule, on the other hand, classifies any trade as a buy (sell) if its price is above (below) the most recent transaction price that is different from the price of the transaction being classified. The LR and EMO methods combine the quote and tick rules to various degrees. The LR approach uses the tick rule to classify only at-midspread trades and the quote rule for all the other trades. The EMO method uses the quote rule for at-quote trades and the tick rule for all the other trades.'' (Savickas and Wilson, 2003, p. 885)
\par
``Each of the four classification methods cannot classify some trades. The quote rule is unable to classify midspread trades. The tick rule cannot classify the'' (Savickas and Wilson, 2003, p. 885)
\par
``first trade of the day for a given security. If the second trade of the day takes place at the same price as the first one, both trades cannot be classified by the tick rule. The LR and EMO rules also lose some trades since they are combinations of the tick and the quote rules.'' (Savickas and Wilson, 2003, p. 886)
\par
``The degree of success varies inversely with a rule's reliance on past transaction prices: 82.78\% for the quote rule, 80.11\% for LR, 76.54\% for EMO, and 59.4\% for the tick method.'' (Savickas and Wilson, 2003, p. 886)
\par
``To address this issue, we form a common sample in which every trade can be classified by each of the four rules.'' (Savickas and Wilson, 2003, p. 887)
\par
``For each of the four classification rules, we use eight variables that may potentially explain observed performance. These variables are: the trade size,6 time from the previous trade, time from the previous quote, the moneyness ratio, trading days to maturity, the absolute value of the relative change in the underlying price, the location of trade relative to the bid-ask quotes, and whether a trade is executed on RAES.'' (Savickas and Wilson, 2003, p. 887)
\par
``trades inside the quotes are more likely to be misclassified.'' (Savickas and Wilson, 2003, p. 888)
\par
``On the one hand, we would expect that the greater (smaller) the transaction price relative to the midspread, the more likely that the transaction is a buy (sell) and occurs on an uptick (a downtick), implying higher classification success for outside-quote trades, especially for large trades in which the trade initiator is willing to pay a premium for the execution of his large order.'' (Savickas and Wilson, 2003, p. 888)
\par
``Trade size affects classification precision in two related ways. First, as found by EMO (2000), small trades are more likely to be executed at the quotes, implying an inverse relation between classification precision and trade size.'' (Savickas and Wilson, 2003, p. 888)
\par
``The longer the time from the previous trade, the less relevant is the information in the previous trade price, adversely affecting tick-based classifications.'' (Savickas and Wilson, 2003, p. 888)
\par
``First, as in the case of the past price, the older the quotes, the less relevant the information they convey, making quote-based classification less precise. Second, the older the quote, the greater underlying asset price change since that quote, which can enhance quote-based classification as described in the next paragraph.'' (Savickas and Wilson, 2003, p. 888)
\par
``Option moneyness has two indirect effects on classification precision. First, deep in-the-money options have deltas close to one (in absolute value) and, therefore, are more sensitive to the underlying asset price changes. Second, there is a positive relation between option moneyness and trade size'' (Savickas and Wilson, 2003, p. 889)
\par
``Time to maturity also has an indirect effect on classification precision. Trades in options with longer maturity tend to be smaller, resulting in negative correlation between the effects of maturity and of trade size.'' (Savickas and Wilson, 2003, p. 889)
\par
``Specifically, one of the most noticeable regularities is that smaller trades are classified more precisely. This is because these trades are more likely to be executed at quotes and are less prone to reversed-quote trading (partially due to the fact that many small trades are executed on RAES)'' (Savickas and Wilson, 2003, p. 889)
\par
``Thus, there seems to be a direct relation between using the quote information and successful option trade classification. This relation implies that the information conveyed by the past transaction prices is less relevant than information contained in quotes.'' (Savickas and Wilson, 2003, p. 891)
\par
``To determine the marginal impacts of individual variables, we estimate the following logit model, ProbabilityYi 1 1 ProbabilityYi 1 + 11 k1 k Xk i + i i 1N (1) where N is the total number of trades in the sample; Yi1 if a trade is misclassified and Yi 0otherwise;X1 is the size of the trade in contracts; X2 1ifthetrade occurs at the bid-ask spread midpoint and X2 0otherwise;X3 1ifthetrade occurs at the quotes and X3 0otherwise;X4 1 if the trade occurs outside the quotes and X4 0otherwise;X5 is the time from the previous trade in seconds; X6 is the time from the previous quote in seconds; X7 is the moneyness ratio ([underlying price]/[strike price] for calls and [strike price]/[underlying price] for puts); X8 is the absolute value of the relative underlying change since the last h t t p s : / / d o i . o r g / 1 0 . 2 3 0 7 / 4 1 2 6 7 4 7 P u b l i s h e d o n l i n e b y C a m b r i d g e U n i v e r s i t y P r e s'' (Savickas and Wilson, 2003, p. 892)
\par
``Savickas and Wilson 893 quotes; X9 is the time to expiration in trading days; X10 0 if it is a call option and X10 1 if it is a put; X11 1 if the trade is executed on RAES and X11 0 otherwise.'' (Savickas and Wilson, 2003, p. 893)
\par
``The most economically significant variables in all four regressions are the trade location relative to quotes (including the RAES dummy), the absolute value of the relative underlying price change, and the put/call dummy.'' (Savickas and Wilson, 2003, p. 893)
\par
``s Table 4 indicates, larger trades have a higher probability of being traded on reversed quotes, as do shorter-term options. When the underlying price change is controlled for, we observe a positive relation between option moneyness and the probability of an RQ trade.'' (Savickas and Wilson, 2003, p. 895)
\par
``hese trades are correctly classified by the quote rule and are misclassified by our ``true'' classification, which assumes that all MC trades are customer-initiated. This suggests that the overall misclassification rate for quotebased rules may in fact be substantially lower than that reported in Table 1. Another factor not included in the regressions above is related to corporate dividend capture.'' (Savickas and Wilson, 2003, p. 896)
\par
``The quote and the tick rules are the two extremes. The LR and EMO methods take their respective places on the continuum between the quote and the tick rules. The EMO approach uses the quote rule to a lesser extent than does the LR algorithm; therefore, the EMO method exhibits a lower degree of spread overestimation than does the LR method.'' (Savickas and Wilson, 2003, p. 897)
\par
``In other words, the lower limit on the classification rate by a variety of methods seems to be 50\%, rather than 0\%. Eliminating a small subset of trades that are classified correctly about half the time would produce improvements, an approach that we employ in Section VI.B.'' (Savickas and Wilson, 2003, p. 898)
\par
``Using the earliest available time stamp for each trade results in an increase in the quote rule's classification precision by about 2.5 percentage points.'' (Savickas and Wilson, 2003, p. 898)
\par
``Experimentation with time adjustments of one second to 10 minutes results in the deterioration of the quote rule's performance. Thus, although the reporting delays are also present in the options market, they are less uniform and are found to be uncorrelated to option or trade parameters.'' (Savickas and Wilson, 2003, p. 898)
\par
``Index option trading involves many complex trades because taking covered positions in index options is not as easy (or possible) as in equity options. Frequently, the only alternatives to naked positions in index options are complex options.'' (Savickas and Wilson, 2003, p. 899)
\par
``Therefore, one way to reduce the problem of complex trades is to exclude all index trades.'' (Savickas and Wilson, 2003, p. 899)
\par
``Eliminating all complex trades cuts the sample in half. To avoid this large sample reduction, we focus only on index complex trades, summarized in Table 6. To further preserve the sample, we do not eliminate index complex trades executed on RAES (the RAES dummy is available in BODB).'' (Savickas and Wilson, 2003, p. 899)
\par
``The main sources of misclassification by all rules are reversed-quote trades (buys at the bid and sells at the ask) and wrong-side trades (buys below the bid and sells above the ask), which occur more frequently in options than in stocks and are associated with large orders to trade deep in-the-money, near-maturity options.'' (Savickas and Wilson, 2003, p. 901)
\par
``e are able to isolate a category of trades that are highly misclassified (approximately 50\%\textendash 60\% of the time) by all methods.'' (Savickas and Wilson, 2003, p. 901)}
}

@article{schaferRecommenderSystemsEcommerce1999,
  title = {Recommender Systems in {{E-commerce}}},
  author = {Schafer, J Ben and Konstan, Joseph and Riedl, John},
  year = {1999},
  doi = {10.1145/336992.337035}
}

@article{schapireBoostingMarginNew1998,
  title = {Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods},
  author = {Schapire, Robert E. and Bartlett, Peter and Freund, Yoav and Lee, Wee Sun},
  year = {1998},
  journal = {The Annals of Statistics},
  volume = {26},
  number = {5},
  doi = {10.1214/aos/1024691352}
}

@article{schapireStrengthWeakLearnability1990,
  title = {The Strength of Weak Learnability},
  author = {Schapire, Robert E.},
  year = {1990},
  journal = {Machine Learning},
  volume = {5},
  number = {2},
  doi = {10.1007/BF00116037}
}

@article{Schmidt_2011,
  title = {Empirical Market Microstructure},
  author = {Schmidt, Anatoly B. and Schmidt, Anatoly B.},
  year = {2011},
  journal = {null},
  doi = {10.1002/9781118268094.ch6},
  mag_id = {1548335088},
  pmcid = {null},
  pmid = {null}
}

@article{shahriariTakingHumanOut2016,
  title = {Taking the Human out of the Loop: A Review of Bayesian Optimization},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {\noopsort{freitas}}{de Freitas}, Nando},
  year = {2016},
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  doi = {10.1109/JPROC.2015.2494218}
}

@article{shallueMeasuringEffectsData,
  title = {Measuring the {{Effects}} of {{Data Parallelism}} on {{Neural Network Training}}},
  author = {Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and {Sohl-Dickstein}, Jascha and Frostig, Roy and Dahl, George E}
}

@article{shaniEvaluatingRecommendationSystems,
  title = {Evaluating Recommendation Systems},
  author = {Shani, Guy and Gunawardana, Asela},
  doi = {10.1007/978-0-387-85820-3_8}
}

@inproceedings{shavittRegularizationLearningNetworks2018,
  title = {Regularization Learning Networks: Deep Learning for Tabular Datasets},
  booktitle = {32nd {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Shavitt, Ira and Segal, Eran},
  year = {2018},
  address = {{Montr\'eal}},
  note = {\section{Annotations\\
(05/11/2022, 08:06:33)}

\par
``e propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss.'' (Shavitt and Segal, 2018, p. 1)
\par
``n contrast, the relative contribution of the input features in the electronic health records example can vary greatly: Changing a single input such as the age of the patient can profoundly impact the life expectancy of the patient, while changes in other input features, such as the time that passed since the last test was taken, may have smaller effects'' (Shavitt and Segal, 2018, p. 1)
\par
``We hypothesized that this potentially large variability in the relative importance of different input features may partly explain the lower performance of DNNs on such tabular datasets [11]. One way to overcome this limitation could be to assign a different regularization coefficient to every weight, which might allow the network to accommodate the non-distributed representation and the variability in relative importance found in tabular datasets'' (Shavitt and Segal, 2018, p. 2)
\par
``Here, we present a new hyperparameter tuning technique, in which we optimize the regularization coefficients using a newly introduced loss function, which we term the Counterfactual Loss, orLCF . We term the networks that apply this technique Regularization Learning Networks (RLNs).'' (Shavitt and Segal, 2018, p. 2)
\par
``When running each model separately, GBTs achieve the best results on all of the tested traits, but it is only significant on 3 of them (Figure 2). DNNs achieve the worst results, with 15\% {$\pm$} 1\% less explained variance than GBTs on average.'' (Shavitt and Segal, 2018, p. 5)
\par
``Constructing an ensemble of models is a powerful technique for improving performance, especially for models which have high variance, like neural networks in our task.'' (Shavitt and Segal, 2018, p. 5)
\par
``Despite the improvement, DNN ensembles still achieve the worst results on all of the traits except for Gender and achieve results 9\% {$\pm$} 1\% lower than GBT ensembles (Figure 4).'' (Shavitt and Segal, 2018, p. 6)
\par
``Indeed, as shown in Figure 5, the best performance is obtained with an ensemble of RLN and GBT, which achieves the best results on all traits except Gender, and outperforms all other ensembles significantly on Age, BMI, and HDL cholesterol (Table 1)'' (Shavitt and Segal, 2018, p. 6)
\par
``Evaluating feature importance is difficult, especially in domains in which little is known such as the gut microbiome. One possibility is to examine the information it supplies'' (Shavitt and Segal, 2018, p. 8)
\par
``Another possibility is to evaluate its consistency across different instantiations of the model. We expect that a good feature importance technique will give similar importance distributions regardless of instantiation'' (Shavitt and Segal, 2018, p. 9)
\par
``We hypothesize that modular regularization could boost the performance of DNNs on such tabular datasets. We introduce the Counterfactual Loss, LCF , and Regularization Learning Networks (RLNs) which use the Counterfactual Loss to tune its regularization hyperparameters efficiently during learning together with the learning of the weights of the network.'' (Shavitt and Segal, 2018, p. 9)
\par
``We test our method on the task of predicting human traits from covariates and microbiome data and show that RLNs significantly and substantially improve the performance over classical DNNs, achieving an increased explained variance by a factor of 2.75 {$\pm$} 0.05 and comparable results with GBTs.'' (Shavitt and Segal, 2018, p. 9)}
}

@misc{shazeerAdafactorAdaptiveLearning2018,
  title = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author = {Shazeer, Noam and Stern, Mitchell},
  year = {2018},
  number = {arXiv:1804.04235},
  eprint = {1804.04235},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@misc{shazeerGLUVariantsImprove2020,
  title = {{{GLU}} Variants Improve Transformer},
  author = {Shazeer, Noam},
  year = {2020},
  number = {arXiv:2002.05202},
  eprint = {2002.05202},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(28/11/2022, 07:35:38)}

\par
``Gated Linear Units [Dauphin et al., 2016] consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function.'' (Shazeer, 2020, p. 1)
\par
``We test these variants in the feedforward sublayers of the Transformer [Vaswani et al., 2017] sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.'' (Shazeer, 2020, p. 1)
\par
``The FFN takes a vector x (the hidden representation at a particular position in the sequence) and passes it through two learned linear transformations, (represented by the matrices W1 and W2 and bias vectors b1 and b2).'' (Shazeer, 2020, p. 1)
\par
``[Dauphin et al., 2016] introduced Gated Linear Units (GLU), a neural network layer defined as the componentwise product of two linear transformations of the input, one of which is sigmoid-activated.'' (Shazeer, 2020, p. 1)
\par
``GEGLU(x, W, V, b, c) = GELU(xW + b) {$\otimes$} (xV + c)'' (Shazeer, 2020, p. 2)
\par
``In this paper, we propose additional variations on the Transformer FFN layer which use GLU or one of its variants in place of the first linear transformation and the activation function.'' (Shazeer, 2020, p. 2)
\par
``FFNGEGLU(x, W, V, W2) = (GELU(xW ) {$\otimes$} xV )W2'' (Shazeer, 2020, p. 2)
\par
``All of these layers have three weight matrices, as opposed to two for the original FFN. To keep the number of parameters and the amount of computation constant, we reduce the number of hidden units dff (the second dimension of W and V and the first dimension of W2) by a factor of 2 3 when comparing these layers to the original two-matrix version'' (Shazeer, 2020, p. 2)
\par
``In a transfer-learning setup, the new variants seem to produce better perplexities for the de-noising objective used in pre-training, as well as better results on many downstream language-understanding tasks. These architectures are simple to implement, and have no apparent computational drawbacks.'' (Shazeer, 2020, p. 3)
\par
``FFNGEGLU 73.96'' (Shazeer, 2020, p. 4)
\par
``FFNSwiGLU 74.56'' (Shazeer, 2020, p. 4)}
}

@article{shiQuantizedTrainingGradient,
  title = {Quantized Training of Gradient Boosting Decision Trees},
  author = {Shi, Yu and Ke, Guolin and Chen, Zhuoming and Zheng, Shuxin and Liu, Tie-Yan}
}

@article{shumwayDelistingBiasCRSP1997,
  title = {The Delisting Bias in {{CRSP}} Data},
  author = {Shumway, Tyler},
  year = {1997},
  journal = {The Journal of Finance},
  volume = {52},
  number = {1},
  doi = {10.1111/j.1540-6261.1997.tb03818.x}
}

@misc{shwartz-zivTabularDataDeep2021,
  title = {Tabular Data: Deep Learning Is Not All You Need},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = {2021},
  number = {arXiv:2106.03253},
  eprint = {2106.03253},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@inproceedings{smiejaProcessingMissingData2018,
  title = {Processing of Missing Data by Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{\'S}mieja, Marek and Struski, {\L}ukasz and Tabor, Jacek and Zieli{\'n}ski, Bartosz and Spurek, Przemys{\l}aw},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@misc{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  number = {arXiv:1706.03825},
  eprint = {1706.03825},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03825},
  archiveprefix = {arXiv},
  note = {Comment: 10 pages}
}

@misc{smithCyclicalLearningRates2017,
  title = {Cyclical Learning Rates for Training Neural Networks},
  author = {Smith, Leslie N.},
  year = {2017},
  number = {arXiv:1506.01186},
  eprint = {1506.01186},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(25/10/2022, 08:54:56)}

\par
``It is well known that too small a learning rate will make a training algorithm converge slowly while too large a learning rate will make the training algorithm diverge [2]'' (Smith, 2017, p. 1)
\par
``In addition, this cyclical learning rate (CLR) method practically eliminates the need to tune the learning rate yet achieve near optimal classification accuracy.'' (Smith, 2017, p. 1)
\par
``A methodology for setting the global learning rates for training neural networks that eliminates the need to perform numerous experiments to find the best values and schedule with essentially no additional computation.'' (Smith, 2017, p. 1)
\par
``allowing 1Hyper-parameters and architecture were obtained in April 2015 from caffe.berkeleyvision.org/gathered/examples/cifar10.html arXiv:1506.01186v6 [cs.CV] 4 Apr 201'' (Smith, 2017, p. 1)
\par
``the learning rate to rise and fall is beneficial overall even though it might temporarily harm the network's performance.'' (Smith, 2017, p. 2)
\par
``Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive learning rates'' (Smith, 2017, p. 2)
\par
``The essence of this learning rate policy comes from the observation that increasing the learning rate might have a short term negative effect and yet achieve a longer term beneficial effect.'' (Smith, 2017, p. 2)
\par
``This led to adopting a triangular window (linearly increasing then linearly decreasing), which is illustrated in Figure 2, because it is the simplest function that incorporates this idea. The rest of this paper refers to this as the triangular learning rate policy'' (Smith, 2017, p. 2)
\par
``An intuitive understanding of why CLR methods work comes from considering the loss function topology. Dauphin et al. [4] argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima'' (Smith, 2017, p. 2)
\par
``Saddle points have small gradients that slow the learning process. However, increasing the learning rate allows more rapid traversal of saddle point plateaus. A more practical reason as to why CLR works is that, by following the methods in Section 3.3, it is likely the optimum learning rate will be between the bounds and near optimal learning rates will be used throughout training.'' (Smith, 2017, p. 3)
\par
``he length of a cycle and the input parameter stepsize can be easily computed from the number of iterations in an epoch. An epoch is calculated by dividing the number of training images by the batchsize used.'' (Smith, 2017, p. 3)
\par
``There is a simple way to estimate reasonable minimum and maximum boundary values with one training run of the network for a few epochs. It is a ``LR range test''; run your model for several epochs while letting the learning rate increase linearly between low and high LR values. This test is enormously valuable whenever you are facing a new architecture or dataset.'' (Smith, 2017, p. 3)
\par
``The triangular learning rate policy provides a simple mechanism to do this. For example, in Caffe, set base lr to the minimum value and set max lr to the maximum value. Set both the stepsize and max iter to the same number of iterations. In this case, the learning rate will increase linearly from the minimum value to the maximum value during this short run. Next, plot the accuracy versus learning rate.'' (Smith, 2017, p. 3)
\par
``A short run of only a few epochs where the learning rate linearly increases is sufficient to estimate boundary learning rates for the CLR policies. Then a policy where the learning rate cyclically varies between these bounds is sufficient to obtain near optimal classification results, often with fewer iterations.'' (Smith, 2017, p. 8)
\par
``This policy is easy to implement and unlike adaptive learning rate methods, incurs essentially no additional computational expense.'' (Smith, 2017, p. 8)
\par
Comment: Presented at WACV 2017; see https://github.com/bckenstler/CLR for instructions to implement CLR in Keras}
}

@misc{smithDonDecayLearning2018,
  title = {Don't Decay the Learning Rate, Increase the Batch Size},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year = {2018},
  number = {arXiv:1711.00489},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: 11 pages, 8 figures. Published as a conference paper at ICLR 2018}
}

@inproceedings{snoekPracticalBayesianOptimization2012,
  title = {Practical Bayesian Optimization of Machine Learning Algorithms},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}}
}

@misc{somepalliSAINTImprovedNeural2021,
  title = {Saint: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training},
  author = {Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C. Bayan and Goldstein, Tom},
  year = {2021},
  number = {arXiv:2106.01342},
  eprint = {2106.01342},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(24/01/2023, 18:08:14)}

\par
``Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce.'' (Somepalli et al., 2021, p. 1)
\par
``SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.'' (Somepalli et al., 2021, p. 1)
\par
``First, tabular data often contain heterogeneous features that represent a mixture of continuous, categorical, and ordinal values, and these values can be independent or correlated. Second, there is no inherent positional information in tabular data, meaning that the order of columns is arbitrary.'' (Somepalli et al., 2021, p. 1)
\par
``Tabular models must handle features from multiple discrete and continuous distributions, and they must discover correlations without relying on the positional information.'' (Somepalli et al., 2021, p. 1)
\par
``SAINT projects all features \textendash{} categorical and continuous into a combined dense vector space. These projected values are passed as tokens into a transformer encoder which uses attention in the following two ways. First, there is ``self-attention,'' which attends to individual features within each data sample. Second, we propose a novel ``intersample attention,'' which enhances the classification of a row (i.e., a data sample) by relating it to other rows in the table. Intersample attention is akin to a nearest-neighbor classification, where the distance metric is learned end-to-end rather than fixed. In addition to this hybrid attention mechanism, we also leverage self-supervised contrastive pre-training to boost performance for semi-supervised problems.'' (Somepalli et al., 2021, p. 2)
\par
``TabNet [1] uses neural networks to mimic decision trees by placing importance on only a few features at each layer. The attention layers in that model do not use the regular dot-product self-attention common in transformer-based models, rather there is a type of sparse layer that allows only certain features to pass through'' (Somepalli et al., 2021, p. 2)
\par
``Transformer models for more general tabular data include TabTransformer [18], which uses a transformer encoder to learn contextual embeddings only on categorical features.'' (Somepalli et al., 2021, p. 2)
\par
``The continuous features are concatenated to the embedded features and fed to an MLP. The main issue with this model is that continuous data do not go through the self-attention block. That means any information about correlations between categorical and continuous features is lost.'' (Somepalli et al., 2021, p. 2)
\par
``Self-Supervised Learning Self-supervision via a `pretext task' on unlabeled data coupled with finetuning on labeled data is widely used for improving model performance in language and computer vision. Some of the tasks previously used for self-supervision on tabular data include masking, denoising, and replaced token detection. Masking (or Masked Language Modeling(MLM)) is when individual features are masked and the model's objective is to impute their value [1, 18, 32]. Denoising injects various types of noise into the data, and the objective there is to recover the original values [43, 49]. Replaced token detection (RTD) inserts random values into a given feature vector and seeks to detect the location of these replacements [18, 20]. Contrastive pre-training, where the distance between two views of the same point is minimized while maximizing the distance between two different points [5, 12, 15], is another pretext task that applies to tabular data. In this paper, to the best of our knowledge, we are the first to adopt contrastive learning for tabular data. We couple this strategy with denoising to perform pre-training on a plethora of datasets with varied volumes of labeled data, and we show that our method outperforms traditional boosting methods.'' (Somepalli et al., 2021, p. 3)
\par
``Encoding the Data In language models, all tokens are embedded using the same procedure. However, in the tabular domain, different features can come from distinct distributions, necessitating a heterogeneous embedding approach. Note that tabular data can contain multiple categorical features which may use the same set of tokens. Unless it is known that common tokens possess identical relationships within multiple columns, it is important to embed these columns independently. Unlike the embedding of TabTransformer[18], which uses attention to embed only categorical features, we propose also projecting continuous features into a d-dimensional space before passing their embedding through the transformer encoder.'' (Somepalli et al., 2021, p. 4)
\par
``With this simple trick alone, we significantly improve the performance of the TabTransformer model as discussed in Section 5.1.'' (Somepalli et al., 2021, p. 4)
\par
``SAINT is inspired by the transformer encoder of Vaswani et al. [41], designed for natural language, where the model takes in a sequence of feature embeddings and outputs contextual representations of the same dimensi'' (Somepalli et al., 2021, p. 4)
\par
``Contrastive learning, in which models are pre-trained to be invariant to reordering, cropping, or other label-preserving ``views'' of the data [5, 12, 15, 32, 43], is a powerful tool in the vision and language domains that has never (to our knowledge) been applied to tabular data.'' (Somepalli et al., 2021, p. 5)
\par
``Existing self-supervised objectives for tabular data include denoising [43], a variation of which was used by VIME [49], masking, and replaced token detection as used by TabTransformer [18]. We find that, while these methods are effective, superior results are achieved by contrastive learning.'' (Somepalli et al., 2021, p. 5)
\par
``It is difficult to craft invariance transforms for tabular data. The authors of VIME [49] use mixup in the non-embedded space as a data augmentation method, but this is limited to continuous data. We instead use CutMix [50] to augment samples in the input space and we use mixup [51] in the embedding space. These two augmentations combined yield a challenging and effective self-supervision task'' (Somepalli et al., 2021, p. 5)
\par
``Semi-supervised setting We perform 3 sets of experiments with 50, 200, and 500 labeled data points (in each case the rest are unlabeled). See Table 3 for numerical results. In all cases, the pre-trained SAINT model (with both self and intersample attention) performs the best. Interestingly, we note that when all the training data samples are labeled, pre-training does not contribute appreciably, hence the results with and without pre-training are fairly close.'' (Somepalli et al., 2021, p. 8)
\par
``Effect of embedding continuous features To understand the effect of learning embeddings for continuous data, we perform a simple experiment with TabTransformer. We modify TabTransformer by embedding continuous features into d dimensions using a single layer ReLU MLP, just as they use on categorical features, and we pass the embedded features through the transformer block. We keep the entire architecture and all training hyper-parameters the same for both TabTransformer and its modified version. The average AUROC of the original TabTransformer is 89.38. Just by embedding the continuous features, the performance jumps to 91.72. This experiment shows that embedding the continuous data is important and can boost the performance of the model significantly.'' (Somepalli et al., 2021, p. 8)
\par
``Positional Encoding Transformers for vision and language typically employ positional encodings along with the patch/word embeddings to retain spatial information. These encodings are necessary when all features in a data point are of same type, hence these models use the same function to embed all inputs. This is not the case with most of the datasets used in this paper; each feature may be of a different type and thus possesses a unique embedding function. However, when we train the model on MNIST (treated as tabular data), positional encodings are used since all pixels are of the same type and share a single embedding function.'' (Somepalli et al., 2021, p. 15)}
}

@inproceedings{songAutoIntAutomaticFeature2019,
  title = {{{AutoInt}}: {{Automatic Feature Interaction Learning}} via {{Self-Attentive Neural Networks}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
  year = {2019},
  eprint = {1810.11921},
  eprinttype = {arxiv},
  doi = {10.1145/3357384.3357925},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(25/01/2023, 14:28:49)}

\par
``4.3 Embedding Layer Since the feature representations of the categorical features are very sparse and high-dimensional, a common way is to represent them into low-dimensional spaces (e.g., word embeddings). Specifically, we represent each categorical feature with a low-dimensional vector, i.e., ei = Vixi, (2) where Vi is an embedding matrix for field i, and xi is an one-hot vector. Often times categorical features can be multi-valued, i.e., xi is a multi-hot vector. Take movie watching prediction as an example, there could be a feature field Genre which describes the types of a movie and it may be multi-valued (e.g., Drama and Romance for movie ``Titanic''). To be compatible with multi-valued inputs, we further modify the Equation 2 and represent the multi-valued feature field as the average of corresponding feature embedding vectors: ei = 1 q Vixi, (3) where q is the number of values that a sample has for i-th field and xi is the multi-hot vector representation for this field. To allow the interaction between categorical and numerical features, we also represent the numerical features in the same lowdimensional feature space. Specifically, we represent the numerical feature as em = vmxm, (4) where vm is an embedding vector for field m, and xm is a scalar value. By doing this, the output of the embedding layer would be a concatenation of multiple embedding vectors, as presented in Figure 2.'' (Song et al., 2019, p. 4)
\par
Comment: Accepted at CIKM2019}
}

@misc{SparseAutoencodersUsing2020,
  title = {Sparse Autoencoders Using L1 Regularization with {{PyTorch}}},
  year = {2020},
  journal = {DebuggerCafe}
}

@article{srivastavaDropoutSimpleWay,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan}
}

@misc{statquestwithjoshstarmerGradientBoostPart2019,
  title = {Gradient Boost Part 2 (of 4): Regression Details},
  author = {{StatQuest with Josh Starmer}},
  year = {2019}
}

@article{Stephan_1990,
  title = {Intraday Price Change and Trading Volume Relations in the Stock and Stock Option Markets},
  author = {Stephan, Jens A. and Whaley, Robert E.},
  year = {1990},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1990.tb05087.x},
  mag_id = {2003268269},
  pmcid = {null},
  pmid = {null}
}

@book{stewartCalculusEarlyTranscendentals2016,
  title = {Calculus: Early Transcendentals},
  author = {Stewart, James},
  year = {2016},
  edition = {Eighth edition},
  publisher = {{Cengage Learning}},
  address = {{Boston, MA, USA}}
}

@book{strangIntroductionLinearAlgebra2016,
  title = {Introduction to Linear Algebra},
  author = {Strang, Gilbert},
  year = {2016},
  edition = {5th edition},
  publisher = {{Cambridge press}},
  address = {{Wellesley}}
}

@article{stutzUnderstandingImprovingRobustness,
  title = {Understanding and Improving Robustness and Uncertainty Estimation in Deep Learning},
  author = {Stutz, David}
}

@misc{sukhbaatarAugmentingSelfattentionPersistent2019,
  title = {Augmenting Self-Attention with Persistent Memory},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  year = {2019},
  number = {arXiv:1907.01470},
  eprint = {1907.01470},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 13:27:46)}

\par
``Both the multi-head self-attention and the feed-forward layer are followed by an add-norm operation. This transformation is simply a residual connection [17] followed by layer normalization [23]. The layer normalization computes the average and standard deviation of the output activations of a given sublayer and normalizes them accordingly. This guarantees that the input yt of the following sublayer is well conditioned, i.e., that yT t 1 = 0 and yT t yt = {$\surd$}d.'' (Sukhbaatar et al., 2019, p. 3)
\par
``More precisely, the AddNorm operation is defined as: AddNorm(xt) = LayerNorm(xt + Sublayer(xt)), (6) where Sublayer is either a multi-head self-attention or a feedforward sublayer.'' (Sukhbaatar et al., 2019, p. 3)
\par
``Transformer layer. The overall transformer layer has the following set of equations: zt = AddNorm(MultiHead(xt)), (7) yt = AddNorm(FF(zt)), (8) where MultiHead is the multi-head self-attention sublayer. This is shown on the left panel of Fig.'' (Sukhbaatar et al., 2019, p. 4)
\par
``These vectors are added to capture information that does not depend on the immediate context, like general knowledge about the task. They are shared across the data and, in some sense, forms a persistent memory similar to the feedforward layer. Therefore we call them persistent vectors. More precisely, the persistent vectors are a set of N pairs of key-value vectors, respectively stacked in two dh \texttimes{} N dimensional matrices Mk and Mv. As discussed in Section 4.1, Mk and Mv can be interpreted as V and U of a feedforward sublayer'' (Sukhbaatar et al., 2019, p. 4)
\par
``The right panel of Fig. 1 summarize the all-attention layer in the case of a single head: we remove the feedforward sublayer and add unconditioned persistent vectors to the self-attention sublayer. While the persistent vectors are directly comparable to a feedforward sublayer in the case of a single head, a multi-head version is more comparable to multiple small feedforward layers working in parallel.'' (Sukhbaatar et al., 2019, p. 5)
\par
``It extends the self-attention layer of a transformer with a set of persistent vectors that are capable of storing information that is complementary to the short term information in contexts. We also show that these persistent vectors can replace the feedforward layers in a transformer network with no loss of performance.'' (Sukhbaatar et al., 2019, p. 9)}
}

@incollection{sunAdaBoostLSTMEnsembleLearning2018,
  title = {{{AdaBoost-LSTM}} Ensemble Learning for Financial Time Series Forecasting},
  booktitle = {Computational {{Science}} \textendash{} {{ICCS}} 2018},
  author = {Sun, Shaolong and Wei, Yunjie and Wang, Shouyang},
  editor = {Shi, Yong and Fu, Haohuan and Tian, Yingjie and Krzhizhanovskaya, Valeria V. and Lees, Michael Harold and Dongarra, Jack and Sloot, Peter M. A.},
  year = {2018},
  volume = {10862},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93713-7_55}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(30/01/2023, 09:47:25)}

\par
``In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector'' (Sutskever et al., 2014, p. 1)
\par
``First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously [18].'' (Sutskever et al., 2014, p. 3)
\par
``The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the ``encoder'' LSTM and 32M for the ``decoder'' LSTM). The complete training details are given below:'' (Sutskever et al., 2014, p. 5)
\par
Comment: 9 pages}
}

@misc{takaseLayerNormalizationsResidual2022,
  title = {On Layer Normalizations and Residual Connections in Transformers},
  author = {Takase, Sho and Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun},
  year = {2022},
  number = {arXiv:2206.00330},
  eprint = {2206.00330},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 14:01:44)}

\par
``the impact of the layer normalization positions [32, 33]. There are currently two major layer normalization positions in Transformers: Pre-Layer Normalization (Pre-LN) and Post-Layer Normalization (Post-LN). Pre-LN applies the layer normalization to an input for each sub-layer, and Post-LN places the layer normalization after each residual connection. The original Transformer [28] employs PostLN. However, recent studies often suggest using Pre-LN [32, 2, 5] because the training in Post-LN with deep Transformers (e.g., ten or more layers) often becomes unstable, resulting in useless models. Figure 1 shows an actual example; loss curves of training 18L-18L Transformer encoder-decoders on a widely used WMT English-to-German machine translation dataset. Here, XL-Y L represents the number of layers in encoder and decoder, where X and Y correspond to encoder and decoder, respectively. These figures clearly show that 18L-18L Post-LN Transformer encoder-decoder fails to train the model. However, in contrast, Liu et al. [13] reported that Post-LN consistently achieved better performance than Pre-LN in the machine translation task when they used 6L-6L (relatively shallow) Transformers.'' (Takase et al., 2022, p. 2)}
}

@inproceedings{tangAnalysisAttentionMechanisms2018,
  title = {An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation},
  booktitle = {Proceedings of the {{Third Conference}} on {{Machine Translation}}: {{Research Papers}}},
  author = {Tang, Gongbo and Sennrich, Rico and Nivre, Joakim},
  year = {2018},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-6304}
}

@article{tanhaSemisupervisedSelftrainingDecision2017,
  title = {Semi-Supervised Self-Training for Decision Tree Classifiers},
  author = {Tanha, Jafar and {\noopsort{someren}}{van Someren}, Maarten and Afsarmanesh, Hamideh},
  year = {2017},
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {8},
  number = {1},
  doi = {10.1007/s13042-015-0328-7},
  note = {\section{Annotations\\
(02/11/2022, 08:50:18)}

\par
``We show that standard decision tree learning as the base learner cannot be effective in a self-training algorithm to semi-supervised learning. The main reason is that the basic decision tree learner does not produce reliable probability estimation to its predictions.'' (Tanha et al., 2017, p. 355)
\par
``Therefore, it cannot be a proper selection criterion in self-training.'' (Tanha et al., 2017, p. 355)
\par
``tree learner that produce better probability estimation than using the distributions at the leaves of the tree. We show that these modifications do not produce better performance when used on the labeled data only, but they do benefit more from the unlabeled data in self-training'' (Tanha et al., 2017, p. 355)
\par
``The modifications that we consider are Naive Bayes Tree, a combination of No-pruning and Laplace correction, grafting, and using a distance-based measure.'' (Tanha et al., 2017, p. 355)
\par
``Semi-supervised learning algorithms use not only the labeled data but also unlabeled data to construct a classifier. The goal of semi-supervised learning is to use unlabeled instances and combine the information in the unlabeled data with the explicit classification information of labeled data for improving the classification performance.'' (Tanha et al., 2017, p. 355)
\par
``A self-training algorithm is an iterative method for semi-supervised learning, which wraps around a base learner. It uses its own predictions to assign labels to unlabeled data. Then, a set of newly-labeled data, which we call a set of high-confidence predictions, are selected to be added to the training set for the next iterations. The performance of the self-training algorithm strongly depends on the selected newly-labeled data at each iteration of the training procedure.'' (Tanha et al., 2017, p. 355)
\par
``In this paper we focus on self-training with a decision tree learner as the base learner.'' (Tanha et al., 2017, p. 356)
\par
``We will show that using this as the selection metric in self-training does not improve the classification performance of a self-training algorithm and thus the algorithm does not benefit from the unlabeled data.'' (Tanha et al., 2017, p. 356)
\par
``These include: (1) the sample size at the leaves is almost always small, there is a limited number of labeled data, and (2) all instances at a leaf get the same probability.'' (Tanha et al., 2017, p. 356)
\par
``Our hypothesis is that these modified decision tree learners will show classification accuracy similar to the standard decision tree learner when applied to the labeled data only, but will benefit from the unlabeled data when used as the base classifier in self-training, because they make better probability estimates which is vital for the selection step of selftraining.'' (Tanha et al., 2017, p. 356)
\par
``We then extend our analysis from single decision trees to ensembles of decision trees, in particular the Random Subspace Method [19] and Random Forests [9].'' (Tanha et al., 2017, p. 356)
\par
``In this case, probability is estimated by combining the predictions of multiple trees. However, if the trees in the ensemble suffer from poor probability estimation, the ensemble learner will not benefit much from self-training on unlabeled data. Using the modified decision tree learners as the base learner for the ensemble will improve the performance of self-training with the ensemble classifier as the base learner.'' (Tanha et al., 2017, p. 356)
\par
``In general, self-training is a wrapper algorithm, and is hard to analyze. However, for specific base classifiers, theoretical analysis is feasible, for example [17] showed that the Yarowsky algorithm [45] minimizes an upperbound on a new definition of cross entropy based on a specific instantiation of the Bregman distance.'' (Tanha et al., 2017, p. 357)
\par
``The goal of the selection step in Algorithm 1 is to find a set unlabeled examples with high-confidence predictions, above a threshold T. This is important, because selection of incorrect predictions will propagate to produce further classification errors.'' (Tanha et al., 2017, p. 357)
\par
``Although for many domains decision tree classifiers produce good classifiers, they provide poor probability estimates [28, 31].'' (Tanha et al., 2017, p. 357)
\par
``The reason is that the sample size at the leaves is almost always small, and all instances at a leaf get the same probability. The probability estimate is simply the proportion of the majority class at the leaf of a (pruned) decision tree. A trained decision tree indeed uses the absolute class frequencies of each leaf of the tree as follows: Int. J. Mach. Learn. \& Cyber. (2017) 8:355\textendash 370 357 12'' (Tanha et al., 2017, p. 357)
\par
``p\dh kjx\TH{$\frac{1}{4}$}K N \dh 1\TH{} where K is the number of instances of the class k out of N instances at a leaf. However, these probabilities are based on very few data points, due to the fragmentation of data over the decision tree. For example, if a leaf node has subset of 50 examples of which 45 examples belong to one class, then any example that corresponds to this leaf will get 0:9 probability where a leaf with 3 examples of one class get a probability of 1:0. In semi-supervised learning this problem is even more serious, because in applications the size of initial set of labeled data is typically quite small. Here, we consider several methods for improving the probability estimates at the leaves of decision trees.'' (Tanha et al., 2017, p. 358)
\par
``One candidate improvement is the Laplacian correction (or Laplace estimator) which smooths the probability values at the leaves of the decision tree [31].'' (Tanha et al., 2017, p. 358)
\par
``Another possible improvement is a decision tree learner that does not do any pruning. Although this introduces the risk of ``overfitting'', it may be a useful method because of the small amount of training data.'' (Tanha et al., 2017, p. 358)
\par
``In an NBTree, a local Naive Bayes Classifier is constructed at each leaf of decision tree that is built by a standard decision tree learning algorithm like C4.5.'' (Tanha et al., 2017, p. 358)
\par
``The idea behind grafting is that some regions in the data space are more sparsely populated.'' (Tanha et al., 2017, p. 358)
\par
``Another way to select from the unlabeled examples is to use a combination of distance-based approach and the probability estimation.'' (Tanha et al., 2017, p. 359)
\par
``In an ensemble classifier, probability estimation is estimated by combining the confidences of their components. This tends to improve both the classification accuracy and the probability estimation. However, if a standard decision tree learner is used as the base learner, then the problems, that we noted above, carry over to the ensemble. We therefore expect that improving the probability estimates of the base learner will enable the ensemble learner to benefit more from the unlabeled data than if the standard decision tree learner is used.'' (Tanha et al., 2017, p. 360)
\par
``There are several ways to compute the probability estimation in a random forest such as averaging class probability distributions estimated by the relative class frequency, the Laplace estimate and the m-estimate respectively. The standard random forest uses the relative class frequency as its probability estimation which is not suitable for self-training as we discussed'' (Tanha et al., 2017, p. 360)
\par
``In Table 3, the columns DT and ST-DT show the classification accuracy of J48 base learner and self-training respectively. As can be seen, self-training does not benefit from unlabeled data and there is no difference in accuracy between learning from the labeled data only and selftraining from labeled and unlabeled data.'' (Tanha et al., 2017, p. 362)
\par
``As can be seen, unlike the basic decision tree learner, C4.4 enables self-training to become effective for nearly all the datasets. The average improvement over the used datasets is 1.9 \%. The reason for improvement is that using Laplacian correction and No-pruning give better rank for probability estimation of the decision tree, which leads to select a set of high-confidence predictions.'' (Tanha et al., 2017, p. 362)
\par
``The results show that RSM with C4.4graft as the base learner in self-training improves the classification performance of RSM on 13 out of 14 datasets and the average improvement over all datasets is 2.7 \%. The same results are shown in Table 7 for RSM, when the NBTree is the base learner.'' (Tanha et al., 2017, p. 364)
\par
``Finally, comparing Tables 6\textendash 8, shows that ensemble methods outperform the single classifier especially for web-page datasets. The results also verify that improving both the classification accuracy and the probability estimates of the base learner in self-training are effective for improving the performance.'' (Tanha et al., 2017, p. 364)
\par
``Consistent with our hypothesis we observe that difference between supervised algorithms and self-training methods decreases when the number of labeled data increases. Another interesting observation is that RF improves the classification performance of self-training when more labeled data are available, because with more labeled data the bagging approach, used in RF, generates diverse decision trees.'' (Tanha et al., 2017, p. 365)
\par
``The main contribution of this paper is the observation that when a learning algorithm is used as the base learner in self-training, it is very important that the confidence of prediction is correctly estimated, probability estimation.'' (Tanha et al., 2017, p. 368)
\par
``The standard technique of using the distribution at the leaves of decision tree as probability estimation does not enable self-training with a decision tree learner to benefit from unlabeled data. The accuracy is the same as when the decision tree learner is applied to only the labeled data.'' (Tanha et al., 2017, p. 368)
\par
``Although to a lesser extent, the same is true when the modified decision tree learners are used as the base learner in an ensemble learner.'' (Tanha et al., 2017, p. 368)
\par
``Based on the results of the experiments we conclude that improving the probability estimation of the tree classifiers leads to better selection metric for the self-training algorithm and produces better classification model.'' (Tanha et al., 2017, p. 368)
\par
``We observe that using Laplacian correction, No-pruning, grafting, and NBTree produce better probability estimation in tree classifiers.'' (Tanha et al., 2017, p. 368)}
}

@misc{tayEfficientTransformersSurvey2022,
  title = {Efficient Transformers: A Survey},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2022},
  number = {arXiv:2009.06732},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 14:06:29)}

\par
``The new tensor is then additively composed with positional encodings and passed through a multiheaded self-attention module.'' (Tay et al., 2022, p. 3)
\par
``Positional encodings can take the form of a sinusoidal input (as per (Vaswani et al., 2017)) or be trainable embeddings'' (Tay et al., 2022, p. 3)
\par
``The inputs and output of the multi-headed self-attention module are connected by residual connectors and a layer normalization layer. The output of the multi-headed selfattention module is then passed to a two-layered feed-forward network which has its inputs/outputs similarly connected in a residual fashion with layer normalization. The sublayer residual connectors with layer norm is expressed as: X = LayerNorm(FS(X)) + X where FS is the sub-layer module which is either the multi-headed self-attention or the position-wise feed-forward layers.'' (Tay et al., 2022, p. 3)
\par
``2.1 Multi-Head Self-Attention The Transformer model leverages a multi-headed self-attention mechanism. The key idea behind the mechanism is for each element in the sequence to learn to gather from other tokens in the sequence. The operation for a single head is defined as: Ah = Softmax({$\alpha$}QhK{$>$} h )Vh, where X is a matrix in RN\texttimes d, {$\alpha$} is a scaling factor that is typically set to 1 {$\surd$}d , Qh = XWq, Kh = XWk and Vh = XWv are linear transformations applied on the temporal dimension of the input sequence, Wq, Wk, Wv {$\in$} Rd\texttimes{} d H are the weight matrices (parameters) for the query, key, and value projections that project the input X to an output tensor of d dimensions, and NH is the number of heads. Softmax is applied row-wise. The outputs of heads A1 {$\cdot$} {$\cdot$} {$\cdot$} AH are concatenated together and passed into a dense layer. The output Y can thus be expressed as Y = Wo[A1 {$\cdot$} {$\cdot$} {$\cdot$} AH ], where Wo is an output linear projection. Note that the computation of A is typically done in a parallel fashion by considering tensors of RB \texttimes{} RN \texttimes{} RH \texttimes{} R d H and computing the linear transforms for all heads in parallel. The attention matrix A = QK{$>$} is chiefly responsible for learning alignment scores between tokens in the sequence. In this formulation, the dot product between each element/token in the query (Q) and key (K) is taken. This drives the self-alignment process in self-attention whereby tokens learn to gather from each other. 2.2 Position-wise Feed-forward Layers The outputs of the self-attention module are then passed into a two-layered feed-forward network with ReLU activations. This feed-forward layer operates on each position independently. This is expressed as follows: F2(ReLU (F1(XA))) where F1 and F2 are feed-forward functions of the form W x + b. 2.3 Putting it all together Each Transformer block can be expressed as: XA = LayerNorm(MultiheadAttention(X, X)) + X XB = LayerNorm(PositionFFN(XA)) + XA where X is the input of the Transformer block and XB is the output of the Transformer block. Note that the MultiheadAttention() function accepts two argument tensors, one for query and the other for key-values. If the first argument and second argument is the same input tensor, this is the MultiheadSelfAttention mechanism.'' (Tay et al., 2022, p. 4)
\par
``The Transformer model leverages a multi-headed self-attention mechanism. The key idea behind the mechanism is for each element in the sequence to learn to gather from other tokens in the sequence.'' (Tay et al., 2022, p. 4)
\par
``2.4 On the compute cost of Transformers The computation costs of Transformers is derived from multiple factors. Firstly, the memory and computational complexity required to compute the attention matrix is quadratic in the'' (Tay et al., 2022, p. 4)
\par
``Efficient Transformers: A Survey input sequence length, i.e., N \texttimes{} N . In particular, the QK{$>$} matrix multiplication operation alone consumes N 2 time and memory. This restricts the overall utility of self-attentive models in applications which demand the processing of long sequences. Memory restrictions are tend to be applicable more to training (due to gradient updates) and are generally of lesser impact on inference (no gradient updates). The quadratic cost of self-attention impacts speed1 in both training and inference. The compute costs of the self-attention mechanism contributes partially to the overall compute cost of the Transformer. A non-trivial amount of compute still stems from the two layer feed-forward layers at every Transformer block (approximately half the compute time and/or FLOPs). The complexity of the FFN is linear with respect to sequence length but is generally still costly. Hence, a large portion of recent work have explored sparsity (Lepikhin et al., 2020; Fedus et al., 2021) as a means to scale up the FFN without incurring compute costs.'' (Tay et al., 2022, p. 5)
\par
``In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens.'' (Tay et al., 2022, p. 5)
\par
``In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not.'' (Tay et al., 2022, p. 5)
\par
``Relative Positional Encodings Transformer-XL introduces novel relative position encodings. In this scheme, absolute positional encodings are not added to the content embeddings. Instead, they are only considered while computing attention weights where they can be replaced with relative position encodings. Since the relative position encodings are not directly relevant to the efficiency of the model, we refer interested readers to Dai et al. (2019) for more details.'' (Tay et al., 2022, p. 24)
\par
Comment: Version 2: 2022 edition
\par
Comment: Version 2: 2022 edition}
}

@inproceedings{tenneyBERTRediscoversClassical2019,
  title = {{{BERT Rediscovers}} the {{Classical NLP Pipeline}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1452},
  note = {\section{Annotations\\
(29/01/2023, 18:46:10)}

\par
``We build on this latter line of work, focusing on the BERT model (Devlin et al., 2019), and use a suite of probing tasks (Tenney et al., 2019) derived from the traditional NLP pipeline to quantify where specific types of linguistic information are encoded.'' (Tenney et al., 2019, p. 4593)
\par
``Building on observations (Peters et al., 2018b) that lower layers of a language model encode more local syntax while higher layers capture more complex semantics, we present two novel contributions.'' (Tenney et al., 2019, p. 4593)
\par
``First, we present an analysis that spans the common components of a traditional NLP pipeline. We show that the order in which specific abstractions are encoded reflects the traditional hierarchy of these tasks. Second, we qualitatively analyze how individual sentences are processed by the BERT network, layer-by-layer. We show that while the pipeline order holds in aggregate, the model can allow individual decisions to depend on each other in arbitrary ways, deferring ambiguous decisions or revising incorrect ones based on higher-level information.'' (Tenney et al., 2019, p. 4593)
\par
``That is, it appears that basic syntactic information appears earlier in the network, while high-level semantic information appears at higher layers. We note that this finding is consistent with initial observations by Peters et al. (2018b), which found that constituents are represented earlier than coreference'' (Tenney et al., 2019, p. 4596)
\par
``We find that while this traditional pipeline order holds in the aggregate, on individual examples the network can resolve out-oforder, using high-level information like predicateargument relations to help disambiguate low-level decisions like part-of-speech. This provides new evidence corroborating that deep language models can represent the types of syntactic and semantic abstractions traditionally believed necessary for language processing, and moreover that they can model complex interactions between different levels of hierarchical information.'' (Tenney et al., 2019, p. 4597)}
}

@article{theissenTestAccuracyLee2000,
  title = {A Test of the Accuracy of the Lee/Ready Trade Classification Algorithm},
  author = {Theissen, Erik},
  year = {2000},
  journal = {Journal of International Financial Markets, Institutions and Money},
  volume = {11},
  note = {\section{Annotations\\
(24/10/2022, 05:42:51)}

\par
``As only transaction data is needed [for the tick test], data requirements for the application of the tick test are low'' (Theissen, 2000, p. 1)
\par
``Aitken / Frino (1996) use data from the Australian Stock Exchange, an electronic open limit order book. They find that the tick rule classifies only 74\% of the transactions correctly. Odders-White (1999) analyzes all steps of the algorithm using the TORQ database provided by the New York Stock Exchange. She finds that the algorithm on average correctly classifies 85\% of the transactions. Ellis / Michaely / O'Hara (1999) provide an analysis of trade classification accuracy using NASDAQ data. In their sample, 81.4\% of the transactions are classified correctly'' (Theissen, 2000, p. 2)
\par
``The extent to which inaccurate trade classification biases the results of empirical research partly depends on whether the misclassifications occur randomly or follow a systematic pattern. There are two ways to address this issue.'' (Theissen, 2000, p. 2)
\par
``Ellis / Michaely / O'Hara (1999) estimate a logit model and find that the proximity of the transaction price to the quotes is the most important determinant of the probability of misclassification. The second method, chosen by Lightfoot et al. (1999) and Odders-White (1999), entails estimating empirical model'' (Theissen, 2000, p. 2)
\par
``3 using the inferred trade classifications and comparing the results to those obtained when using the true classifications instead.'' (Theissen, 2000, p. 3)
\par
``Second, the present paper uses a different definition of the true trade classification than both Odders-White (1999) and Lightfoot et al. (1999). These authors consider a transaction to be buyer-initiated [seller-initiated] if the buy order [sell order] was placed last, chronologically. In contrast, we use a definition based on the position taken by the Makler (the equivalent of the specialist). If the Makler sold [bought] shares the transaction is classified as being buyer-initiated [seller-initiated]. This is similar to the approach in Ellis / Michaely / O'Hara (1999).'' (Theissen, 2000, p. 3)
\par
``The true classification is based on the position taken by the Makler. We define a trade to be buyer-initiated when the Makler sold shares and to be seller-initiated when the Makler bought shares'' (Theissen, 2000, p. 7)
\par
``The percentage of transactions classified correctly by the tick test is only slightly lower than the corresponding percentage for the Lee / Ready method (72.22\% compared to 72.77\%).'' (Theissen, 2000, p. 8)
\par
``The fraction of correctly classified trades appears to be higher for more liquid stocks.'' (Theissen, 2000, p. 8)
\par
``The performance of the tick test deteriorates dramatically when transactions occurring on a zero tick are considered. For these trades, the classification obtained when using the tick test appears to be unrelated to the true classification. The percentage of correct classifications is only 52.6\% and is not significantly different from 50\%'' (Theissen, 2000, p. 10)
\par
``Overall, the results indicate that, at least for the German stock market, the accuracy of the Lee / Ready trade classification method is limited. The misclassification probability of 27.23\% is higher than the corresponding percentages reported for the NYSE and NASDAQ'' (Theissen, 2000, p. 10)}
}

@misc{thoppilanLaMDALanguageModels2022,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and {Meier-Hellstern}, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and {Hoffman-John}, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and {Aguera-Arcas}, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  year = {2022},
  number = {arXiv:2201.08239},
  eprint = {2201.08239},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{tobekDoesSourceFundamental,
  title = {Does the Source of Fundamental Data Matter?},
  author = {Tobek, Ondrej and Hronec, Martin},
  doi = {10.2139/ssrn.3150654}
}

@misc{TransformerArchitecturePositional,
  title = {Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog},
  howpublished = {https://kazemnejad.com/blog/transformer\_architecture\_positional\_encoding/}
}

@misc{TransformersLucasBeyer,
  title = {Transformers with Lucas Beyer, Google Brain - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=EixI6t5oif0}
}

@article{tsaiPredictingStockReturns2011,
  title = {Predicting Stock Returns by Classifier Ensembles},
  author = {Tsai, Chih-Fong and Lin, Yuah-Chiao and Yen, David C. and Chen, Yan-Min},
  year = {2011},
  journal = {Applied Soft Computing},
  volume = {11},
  number = {2},
  doi = {10.1016/j.asoc.2010.10.001}
}

@misc{tuningplaybookgithub,
  title = {Deep Learning Tuning Playbook},
  author = {Godbole, Varun and Dahl, George E. and Gilmer, Justin and Shallue, Christopher J. and Nado, Zachary},
  year = {2023},
  note = {Version 1.0}
}

@article{tunstallNaturalLanguageProcessing2022,
  title = {Natural Language Processing with Transformers},
  author = {Tunstall, Lewis},
  year = {2022},
  note = {\section{Annotations\\
(09/01/2023, 07:53:54)}

\par
``Naturally, we want to avoid being so wasteful with our model parameters since models are expensive to train, and larger models are more difficult to maintain. A common approach is to limit the vocabulary and discard rare words by considering, say, the 100,000 most common words in the corpus. Words that are not part of the vocabulary are classified as ``unknown'' and mapped to a shared UNK token. This means that we lose some potentially important information in the process of word tokenization, since the model has no information about words associated with UNK.'' (Tunstall, 2022, p. 32)
\par
``Positional embeddings are based on a simple, yet very effective idea: augment the token embeddings with a position-dependent pattern of values arranged in a vector. If the pattern is characteristic for each position, the attention heads and feed-forward layers in each stack can learn to incorporate positional information into their transformations.'' (Tunstall, 2022, p. 73)
\par
``Absolute positional representations Transformer models can use static patterns consisting of modulated sine and cosine signals to encode the positions of the tokens. This works especially well when there are not large volumes of data available.'' (Tunstall, 2022, p. 74)
\par
``Relative positional representations Although absolute positions are important, one can argue that when computing an embedding, the surrounding tokens are most important. Relative positional representations follow that intuition and encode the relative positions between tokens. This cannot be set up by just introducing a new relative embedding layer at the beginning, since the relative embedding changes for each token depending on where from the sequence we are attending to it'' (Tunstall, 2022, p. 74)
\par
\section{Annotations\\
(23/01/2023, 13:40:44)}

\par
``Naturally, we want to avoid being so wasteful with our model parameters since models are expensive to train, and larger models are more difficult to maintain. A common approach is to limit the vocabulary and discard rare words by considering, say, the 100,000 most common words in the corpus. Words that are not part of the vocabulary are classified as ``unknown'' and mapped to a shared UNK token. This means that we lose some potentially important information in the process of word tokenization, since the model has no information about words associated with UNK.'' (Tunstall, 2022, p. 32)
\par
``The feed-forward sublayer in the encoder and decoder is just a simple two-layer fully connected neural network, but with a twist: instead of processing the whole sequence of embeddings as a single vector, it processes each embedding independently. For this reason, this layer is often referred to as a position-wise feed-forward layer.'' (Tunstall, 2022, p. 70)
\par
``A rule of thumb from the literature is for the hidden size of the first layer to be four times the size of the embeddings, and a GELU activation function is most commonly used. This is where most of the capacity and memorization is hypothesized to happen, and it's the part that is most often scaled when scaling up the models.'' (Tunstall, 2022, p. 70)
\par
``Adding Layer Normalization'' (Tunstall, 2022, p. 71)
\par
``As mentioned earlier, the Transformer architecture makes use of layer normalization and skip connections. The former normalizes each input in the batch to have zero mean and unity variance. Skip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor.'' (Tunstall, 2022, p. 71)
\par
``Post layer normalization This is the arrangement used in the Transformer paper; it places layer normalization in between the skip connections. This arrangement is tricky to train from scratch as the gradients can diverge. For this reason, you will often see a concept known as learning rate warm-up, where the learning rate is gradually increased from a small value to some maximum value during training.'' (Tunstall, 2022, p. 71)
\par
``Pre layer normalization This is the most common arrangement found in the literature; it places layer normalization within the span of the skip connections. This tends to be much more stable during training, and it does not usually require any learning rate warm-up.'' (Tunstall, 2022, p. 71)
\par
``We've now implemented our very first transformer encoder layer from scratch! However, there is a caveat with the way we set up the encoder layers: they are totally 72 | Chapter 3: Transformer Anatom'' (Tunstall, 2022, p. 72)
\par
``4 In fancier terminology, the self-attention and feed-forward layers are said to be permutation equivariant\textemdash if the input is permuted then the corresponding output of the layer is permuted in exactly the same way. invariant to the position of the tokens. Since the multi-head attention layer is effectively a fancy weighted sum, the information on token position is lost.'' (Tunstall, 2022, p. 73)
\par
``the self-attention and feed-forward layers are said to be permutation equivariant'' (Tunstall, 2022, p. 73)
\par
``Positional embeddings are based on a simple, yet very effective idea: augment the token embeddings with a position-dependent pattern of values arranged in a vector. If the pattern is characteristic for each position, the attention heads and feed-forward layers in each stack can learn to incorporate positional information into their transformations.'' (Tunstall, 2022, p. 73)
\par
``Absolute positional representations Transformer models can use static patterns consisting of modulated sine and cosine signals to encode the positions of the tokens. This works especially well when there are not large volumes of data available.'' (Tunstall, 2022, p. 74)
\par
``Relative positional representations Although absolute positions are important, one can argue that when computing an embedding, the surrounding tokens are most important. Relative positional representations follow that intuition and encode the relative positions between tokens. This cannot be set up by just introducing a new relative embedding layer at the beginning, since the relative embedding changes for each token depending on where from the sequence we are attending to it'' (Tunstall, 2022, p. 74)}
}

@misc{turnerBayesianOptimizationSuperior2021,
  title = {Bayesian Optimization Is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020},
  author = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  year = {2021},
  number = {arXiv:2104.10201},
  eprint = {2104.10201},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(02/11/2022, 05:35:15)}

\par
``In black-box optimization we aim to solve the problem minx{$\in\Omega$} f (x), where f is a computationally expensive black-box function and the domain {$\Omega$} is commonly a hyper-rectangle.'' (Turner et al., 2021, p. 1)
\par
``Using this surrogate model, an acquisition function is used to determine the most promising point to evaluate next, where popular options include expected improvement (EI) [35], knowledge gradient (KG) [18], and entropy search (ES) [31]. There are also other surrogate optimization methods that rely on deterministic surrogate models such as radial basis functions [16, 66], see Forrester et al. [17] for an overview.'' (Turner et al., 2021, p. 1)
\par
``This is a new competition as there have been no ML-oriented black-box optimization competitions in the past.3 Th'' (Turner et al., 2021, p. 3)
\par
``We obtain novel optimization problems via the Cartesian product of datasets, ML models, and evaluation metrics'' (Turner et al., 2021, p. 4)
\par
``Note that only the (local) practice problems were visible to the participants; both the feedback and final problems were hidden.'' (Turner et al., 2021, p. 4)
\par
``For the local practice optimization problems, the evaluation of the objective functions was done on the participants' hardware. However, for the test problems (the feedback and final leaderboards), the objective function had to be hidden, and therefore the participants' submissions were run inside a Docker container in a cloud environment.'' (Turner et al., 2021, p. 5)
\par
``61 beat the baseline random search and 23 beat TuRBO which was the strongest baseline provided in the starter kit.'' (Turner et al., 2021, p. 5)
\par
``Note that this comparison does not necessarily show that one package is better than another; it rather compares the performance of their default methods.'' (Turner et al., 2021, p. 6)
\par
``The submissions immediately bring one significant realization to the forefront: surrogate-assisted optimization is very effective'' (Turner et al., 2021, p. 6)
\par
``All of the top-20 participants used some form of surrogate-assisted optimization. This is strongly indicative of the value of using a ``surrogate model'' and that intelligent modeling/decision'' (Turner et al., 2021, p. 6)
\par
``Top methods on the final leaderboard and examples submissions vs random search (RS): On the left, we show what RS would have done given more function evaluations than allowed in the challenge (128). This performance curve is based on an unbiased estimate from pooling the data of N = 256 RS runs, which gives 256 \texttimes{} 128 = 32,768 function evaluations for each problem.'' (Turner et al., 2021, p. 8)
\par
``Many published papers on BO propose using only one surrogate model and one acquisition function, despite some prior research having discussed the benefits of ensembling BO methods [34].'' (Turner et al., 2021, p. 8)
\par
``This analysis hints that ensembling may be useful in avoiding failed models where an individual BO algorithm makes little progress.'' (Turner et al., 2021, p. 9)
\par
``The competition was divided into a feedback session (which the participants could monitor thorough a practice leaderboard) and a final testing session (the results of which produced the final leaderboard, as seen in Table 2).'' (Turner et al., 2021, p. 10)
\par
``But we were so excited by the effort put in to meta-learning by these teams that we reran all submissions with full visibility into parameter names. This allowed teams to employ strategies such as making initial guesses using the found optima from problems with the same variable names under the premise that the objective functions are likely similar.'' (Turner et al., 2021, p. 10)
\par
``As such, it demonstrated decisively the benefits of Bayesian optimization over random search. The top submissions showed over 100\texttimes{} sample efficiency gains compared to random search. First, all of the top teams used some form of BO ensemble; sometimes with very simple and easy to productionize strategies such as alternating the surrogate, acquisition function, or potentially entire optimization algorithms.. Second, the warm start leaderboard demonstrated how warm starting from even loosely related problems often yields large performance gains.'' (Turner et al., 2021, p. 11)}
}

@article{twalaGoodMethodsCoping2008,
  title = {Good Methods for Coping with Missing Data in Decision Trees},
  author = {Twala, B.E.T.H. and Jones, M.C. and Hand, D.J.},
  year = {2008},
  journal = {Pattern Recognition Letters},
  volume = {29},
  number = {7},
  doi = {10.1016/j.patrec.2008.01.010}
}

@inproceedings{ucarSubTabSubsettingFeatures2021,
  title = {{{SubTab}}: Subsetting Features of Tabular Data for Self-Supervised Representation Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ucar, Talip and Hajiramezanali, Ehsan and Edwards, Lindsay},
  year = {2021},
  volume = {34},
  publisher = {{Curran Associates, Inc.}},
  note = {\section{Annotations\\
(24/01/2023, 09:31:46)}

\par
``In recent years, the self-supervised learning has successfully been used to learn meaningful representations of the data in natural language processing [34, 41, 11, 28, 10, 21, 9]. A similar success has been achieved in image and audio domains [7, 15, 37, 5, 17, 13, 8]. This progress is mainly enabled by taking advantage of spatial, semantic, or temporal structure in the data through data augmentation [7], pretext task generation [11] and using inductive biases through architectural choices (e.g. CNN for images). However, these methods can be less effective in the lack of such structures and biases in the tabular data commonly used in many fields such as healthcare, advertisement, finance, and law. And some augmentation methods such as cropping, rotation, color transformation etc. are domain specific, and not suitable for tabular setting. The difficulty in designing similarly effective methods tailored for tabular data is one of the reasons why self-supervised learning is under-studied in this domain [46].'' (Ucar et al., 2021, p. 1)
\par
``he most common approach in tabular data is to corrupt data through adding noise [43]. An autoencoder maps corrupted examples of data to a latent space, from which it maps back to uncorrupted data. Through this process, it learns a representation robust to the noise in the input. This approach may not be as effective since it treats all features equally as if features are equally informative. However, perturbing uninformative features may not result in the intended goal of the corruption. A recent work takes advantage of self-supervised learning in tabular data setting by introducing a pretext task [46], in which a de-noising autoencoder with a classifier attached to representation layer is trained on 35th Conference on Neural Information Processing Systems (NeurIPS 2021)'' (Ucar et al., 2021, p. 1)
\par
``Figure 1: SubTab framework: i) Dividing the features into subsets (similar to feature bagging, or cropping images), ii) Reconstruction of either subsets of features ({\~ } x1,{\~ } x2,{\~ } x3), or complete feature space ({\~ } X1,{\~ } X2,{\~ } X3), which are used to compute reconstruction loss. iii) Generating projections used to compute contrastive and distance loss. E â Encoder, D â Decoder, G â Projection. corrupted data. The classifier's task is to predict the location of corrupted features. However, this framework still relies on noisy data in the input. Additionally, training a classifier on an imbalanced binary mask for a high-dimensional data may not be ideal to learn meaningful representations.'' (Ucar et al., 2021, p. 2)
\par
``In this work, we turn the problem of learning representation from a single-view of the data into the one learnt from its multiple views by dividing the features into subsets, akin to cropping in image domain or feature bagging in ensemble learning, to generate different views of the data. Each subset can be considered a different view. We show that reconstructing data from the subset of its features forces the encoder to learn better representation than the ones learned through the existing methods such as adding noise.'' (Ucar et al., 2021, p. 2)}
}

@article{vandermaatenVisualizingDataUsing2008,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{\noopsort{maaten}}{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86}
}

@article{vanengelenSurveySemisupervisedLearning2020,
  title = {A Survey on Semi-Supervised Learning},
  author = {{\noopsort{engelen}}{van Engelen}, Jesper E. and Hoos, Holger H.},
  year = {2020},
  journal = {Machine Learning},
  volume = {109},
  number = {2},
  doi = {10.1007/s10994-019-05855-6}
}

@misc{vasuImprovedOneMillisecond2022,
  title = {An Improved One Millisecond Mobile Backbone},
  author = {Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag},
  year = {2022},
  number = {arXiv:2206.04040},
  eprint = {2206.04040},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}L\{\}ukasz and Polosukhin, Illia},
  year = {2017},
  series = {{{NeurIPS}} 2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  address = {{Long Beach, CA}},
  note = {Comment: 15 pages, 5 figures}
}

@inproceedings{verscheldeGPUAccelerationNewton2014,
  title = {{{GPU}} Acceleration of Newton's Method for Large Systems of Polynomial Equations in Double Double and Quad Double Arithmetic},
  booktitle = {2014 {{IEEE Intl Conf}} on {{High Performance Computing}} and {{Communications}}, 2014 {{IEEE}} 6th {{Intl Symp}} on {{Cyberspace Safety}} and {{Security}}, 2014 {{IEEE}} 11th {{Intl Conf}} on {{Embedded Software}} and {{Syst}} ({{HPCC}},{{CSS}},{{ICESS}})},
  author = {Verschelde, Jan and Yu, Xiangcheng},
  year = {2014},
  publisher = {{IEEE}},
  address = {{Paris, France}},
  doi = {10.1109/HPCC.2014.31}
}

@inproceedings{vigInvestigatingGenderBias2020,
  title = {Investigating {{Gender Bias}} in {{Language Models Using Causal Mediation Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  year = {2020},
  volume = {33},
  publisher = {{Curran Associates, Inc.}}
}

@article{Vijh_1990,
  title = {Liquidity of the {{CBOE}} Equity Options},
  author = {Vijh, Anand M.},
  year = {1990},
  journal = {Journal of Finance},
  doi = {10.1111/j.1540-6261.1990.tb02431.x},
  mag_id = {1979846182},
  pmcid = {null},
  pmid = {null}
}

@article{vijhStockClosingPrice2020,
  title = {Stock Closing Price Prediction Using Machine Learning Techniques},
  author = {Vijh, Mehar and Chandola, Deeksha and Tikkiwal, Vinay Anand and Kumar, Arun},
  year = {2020},
  journal = {Procedia Computer Science},
  volume = {167},
  doi = {10.1016/j.procs.2020.03.326}
}

@inproceedings{voitaAnalyzingMultiHeadSelfAttention2019,
  title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence}},
  doi = {10.18653/v1/P19-1580},
  note = {\section{Annotations\\
(08/01/2023, 15:13:45)}

\par
``For heads judged to be important, we then attempt to characterize the roles they perform.'' (Voita et al., 2019, p. 5797)
\par
``We observe the following types of role: positional (heads attending to an adjacent token), syntactic (heads attending to tokens in a specific syntactic dependency relation) and attention to rare words (heads pointing to the least frequent tokens in the sentence)'' (Voita et al., 2019, p. 5797)
\par
``Previous work analyzing how representations are formed by the Transformer's multi-head attention mechanism focused on either the average or the maximum attention weights over all heads (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads.'' (Voita et al., 2019, p. 5798)
\par
``but neither method explicitly takes into account the varying importance of different heads.'' (Voita et al., 2019, p. 5798)}
}

@article{wangAttentionbasedTransactionalContext,
  title = {Attention-Based Transactional Context Embedding for next-Item Recommendation},
  author = {Wang, Shoujin and Hu, Liang and Cao, Longbing and Huang, Xiaoshui and Lian, Defu and Liu, Wei},
  doi = {10.1609/aaai.v32i1.11851}
}

@article{wangForecastingMethodStock2020,
  title = {Forecasting Method of Stock Market Volatility in Time Series Data Based on Mixed Model of {{ARIMA}} and Xgboost},
  author = {Wang, Yan and Guo, Yuankai},
  year = {2020},
  journal = {China Communications},
  volume = {17},
  number = {3},
  doi = {10.23919/JCC.2020.03.017}
}

@inproceedings{wangLearningDeepTransformer2019,
  title = {Learning Deep Transformer Models for Machine Translation},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F. and Chao, Lidia S.},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1176},
  note = {\section{Annotations\\
(23/01/2023, 14:17:52)}

\par
``Learning Deep Transformer Models for Machine Translation'' (Wang et al., 2019, p. 1810)
\par
``Learning Deep Transformer Models for Machine Translation'' (Wang et al., 2019, p. 1810)
\par
``Transformer'' (Wang et al., 2019, p. 1810)
\par
``Transformer'' (Wang et al., 2019, p. 1810)
\par
``For Transformer, it is not easy to train stacked layers on neither the encoder-side nor the decoderside. Stacking all these sub-layers prevents the efficient information flow through the network, and probably leads to the failure of training. Residual connections and layer normalization are adopted for a solution. Let F be a sub-layer in encoder or decoder, and \texttheta l be the parameters of the sub-layer. A residual unit is defined to be (He et al., 2016b): xl+1 = f (yl) (1) yl = xl + F (xl; \texttheta l) (2) where xl and xl+1 are the input and output of the l-th sub-layer, and yl is the intermediate output followed by the post-processing function f ({$\cdot$}). In this way, xl is explicitly exposed to yl (see Eq. (2)). Moreover, layer normalization is adopted to reduce the variance of sub-layer output because hidden state dynamics occasionally causes a much longer training time for convergence. There are two ways to incorporate layer normalization into the residual network. \textbullet{} Post-Norm. In early versions of Transformer (Vaswani et al., 2017), layer normalization is placed after the element-wise residual addition (see Figure 1(a)), like this: xl+1 = LN(xl + F (xl; \texttheta l)) (3) where LN({$\cdot$}) is the layer normalization function, whose parameter is dropped for simplicity. It can be seen as a post-processing step of the output (i.e., f (x) = LN(x)). \textbullet{} Pre-Norm. In recent implementations (Klein et al., 2017; Vaswani et al., 2018; Domhan, 2018), layer normalization is applied to the input of every sub-layer (see Figure 1(b)): xl+1 = xl + F (LN(xl); \texttheta l) (4'' (Wang et al., 2019, p. 1811)
\par
``1812 Eq. (4) regards layer normalization as a part of the sub-layer, and does nothing for postprocessing of the residual connection (i.e., f (x) = x).3 Both of these methods are good choices for implementation of Transformer. In our experiments, they show comparable performance in BLEU for a system based on a 6-layer encoder (Section 5.1).'' (Wang et al., 2019, p. 1812)
\par
``2.2 On the Importance of Pre-Norm for Deep Residual Network The situation is quite different when we switch to deeper models. More specifically, we find that prenorm is more efficient for training than post-norm if the model goes deeper. This can be explained by seeing back-propagation which is the core process to obtain gradients for parameter update. Here we take a stack of L sub-layers as an example. Let E be the loss used to measure how many errors occur in system prediction, and xL be the output of the topmost sub-layer. For post-norm Transformer, given a sub-layer l, the differential of E with respect to xl can be computed by the chain rule, and we have {$\partial$}E {$\partial$}xl = {$\partial$}E {$\partial$}xL \texttimes{} L-1 {$\prod$} k=l {$\partial$}LN(yk) {$\partial$}yk \texttimes{} L-1 {$\prod$} k=l ( 1 + {$\partial$}F (xk; \texttheta k) {$\partial$}xk ) (5) where {$\prod$}L-1 k=l {$\partial$} LN(yk ) {$\partial$}yk means the backward pass of the layer normalization, and {$\prod$}L-1 k=l (1 + {$\partial$}F(xk;\texttheta k) {$\partial$}xk ) means the backward pass of the sub-layer with the residual connection. Likewise, we have the gradient for pre-norm 4: {$\partial$}E {$\partial$}xl = {$\partial$}E {$\partial$}xL \texttimes{} ( 1+ L-1 {$\sum$} k=l {$\partial$}F (LN(xk); \texttheta k) {$\partial$}xl ) (6) Obviously, Eq. (6) establishes a direct way to pass error gradient {$\partial$}E {$\partial$}xL from top to bottom. Its merit lies in that the number of product items on the right side does not depend on the depth of the stack. In contrast, Eq. (5) is inefficient for passing gradients back because the residual connection is not 3We need to add an additional function of layer normalization to the top layer to prevent the excessively increased value caused by the sum of unnormalized output. 4For a detailed derivation, we refer the reader to Appendix A. a bypass of the layer normalization unit (see Figure 1(a)). Instead, gradients have to be passed through LN({$\cdot$}) of each sub-layer. It in turn introduces term {$\prod$}L-1 k=l {$\partial$} LN(yk ) {$\partial$}yk into the right hand side of Eq. (5), and poses a higher risk of gradient vanishing or exploring if L goes larger. This was confirmed by our experiments in which we successfully trained a pre-norm Transformer system with a 20-layer encoder on the WMT English-German task, whereas the post-norm Transformer system failed to train for a deeper encoder (Section 5.1).'' (Wang et al., 2019, p. 1812)}
}

@misc{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: Self-Attention with Linear Complexity},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  year = {2020},
  number = {arXiv:2006.04768},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@incollection{wangPerceivingNextChoice2017,
  title = {Perceiving the next Choice with Comprehensive Transaction Embeddings for Online Recommendation},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Wang, Shoujin and Hu, Liang and Cao, Longbing},
  editor = {Ceci, Michelangelo and Hollm{\'e}n, Jaakko and Todorovski, Ljup{\v c}o and Vens, Celine and D{\v z}eroski, Sa{\v s}o},
  year = {2017},
  volume = {10535},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-71246-8_18},
  note = {\section{Annotations\\
(23/01/2023, 14:17:11)}

\par
``transaction-based recommender systems (TBRS).'' (Wang et al., 2017, p. 1)
\par
``on one hand, the context is dynamic along with the shopping'' (Wang et al., 2017, p. 1)
\par
``called transaction-based RS (TBRS)'' (Wang et al., 2017, p. 2)
\par
``context, most of them treat the context as static rather than dynamic and can only work on static transactional data, an example is the pattern-based RS [23].'' (Wang et al., 2017, p. 2)
\par
``They tend to recommend those popular and long-released items while ignore those less popular or newly-released ones.'' (Wang et al., 2017, p. 2)
\par
``In practice, the next choice is not only affected by one item or part of items in front of the target item, but by all items bought in the transaction event (i.e., the whole context).'' (Wang et al., 2017, p. 3)
\par
``It is noted that a deep structure is not efficient for online recommendation due to the long computational time needed to deal with thousands of items in real time.'' (Wang et al., 2017, p. 4)
\par
``Our model, NTEM has a three-layer network structure consisting of input layer, embedding layer and output layer as shown in Fig. 1. The input layer contains double wide-in data vectors, the contextual itemset is collected from one while their corresponding features are acquired from the other.'' (Wang et al., 2017, p. 4) 1 input layer, 2 embedding layer, 1 output layer
\par
``A TBRS model is proposed, which does not require the strict order over items within one transaction. This is more consistent with the real-world case.'' (Wang et al., 2017, p. 4)
\par
``We incorporate item features into the model and encode the feature-item relevance coupling relations into feature embeddings, which makes our model also work well on cold start cases.'' (Wang et al., 2017, p. 4)
\par
``Markov Chain to estimate the transition probability from current item to the next item and thus make prediction based on this probability.'' (Wang et al., 2017, p. 5)
\par
``I = \{i1, i2...i|I|\}'' (Wang et al., 2017, p. 6)
\par
``have a rigid order,'' (Wang et al., 2017, p. 6)
\par
``c, our NTEM is constructed and trained as a probabilistic classifier that learns to predict a conditional probability distribution P (is|c), where c {$\subseteq$} t\textbackslash is is the context from a transaction t {$\in$} T w.r.t. the target item is'' (Wang et al., 2017, p. 6) Ich brauche also noch einen weg meine target variable zu isolieren.
\par
``Similar to BOW, for each target item is {$\in$} t, the transaction context is c = t\textbackslash is, namely all the items except the target one in the transaction are picked up as the context.'' (Wang et al., 2017, p. 6) umsetzen
\par
``|t| training instances are built for each transaction t by picking up one item as the target one each time.'' (Wang et al., 2017, p. 6) relevant!!
\par
``features of items are added to the model as part of context,'' (Wang et al., 2017, p. 6) context codieren statt einzelnen items?
\par
``{$<$} c, Fc'' (Wang et al., 2017, p. 6)
\par
``c and Fc'' (Wang et al., 2017, p. 6)
\par
``Fc'' (Wang et al., 2017, p. 6)
\par
``{$<$} c, Fc'' (Wang et al., 2017, p. 6)
\par
``one-hot encoding vector'' (Wang et al., 2017, p. 6)
\par
(Wang et al., 2017, p. 6) i selbst entfernen.
\par
``categorical features.'' (Wang et al., 2017, p. 6)
\par
``\texttimes{} m vector using one-hot encoding'' (Wang et al., 2017, p. 6)
\par
``f k'' (Wang et al., 2017, p. 6)
\par
``1 \texttimes{} V'' (Wang et al., 2017, p. 6)
\par
``Thus the input layer for each training example consists of |c| item vector with length |I| and |c| item feature vectors with length'' (Wang et al., 2017, p. 7) \"andern!
\par
``|c|'' (Wang et al., 2017, p. 7)
\par
``one-hot item and feature vectors.'' (Wang et al., 2017, p. 7)
\par
``The transaction context weight matrix Wt {$\in$} RK\texttimes |I| is used to fully connect between inputlayer and embedding-layer.'' (Wang et al., 2017, p. 7)
\par
``t :,i)'' (Wang et al., 2017, p. 7)
\par
``c,'' (Wang et al., 2017, p. 7)
\par
``As illustrated in the following equation, the transaction context embedding is built as a combination of Ei,'' (Wang et al., 2017, p. 7)
\par
``transaction as unordered,'' (Wang et al., 2017, p. 7)
\par
``{$\Theta$} are learned by back propagation.'' (Wang et al., 2017, p. 8)
\par
``The transaction table contains multiple transactions and each transaction consists of multiple items. Note that those transactions containing only one item are removed as they can not fit our model. This is because, we use at least one item as context for constructing the embeddings.'' (Wang et al., 2017, p. 9)
\par
``The item information table is used in both training and testing processes.'' (Wang et al., 2017, p. 9)
\par
``In the testing process, the learned embeddings are used to predict the target item given the (n - 1) ones.'' (Wang et al., 2017, p. 9)
\par
``global novelty and local novelty by comparing the recommendation list and the whole item population and the given context item set respectively.'' (Wang et al., 2017, p. 10)
\par
``We use the following widely used accuracy metrics for transactionbased recommendation to evaluate all the comparison approaches.The result of each approach is given in Table 2.'' (Wang et al., 2017, p. 10)
\par
``50 and 20 respectively'' (Wang et al., 2017, p. 11)
\par
``The reason is multifaceted. First of all, we use a complete context to predict one target item, and we donot assume a rigid order between the items within one transaction.'' (Wang et al., 2017, p. 12)
\par
``Second, more information is used by our model by incarnating the item features into it.'' (Wang et al., 2017, p. 12)
\par
``What's more important, our model has a very concise structure which is easy to train. This shallow structure is efficient enough to retrain and recompute the score for ranking of all candidate items in an incremental dataset for online recommendations.'' (Wang et al., 2017, p. 12)
\par
``Recall that in this paper we also try to recommend those infrequent or unpopular items, namely novel items, so the novelty is particularly important to measure the recommendation quality of our model.'' (Wang et al., 2017, p. 12)}
}

@article{wangSurveySessionbasedRecommender2020,
  title = {A Survey on Session-Based Recommender Systems},
  author = {Wang, Shoujin and Cao, Longbing and Wang, Yan and Sheng, Quan Z. and Orgun, Mehmet and Lian, Defu},
  year = {2020},
  journal = {arXiv:1902.04864 [cs]},
  eprint = {1902.04864},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{wangTransTabLearningTransferable,
  title = {{{TransTab}}: {{Learning Transferable Tabular Transformers Across Tables}}},
  author = {Wang, Zifeng and Sun, Jimeng},
  note = {\section{Annotations\\
(24/01/2023, 13:10:01)}

\par
``To answer all those questions, we propose to relax fixed table structures by introducing a Transferable Tabular Transformer (TransTab) for tables. The goal of TransTab is to convert each sample (a row in the table) to a generalizable embedding vector, and then apply stacked transformers for feature encoding.'' (Wang and Sun, p. 1)
\par
``The other insight is to introduce supervised and self-supervised pretraining to improve model performance. We compare TransTab with multiple baseline methods on diverse benchmark datasets and five oncology clinical trial datasets. Overall, TransTab ranks 1.00, 1.00, 1.78 out of 12 methods in supervised learning, feature incremental learning, and transfer learning scenarios, respectively; and the proposed pretraining leads to 2.3\% AUC lift on average over the supervised learning.'' (Wang and Sun, p. 1)
\par
``Recent works enhance tabular ML modeling using deep networks [5, 6, 7, 8] or designing self-supervision [2, 9, 10, 11]. Those existing works require the same table structure in training and testing data. However, there can be multiple tables sharing partially overlapped columns in the real world. Hence, learning across tables is inapplicable.'' (Wang and Sun, p. 1)
\par
``Columns are mapped to unique indexes then models take the cell values for training and inference. The premise of this modeling formulation is to keep the same column structure in all the tables. But tables often have divergent protocols where the nomenclatures of columns and cells differ. By contrast, our proposed work contextualizes the columns and cells. For example, previous methods represent a cell valued man under the column gender by 0 referring to the codebook \{man : 0, woman : 1\}. Our model converts the tabular input into a sequence input (e.g., gender is man), which can be modeled with downstream sequence models. We argue such featurizing protocol is generalizable across tables, thus enabling models to apply to different tables.'' (Wang and Sun, p. 2)
\par
``We build the input processor (1) to accept variable-column tables (2) to retain knowledge across tabular datasets. The idea is to convert tabular data (cells in columns) into a sequence of semantically encoded tokens. We utilize the following observation to create the sequence: the column description (e.g., column name) decides the meaning of cells in that column. For example, if a cell in column smoking history has value 1, it indicates the individual has a smoking history. Similarly, cell value 60 in column weight indicates 60 kg in weight instead of 60 years old. Motivated by the discussion, we'' (Wang and Sun, p. 3)
\par
``propose to include column names into the tabular modeling. As a result, TransTab treats any tabular data as the composition of three elements: text (for categorical \& textual cells and column names), continuous values (for numerical cells), and boolean values (for binary cells) .'' (Wang and Sun, p. 4)
\par
``Categorical/Textual feature. A category or textual feature contains a sequence of text tokens. For the categorical feature cat, we concatenate the column name with the feature value xc, which forms as a sequence of tokens. This sentence is then tokenized and matched to the token embedding matrix to generate the feature embedding Ec {$\in$} Rnc\texttimes d where d is the embedding dimension and nc is the number of tokens. Binary feature. The binary feature bin is usually an assertive description and its value xb {$\in$} \{0, 1\}. If xb = 1, then bin is tokenized and encoded to the embeddings Eb {$\in$} Rnb\texttimes d; if not, it will not be processed to the subsequent steps. This design significantly reduces the computational and memory cost when the inputs have high-dimensional and sparse one-hot features. Numerical feature. We do not concatenate column names and values for numerical feature because the tokenization-embedding paradigm was notoriously known to be bad at discriminating numbers [21]. Instead, we process them separately. num is encoded as same as cat and bin to get Eu,col {$\in$} Rnu\texttimes d. We then multiply the numerical features with the column embedding to yield the numerical embedding as Eu = xu \texttimes{} Eu,col2, which we identify gets an edge on more complicated numerical embedding techniques empirically.'' (Wang and Sun, p. 4)
\par
``Self-supervised learning \& contrastive learning. SSL uses unlabeled data with pretext tasks to learn useful representations and most of them are in CV and NLP [20, 17, 15, 16, 58, 23, 59, 60, 61, 62, 63]. Recent SSL tabular models can be classified into reconstruction and contrastive based methods: TabNet [30] and VIME [2] try to recover the corrupted inputs with auto-encoding loss; SCARF [11] takes a SimCLR-like [64] contrastive loss between the sample and its corrupted version; SubTab [9] takes a combination of both. Nevertheless, all fail to learn transferable models across tables such that cannot benefit from pretraining with scale. Contrastive learning can also be applied to supervised learning by leveraging class labels to build positive samples [26]. Our work extends it to to the tabular domain, which we prove works better than vanilla supervised pretraining. The vertical partition sampling also enjoys high query speed from large databases which are often column-oriented [25]. Another line of research takes table pretraining table semantic parsing [65, 66, 67, 68, 69] or table-to-text generation [70, 71]. But these methods either encode the whole table instead of each row or do not demonstrate to benefit tabular prediction yet.'' (Wang and Sun, p. 9)}
}

@article{waszczukAssemblingInternationalEquity2014,
  title = {Assembling International Equity Datasets \textendash{} Review of Studies on the Cross-Section of Returns},
  author = {Waszczuk, Antonina},
  year = {2014},
  journal = {Procedia Economics and Finance},
  volume = {15},
  doi = {10.1016/S2212-5671(14)00631-5}
}

@article{welchComprehensiveLookEmpirical2008,
  title = {A Comprehensive Look at the Empirical Performance of Equity Premium Prediction},
  author = {Welch, Ivo and Goyal, Amit},
  year = {2008},
  journal = {Review of Financial Studies},
  volume = {21},
  number = {4},
  doi = {10.1093/rfs/hhm014}
}

@misc{wengLearningNotEnough2021,
  title = {Learning with Not Enough Data Part 1: Semi-Supervised Learning},
  author = {Weng, Lilian},
  year = {2021},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2021-12-05-semi-supervised/}
}

@misc{wiegreffeAttentionNotNot2019,
  title = {Attention Is Not Not Explanation},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  year = {2019},
  number = {arXiv:1908.04626},
  eprint = {1908.04626},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Accepted to EMNLP 2019; related blog post at https://medium.com/@yuvalpinter/attention-is-not-not-explanation-dbc25b534017
\par
Comment: Accepted to EMNLP 2019; related blog post at https://medium.com/@yuvalpinter/attention-is-not-not-explanation-dbc25b534017}
}

@book{wittenDataMiningPractical2017,
  title = {Data Mining: Practical Machine Learning Tools and Techniques},
  editor = {Witten, I. H. and Witten, I. H.},
  year = {2017},
  edition = {Fourth Edition},
  publisher = {{Elsevier}},
  address = {{Amsterdam}}
}

@misc{wuMemorizingTransformers2022,
  title = {Memorizing Transformers},
  author = {Wu, Yuhuai and Rabe, Markus N. and Hutchins, DeLesley and Szegedy, Christian},
  year = {2022},
  number = {arXiv:2203.08913},
  eprint = {2203.08913},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Published as a conference paper at ICLR 2022 (spotlight)}
}

@misc{xiongLayerNormalizationTransformer2020,
  title = {On Layer Normalization in the Transformer Architecture},
  author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tie-Yan},
  year = {2020},
  number = {arXiv:2002.04745},
  eprint = {2002.04745},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(23/01/2023, 14:11:33)}

\par
``To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyperparameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.'' (Xiong et al., 2020, p. 1)
\par
``Residual connection and layer normalization Besides the two sub-layers described above, the residual connection and layer normalization are also key components to the Transformer. For any vector v, the layer normalization is computed as LayerNorm(v) = {$\gamma$} v-{$\mu$} {$\sigma$} + {$\beta$}, in which {$\mu$}, {$\sigma$} are the mean and standard deviation of the elements in v, i.e., {$\mu$} = 1 d {$\sum$}d k=1 vk and {$\sigma$}2 = 1 d {$\sum$}d k=1(vk - {$\mu$})2. Scale {$\gamma$} and bias vector {$\beta$} are parameters'' (Xiong et al., 2020, p. 3)
\par
``Different orders of the sub-layers, residual connection and layer normalization in a Transformer layer lead to variants of Transformer architectures. One of the original and most popularly used architecture for the Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2018) follows ``selfattention (FFN) sub-layer \textrightarrow{} residual connection \textrightarrow{} layer normalization'', which we call the Transformer with PostLayer normalization (Post-LN Transformer), as illustrated in Figure 1.'' (Xiong et al., 2020, p. 3)}
}

@article{yangStockPricePrediction2021,
  title = {Stock Price Prediction Based on Xgboost and {{LightGBM}}},
  author = {Yang, Yue and Wu, Yang and Wang, Peikun and Jiali, Xu},
  editor = {Wen, F. and Ziaei, S.M.},
  year = {2021},
  journal = {E3S Web of Conferences},
  volume = {275},
  doi = {10.1051/e3sconf/202127501040}
}

@inproceedings{yanMachineLearningStock2007,
  title = {Machine Learning for Stock Selection},
  booktitle = {Proceedings of the 13th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining  - {{KDD}} '07},
  author = {Yan, Robert J. and Ling, Charles X.},
  year = {2007},
  publisher = {{ACM Press}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/1281192.1281307}
}

@misc{yaoZeroQuantEfficientAffordable2022,
  title = {{{ZeroQuant}}: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author = {Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  year = {2022},
  number = {arXiv:2206.01861},
  eprint = {2206.01861},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.01861},
  archiveprefix = {arXiv},
  note = {Comment: 11 pages, 4 figures}
}

@inproceedings{yarowskyUnsupervisedWordSense1995,
  title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Yarowsky, David},
  year = {1995},
  publisher = {{Association for Computational Linguistics}},
  address = {{Cambridge, MA}},
  doi = {10.3115/981658.981684},
  note = {\section{Annotations\\
(25/10/2022, 15:49:00)}

\par
``If one begins with a small set of seed examples representative of two senses of a word, one can incrementally augment these seed examples with additional examples of each sense, using a combination of the one-senseper-collocation and one-sense-per-discourse tendencies.'' (Yarowsky, 1995, p. 190)
\par
``Indeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here.'' (Yarowsky, 1995, p. 190)
\par
``This could be accomplished by hand tagging a subset of the training sentences'' (Yarowsky, 1995, p. 191)
\par
``The remainder of the examples (typically 85-98\%) constitute an untagged residual.'' (Yarowsky, 1995, p. 191)
\par
``Train the supervised classification algorithm on the SENSE-A/SENSE-B seed sets.'' (Yarowsky, 1995, p. 191)
\par
``Apply the resulting classifier to the entire sample set. Take those members in the residual that are tagged as SENSE-A or SENSE-B with probability above a certain threshold, and add those examples to the growing seed sets.'' (Yarowsky, 1995, p. 192)
\par
``Optionally, the one-sense-per-discourse constraint is then used both to filter and augment this addition.'' (Yarowsky, 1995, p. 192)
\par
``When the training parameters are held constant, the algorithm will converge on a stable residual set.'' (Yarowsky, 1995, p. 192)
\par
``Repeat Step 3 iteratively. The training sets (e.g. SENSE-A seeds plus newly added examples) will tend to grow, while the residual will tend to shrink'' (Yarowsky, 1995, p. 192)
\par
``The classification procedure learned from the final supervised training step may now be applied to new data, and used to annotate the original untagged corpus with sense tags and probabilities.'' (Yarowsky, 1995, p. 193)
\par
``Also, for an unsupervised algorithm it works surprisingly well, directly outperforming Schiitze's unsupervised algorithm 96.7 \% to 92.2 \%, on a test of the same 4 words. More impressively, it achieves nearly the same performance as the supervised algorithm given identical training contexts (95.5 \% 19'' (Yarowsky, 1995, p. 195)
\par
``vs. 96.1\%) , and in some cases actually achieves superior performance when using the one-sense-perdiscourse constraint (96.5 \% vs. 96.1\%). This would indicate that the cost of a large sense-tagged training corpus may not be necessary to achieve accurate word-sense disambiguation.'' (Yarowsky, 1995, p. 196)}
}

@inproceedings{yinTaBERTPretrainingJoint2020,
  title = {{{TaBERT}}: {{Pretraining}} for {{Joint Understanding}} of {{Textual}} and {{Tabular Data}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  year = {2020},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.745},
  note = {\section{Annotations\\
(24/01/2023, 08:48:24)}

\par
``n this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables.'' (Yin et al., 2020, p. 8413)
\par
``These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019).'' (Yin et al., 2020, p. 8413)
\par
``These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables.'' (Yin et al., 2020, p. 8414)}
}

@article{yiWhyNotUse2020,
  title = {Why Not to Use Zero Imputation? {{Correcting}} Sparsity Bias in Training Neural Networks},
  author = {Yi, Joonyoung and Lee, Juhyuk and Kim, Kwang Joon and Hwang, Sung Ju and Yang, Eunho},
  year = {2020},
  note = {\section{Annotations\\
(28/11/2022, 09:24:44)}

\par
``Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks'' (Yi et al., 2020, p. 1)
\par
``While various imputing techniques, from imputing using global statistics such as mean, to individually imputing by learning auxiliary models such as GAN, can be applied with their own pros and cons, the most simple and natural way to do this is zero imputation, where we simply treat a missing feature as zero. In neural networks, at first glance, zero imputation can be thought of as a reasonable solution since it simply drops missing input nodes by preventing the weights associated with them from being updated'' (Yi et al., 2020, p. 1)
\par
``Some what surprisingly, however, many previous studies have reported that this intuitive approach has an adverse effect on model performances (Hazan et al., 2015; Luo et al., 2018;{\' } Smieja et al., 2018), and none of them has investigated the reasons of such performance degradations.'' (Yi et al., 2020, p. 1)
\par
``In this work, we find that zero imputation causes the output of a neural network to largely vary with respect to the number of missing entries in the input.'' (Yi et al., 2020, p. 1)
\par
``To best of our knowledge, we are the first in exploring the adverse effect of zero imputation, both theoretically and empirically.'' (Yi et al., 2020, p. 2)
\par
``We formally define the Variable Sparsity Problem (VSP) as follows: a phenomenon in which the expected value of the output layer of a neural network (over the weight and input distributions) depends on the sparsity (the number of zero values) of the input data (Figure 2a).'' (Yi et al., 2020, p. 3)
\par
``Assumption 1. (i) Every coordinate of input vector, h0 l , is generated by the element-wise multiplication of two random variables{\~ } h0 l and ml where ml is binary mask indicating missing value and{\~ } h0 l is a (possibly unobserved) feature value. Here, missing mask ml is MCAR (missing completely at random), with no dependency with other mask variables or their values{\~ } h0. All ml follow some identical distribution with mean {$\mu$}m. (ii) The elements of matrix W i are mutually independent and follow the identical distribution with mean {$\mu$}iw. Similarly, bi and{\~ } h0 consist of i.i.d. coordinates with mean {$\mu$}i b and {$\mu$}x, respectively. (iii) {$\mu$}iw is not zero uniformly over all i.'' (Yi et al., 2020, p. 3)
\par
``Here, missing mask ml is MCAR (missing completely at random), with no dependency with other mask variables or their values{\~ } h0.'' (Yi et al., 2020, p. 3)
\par
``(Case 1) For simplicity, let us first consider networks without the non-linearity nor the bias term. Theorem 1 shows that the average value of the output layer E[hL l ] is directly proportional to the expectation of the mask vector {$\mu$}m: Theorem 1. Suppose that activation {$\sigma$} is an identity function and that bi l is uniformly fixed as zero under Assumption 1. Then, we have E[hL l ] = {$\prod$}L i=1 ni-1{$\mu$}iw{$\mu$}x{$\mu$}m. (Case 2) When the activation function is affine but now with a possibly nonzero bias, E[hL l ] is influenced by {$\mu$}m in the following way: Theorem 2. Suppose that activation {$\sigma$} is an affine function under Assumption 1. Suppose further that fi(x) is defined as {$\sigma$}(ni-1{$\mu$}iwx + {$\mu$}i b). Then, E[hL l ] = fL {$\smwhtcircle$} {$\cdot$} {$\cdot$} {$\cdot$} {$\smwhtcircle$} f1({$\mu$}x{$\mu$}m). (Case 3) Finally, when the activation function is non-linear but non-decreasing and convex, we can show that E[hL l ] is lower-bounded by some quantity involving {$\mu$}m: Theorem 3. Suppose that {$\sigma$} is a non-decreasing convex function under Assumption 1. Suppose further that fi(x) is defined as {$\sigma$}(ni-1{$\mu$}iwx + {$\mu$}i b) and {$\mu$}iw {$>$} 0. Then, E[hL l ] {$\geq$} fL {$\smwhtcircle$} {$\cdot$} {$\cdot$} {$\cdot$} {$\smwhtcircle$} f1({$\mu$}x{$\mu$}m)'' (Yi et al., 2020, p. 4)
\par
``If the expected value of the output layer (or the lower bound of it) depends on the level of sparsity/missingness as in Theorem 1-3, even similar data instances may have different output values depending on their sparsity levels, which would hinder fair and correct inference of the model.'' (Yi et al., 2020, p. 4)
\par
``Missing handling techniques Missing imputation can be understood as a technique to increase the generalization performance by injecting plausible noise into data. Noise injection using global statistics like mean or median values is the simplest way to do this (Lipton et al., 2016;{\' } Smieja et al., 2018). However, it could lead to highly incorrect estimation since they do not take into consideration the characteristics of each data instance (Tresp et al., 1994; Che et al., 2018). To overcome this limitation, researchers have proposed various ways to model individualized noise using autoencoders (Pathak et al., 2016; Gondara \& Wang, 2018), or GANs (Yoon et al., 2018; Li et al., 2019). However, those model based imputation techniques have not properly worked for high dimensional datasets with the large number of features and/or extremely high missing rates (Yoon et al., 2018) because excessive noise can ruin the training of neural networks rather increasing generalization performance.'' (Yi et al., 2020, p. 9)}
}

@inproceedings{yoonVIMEExtendingSuccess2020,
  title = {Vime: Extending the Success of Self- and Semi-Supervised Learning to Tabular Domain},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and {\noopsort{schaar}}{van der Schaar}, Mihaela},
  year = {2020},
  series = {{{NeurIPS}} 2020},
  volume = {34},
  publisher = {{Curran Associates, Inc.}},
  address = {{Red Hook, NY}},
  note = {\section{Annotations\\
(24/01/2023, 10:36:56)}

\par
``Datasets like these present huge opportunities for self- and semi-supervised learning algorithms, which can leverage the unlabeled data to further improve the performance of a predictive model.'' (Yoon et al., 2020, p. 1)
\par
``Unfortunately, existing self- and semi-supervised learning algorithms are not effective for tabular data1 because they heavily rely on the spatial or semantic structure of image or language data.'' (Yoon et al., 2020, p. 1)
\par
``Tabular data is a database that is structured in a tabular form. It arranges data elements in vertical columns (features) and horizontal rows (samples)'' (Yoon et al., 2020, p. 1)
\par
``Standard semi-supervised learning methods also suffer from the same problem, since the regularizers they use for the predictive model are based on some prior knowledge of these data structures'' (Yoon et al., 2020, p. 2)
\par
``The notion of rotation simply does not exist in tabular data.'' (Yoon et al., 2020, p. 2)
\par
``Even in a setting where all variables are continuous, there is no guarantee that the data manifold is convex and as such taking convex combinations will either generate out-of-distribution samples (therefore degrading model performances) or be restricted to generating samples that are very close to real samples'' (Yoon et al., 2020, p. 2)
\par
``In this paper, we propose novel self- and semi-supervised learning frameworks for tabular data. For self-supervised learning, we introduce a novel pretext task, mask vector estimation in addition to feature vector estimation.'' (Yoon et al., 2020, p. 2)
\par
``Self-supervised learning (Self-SL) frameworks are representation learning methods using unlabeled data. It can be categorized into two types: using pretext task(s) and contrastive learning. Most existing works with pretext tasks are appropriate only for images or natural language:'' (Yoon et al., 2020, p. 2)
\par
``Most existing works with contrastive learning are also applicable only for image or natural languages due to their data augmentation scheme, and temporal and spatial relationships for defining the similarity'' (Yoon et al., 2020, p. 2)
\par
``There is some existing work on self-supervised learning which can be applied to tabular data. In Denoising auto-encoder [21], the pretext task is to recover the original sample from a corrupted sample. In Context Encoder [22], the pretext task is to reconstruct the original sample from both the corrupted sample and the mask vector. The pretext task for self-supervised learning in TabNet [23] and TaBERT [24] is also recovering corrupted tabular data.'' (Yoon et al., 2020, p. 2)
\par
``n this paper, we propose a new pretext task: to recover the mask vector, in addition to the original sample with a novel corrupted sample generation scheme. Also, we propose a novel tabular data augmentation scheme that can be combined with various contrastive learning frameworks to extend the self-supervised learning to tabular domains.'' (Yoon et al., 2020, p. 2)
\par
``emi-supervised learning (Semi-SL) frameworks can be categorized into two types: entropy minimization and consistency regularization. Entropy minimization encourages a classifier to output low entropy predictions on unlabeled data. For instance, [25] constructs hard labels from high-confidence predictions on unlabeled data, and train the network using these pseudo-labels together with labeled data in a supervised way. Consistency regularization encourages some sort of consistency between a sample and some stochastically altered version of itself. {$\Pi$}-model [26] uses an L2 loss to encourage consistency between predictions. Mean teacher [27] uses an L2 loss to encourage consistency between the intermediate representations. Virtual Adversarial Training (VAT) [28] encourages prediction consistency by minimizing the maximum difference in predictions between a sample and multiple'' (Yoon et al., 2020, p. 2)
\par
``augmented versions. MixMatch [29] and ReMixMatch [30] combine entropy minimization with consistency regularization in one unified framework with MixUp [10] as the data augmentation method. There is a series of interesting works on graph-based semi-supervised learning [31, 32, 33] which consider a special case of network data where samples are connected by a given edge, i.e. a citation network where an article is connected with its citations. Here, we introduce a novel data augmentation method for general tabular data which can be combined with various semi-supervised learning frameworks to train a predictive model in a semi-supervised way.'' (Yoon et al., 2020, p. 3)
\par
``When only limited labeled samples from pX,Y are available, a predictive model f : X \textrightarrow{} Y solely trained by supervised learning is likely to overfit the training samples since the empirical supervised loss {$\sum$}Nl i=1 l(f (xi), yi ) we minimize deviates significantly from the expected supervised loss E(x,y){$\sim$}pX,Y [l(f (x), y)], where l({$\cdot$}, {$\cdot$}) is some standard supervised loss function (e.g. cross-entropy).'' (Yoon et al., 2020, p. 3)
\par
``Self-supervised learning aims to learn informative representations from unlabeled data.'' (Yoon et al., 2020, p. 3)
\par
``In this subsection, we focus on self-supervised learning with various self-supervised/pretext tasks for a pretext model to solve. These tasks are set to be challenging but highly relevant to the downstream tasks that we attempt to solve. Ideally, the pretext model will extract some useful information from the raw data in the process of solving the pretext tasks. Then the extracted information can be utilized by the predictive model f in the downstream tasks.'' (Yoon et al., 2020, p. 3)
\par
``We define the pretext predictive model as h : Z \textrightarrow{} Ys, which is trained jointly with the encoder function e by minimizing the expected self-supervised loss function lss as follows, min e,h E(xs,ys){$\sim$}pXs,Ys [ lss (ys, (h {$\smwhtcircle$} e)(xs)) ] (1) where pXs,Ys is a pretext distribution that generates pseudo-labeled samples (xs, ys) for training the encoder e and pretext predictive model h.'' (Yoon et al., 2020, p. 3)
\par
``Semi-supervised learning optimizes the predictive model f by minimizing the supervised loss function jointly with some unsupervised loss function defined over the output space Y. Formally, semi-supervised learning is formulated as an optimization problem as follows, min f E(x,y){$\sim$}pXY [ l (y, f (x)) ] + {$\beta$} {$\cdot$} Ex{$\sim$}pX ,x{${'}\sim$}{\~ } pX (x{${'}$}|x) [ lu (f (x), f (x{${'}$})) ] (2) where lu : Y \texttimes{} Y \textrightarrow{} R is an unsupervised loss function, and a hyperparameter {$\beta$} {$\geq$} 0 is introduced to control the trade-off between the supervised and unsupervised losses. x{${'}$} is a perturbed version of x assumed to be drawn from a conditional distribution{\~ } pX (x{${'}$}|x).'' (Yoon et al., 2020, p. 3)
\par
``Image and tabular data are very different. The spatial correlations between pixels in images or the sequential correlations between words in text data are well-known and consistent across different datasets. By contrast, the correlation structure among features in tabular data is unknown and varies across different datasets. In other words, there is no ``common'' correlation structure in tabular data (unlike in image and text data). This makes the self- and semi-supervised learning in tabular data more challenging. Note that promising methods for image domain do not guarantee the favorable results on tabular domain (vice versa). Also, most augmentations and pretext tasks used in image data are not applicable to tabular data; because they directly utilize the spatial relationship of the image for augmentation (e.g., rotation) and pretext tasks (e.g., jigsaw puzzle and colorization).'' (Yoon et al., 2020, p. 9)}
}

@misc{zengAreTransformersEffective2022,
  title = {Are Transformers Effective for Time Series Forecasting?},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  year = {2022},
  number = {arXiv:2205.13504},
  eprint = {2205.13504},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  note = {Comment: Code is available at https://github.com/cure-lab/LTSF-Linear}
}

@article{zhangDiveDeepLearning2021,
  title = {Dive into Deep Learning},
  author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year = {2021},
  journal = {arXiv:2106.11342},
  eprint = {2106.11342},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  note = {\section{Annotations\\
(29/01/2023, 10:33:52)}

\par
``he core idea behind the Transformer model is the attention mechanism, an innovation that was originally envisioned as an enhancement for encoder-decoder RNNs applied to sequenceto-sequence applications, like machine translations (Bahdanau et al., 2014)'' (Zhang et al., 2021, p. 384)
\par
``The intuition behind attention is that rather than compressing the input, it might be better for the decoder to revisit the input sequence at every step. Moreover, rather than always seeing the same representation of the input, one might imagine that the decoder should selectively focus on particular parts of the input sequence at particular decoding steps.'' (Zhang et al., 2021, p. 384)
\par
``Bahdanau's attention mechanism provided a simple means by which the decoder could dynamically attend to different parts of the input at each decoding step.'' (Zhang et al., 2021, p. 384)
\par
``the attention mechanism (Bahdanau et al., 2014). We will cover the specifics of its application to machine translation later. For now, simply consider the following: denote by D def = f(k1; v1); : : : (km; vm)g a database of m tuples of keys and values. Moreover, denote by q a query. Then we can define the attention over D as Attention(q; D) def = m {$\sum$} i=1 (q; ki)vi; (11.1.1) where (q; ki) 2 R (i = 1; : : : ; m) are scalar attention weights.'' (Zhang et al., 2021, p. 386)
\par
``The name attention derives from the fact that the operation pays particular attention to the terms for which the weight is significant (i.e., large)'' (Zhang et al., 2021, p. 386)
\par
``A common strategy to ensure that the weights sum up to 1 is to normalize them via (q; ki) = (q; ki) {$\sum$} j(q; kj) : (11.1.2) In particular, to ensure that the weights are also nonnegative, one can resort to exponentiation. This means that we can now pick any function a(q; k) and then apply the softmax operation used for multinomial models to it via (q; ki) = exp(a(q; ki)) {$\sum$} j exp(a(q; kj)) : (11.1.3)'' (Zhang et al., 2021, p. 386)
\par
``One of the benefits of the attention mechanism is that it can be quite intuitive, particularly when the weights are nonnegative and sum to 1. In this case we might interpret large weights as a way for the model to select components of relevance. While this is a good intuition, it is important to remember that it is just that, an intuition. Regardless, we may want to visualize its effect on the given set of keys, when applying a variety of different queries.'' (Zhang et al., 2021, p. 387)
\par
``By design, the attention mechanism provides a differentiable means of control by which a neural network can select elements from a set and to construct an associated weighted sum over representations'' (Zhang et al., 2021, p. 388)
\par
``In Section 11.2, we used a number of different distance-based kernels, including a Gaussian kernel to model interactions between queries and keys. As it turns out, distance functions are slightly more expensive to compute than inner products. As such, with the softmax operation to ensure nonnegative attention weights, much of the work has gone into attention scoring functions a in (11.1.3) and Fig.11.3.1 that are simpler to compute.'' (Zhang et al., 2021, p. 393)
\par
``Last, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query q 2 Rd and the key ki 2 Rd are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of d.'' (Zhang et al., 2021, p. 394)
\par
``To ensure that the variance of the dot product still remains one regardless of vector length, we use the scaled dot-product attention scoring function. That is, we rescale the dot-product by 1/pd.'' (Zhang et al., 2021, p. 394)
\par
``Let's return to the dot-product attention introduced in (11.3.2). In general, it requires that both the query and the key have the same vector length, say d, even though this can be addressed easily by replacing q{$\top$}k with q{$\top$}Mk where M is a suitably chosen matrix to translate between both spaces. For now assume that the dimensions match.'' (Zhang et al., 2021, p. 396)
\par
``In practice, we often think in minibatches for efficiency, such as computing attention for n queries and m key-value pairs, where queries and keys are of length d and values are of length v. The scaled dot-product attention of queries Q 2 Rnd, keys K 2 Rmd, and values V 2 Rmv thus can be written as softmax ( QK{$\top$} pd ) V 2 Rnv:'' (Zhang et al., 2021, p. 396)
\par
``In practice, given the same set of queries, keys, and values we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values.'' (Zhang et al., 2021, p. 404)
\par
``To this end, instead of performing a single attention pooling, queries, keys, and values can be transformed with h independently learned linear projections. Then these h projected queries, keys, and values are fed into attention pooling in parallel. In the end, h attention pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention, where each of the h attention pooling outputs is a head (Vaswani et al., 2017). Using fully connected layers to perform learnable linear transformations, Fig.11.5.1 describes multi-head attention.'' (Zhang et al., 2021, p. 404)
\par
``Suppose that the input representation X 2 Rnd contains the d-dimensional embeddings for n tokens of a sequence. The positional encoding outputs X+P using a positional embedding matrix P 2 Rnd of the same shape, whose element on the ith row and the (2j)th or the (2j + 1)th column is pi;2j = sin (i 100002j/d ) ; pi;2j+1 = cos (i 100002j/d ) : (11.6.2) At first glance, this trigonometric-function design looks weird. Before explanations of this design, let's first implement it in the following PositionalEncoding class.'' (Zhang et al., 2021, p. 409)
\par
``In the positional embedding matrix P, rows correspond to positions within a sequence and columns represent different positional encoding dimensions. In the example below, we can see that the 6th and the 7th columns of the positional embedding matrix have a higher frequency than the 8th and the 9th columns. The offset between the 6th and the 7th (same for the 8th and the 9th) columns is due to the alternation of sine and cosine functions.'' (Zhang et al., 2021, p. 409)
\par
``To see how the monotonically decreased frequency along the encoding dimension relates to absolute positional information, let's print out the binary representations of 0; 1; : : : ; 7. As we can see, the lowest bit, the second-lowest bit, and the third-lowest bit alternate on every number, every two numbers, and every four numbers, respectively.'' (Zhang et al., 2021, p. 410)
\par
``In binary representations, a higher bit has a lower frequency than a lower bit. Similarly, as demonstrated in the heat map below, the positional encoding decreases frequencies along the encoding dimension by using trigonometric functions. Since the outputs are float numbers, such continuous representations are more space-efficient than binary representations.'' (Zhang et al., 2021, p. 410)}
}

@article{zhangUptodateComparisonStateoftheart2017,
  title = {An Up-to-Date Comparison of State-of-the-Art Classification Algorithms},
  author = {Zhang, Chongsheng and Liu, Changchang and Zhang, Xiangliang and Almpanidis, George},
  year = {2017},
  journal = {Expert Systems with Applications},
  volume = {82},
  doi = {10.1016/j.eswa.2017.04.003}
}

@article{Zheng_2013,
  title = {Price Jump Prediction in a Limit Order Book},
  author = {Zheng, Ban and Moulines, Eric and Abergel, Fr{\'e}d{\'e}ric},
  year = {2013},
  journal = {Journal of Mathematical Finance},
  doi = {10.4236/jmf.2013.32024},
  mag_id = {3022782557},
  pmcid = {null},
  pmid = {null}
}

@article{zhengFeatureEngineeringMachine,
  title = {Feature Engineering for Machine Learning},
  author = {Zheng, Alice and Casari, Amanda},
  note = {\section{Annotations\\
(20/12/2022, 08:43:13)}

\par
``Feature Scaling or Normalization Some features, such as latitude or longitude, are bounded in value. Other numeric features, such as counts, may increase without bound. Models that are smooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. Tree-based models, on the Feature Scaling or Normalization | 2'' (Zheng and Casari, p. 29)
\par
``other hand, couldn't care less. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature. Sometimes people also call it feature normalization. Feature scaling is usually done individually to each feature. Next, we will discuss several types of common scaling operations, each resulting in a different distribution of feature values.'' (Zheng and Casari, p. 30)
\par
``Min-Max Scaling Let x be an individual feature value (i.e., a value of the feature in some data point), and min(x) and max(x), respectively, be the minimum and maximum values of this feature over the entire dataset. Min-max scaling squeezes (or stretches) all feature values to be within the range of [0, 1]. Figure 2-15 demonstrates this concept. The formula for min-max scaling is: x{\~ } = x \textendash{} min(x) max(x) \textendash{} min(x)'' (Zheng and Casari, p. 30)
\par
``Standardization (Variance Scaling) Feature standardization is defined as: x{\~ } = x \textendash{} mean(x) sqrt(var(x)) It subtracts off the mean of the feature (over all data points) and divides by the variance. Hence, it can also be called variance scaling. The resulting scaled feature has a mean of 0 and a variance of 1. If the original feature has a Gaussian distribution, then the scaled feature does too. Figure 2-16 is an illustration of standardization.'' (Zheng and Casari, p. 31)}
}

@misc{zhuClusteringStructureMicrostructure2021,
  title = {Clustering Structure of Microstructure Measures},
  author = {Zhu, Liao and Sun, Ningning and Wells, Martin T.},
  year = {2021},
  number = {arXiv:2107.02283},
  eprint = {2107.0228},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@article{zhuSemiSupervisedLearningLiterature,
  title = {Semi-Supervised Learning Literature Survey},
  author = {Zhu, Xiaojin},
  note = {\section{Annotations\\
(13/10/2022, 08:23:45)}

\par
``It is a special form of classification. Traditional classifiers use only labeled data (feature / label pairs) to train. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers.'' (Zhu, p. 4)
\par
``Self-training is a commonly used technique for semi-supervised learning. In selftraining a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. The procedure is also called self-teaching or bootstrapping (not to be confused with the statistical procedure with the same name). The generative model and EM approach of section 2 can be viewed as a special case of `soft' self-training.'' (Zhu, p. 11)}
}

@misc{zouStockMarketPrediction2022,
  title = {Stock Market Prediction via Deep Learning Techniques: A Survey},
  author = {Zou, Jinan and Zhao, Qingying and Jiao, Yang and Cao, Haiyao and Liu, Yanxi and Yan, Qingsen and Abbasnejad, Ehsan and Liu, Lingqiao and Shi, Javen Qinfeng},
  year = {2022},
  number = {arXiv:2212.12717},
  eprint = {2212.12717},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv}
}

@preamble{ "\providecommand{\noopsort}[1]{} " }
