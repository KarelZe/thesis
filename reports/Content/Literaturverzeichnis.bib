@incollection{abeDeepLearningForecasting2018,
  title = {Deep Learning for Forecasting Stock Returns in the Cross-Section},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Abe, Masaya and Nakayama, Hideki},
  editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
  year = {2018},
  volume = {10937},
  pages = {273--284},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93034-3_22},
  abstract = {Many studies have been undertaken by using machine learning techniques, including neural networks, to predict stock returns. Recently, a method known as deep learning, which achieves high performance mainly in image recognition and speech recognition, has attracted attention in the machine learning field. This paper implements deep learning to predict one-month-ahead stock returns in the cross-section in the Japanese stock market and investigates the performance of the method. Our results show that deep neural networks generally outperform shallow neural networks, and the best networks also outperform representative machine learning models. These results indicate that deep learning shows promise as a skillful machine learning method to predict stock returns in the cross-section.},
  isbn = {978-3-319-93033-6 978-3-319-93034-3},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Abe und Nakayama - 2018 - Deep Learning for Forecasting Stock Returns in the.pdf}
}

@misc{adaloglouHowPositionalEmbeddings2021,
  title = {How Positional Embeddings Work in Self-Attention (Code in Pytorch)},
  author = {Adaloglou, Nikolas},
  year = {2021},
  month = feb,
  journal = {AI Summer},
  abstract = {Understand how positional embeddings emerged and how we use the inside self-attention to model highly structured data such as images},
  howpublished = {https://theaisummer.com/positional-embeddings/},
  langid = {english},
  keywords = {Attention,Transformer},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\86EQ2XCJ\\positional-embeddings.html}
}

@misc{agarapImplementingAutoencoderPyTorch2020,
  title = {Implementing an Autoencoder in {{PyTorch}}},
  author = {Agarap, Abien Fred},
  year = {2020},
  month = oct,
  journal = {PyTorch},
  abstract = {Building an autoencoder model for reconstruction},
  langid = {english},
  keywords = {Attention,Pytorch,Transformer},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\5E72NBVE\\implementing-an-autoencoder-in-pytorch-19baa22647d1.html}
}

@article{aggarwalFrameworkProjectedClustering,
  title = {A Framework for Projected Clustering of High Dimensional Data Streams},
  author = {Aggarwal, Charu C and Han, Jiawei and Wang, Jianyong and Yu, Philip S},
  pages = {12},
  doi = {10.1016/B978-012088469-8.50075-9},
  abstract = {The data stream problem has been studied extensively in recent years, because of the great ease in collection of stream data. The nature of stream data makes it essential to use algorithms which require only one pass over the data. Recently, single-scan, stream analysis methods have been proposed in this context. However, a lot of stream data is highdimensional in nature. High-dimensional data is inherently more complex in clustering, classification, and similarity search. Recent research discusses methods for projected clustering over high-dimensional data sets. This method is however difficult to generalize to data streams because of the complexity of the method and the large volume of the data streams.},
  langid = {english},
  annotation = {232 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\Q4TTFNKA\\Aggarwal et al. - A Framework for Projected Clustering of High Dimen.pdf}
}

@book{aggarwalRecommenderSystems2016,
  title = {Recommender Systems},
  author = {Aggarwal, Charu C.},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-29659-3},
  isbn = {978-3-319-29657-9 978-3-319-29659-3},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Aggarwal - 2016 - Recommender Systems.pdf}
}

@article{aitkenIntradayAnalysisProbability1995,
  title = {An {{Intraday Analysis}} of the {{Probability}} of {{Trading}} on the {{Asx}} at the {{Asking Price}}},
  author = {Aitken, Michael and Kua, Amaryllis and Brown, Philip and Watter, Terry and Y. Izan, H.},
  year = {1995},
  month = dec,
  journal = {Australian Journal of Management},
  volume = {20},
  number = {2},
  pages = {115--154},
  doi = {10.1177/031289629502000202},
  langid = {english},
  keywords = {üíé},
  annotation = {16 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Aitken et al. - 1995 - An Intraday Analysis of the Probability of Trading.pdf}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: A next-Generation Hyperparameter Optimization Framework},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {{Association for Computing Machinery}},
  address = {{Anchorage, AK}},
  doi = {10.1145/3292500.3330701},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  keywords = {üíé,Computer Science - Machine Learning},
  annotation = {579 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf;C\:\\Users\\Markus\\Zotero\\storage\\WQQTMXTA\\1907.html}
}

@article{aktasTradeClassificationAccuracy2014,
  title = {Trade {{Classification Accuracy}} for the {{Bist}}},
  author = {Aktas, Osman Ulas and Kryzanowski, Lawrence},
  year = {2014},
  month = nov,
  journal = {Journal of International Financial Markets, Institutions and Money},
  volume = {33},
  pages = {259--282},
  doi = {10.1016/j.intfin.2014.08.003},
  langid = {english},
  keywords = {üíé},
  annotation = {11 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Aktas and Kryzanowski - 2014 - Trade classification accuracy for the BIST.pdf}
}

@article{amel-zadehMachineLearningBasedFinancial2020,
  title = {Machine Learning-Based Financial Statement Analysis},
  author = {{Amel-Zadeh}, Amir and Calliess, Jan-Peter and Kaiser, Daniel and Roberts, Stephen},
  year = {2020},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3520684},
  abstract = {This paper explores the application of machine learning methods to financial statement analysis. We compare a range of models in the machine learning repertoire in their ability to predict the sign and magnitude of abnormal stock returns around earnings announcements based on past financial statement data alone. Random Forests produce the most accurate forecasts and the highest abnormal returns. (Nonlinear) neural network-based models perform relatively better for predictions of extreme market reactions, while the linear methods are relatively better in predicting moderate market reactions. Long-short portfolios based on model predictions generate sizable abnormal returns, which seem to decay over time. Abnormal returns are robust to various risk factors and load in expected ways on size, value and accruals. Analysing the underlying economic drivers of the performance of the Random Forests, we find that the models select as most important predictors financial variables required to forecast free cash flows and firm characteristics that are known cross-sectional predictors of stock returns.},
  langid = {english},
  annotation = {4 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RW89NNLS\\Amel-Zadeh et al. - 2020 - Machine Learning-Based Financial Statement Analysi.pdf}
}

@misc{AnnotatedTransformer,
  title = {The Annotated Transformer},
  howpublished = {http://nlp.seas.harvard.edu/2018/04/03/attention.html},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\8WIXUY4I\\attention.html}
}

@misc{arikTabNetAttentiveInterpretable2020,
  title = {Tabnet: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {Tabnet},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = {2020},
  number = {arXiv:1908.07442},
  eprint = {1908.07442},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\TPDKX93V\\Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf;C\:\\Users\\Markus\\Zotero\\storage\\EMGJQDZJ\\1908.html}
}

@article{arpitWhyRegularizedAutoEncoders2016,
  title = {Why Regularized Auto-Encoders Learn Sparse Representation?},
  author = {Arpit, Devansh and Zhou, Yingbo and Ngo, Hung and Govindaraju, Venu},
  year = {2016},
  month = jun,
  journal = {arXiv:1505.05561 [cs, stat]},
  eprint = {1505.05561},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textbackslash textit\{Internal Covariate Shift\}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size \$ 1 \$ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textbackslash textit\{Normalization Propagation\}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Arpit et al. - 2016 - Why Regularized Auto-Encoders learn Sparse Represe.pdf;C\:\\Users\\Markus\\Zotero\\storage\\H2VP2YAC\\1505.html}
}

@article{averyRecommenderSystemsEvaluating1997,
  title = {Recommender Systems for Evaluating Computer Messages},
  author = {Avery, Christopher and Zeckhauser, Richard},
  year = {1997},
  month = mar,
  journal = {Communications of the ACM},
  volume = {40},
  number = {3},
  pages = {88--89},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/245108.245127},
  langid = {english},
  annotation = {50 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Avery und Zeckhauser - 1997 - Recommender systems for evaluating computer messag.pdf}
}

@misc{bahriSCARFSelfSupervisedContrastive2022,
  title = {{{SCARF}}: {{Self-Supervised Contrastive Learning}} Using {{Random Feature Corruption}}},
  author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  year = {2022},
  number = {arXiv:2106.15147},
  eprint = {2106.15147},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Bahri et al. - 2022 - SCARF Self-Supervised Contrastive Learning using .pdf;C\:\\Users\\Markus\\Zotero\\storage\\BXCIPLS5\\2106.html}
}

@article{baileyAdvancesFinancialMachine,
  title = {Advances in Financial Machine Learning},
  author = {Bailey, Dr David H},
  pages = {393},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Bailey - Advances in Financial Machine Learning.pdf}
}

@article{baLayerNormalization2016,
  title = {Layer Normalization},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ba et al. - 2016 - Layer Normalization.pdf;C\:\\Users\\Markus\\Zotero\\storage\\RDL9NPAW\\1607.html}
}

@book{banachewiczKaggleBookData2022,
  title = {The {{Kaggle Book}}: {{Data Analysis}} and {{Machine Learning}} for {{Competitive Data Science}}},
  shorttitle = {The {{Kaggle Book}}},
  author = {Banachewicz, Konrad and Massaron, Luca},
  year = {2022},
  edition = {[First edition]},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  abstract = {Get a step ahead of your competitors with insights from over 30 Kaggle Masters and Grandmasters. Discover tips, tricks, and best practices for competing effectively on Kaggle and becoming a better data scientist. Key Features Learn how Kaggle works and how to make the most of competitions from over 30 expert Kagglers Sharpen your modeling skills with ensembling, feature engineering, adversarial validation and AutoML A concise collection of smart data handling techniques for modeling and parameter tuning Book Description Millions of data enthusiasts from around the world compete on Kaggle, the most famous data science competition platform of them all. Participating in Kaggle competitions is a surefire way to improve your data analysis skills, network with an amazing community of data scientists, and gain valuable experience to help grow your career. The first book of its kind, The Kaggle Book assembles in one place the techniques and skills you'll need for success in competitions, data science projects, and beyond. Two Kaggle Grandmasters walk you through modeling strategies you won't easily find elsewhere, and the knowledge they've accumulated along the way. As well as Kaggle-specific tips, you'll learn more general techniques for approaching tasks based on image, tabular, textual data, and reinforcement learning. You'll design better validation schemes and work more comfortably with different evaluation metrics. Whether you want to climb the ranks of Kaggle, build some more data science skills, or improve the accuracy of your existing models, this book is for you. What you will learn Get acquainted with Kaggle as a competition platform Make the most of Kaggle Notebooks, Datasets, and Discussion forums Create a portfolio of projects and ideas to get further in your career Design k-fold and probabilistic validation schemes Get to grips with common and never-before-seen evaluation metrics Understand binary and multi-class classification and object detection Approach NLP and time series tasks more effectively Handle simulation and optimization competitions on Kaggle Who this book is for This book is suitable for anyone new to Kaggle, veteran users, and anyone in between. Data analysts/scientists who are trying to do better in Kaggle competitions and secure jobs with tech giants will find this book useful. A basic understanding of machine learning concepts will help you make the most of this book},
  isbn = {978-1-80181-221-4},
  langid = {english},
  keywords = {üíé},
  annotation = {OCLC: 1313909123},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\GGDTHYB5\\Banachewicz and Massaron - 2022 - The Kaggle book data analysis and machine learnin.pdf}
}

@article{baptistaRelationPrognosticsPredictor2022,
  title = {Relation between Prognostics Predictor Evaluation Metrics and Local Interpretability {{SHAP}} Values},
  author = {Baptista, Marcia L. and Goebel, Kai and Henriques, Elsa M.P.},
  year = {2022},
  month = may,
  journal = {Artificial Intelligence},
  volume = {306},
  pages = {103667},
  doi = {10.1016/j.artint.2022.103667},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\MD6LCD6F\\Baptista et al. - 2022 - Relation between prognostics predictor evaluation .pdf}
}

@article{basakPredictingDirectionStock2019,
  title = {Predicting the Direction of Stock Market Prices Using Tree-Based Classifiers},
  author = {Basak, Suryoday and Kar, Saibal and Saha, Snehanshu and Khaidem, Luckyson and Dey, Sudeepa Roy},
  year = {2019},
  journal = {The North American Journal of Economics and Finance},
  volume = {47},
  pages = {552--567},
  doi = {10.1016/j.najef.2018.06.013},
  abstract = {Predicting returns in the stock market is usually posed as a forecasting problem where prices are predicted. Intrinsic volatility in the stock market across the globe makes the task of prediction challenging. Consequently, forecasting and diffusion modeling undermines a diverse range of problems encountered in predicting trends in the stock market. Minimizing forecasting error would minimize investment risk. In the current work, we pose the problem as a direction-predicting exercise signifying gains and losses. We develop an experimental framework for the classification problem which predicts whether stock prices will increase or decrease with respect to the price prevailing n days earlier. Two algorithms, random forests, and gradient boosted decisio`n trees (using XGBoost) facilitate this connection by using ensembles of decision trees. We test our approach and report the accuracies for a variety of companies as improvement over existing predictions. A novelty of the current work is about the selection of technical indicators and their use as features, with high accuracy for medium to long-run prediction of stock price direction.},
  langid = {english},
  annotation = {95 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\UM37YBUJ\\Basak et al. - 2019 - Predicting the direction of stock market prices us.pdf}
}

@misc{batesCrossvalidationWhatDoes2022,
  title = {Cross-{{Validation}}: {{What Does It Estimate}} and {{How Well Does It Do It}}?},
  shorttitle = {Cross-{{Validation}}},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  year = {2022},
  month = jul,
  number = {arXiv:2104.00673},
  eprint = {2104.00673},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow's Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and we show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail.},
  archiveprefix = {arXiv},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Bates et al. - 2022 - Cross-validation what does it estimate and how we.pdf;C\:\\Users\\Markus\\Zotero\\storage\\DQDXNDVM\\2104.html}
}

@incollection{bengioPracticalRecommendationsGradientBased2012,
  title = {Practical {{Recommendations}} for {{Gradient-Based Training}} of {{Deep Architectures}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {Bengio, Yoshua},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {437--478},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_26},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {Deep Learn,Generalization Error,Grid Search,Hide Unit,Sparse Code},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\L7XB782W\\Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf}
}

@article{bessembinderIssuesAssessingTrade2003,
  title = {Issues in {{Assessing Trade Execution Costs}}},
  author = {Bessembinder, Hendrik},
  year = {2003},
  month = may,
  journal = {Journal of Financial Markets},
  volume = {6},
  number = {3},
  pages = {233--257},
  doi = {10.1016/S1386-4181(02)00064-2},
  langid = {english},
  keywords = {üíé},
  annotation = {260 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Bessembinder - 2003 - Issues in assessing trade execution costs.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{blackPricingOptionsCorporate1973,
  title = {The Pricing of Options and Corporate Liabilities},
  author = {Black, Fischer and Scholes, Myron},
  year = {1973},
  journal = {The Journal of Political Economy},
  volume = {81},
  number = {3},
  pages = {637--654},
  doi = {10.1086/260062},
  keywords = {options},
  annotation = {16248 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Black und Scholes - 1973 - The Pricing of Options and Corporate Liabilities.pdf}
}

@article{blazejewskiLocalNonparametricModel2005,
  title = {A {{Local Non-Parametric Model}} for {{Trade Sign Inference}}},
  author = {Blazejewski, Adam and Coggins, Richard},
  year = {2005},
  month = mar,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {348},
  pages = {481--495},
  issn = {03784371},
  doi = {10.1016/j.physa.2004.09.033},
  abstract = {We investigate a regularity in market order submission strategies for 12 stocks with large market capitalization on the Australian Stock Exchange. The regularity is evidenced by a predictable relationship between the trade sign (trade initiator), size of the trade, and the contents of the limit order book before the trade. We demonstrate this predictability by developing an empirical inference model to classify trades into buyer-initiated and sellerinitiated. The model employs a local non-parametric method, k-nearest neighbor, which in the past was used successfully for chaotic time series prediction. The k-nearest neighbor with three predictor variables achieves an average out-of-sample classification accuracy of 71.40\%, compared to 63.32\% for the linear logistic regression with seven predictor variables. The result suggests that a non-linear approach may produce a more parsimonious trade sign inference model with a higher out-of-sample classification accuracy. Furthermore, for most of our stocks the observed regularity in market order submissions seems to have a memory of at least 30 trading days.},
  langid = {english},
  keywords = {üíé},
  annotation = {5 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Blazejewski and Coggins - 2005 - A local non-parametric model for trade sign infere.pdf}
}

@misc{boardofgovernorsofthefederalreservesystemus1YearTreasuryBill1959,
  title = {1-Year Treasury Bill Secondary Market Rate},
  shorttitle = {{{DTB1YR}}},
  author = {{Board of Governors of the Federal Reserve System (US)}},
  year = {1959},
  month = jul,
  journal = {FRED, Federal Reserve Bank of St. Louis},
  publisher = {{FRED, Federal Reserve Bank of St. Louis}},
  abstract = {Discount Basis},
  howpublished = {https://fred.stlouisfed.org/series/DTB1YR}
}

@misc{borisovDeepNeuralNetworks2022,
  title = {Deep {{Neural Networks}} and {{Tabular Data}}: {{A Survey}}},
  shorttitle = {Deep {{Neural Networks}} and {{Tabular Data}}},
  author = {Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2022},
  number = {arXiv:2110.01889},
  eprint = {2110.01889},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Borisov et al. - 2022 - Deep Neural Networks and Tabular Data A Survey.pdf}
}

@article{boweNewClassicalBayesian,
  title = {New {{Classical}} and {{Bayesian Estimators}} for {{Classifying Trade Direction}} in the {{Absence}} of {{Quotes}}},
  author = {Bowe, Michael and Cho, Sungjun and Hyde, Stuart and Sung, Iljin},
  year = {2018},
  pages = {78},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\UBVF223Y\\Bowe et al. - New Classical and Bayesian Estimators for Classify.pdf}
}

@article{breedenPricesStateContingentClaims1978,
  title = {Prices of {{State-Contingent Claims Implicit}} in {{Option Prices}}},
  author = {Breeden, Douglas T. and Litzenberger, Robert H.},
  year = {1978},
  journal = {The Journal of Business},
  volume = {51},
  number = {4},
  pages = {621},
  doi = {10.1086/296025},
  langid = {english},
  annotation = {1435 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\JU2TCMJU\\Breeden and Litzenberger - 1978 - Prices of State-Contingent Claims Implicit in Opti.pdf}
}

@article{breimanBaggingPredictors1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = {1996},
  month = aug,
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  doi = {10.1007/BF00058655},
  abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  langid = {english},
  keywords = {üíé},
  annotation = {10776 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Breiman - 1996 - Bagging predictors.pdf}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {1984},
  edition = {First},
  publisher = {{CLC Press}},
  address = {{Boca Raton, FL}},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Breiman et al. - 2017 - Classification And Regression Trees2.pdf}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  doi = {10.1023/A:1010933404324},
  keywords = {üíé},
  annotation = {56565 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Breiman - 2001 - Random Forests.pdf}
}

@misc{breuelEffectsHyperparametersSGD2015,
  title = {The {{Effects}} of {{Hyperparameters}} on {{SGD Training}} of {{Neural Networks}}},
  author = {Breuel, Thomas M.},
  year = {2015},
  month = aug,
  number = {arXiv:1508.02788},
  eprint = {1508.02788},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {The performance of neural network classifiers is determined by a number of hyperparameters, including learning rate, batch size, and depth. A number of attempts have been made to explore these parameters in the literature, and at times, to develop methods for optimizing them. However, exploration of parameter spaces has often been limited. In this note, I report the results of large scale experiments exploring these different parameters and their interactions.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\G8RSPV6D\\Breuel - 2015 - The Effects of Hyperparameters on SGD Training of .pdf;C\:\\Users\\Markus\\Zotero\\storage\\MWTK328K\\1508.html}
}

@article{breunigLOFIdentifyingDensitybased,
  title = {{{LOF}}: Identifying Density-Based Local Outliers},
  author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  pages = {12},
  abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using realworld datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
  langid = {english},
  keywords = {‚ùì Multiple DOI},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Breunig et al. - LOF Identifying Density-Based Local Outliers.pdf}
}

@article{burkeHybridRecommenderSystems2002,
  title = {Hybrid Recommender Systems: Survey and Experiments\textdagger},
  author = {Burke, Robin},
  year = {2002},
  journal = {User Modeling and User-Adapted Interaction},
  volume = {12},
  number = {4},
  pages = {331--370},
  issn = {09241868},
  doi = {10.1023/A:1021240730564},
  abstract = {Recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. They have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences. A variety of techniques have been proposed for performing recommendation, including content-based, collaborative, knowledge-based and other techniques. To improve performance, these methods have sometimes been combined in hybrid recommenders. This paper surveys the landscape of actual and possible hybrid recommenders, and introduces a novel hybrid, EntreeC, a system that combines knowledge-based recommendation and collaborative filtering to recommend restaurants. Further, we show that semantic ratings obtained from the knowledge-based part of the system enhance the effectiveness of collaborative filtering.},
  langid = {english},
  annotation = {2074 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Burke - 2002 - Hybrid Recommender Systems Survey and Experiments.pdf}
}

@article{busariCrudeOilPrice2021,
  title = {Crude Oil Price Prediction: {{A}} Comparison between {{AdaBoost-LSTM}} and {{AdaBoost-GRU}} for Improving Forecasting Performance},
  shorttitle = {Crude Oil Price Prediction},
  author = {Busari, Ganiyu Adewale and Lim, Dong Hoon},
  year = {2021},
  month = dec,
  journal = {Computers \& Chemical Engineering},
  volume = {155},
  pages = {107513},
  issn = {00981354},
  doi = {10.1016/j.compchemeng.2021.107513},
  langid = {english},
  annotation = {17 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Busari and Lim - 2021 - Crude oil price prediction A comparison between A.pdf}
}

@article{butcherFeatureEngineeringSelection2020,
  title = {Feature {{Engineering}} and {{Selection}}: {{A Practical Approach}} for {{Predictive Models}}: By {{Max Kuhn}} and {{Kjell Johnson}}. {{Boca Raton}}, {{FL}}: {{Chapman}} \& {{Hall}}/{{CRC Press}}, 2019, Xv + 297 Pp., \$79.95({{H}}), {{ISBN}}: 978-1-13-807922-9.},
  shorttitle = {Feature {{Engineering}} and {{Selection}}},
  author = {Butcher, Brandon and Smith, Brian J.},
  year = {2020},
  month = jul,
  journal = {The American Statistician},
  volume = {74},
  number = {3},
  pages = {308--309},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2020.1790217},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Butcher and Smith - 2020 - Feature Engineering and Selection A Practical App.pdf;C\:\\Users\\Markus\\Zotero\\storage\\WIWDCH5M\\Butcher and Smith - 2020 - Feature Engineering and Selection A Practical App.pdf}
}

@article{carrionTradeSigningFast2020,
  title = {Trade Signing in Fast Markets},
  author = {Carrion, Allen and Kolay, Madhuparna},
  year = {2020},
  month = aug,
  journal = {Financial Review},
  volume = {55},
  number = {3},
  pages = {385--404},
  issn = {0732-8516, 1540-6288},
  doi = {10.1111/fire.12218},
  langid = {english},
  keywords = {üíé},
  annotation = {2 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Carrion and Kolay - 2020 - Trade signing in fast markets.pdf}
}

@misc{caruanaObtainingCalibratedProbabilities,
  title = {Obtaining {{Calibrated Probabilities}} from {{Boosting}}},
  author = {Caruana, Alexandru Niculescu-Mizil Rich},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RA8HXVK6\\Caruana - Obtaining Calibrated Probabilities from Boosting.pdf}
}

@article{chakrabartyEvaluatingTradeClassification2015,
  title = {Evaluating {{Trade Classification Algorithms}}: {{Bulk Volume Classification Versus}} the {{Tick Rule}} and the {{Lee-Ready Algorithm}}},
  shorttitle = {Evaluating {{Trade Classification Algorithms}}},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = {2015},
  month = sep,
  journal = {Journal of Financial Markets},
  volume = {25},
  pages = {52--79},
  doi = {10.1016/j.finmar.2015.06.001},
  langid = {english},
  keywords = {üíé},
  annotation = {31 citations (Crossref) [2022-10-23]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chakrabarty et al. - 2015 - Evaluating trade classification algorithms Bulk v.pdf}
}

@article{chakrabartyEvaluatingTradeClassification2015a,
  title = {Evaluating {{Trade Classification Algorithms}}: {{Bulk Volume Classification Versus}} the {{Tick Rule}} and the {{Lee-Ready Algorithm}}},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = {2015},
  journal = {Journal of Financial Markets},
  volume = {25},
  pages = {52--79},
  doi = {10.1016/j.finmar.2015.06.001},
  langid = {english},
  annotation = {32 citations (Crossref) [2022-11-07]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chakrabarty et al. - 2015 - Evaluating trade classification algorithms Bulk v2.pdf}
}

@article{chakrabartyTradeClassificationAlgorithms2007,
  title = {Trade {{Classification Algorithms}} for {{Electronic Communications Network Trades}}},
  author = {Chakrabarty, Bidisha and Li, Bingguang and Nguyen, Vanthuan and Van Ness, Robert A.},
  year = {2007},
  journal = {Journal of Banking \& Finance},
  volume = {31},
  number = {12},
  pages = {3806--3821},
  doi = {10.1016/j.jbankfin.2007.03.003},
  keywords = {üíé},
  annotation = {55 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chakrabarty et al. - 2007 - Trade classification algorithms for electronic com.pdf}
}

@article{chakrabartyTradeClassificationAlgorithms2012,
  title = {Trade {{Classification Algorithms}}: {{A Horse Race Between}} the {{Bulk-Based}} and the {{Tick-Based Rules}}},
  shorttitle = {Trade {{Classification Algorithms}}},
  author = {Chakrabarty, Bidisha and Pascual, Roberto and Shkilko, Andriy},
  year = {2012},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2182819},
  abstract = {We compare bulk-volume classification (BVC) proposed by Easley, Lopez de Prado, and O'Hara (2012b) to the traditional tick rule (TR) for a sample of equity trades executed on NASDAQ's INET platform. Applying BVC leads to substantial time savings when a researcher uses pre-compressed data like Bloomberg and to smaller time savings when a researcher uses TAQ. Notably, this efficiency comes at a significant loss of accuracy. Specifically, misclassification increases by 7.4 to 16.3 percentage points (or 46\% to 291\%) when switching from TR to BVC. Additionally, TR produces more accurate estimates of order imbalances and of order flow toxicity (VPIN).},
  langid = {english},
  keywords = {üíé},
  annotation = {12 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chakrabarty et al. - 2012 - Trade Classification Algorithms A Horse Race betw.pdf;C\:\\Users\\Markus\\Zotero\\storage\\5RWC7HYS\\Chakrabarty et al. - 2012 - Trade Classification Algorithms A Horse Race betw.pdf}
}

@article{chanAlgorithmicTrading,
  title = {Algorithmic Trading},
  author = {Chan, Ernie},
  pages = {225},
  doi = {10.1002/9781118676998},
  langid = {english},
  annotation = {20 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chan - Algorithmic Trading.pdf}
}

@misc{chanTransformersGeneralizeDifferently2022,
  title = {Transformers Generalize Differently from Information Stored in Context vs in Weights},
  author = {Chan, Stephanie C. Y. and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K. and Hill, Felix},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05675},
  eprint = {2210.05675},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Transformer models can use two fundamentally different kinds of information: information stored in weights during training, and information provided ``in-context'' at inference time. In this work, we show that transformers exhibit different inductive biases in how they represent and generalize from the information in these two sources. In particular, we characterize whether they generalize via parsimonious rules (rule-based generalization) or via direct comparison with observed examples (exemplar-based generalization). This is of important practical consequence, as it informs whether to encode information in weights or in context, depending on how we want models to use that information. In transformers trained on controlled stimuli, we find that generalization from weights is more rule-based whereas generalization from context is largely exemplar-based. In contrast, we find that in transformers pre-trained on natural language, in-context learning is significantly rule-based, with larger models showing more rule-basedness. We hypothesise that rule-based generalization from in-context information might be an emergent consequence of large-scale training on language, which has sparse rule-like structure. Using controlled stimuli, we verify that transformers pretrained on data containing sparse rule-like structure exhibit more rule-based generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chan et al. - 2022 - Transformers generalize differently from informati.pdf;C\:\\Users\\Markus\\Zotero\\storage\\VKBN7SEJ\\2210.html}
}

@inproceedings{chapelleSemiSupervisedClassificationLow2005,
  title = {Semi-{{Supervised Classification}} by {{Low Density Separation}}},
  booktitle = {Proceedings of the {{Tenth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chapelle, Olivier and Zien, Alexander},
  year = {2005},
  pages = {57--64},
  abstract = {We believe that the cluster assumption is key to successful semi-supervised learning. Based on this, we propose three semi-supervised algorithms: 1. deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM; 2. optimizing the Transductive SVM objective function, which places the decision boundary in low density regions, by gradient descent; 3. combining the first two to make maximum use of the cluster assumption. We compare with state of the art algorithms and demonstrate superior accuracy for the latter two methods.},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RMGB854L\\Chapelle and Zien - Semi-Supervised ClassiÔ¨Åcation by Low Density Separ.pdf}
}

@book{chapelleSemisupervisedLearning2006,
  title = {Semi-Supervised Learning},
  editor = {Chapelle, Olivier and Sch{\"o}lkopf, Bernhard and Zien, Alexander},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-03358-9},
  langid = {english},
  lccn = {Q325.75 .S42 2006},
  keywords = {Supervised learning (Machine learning)},
  annotation = {OCLC: ocm64898359},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\ANT43KIN\\Chapelle et al. - 2006 - Semi-supervised learning.pdf}
}

@article{chaumUntraceableElectronicMail1981,
  title = {Untraceable Electronic Mail, Return Addresses, and Digital Pseudonyms},
  author = {Chaum, David L.},
  year = {1981},
  month = feb,
  journal = {Communications of the ACM},
  volume = {24},
  number = {2},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/358549.358563},
  langid = {english},
  annotation = {2441 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chaum - 1981 - Untraceable electronic mail, return addresses, and.pdf;C\:\\Users\\Markus\\Zotero\\storage\\GDA79KWN\\Chaum - 1981 - Untraceable electronic mail, return addresses, and.pdf}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: Synthetic Minority over-Sampling Technique},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {16},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ``normal'' examples with only a small percentage of ``abnormal'' or ``interesting'' examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  langid = {english},
  annotation = {11111 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf}
}

@misc{chenDeepLearningAsset2021,
  title = {Deep Learning in Asset Pricing},
  author = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
  year = {2021},
  number = {1904.00745},
  eprint = {1904.00745},
  eprinttype = {arxiv},
  abstract = {We use deep neural networks to estimate an asset pricing model for individual stock returns that takes advantage of the vast amount of conditioning information, while keeping a fully flexible form and accounting for time-variation. The key innovations are to use the fundamental no-arbitrage condition as criterion function, to construct the most informative test assets with an adversarial approach and to extract the states of the economy from many macroeconomic time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in terms of Sharpe ratio, explained variation and pricing errors and identifies the key factors that drive asset prices.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Finance - Statistical Finance},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chen et al. - 2021 - Deep Learning in Asset Pricing.pdf;C\:\\Users\\Markus\\Zotero\\storage\\K3UQXG3C\\Chen et al. - Internet Appendix for Deep Learning in Asset Prici.pdf;C\:\\Users\\Markus\\Zotero\\storage\\XE9BJR66\\1904.html}
}

@article{chenSimulationVerificationDynamic,
  title = {Simulation and Verification of a Dynamic Online Auction},
  author = {Chen, Bo and Sadaoui, Samira},
  pages = {7},
  abstract = {This paper deals with the formal specification, simulation and model-checking verification of an agent-based online auction. The need to understand dynamic behavior in auctions is increasing with the popularization of Internet auctions. This provides a strong motivation for the simulation of these complex systems. Since the auction interaction protocol is not trivial, it is suitable to use formal methods to ensure its correct functioning. Therefore, we investigate on the applicability of well-established techniques and tools from distributed systems, such as the formal specification language LOTOS, to specify and verify properties of interaction protocols in multiagent systems, and simulate their behavior before the implementation.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chen und Sadaoui - Simulation and VeriÔ¨Åcation of a Dynamic Online Auc.pdf}
}

@article{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: A Scalable Tree Boosting System},
  shorttitle = {{{XGBoost}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {9874 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chen und Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@inproceedings{chenXGBoostScalableTree2016a,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {785--794},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;C\:\\Users\\Markus\\Zotero\\storage\\HS8CGG68\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@article{christianoDeepReinforcementLearning2017,
  title = {Deep Reinforcement Learning from Human Preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = {2017},
  month = jul,
  journal = {arXiv:1706.03741 [cs, stat]},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf;C\:\\Users\\Markus\\Zotero\\storage\\UYXT6YJA\\1706.html}
}

@article{chuanSuccessAdaBoostIts2021,
  title = {The Success of {{AdaBoost}} and Its Application in Portfolio Management},
  author = {Chuan, Yijian and Zhao, Chaoyi and He, Zhenrui and Wu, Lan},
  year = {2021},
  month = jun,
  journal = {International Journal of Financial Engineering},
  volume = {08},
  number = {02},
  pages = {2142001},
  issn = {2424-7863, 2424-7944},
  doi = {10.1142/S2424786321420019},
  abstract = {We develop a novel approach to explain why AdaBoost is a successful classifier. By introducing a measure of the influence of the noise points (ION) in the training data for the binary classification problem, we prove that there is a strong connection between the ION and the test error. We further identify that the ION of AdaBoost decreases as the iteration number or the complexity of the base learners increases. We confirm that it is impossible to obtain a consistent classifier without deep trees as the base learners of AdaBoost in some complicated situations. We apply AdaBoost in portfolio management via empirical studies in the Chinese market, which corroborates our theoretical propositions.},
  langid = {english},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\8ZHDPI5X\\Chuan et al. - 2021 - The success of AdaBoost and its application in por.pdf}
}

@misc{clarkELECTRAPretrainingText2020,
  title = {Electra: {{Pre-Training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  number = {arXiv:2003.10555},
  eprint = {2003.10555},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminat.pdf;C\:\\Users\\Markus\\Zotero\\storage\\BRLB5NP4\\2003.html}
}

@article{congDEEPSEQUENCEMODELING,
  title = {Deep {{Sequence Modeling}}: {{Development}} and {{Applications}} in {{Asset Pricing}}},
  author = {Cong, Lin William and Tang, Ke and Wang, Jingyuan and Zhang, Yang},
  pages = {22},
  abstract = {We predict asset returns and measure risk premiums using a prominent technique from artificial intelligence: deep sequence modeling. Because asset returns often exhibit sequential dependence that may not be effectively captured by conventional time- series models, sequence modeling offers a promising path with its data-driven approach and superior performance. In this paper, we first overview the development of deep sequence models, introduce their applications in asset pricing, and discuss their advantages and limitations. We then perform a comparative analysis of these methods using data on U.S. equities. We demonstrate how sequence modeling benefits investors in general through incorporating complex historical path dependence and that l ong- and s hort-term m emory based models tend to have the best out-of-sample performance.},
  langid = {english},
  keywords = {‚ùì Multiple DOI,stock-prediction},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\LAU8MMFZ\\Cong et al. - Deep Sequence Modeling Development and Applicatio.pdf}
}

@article{cowgillAlgorithmicFairnessEconomics,
  title = {Algorithmic Fairness and Economics},
  author = {Cowgill, Bo and Tucker, Catherine},
  pages = {31},
  abstract = {We develop an economic perspective on algorithmic fairness and the surrounding empirical, theoretical and policy issues. Our perspective draws from clear parallels between algorithms and issues in economics of discrimination, crime, personnel and technological innovation; as well as more subtle connections to environmental economics, product safety regulation, behavioral economics and economics of information. We highlight the distinction between biased algorithmic predictions and biased algorithmic objectives. We conclude by discussing economic issues in policy policy and managerial practices around reducing algorithmic unfairness.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\CILJ8XK6\\Cowgill und Tucker - Algorithmic Fairness and Economics.pdf}
}

@article{coxOptionPricingSimplified1979,
  title = {Option Pricing: A Simplified Approach},
  author = {Cox, John C and Ross, A},
  year = {1979},
  pages = {35},
  doi = {10.1016/0304-405X(79)90015-1},
  langid = {english},
  annotation = {3490 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Cox und Ross - 1979 - Option pricing a simplified approach.pdf}
}

@article{coxRelationForwardPrices1981,
  title = {The Relation between Forward Prices and Futures Prices},
  author = {Cox, John C.},
  year = {1981},
  journal = {Journal of Financial Economics},
  pages = {321--346},
  doi = {10.1016/0304-405X(81)90002-7},
  abstract = {This paper consolidate \textbackslash{} the result \textbackslash{} of home recent worh on the rclatlon between forward prices and futures prices. It develops a number of propositIons characterizlnp the two prices. These propositions contam several testable implications about the difference between forward and futures prices. Many of the propositions show that equilibrium forward and futures prices are equal to the values of particular assets, even though they are not in themselves asset prices. The paper then illustrates these results in the context of two valuation models and discusses the effects of taxes and other institutional factors. 1.},
  annotation = {496 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\K57IEQIF\\Cox - 1981 - The Relation Between Forward Prices and Futures Pr.pdf;C\:\\Users\\Markus\\Zotero\\storage\\WM2RX3ST\\download.html}
}

@article{creamerAutomatedTradingBoosting2010,
  title = {Automated Trading with Boosting and Expert Weighting},
  author = {Creamer, Germ{\'a}n and Freund, Yoav},
  year = {2010},
  month = apr,
  journal = {Quantitative Finance},
  volume = {10},
  number = {4},
  pages = {401--420},
  issn = {1469-7688, 1469-7696},
  doi = {10.1080/14697680903104113},
  langid = {english},
  annotation = {37 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Creamer and Freund - 2010 - Automated trading with boosting and expert weighti.pdf}
}

@article{crspDATADESCRIPTIONSGUIDE,
  title = {Data {{Descriptions Guide Crsp Us Stock}} \& {{Us Index Databases}}},
  author = {{CRSP}},
  pages = {130},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\93JDMHTU\\Adams - CRSP CONTACT INFORMATION.pdf}
}

@misc{culurcielloFallRNNLSTM2019,
  title = {The Fall of {{RNN}} / {{LSTM}}},
  author = {Culurciello, Eugenio},
  year = {2019},
  month = jan,
  journal = {Medium},
  abstract = {We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. Now it is time to drop them!},
  howpublished = {https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\BMFPZ9XI\\the-fall-of-rnn-lstm-2d1594c74ce0.html}
}

@misc{culurcielloMemoryAttentionSequences2018,
  title = {Memory, Attention, Sequences},
  author = {Culurciello, Eugenio},
  year = {2018},
  month = jun,
  journal = {Medium},
  abstract = {We have seen the rise and success of categorization neural networks. The next big step in neural network is to make sense of complex\ldots},
  howpublished = {https://towardsdatascience.com/memory-attention-sequences-37456d271992},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\PBLC9WYZ\\memory-attention-sequences-37456d271992.html}
}

@article{davisGradientBoostingQuantitative2019,
  title = {Gradient Boosting for Quantitative Finance},
  author = {Davis, Jesse and Devos, Laurens and Reyners, Sofie and Schoutens, Wim},
  year = {2019},
  journal = {The Journal of Computational Finance},
  volume = {24},
  number = {4},
  pages = {4},
  doi = {10.21314/JCF.2020.403},
  abstract = {In this paper, we discuss how tree-based machine learning techniques can be used in the context of derivatives pricing. Gradient boosted regression trees are employed to learn the pricing map for a couple of classical, time-consuming problems in quantitative finance. In particular, we illustrate this methodology by reducing computation times for pricing exotic derivative products and American options. Once the gradient boosting model is trained, it is used to make fast predictions of new prices. We show that this approach leads to speed-ups of several orders of magnitude, while the loss of accuracy is very acceptable from a practical point of view. Besides the predictive performance of machine learning methods, financial regulators attach more and more importance to the interpretability of pricing models. For both applications, we therefore look under the hood of the gradient boosting model and try to reveal how the price is constructed and interpreted.},
  langid = {english},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\HPCLLYAX\\Davis et al. - 2021 - Gradient boosting for quantitative finance.pdf}
}

@article{deisenrothMathematicsMachineLearning,
  title = {Mathematics for Machine Learning},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  pages = {417},
  doi = {10.1017/9781108679930},
  langid = {english},
  annotation = {89 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Deisenroth et al. - Mathematics for Machine Learning.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {Bert: {{Pre-Training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  volume = {1},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, MN}},
  doi = {10.18653/v1/N19-1423},
  keywords = {üíé},
  annotation = {899 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{dhondtArtificialIntelligenceAlter2020,
  title = {Artificial {{Intelligence Alter Egos}}: {{Who}} Might Benefit from Robo-Investing?},
  shorttitle = {Artificial {{Intelligence Alter Egos}}},
  author = {D'Hondt, Catherine and De Winne, Rudy and Ghysels, Eric and Raymond, Steve},
  year = {2020},
  month = dec,
  journal = {Journal of Empirical Finance},
  volume = {59},
  pages = {278--299},
  issn = {09275398},
  doi = {10.1016/j.jempfin.2020.10.002},
  abstract = {We use a unique data set covering brokerage accounts for a large cross-section of investors over a sample from January 2003 to March 2012, which includes the 2008 financial crisis, to assess the potential benefits of robo-investing. We explore robo-investing strategies commonly used in the industry, including some involving advanced machine learning methods. We shadow each of our investors with a robo-advisor to shed light on possible benefits the emerging robo-advising industry may provide to certain segments of the population, such as low income and/or low education investors.},
  langid = {english},
  annotation = {10 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\D‚ÄôHondt et al. - 2020 - Artificial Intelligence Alter Egos Who might bene.pdf;C\:\\Users\\Markus\\Zotero\\storage\\6IYPQWQY\\D‚ÄôHondt et al. - 2020 - Artificial Intelligence Alter Egos Who might bene.pdf}
}

@article{dieboldComparingPredictiveAccuracy1995,
  title = {Comparing {{Predictive Accuracy}}},
  author = {Diebold, Francis X. and Mariano, Roberto S.},
  year = {1995},
  journal = {Journal of Business \& Economic Statistics},
  volume = {13},
  number = {3},
  pages = {253--263},
  doi = {10.1080/07350015.1995.10524599},
  langid = {english},
  keywords = {evaluation},
  annotation = {380 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Diebold and Mariano - 1995 - Comparing Predictive Accuracy.pdf;C\:\\Users\\Markus\\Zotero\\storage\\YIPKW7LD\\Diebold and Mariano - 1995 - Comparing Predictive Accuracy.pdf}
}

@article{easleyDiscerningInformationTrade2016,
  title = {Discerning {{Information}} from {{Trade Data}}},
  author = {Easley, David and {de Prado}, Marcos Lopez and O'Hara, Maureen},
  year = {2016},
  journal = {Journal of Financial Economics},
  volume = {120},
  number = {2},
  pages = {269--285},
  doi = {10.1016/j.jfineco.2016.01.018},
  langid = {english},
  keywords = {üíé},
  annotation = {53 citations (Crossref) [2022-11-07]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Easley et al. - 2016 - Discerning information from trade data.pdf;C\:\\Users\\Markus\\Zotero\\storage\\88X3XTV7\\SSRN-id1989555.pdf}
}

@article{easleyFlowToxicityLiquidity2012,
  title = {Flow {{Toxicity}} and {{Liquidity}} in a {{High-Frequency World}}},
  author = {Easley, David and {L{\'o}pez de Prado}, Marcos M. and O'Hara, Maureen},
  year = {2012},
  month = may,
  journal = {Review of Financial Studies},
  volume = {25},
  number = {5},
  pages = {1457--1493},
  issn = {0893-9454, 1465-7368},
  doi = {10.1093/rfs/hhs053},
  langid = {english},
  keywords = {üíé},
  annotation = {256 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Easley et al. - 2012 - Flow Toxicity and Liquidity in a High-frequency Wo.pdf}
}

@article{easleyOptionVolumeStock1998,
  title = {Option {{Volume}} and {{Stock Prices}}: {{Evidence}} on {{Where Informed Traders Trade}}},
  shorttitle = {Option {{Volume}} and {{Stock Prices}}},
  author = {Easley, David and O'Hara, Maureen and Srinivas, P.S.},
  year = {1998},
  journal = {The Journal of Finance},
  volume = {53},
  number = {2},
  pages = {431--465},
  doi = {10.1111/0022-1082.194060},
  keywords = {üíé},
  annotation = {695 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Easley et al. - 1998 - Option Volume and Stock Prices Evidence on Where .pdf}
}

@article{ellisAccuracyTradeClassification2000,
  title = {The {{Accuracy}} of {{Trade Classification Rules}}: {{Evidence}} from {{Nasdaq}}},
  author = {Ellis, Katrina and Michaely, Roni and O'Hara, Maureen},
  year = {2000},
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {35},
  number = {4},
  pages = {529--551},
  doi = {10.2307/2676254},
  keywords = {üíé},
  annotation = {301 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ellis et al. - 2000 - The Accuracy of Trade Classification Rules Eviden.pdf}
}

@book{falkPracticalRecommenderSystems2019,
  title = {Practical Recommender Systems},
  author = {Falk, Kim},
  year = {2019},
  publisher = {{Manning}},
  address = {{Shelter Island, NY}},
  abstract = {Online recommender systems help users find movies, jobs, restaurants, even romance! There's an art in combining statistics, demographics, and query terms to achieve results that will delight them. Learn to build a recommender system the right way : it can make or break your application! "Practical recommender systems" explains how recommender systems work and shows how to create and apply them for your site. After covering the basics, you'll see how to collect user data and produce personalized recommendations. You'll learn how to use the most popular recommendation algorithms and see examples of them in action on sites like Amazon and Netflix. Finally, the book covers scaling problems and other issues you'll encounter as your site grows},
  isbn = {978-1-61729-270-5},
  lccn = {QA76.9.I58 F34 2019},
  annotation = {OCLC: ocn970761259},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Falk - 2019 - Practical recommender systems.pdf}
}

@article{famaCAPMWantedDead1996,
  title = {The {{CAPM}} Is Wanted, Dead or Alive},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = {1996},
  month = dec,
  journal = {The Journal of Finance},
  volume = {51},
  number = {5},
  pages = {1947--1958},
  issn = {00221082},
  doi = {10.1111/j.1540-6261.1996.tb05233.x},
  langid = {english},
  annotation = {131 citations (Crossref) [2022-10-18]}
}

@article{famaCommonRiskFactors1993,
  title = {Common Risk Factors in the Returns on Stocks and Bonds},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = {1993},
  month = feb,
  journal = {Journal of Financial Economics},
  volume = {33},
  number = {1},
  pages = {3--56},
  issn = {0304405X},
  doi = {10.1016/0304-405X(93)90023-5},
  langid = {english},
  annotation = {14470 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\7QEMNRBY\\Fama und French - 1993 - Common risk factors in the returns on stocks and b.pdf}
}

@article{famaFivefactorAssetPricing2015,
  title = {A Five-Factor Asset Pricing Model},
  author = {Fama, Eugene F. and French, Kenneth R.},
  year = {2015},
  month = apr,
  journal = {Journal of Financial Economics},
  volume = {116},
  number = {1},
  pages = {1--22},
  issn = {0304405X},
  doi = {10.1016/j.jfineco.2014.10.010},
  abstract = {A five-factor model directed at capturing the size, value, profitability, and investment patterns in average stock returns performs better than the three-factor model of Fama and French (FF, 1993). The five-factor model's main problem is its failure to capture the low average returns on small stocks whose returns behave like those of firms that invest a lot despite low profitability. The model's performance is not sensitive to the way its factors are defined. With the addition of profitability and investment factors, the value factor of the FF three-factor model becomes redundant for describing average returns in the sample we examine.},
  langid = {english},
  annotation = {2878 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\44LQ4BSN\\Fama und French - 2015 - A five-factor asset pricing model.pdf}
}

@article{fardRecommenderSystemBased2013,
  title = {Recommender System Based on Semantic Similarity},
  author = {Fard, Karamollah Bagheri and Nilashi, Mehrbakhsh and Rahmani, Mohsen and Ibrahim, Othman},
  year = {2013},
  volume = {3},
  number = {6},
  pages = {11},
  abstract = {In electronic commerce, in order to help users to find their favourite products, we essentially need a system to classify the products based on the user's interests and needs to recommend them to the users. For the same reason the recommendation systems are designed to help finding information in large websites. They are basically developed to offer products to the customers in an automated fashion to help them to do conveniently their shopping. The developing of such systems is important since there are often a large number of factors involved in purchasing a product that would make it difficult for the customer to make the best decision. Finding relationship among users and relationships among products are important issue in these systems. One of relations is similarity. Measure similarity among users and products is used in the pure methods for calculating similarity degree. In this paper, semantic similarity is used to find a set of k nearest neighbours to the target user, or target item. Thus, because of incorporating semantic similarity in the proposed recommendation system, from the experimental results, the high accuracy was obtained on private building company dataset in comparison with state-of-the-art recommender systems.},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Fard et al. - 2013 - Recommender System Based on Semantic Similarity.pdf}
}

@article{fawziDiscoveringFasterMatrix2022,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and {Romera-Paredes}, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  year = {2022},
  month = oct,
  journal = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-022-05172-4},
  abstract = {Abstract                            Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\textemdash from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero               1               for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\,\texttimes\,4 matrices in a finite field, where AlphaTensor's algorithm improves on Strassen's two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago               2               . We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor's ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
  langid = {english},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\MZTJRUYR\\Fawzi et al. - 2022 - Discovering faster matrix multiplication algorithm.pdf}
}

@misc{FeatureEngineeringHandling00:00:00-05:00,
  title = {Feature Engineering - Handling Cyclical Features},
  year = {00:00:00-05:00},
  journal = {From Neutrinos to Data Science},
  abstract = {SOHCAHTOA amirite?},
  howpublished = {http://blog.davidkaleko.com/feature-engineering-cyclical-features.html},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\YN9HARR7\\feature-engineering-cyclical-features.html}
}

@article{fedeniaMachineLearningCorporate2021,
  title = {Machine {{Learning}} in the {{Corporate Bond Market}} and {{Beyond}}: {{A New Classifier}}},
  author = {Fedenia, Mark A. and Nam, Seunghan and Ronen, Tavy},
  year = {2021},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3848068},
  langid = {english},
  keywords = {üíé},
  annotation = {0 citations (Crossref)},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\HVXHFLIB\\Fedenia et al. - 2021 - Machine Learning in the Corporate Bond Market and .pdf}
}

@misc{federalreservebankofstlouisNBERBasedRecession2022,
  title = {{{NBER}} Based {{Recession Indicators}} for the {{United States}} from the {{Period}} Following the {{Peak}} through the {{Trough}} [{{USREC}}]},
  shorttitle = {{{USREC}}},
  author = {{Federal Reserve Bank of St. Louis}},
  year = {2022},
  month = jul,
  journal = {FRED, Federal Reserve Bank of St. Louis},
  abstract = {This time series is an interpretation of US Business Cycle Expansions and Contractions data provided by The National Bureau of Economic Research (http://www.nber.org/cycles/cyclesmain.html) (NBER). Our time series is composed of dummy variables that represent periods of expansion and recession. The NBER identifies months and quarters of turning points without designating a date within the period that turning points occurred. The dummy variable adopts an arbitrary convention that the turning point occurred at a specific date within the period. The arbitrary convention does not reflect any judgment on this issue by the NBER's Business Cycle Dating Committee. A value of 1 is a recessionary period, while a value of 0 is an expansionary period. For this time series, the recession begins the first day of the period following a peak and ends on the last day of the period of the trough. For more options on recession shading, see the notes and links below. The recession shading data that we provide initially comes from the source as a list of dates that are either an economic peak or trough. We interpret dates into recession shading data using one of three arbitrary methods. All of our recession shading data is available using all three interpretations. The period between a peak and trough is always shaded as a recession. The peak and trough are collectively extrema. Depending on the application, the extrema, both individually and collectively, may be included in the recession period in whole or in part. In situations where a portion of a period is included in the recession, the whole period is deemed to be included in the recession period. The first interpretation, known as the midpoint method, is to show a recession from the midpoint of the peak through the midpoint of the trough for monthly and quarterly data. For daily data, the recession begins on the 15th of the month of the peak and ends on the 15th of the month of the trough. Daily data is a disaggregation of monthly data. For monthly and quarterly data, the entire peak and trough periods are included in the recession shading. This method shows the maximum number of periods as a recession for monthly and quarterly data. The Federal Reserve Bank of St. Louis uses this method in its own publications. One version of this time series is represented using the midpoint method (https://fred.stlouisfed.org/series/USRECM) The second interpretation, known as the trough method, is to show a recession from the period following the peak through the trough (i.e. the peak is not included in the recession shading, but the trough is). For daily data, the recession begins on the first day of the first month following the peak and ends on the last day of the month of the trough. Daily data is a disaggregation of monthly data. The trough method is used when displaying data on FRED graphs. The trough method is used for this series. The third interpretation, known as the peak method, is to show a recession from the period of the peak to the trough (i.e. the peak is included in the recession shading, but the trough is not). For daily data, the recession begins on the first day of the month of the peak and ends on the last day of the month preceding the trough. Daily data is a disaggregation of monthly data. Here is an example of this time series represented using the peak method (https://fred.stlouisfed.org/series/USRECP).},
  howpublished = {https://fred.stlouisfed.org/series/USREC},
  langid = {american}
}

@article{fengDeepLearningPredicting2018,
  title = {Deep Learning for Predicting Asset Returns},
  author = {Feng, Guanhao and He, Jingyu and Polson, Nicholas G.},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.09314},
  eprint = {1804.09314},
  eprinttype = {arxiv},
  abstract = {Deep learning searches for nonlinear factors for predicting asset returns. Predictability is achieved via multiple layers of composite factors as opposed to additive ones. Viewed in this way, asset pricing studies can be revisited using multi-layer deep learners, such as rectified linear units (ReLU) or long-short-term-memory (LSTM) for time-series effects. State-of-the-art algorithms including stochastic gradient descent (SGD), TensorFlow and dropout design provide imple- mentation and efficient factor exploration. To illustrate our methodology, we revisit the equity market risk premium dataset of Welch and Goyal (2008). We find the existence of nonlinear factors which explain predictability of returns, in particular at the extremes of the characteristic space. Finally, we conclude with directions for future research.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning,Economics - Econometrics},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Feng et al. - 2018 - Deep Learning for Predicting Asset Returns.pdf;C\:\\Users\\Markus\\Zotero\\storage\\XA7LADRN\\1804.html}
}

@misc{fiedlerSimpleModificationsImprove2021,
  title = {Simple {{Modifications}} to {{Improve Tabular Neural Networks}}},
  author = {Fiedler, James},
  year = {2021},
  month = aug,
  number = {arXiv:2108.03214},
  eprint = {2108.03214},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\LTL6RA8D\\Fiedler - 2021 - Simple Modifications to Improve Tabular Neural Net.pdf;C\:\\Users\\Markus\\Zotero\\storage\\XU3ZETPV\\2108.html}
}

@article{finucaneDirectTestMethods2000,
  title = {A {{Direct Test}} of {{Methods}} for {{Inferring Trade Direction}} from {{Intra-Day Data}}},
  author = {Finucane, Thomas J.},
  year = {2000},
  journal = {The Journal of Financial and Quantitative Analysis},
  volume = {35},
  number = {4},
  pages = {553--576},
  doi = {10.2307/2676255},
  keywords = {üíé},
  annotation = {174 citations (Semantic Scholar/DOI) [2022-10-24] 115 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Finucane - 2000 - A Direct Test of Methods for Inferring Trade Direc.pdf}
}

@article{frazziniBettingBeta2014,
  title = {Betting against Beta},
  author = {Frazzini, Andrea and Pedersen, Lasse Heje},
  year = {2014},
  month = jan,
  journal = {Journal of Financial Economics},
  volume = {111},
  number = {1},
  pages = {1--25},
  issn = {0304405X},
  doi = {10.1016/j.jfineco.2013.10.005},
  abstract = {We present a model with leverage and margin constraints that vary across investors and time. We find evidence consistent with each of the model's five central predictions: (1) Because constrained investors bid up high-beta assets, high beta is associated with low alpha, as we find empirically for US equities, 20 international equity markets, Treasury bonds, corporate bonds, and futures. (2) A betting against beta (BAB) factor, which is long leveraged low-beta assets and short high-beta assets, produces significant positive riskadjusted returns. (3) When funding constraints tighten, the return of the BAB factor is low. (4) Increased funding liquidity risk compresses betas toward one. (5) More constrained investors hold riskier assets.},
  langid = {english},
  annotation = {991 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Frazzini und Pedersen - 2014 - Betting against beta.pdf}
}

@article{freybergerDissectingCharacteristicsNonparametrically,
  title = {Dissecting Characteristics Nonparametrically},
  author = {Freyberger, Joachim and Neuhierl, Andreas and Weber, Michael},
  year = {2020},
  journal = {The Review of Financial Studies},
  volume = {5},
  number = {33},
  pages = {2326--2377},
  doi = {10.1093/rfs/hhz123},
  abstract = {We propose a nonparametric method to study which characteristics provide incremental information for the cross-section of expected returns. We use the adaptive group LASSO to select characteristics and to estimate how selected characteristics affect expected returns nonparametrically. Our method can handle a large number of characteristics and allows for a flexible functional form. Our implementation is insensitive to outliers. Many of the previously identified return predictors don't provide incremental information for expected returns, and nonlinearities are important. We study our method's properties in simulations and find large improvements in both model selection and prediction compared to alternative selection methods.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
  langid = {english},
  annotation = {110 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Freyberger et al. - 2020 - Dissecting Characteristics Nonparametrically.pdf}
}

@article{friedmanAdditiveLogisticRegression2000,
  title = {Additive {{Logistic Regression}}: {{A Statistical View}} of {{Boosting}} (with {{Discussion}} and a {{Rejoinder}} by the {{Authors}})},
  shorttitle = {Additive {{Logistic Regression}}},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2000},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {28},
  number = {2},
  doi = {10.1214/aos/1016218223},
  keywords = {üíé},
  annotation = {3848 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Friedman et al. - 2000 - Additive logistic regression a statistical view o.pdf;C\:\\Users\\Markus\\Zotero\\storage\\RY269YXQ\\alr.pdf}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  shorttitle = {Greedy Function Approximation},
  author = {Friedman, Jerome H.},
  year = {2001},
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  doi = {10.1214/aos/1013203451},
  keywords = {üíé},
  annotation = {9103 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Friedman - 2001 - Greedy function approximation A gradient boosting.pdf}
}

@misc{gabrielDynamicPricingUsing2021,
  title = {Dynamic Pricing Using Reinforcement Learning and Neural Networks},
  author = {Gabriel, Reslley},
  year = {2021},
  month = feb,
  journal = {Medium},
  abstract = {An intelligent system that can increase e-commerce sales and profits},
  howpublished = {https://towardsdatascience.com/dynamic-pricing-using-reinforcement-learning-and-neural-networks-cc3abe374bf5},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\64VT3HF9\\dynamic-pricing-using-reinforcement-learning-and-neural-networks-cc3abe374bf5.html}
}

@book{gallianContemporaryAbstractAlgebra2021,
  title = {Contemporary Abstract Algebra},
  author = {Gallian, Joseph},
  editor = {Gallian, Joseph A.},
  year = {2021},
  month = jan,
  edition = {Tenth},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781003142331},
  isbn = {978-1-00-314233-1},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Gallian - 2021 - Contemporary Abstract Algebra.pdf;C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Gallian - 2021 - Contemporary abstract algebra2.pdf}
}

@article{garleanuDemandBasedOptionPricing2009,
  title = {Demand-{{Based Option Pricing}}},
  author = {G{\^a}rleanu, Nicolae and Pedersen, Lasse Heje and Poteshman, Allen M.},
  year = {2009},
  month = oct,
  journal = {Review of Financial Studies},
  volume = {22},
  number = {10},
  pages = {4259--4299},
  doi = {10.1093/rfs/hhp005},
  langid = {english},
  keywords = {üíé},
  annotation = {463 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\G√¢rleanu et al. - 2009 - Demand-Based Option Pricing.pdf}
}

@article{glinzRethinkingNotionNonFunctional,
  title = {Rethinking the {{Notion}} of {{Non-Functional Requirements}}},
  author = {Glinz, Martin},
  pages = {10},
  abstract = {Requirements standards and textbooks typically classify requirements into functional requirements on the one hand and attributes or non-functional requirements on the other hand. In this classification, requirements given in terms of required operations and/or data are considered to be functional, while performance requirements and quality requirements (such as requirements about security, reliability, maintainability, etc.) are classified as nonfunctional.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RV9YGNSQ\\Glinz - Rethinking the Notion of Non-Functional Requiremen.pdf}
}

@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep Sparse Rectifier Neural Networks},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  volume = {15},
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  address = {{Fort Lauderdale, FL}},
  issn = {1938-7228},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf}
}

@inproceedings{golovinGoogleVizierService2017,
  title = {Google {{Vizier}}: {{A Service}} for {{Black-Box Optimization}}},
  shorttitle = {Google {{Vizier}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D.},
  year = {2017},
  month = aug,
  pages = {1487--1495},
  publisher = {{ACM}},
  address = {{Halifax NS Canada}},
  doi = {10.1145/3097983.3098043},
  abstract = {Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Google's Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Golovin et al. - 2017 - Google Vizier A Service for Black-Box Optimizatio.pdf;C\:\\Users\\Markus\\Zotero\\storage\\R7K8ISZH\\Golovin et al. - 2017 - Google Vizier A Service for Black-Box Optimizatio.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\2IYU7IVX\\Goodfellow et al. - 2016 - Deep learning.pdf}
}

@book{goossensLaTeXGraphicsCompanion2008,
  title = {The {{Latex Graphics Companion}}},
  editor = {Goossens, Michel},
  year = {2008},
  series = {Addison-{{Wesley}} Series on Tools and Techniques for Computer Typesetting},
  edition = {2nd ed},
  publisher = {{Addison-Wesley}},
  address = {{Upper Saddle River, NJ}},
  isbn = {978-0-321-50892-8},
  langid = {english},
  lccn = {Z253.4.L38 G663 2008},
  keywords = {Computer programs,Computerized typesetting,LaTeX (Computer file),Mathematics printing},
  annotation = {OCLC: ocm85892394},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\JXEX4ZSJ\\Goossens - 2008 - The LaTeX Graphics companion.pdf}
}

@misc{gorishniyEmbeddingsNumericalFeatures2022,
  title = {On {{Embeddings}} for {{Numerical Features}} in {{Tabular Deep Learning}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Babenko, Artem},
  year = {2022},
  number = {arXiv:2203.05556},
  eprint = {2203.05556},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Gorishniy et al. - 2022 - On Embeddings for Numerical Features in Tabular De.pdf;C\:\\Users\\Markus\\Zotero\\storage\\WPE2L8UH\\2203.html}
}

@inproceedings{gorishniyRevisitingDeepLearning2021,
  title = {Revisiting {{Deep Learning Models}} for {{Tabular Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = {2021},
  volume = {34},
  pages = {18932--18943},
  publisher = {{Curran Associates, Inc.}},
  address = {{Red Hook, NY}},
  abstract = {The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution. The source code is available at https://github.com/yandex-research/rtdl.},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Gorishniy et al. - 2021 - Revisiting Deep Learning Models for Tabular Data.pdf}
}

@misc{GradientBoostPart,
  title = {Gradient Boost Part 1 (of 4): Regression Main Ideas - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=3CC4N4z3GJc},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\Z2BTLAQD\\watch.html}
}

@article{grammigDivergingRoadsTheoryBased2020,
  title = {Diverging Roads: Theory-Based vs. Machine Learning-Implied Stock Risk Premia},
  author = {Grammig, Joachim and Hanenberg, Constantin and Schlag, Christian and S{\"o}nksen, Jantje},
  year = {2020},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.3536835},
  abstract = {We assess financial theory-based and machine learning methods to quantify stock risk premia and investigate the potential of hybrid strategies by comparing the quality of the respective excess return forecasts. In the low signal-to-noise environment of a one-month investment horizon, we recommend to rely on a theory-based strategy that exploits the information in current option prices, especially if the risk premium estimate is to be updated at a high frequency. At the one-year horizon, a random forest can improve on the theory-based method, provided that a sufficiently long training period is used. In an effort to connect the opposing philosophies, we identify the use of a random forest to account for the approximation errors of the theory-based approach towards measuring stock risk premia as a promising hybrid strategy. It combines the advantages of two diverging roads in the finance world.},
  langid = {english},
  keywords = {sem-stock-option},
  annotation = {1 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\JCNUHNG4\\Grammig et al. - 2020 - Diverging Roads Theory-Based vs. Machine Learning.pdf;C\:\\Users\\Markus\\Zotero\\storage\\N2JS682N\\ghss_online_appendix_publish.pdf}
}

@article{grauerOptionTradeClassification2022,
  title = {Option {{Trade Classification}}},
  author = {Grauer, Caroline and Schuster, Philipp and {Uhrig-Homburg}, Marliese},
  year = {2022},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4098475},
  abstract = {We evaluate the performance of common stock trade classification algorithms including the quote, tick, Lee and Ready (1991), and Ellis, Michaely, and O'Hara (2000) rule to infer the trade direction of option trades. Using a large sample of matched intraday transactions and Open/Close data, we show that the algorithms' success rate to correctly classify option trades is considerably lower than for stocks. In particular, the prevailing Lee and Ready algorithm is only able to correctly sign between 60\% to 64\% of option trades, which is a similar magnitude as using the quote rule alone. We find that the overall weak performance is due to sophisticated customers who often use limit orders instead of market orders to implement their trading strategies. We develop additional rules that can be used together with existing classification algorithms, improving correct classification by more than 10\%.},
  langid = {english},
  keywords = {üíé,options},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\6P59Q43F\\Grauer et al. - 2022 - Option Trade Classification.pdf}
}

@inproceedings{grinsztajnWhyTreebasedModels2022,
  title = {Why {{Do Tree-Based Models Still Outperform Deep Learning}} on {{Typical Tabular Data}}?},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = {2022},
  series = {{{NeurIPS}} 2022},
  volume = {36},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY}},
  abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ({$\sim$}10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and neural networks. This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural network: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\A3KU4A43\\Grinsztajn et al. - Why do tree-based models still outperform deep lea.pdf}
}

@article{guAutoencoderAssetPricing2021,
  title = {Autoencoder Asset Pricing Models},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  year = {2021},
  journal = {Journal of Econometrics},
  volume = {222},
  number = {1},
  pages = {429--450},
  doi = {10.1016/j.jeconom.2020.07.009},
  abstract = {We propose a new latent factor conditional asset pricing model. Like Kelly, Pruitt, and Su (KPS, 2019), our model allows for latent factors and factor exposures that depend on covariates such as asset characteristics. But, unlike the linearity assumption of KPS, we model factor exposures as a flexible nonlinear function of covariates. Our model retrofits the workhorse unsupervised dimension reduction device from the machine learning literature \textendash{} autoencoder neural networks \textendash{} to incorporate information from covariates along with returns themselves. This delivers estimates of nonlinear conditional exposures and the associated latent factors. Furthermore, our machine learning framework imposes the economic restriction of no-arbitrage. Our autoencoder asset pricing model delivers out-of-sample pricing errors that are far smaller (and generally insignificant) compared to other leading factor models.},
  langid = {english},
  annotation = {44 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\872WT5EZ\\Gu et al. - 2021 - Autoencoder asset pricing models.pdf}
}

@article{guEmpiricalAssetPricing2020,
  title = {Empirical Asset Pricing via Machine Learning},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  year = {2020},
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {5},
  pages = {2223--2273},
  doi = {10.1093/rfs/hhaa009},
  abstract = {Abstract             We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.             Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
  langid = {english},
  keywords = {üíé,sem-stock-option},
  annotation = {304 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Gu et al. - 2020 - Empirical Asset Pricing via Machine Learning.pdf;C\:\\Users\\Markus\\Zotero\\storage\\BUTK5JVG\\hhaa009_supplementary_data.pdf}
}

@article{gunnarssonDeepLearningCredit2021,
  title = {Deep Learning for Credit Scoring: Do or Don't?},
  shorttitle = {Deep Learning for Credit Scoring},
  author = {Gunnarsson, Bj{\"o}rn Rafn and {vanden Broucke}, Seppe and Baesens, Bart and {\'O}skarsd{\'o}ttir, Mar{\'i}a and Lemahieu, Wilfried},
  year = {2021},
  month = nov,
  journal = {European Journal of Operational Research},
  volume = {295},
  number = {1},
  pages = {292--305},
  issn = {03772217},
  doi = {10.1016/j.ejor.2021.03.006},
  langid = {english},
  annotation = {20 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\9KJNBUFK\\Gunnarsson et al. - 2021 - Deep learning for credit scoring Do or don‚Äôt.pdf}
}

@inproceedings{gyamerahStockMarketMovement2019,
  title = {On {{Stock Market Movement Prediction Via Stacking Ensemble Learning Method}}},
  booktitle = {2019 {{IEEE Conference}} on {{Computational Intelligence}} for {{Financial Engineering}} \& {{Economics}} ({{CIFEr}})},
  author = {Gyamerah, Samuel Asante and Ngare, Philip and Ikpe, Dennis},
  year = {2019},
  month = may,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Shenzhen, China}},
  doi = {10.1109/CIFEr.2019.8759062},
  isbn = {978-1-72810-033-3},
  annotation = {3 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Gyamerah et al. - 2019 - On Stock Market Movement Prediction Via Stacking E.pdf}
}

@inproceedings{hanAutoEncoderInspiredUnsupervised2018,
  title = {{{AutoEncoder}} Inspired Unsupervised Feature Selection},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Han, Kai and Wang, Yunhe and Zhang, Chao and Li, Chao and Xu, Chao},
  year = {2018},
  month = apr,
  eprint = {1710.08310},
  eprinttype = {arxiv},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8462261},
  abstract = {High-dimensional data in many areas such as computer vision and machine learning tasks brings in computational and analytical difficulty. Feature selection which selects a subset from observed features is a widely used approach for improving performance and effectiveness of machine learning models with high-dimensional data. In this paper, we propose a novel AutoEncoder Feature Selector (AEFS) for unsupervised feature selection which combines autoencoder regression and group lasso tasks. Compared to traditional feature selection methods, AEFS can select the most important features by excavating both linear and nonlinear information among features, which is more flexible than the conventional self-representation method for unsupervised feature selection with only linear assumptions. Experimental results on benchmark dataset show that the proposed method is superior to the state-of-the-art method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {41 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Han et al. - 2018 - AutoEncoder Inspired Unsupervised Feature Selectio.pdf;C\:\\Users\\Markus\\Zotero\\storage\\DNPYMSJJ\\1710.html}
}

@article{hansenApplicationsMachineLearning,
  title = {Applications of {{Machine Learning}} in {{High-Frequency Trade Direction Classification}}},
  author = {Hansen, Jared E},
  pages = {99},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\C74UETBP\\Hansen - Applications of Machine Learning in High-Frequency.pdf}
}

@article{harveyMultivariateStochasticVariance1994,
  title = {Multivariate Stochastic Variance Models},
  author = {Harvey, A. and Ruiz, E. and Shephard, N.},
  year = {1994},
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {61},
  number = {2},
  pages = {247--264},
  issn = {0034-6527, 1467-937X},
  doi = {10.2307/2297980},
  langid = {english},
  annotation = {736 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Harvey et al. - 1994 - Multivariate Stochastic Variance Models.pdf}
}

@article{harveyTestingEqualityPrediction1997,
  title = {Testing the {{Equality}} of {{Prediction Mean Squared Errors}}},
  author = {Harvey, David and Leybourne, Stephen and Newbold, Paul},
  year = {1997},
  month = jun,
  journal = {International Journal of Forecasting},
  volume = {13},
  number = {2},
  pages = {281--291},
  doi = {10.1016/S0169-2070(96)00719-4},
  langid = {english},
  annotation = {1018 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Harvey et al. - 1997 - Testing the equality of prediction mean squared er.pdf}
}

@article{hasbrouckTradesQuotesInventories1988,
  title = {Trades, {{Quotes}}, {{Inventories}}, and {{Information}}},
  author = {Hasbrouck, Joel},
  year = {1988},
  journal = {Journal of Financial Economics},
  volume = {22},
  number = {2},
  pages = {229--252},
  doi = {10.1016/0304-405X(88)90070-0},
  langid = {english},
  keywords = {üíé},
  annotation = {328 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\F954MPA7\\Hasbrouck - 1988 - Trades, quotes, inventories, and information.pdf}
}

@book{hastietrevorElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor, Sami and Friedman, Harry and Tibshirani, Robert},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer}},
  address = {{New York, NY}},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Tibshirani et al. - The Elements of Statistical Learning.pdf}
}

@misc{heBagTricksImage2018,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = {2018},
  month = dec,
  number = {arXiv:1812.01187},
  eprint = {1812.01187},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\He et al. - 2018 - Bag of Tricks for Image Classification with Convol.pdf;C\:\\Users\\Markus\\Zotero\\storage\\VFQECGHG\\1812.html}
}

@misc{hertzPrompttoPromptImageEditing2022,
  title = {Prompt-to-{{Prompt Image Editing}} with {{Cross Attention Control}}},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and {Cohen-Or}, Daniel},
  year = {2022},
  month = aug,
  number = {arXiv:2208.01626},
  eprint = {2208.01626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Hertz et al. - 2022 - Prompt-to-Prompt Image Editing with Cross Attentio.pdf;C\:\\Users\\Markus\\Zotero\\storage\\GX46DTWD\\2208.html}
}

@article{hidasiRecurrentNeuralNetworks2018,
  title = {Recurrent Neural Networks with Top-k Gains for Session-Based Recommendations},
  author = {Hidasi, Bal{\'a}zs and Karatzoglou, Alexandros},
  year = {2018},
  month = oct,
  journal = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  eprint = {1706.03847},
  eprinttype = {arxiv},
  pages = {843--852},
  doi = {10.1145/3269206.3271761},
  abstract = {RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an sessionbased manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35\% in terms of MRR and Recall@20 over previous sessionbased RNN solutions and up to 53\% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,enge_auswahl},
  annotation = {270 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Hidasi und Karatzoglou - 2018 - Recurrent Neural Networks with Top-k Gains for Ses.pdf}
}

@article{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf;C\:\\Users\\Markus\\Zotero\\storage\\DMCEB5FW\\1207.html}
}

@article{hirtEndtoendProcessModel,
  title = {An End-to-End Process Model for Supervised Machine Learning Classification: From Problem to Deployment in Information Systems},
  author = {Hirt, Robin and Kuhl, Niklas and Satzger, Gerhard},
  pages = {9},
  abstract = {Extracting meaningful knowledge from (big) data represents a key success factor in many industries today. Supervised machine learning (SML) has emerged as a popular technique to learn patterns in complex data sets and to identify hidden correlations. When this insight is turned into action, business value is created. However, common data mining processes are generally not tailored to SML. In addition, they fall short of providing an end-to-end view that not only supports building a ''one off'' model, but also covers its operational deployment within an information system.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\4WWP98Z2\\Hirt et al. - An End-to-End Process Model for Supervised Machine.pdf}
}

@incollection{hjaltasonRankingSpatialDatabases1995,
  title = {Ranking in Spatial Databases},
  booktitle = {Advances in {{Spatial Databases}}},
  author = {Hjaltason, G{\'i}sli R. and Samet, Hanan},
  editor = {Goos, Gerhard and Hartmanis, Juris and Leeuwen, Jan and Egenhofer, Max J. and Herring, John R.},
  year = {1995},
  volume = {951},
  pages = {83--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-60159-7_6},
  abstract = {An algorithm for ranking spatial objects according to increasing distance from a query object is introduced and analyzed. The algorithm makes use of a hierarchical spatial data structure. The intended application area is a database environment, where the spatial data structure serves as an index. The algorithm is incremental in the sense that objects are reported one by one, so that a query processor can use the algorithm in a pipelined fashion for complex queries involving proximity. It is well suited for k nearest neighbor queries, and has the property that k needs not be xed in advance.},
  isbn = {978-3-540-60159-3 978-3-540-49536-9},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\FRDK3U3J\\Hjaltason und Samet - 1995 - Ranking in spatial databases.pdf}
}

@article{hoangMachineLearningMethods,
  title = {Machine Learning Methods in Finance: Recent Applications and Prospects},
  author = {Hoang, Daniel and Wiegratz, Kevin},
  pages = {64},
  abstract = {We study how researchers can apply machine learning (ML) methods in finance. We first establish that the three distinct categories of ML (supervised learning, unsupervised learning, reinforcement learning and others) address fundamentally different problems than traditional econometric approaches. Then, we review the current state of research of ML in finance and identify three archetypes of applications: i) the construction of superior and novel measures, ii) the reduction of prediction error, and iii) the extension of the standard econometric toolset. With this taxonomy, we provide an outlook on potential future directions for both researchers and practitioners. We finally apply ML to typical problems in finance. Our results suggest large benefits of ML methods compared to traditional approaches and indicate that ML holds great potential for future research in finance.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\LE9WNKPY\\Hoang und Wiegratz - Machine Learning Methods in Finance Recent Applic.pdf}
}

@article{hoAxialAttentionMultidimensional2019,
  title = {Axial Attention in Multidimensional Transformers},
  author = {Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.12180 [cs]},
  eprint = {1912.12180},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,transformers},
  annotation = {214 citations (Semantic Scholar/arXiv) [2022-10-23]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ho et al. - 2019 - Axial Attention in Multidimensional Transformers.pdf;C\:\\Users\\Markus\\Zotero\\storage\\LU2GCGAP\\1912.html}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  annotation = {36303 citations (Crossref) [2022-10-18]}
}

@inproceedings{hofferTrainLongerGeneralize2017,
  title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
  shorttitle = {Train Longer, Generalize Better},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance -  known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem.  Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Hoffer et al. - 2017 - Train longer, generalize better closing the gener.pdf}
}

@article{holthausenEffectLargeBlock1987,
  title = {The {{Effect}} of {{Large Block Transactions}} on {{Security Prices}}: {{A Cross-Sectional Analysis}}},
  author = {Holthausen, Robert W. and Leftwich, Richard W. and Mayers, David},
  year = {1987},
  journal = {Journal of Financial Economics},
  volume = {19},
  number = {2},
  pages = {237--267},
  doi = {10.1016/0304-405X(87)90004-3},
  langid = {english},
  keywords = {üíé},
  annotation = {327 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Holthausen et al. - 1987 - The effect of large block transactions on security.pdf}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
  langid = {english},
  keywords = {üíé},
  annotation = {12084 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\68GPN2PV\\Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf}
}

@misc{huangSnapshotEnsemblesTrain2017,
  title = {Snapshot Ensembles: Train 1, Get {{M}} for Free},
  shorttitle = {Snapshot Ensembles},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  year = {2017},
  number = {arXiv:1704.00109},
  eprint = {1704.00109},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  annotation = {590 citations (Semantic Scholar/arXiv) [2022-10-23]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Huang et al. - 2017 - Snapshot Ensembles Train 1, get M for free.pdf;C\:\\Users\\Markus\\Zotero\\storage\\5T776IQD\\1704.html}
}

@misc{huangTabTransformerTabularData2020,
  title = {{{TabTransformer}}: {{Tabular Data Modeling Using Contextual Embeddings}}},
  shorttitle = {Tabtransformer},
  author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
  year = {2020},
  number = {arXiv:2012.06678},
  eprint = {2012.06678},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Huang et al. - 2020 - TabTransformer Tabular Data Modeling Using Contex.pdf;C\:\\Users\\Markus\\Zotero\\storage\\7VBWQD8R\\2012.html}
}

@article{huDoesOptionTrading2014,
  title = {Does {{Option Trading Convey Stock Price Information}}?},
  author = {Hu, Jianfeng},
  year = {2014},
  journal = {Journal of Financial Economics},
  volume = {111},
  number = {3},
  pages = {625--645},
  doi = {10.1016/j.jfineco.2013.12.004},
  langid = {english},
  keywords = {üíé},
  annotation = {114 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Hu - 2014 - Does option trading convey stock price information.pdf}
}

@book{hullOptionsFuturesOther2012,
  title = {Options, Futures, and Other Derivatives},
  author = {Hull, John},
  year = {2012},
  edition = {8th ed},
  publisher = {{Prentice Hall}},
  address = {{Boston}},
  isbn = {978-0-13-216494-8},
  langid = {english},
  lccn = {HG6024.A3 H85 2012},
  keywords = {Derivative securities,Futures,Stock options},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Hull - 2012 - Options, futures, and other derivatives.pdf}
}

@book{huyenDesigningMachineLearning,
  title = {Designing {{Machine Learning Systems}}},
  author = {Huyen, Chip},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\WWK72R8B\\Huyen - Designing Machine Learning Systems.pdf}
}

@book{hyndmanForecastingPrinciplesPractice2021,
  title = {Forecasting: Principles and Practice},
  author = {Hyndman, Rob J and Athanasopoulos, George},
  year = {2021},
  edition = {Third},
  abstract = {"Forecasting is required in many situations. Deciding whether to build another power generation plant in the next five years requires forecasts of future demand. Scheduling staff in a call centre next week requires forecasts of call volumes. Stocking an inventory requires forecasts of stock requirements. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning. This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly. Examples use R with many data sets taken from the authors' own consulting experience. In this third edition, all chapters have been updated to cover the latest research and forecasting methods. One new chapter has been added on time series features."-- Back cover.},
  langid = {english},
  annotation = {OCLC: 1260211835},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\D8GYNBKJ\\161.pdf}
}

@article{inceINDIVIDUALEQUITYRETURN2006,
  title = {Individual Equity Return Data from Thomson Datastream: Handle with Care!},
  author = {Ince, Ozgur S. and Porter, R. Burt},
  year = {2006},
  month = dec,
  journal = {Journal of Financial Research},
  volume = {29},
  number = {4},
  pages = {463--479},
  issn = {0270-2592, 1475-6803},
  doi = {10.1111/j.1475-6803.2006.00189.x},
  abstract = {We compare individual U.S. equity return data from Thomson Datastream (TDS) with similar data from the Center for Research in Security Prices (CRSP) to evaluate TDS for use in studies involving large numbers of individual equities in markets outside the United States. We document important issues of coverage, classification, and data integrity and find that naive use of TDS data can have a large impact on economic inferences. We show that after careful screening of the TDS data, inferences drawn from TDS data are similar to those drawn from CRSP. We illustrate the importance of the screens we develop using U.S. TDS data by applying the screens to TDS data from four European equity markets.},
  langid = {english},
  annotation = {518 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\DHEQUWDV\\Ince und Porter - 2006 - INDIVIDUAL EQUITY RETURN DATA FROM THOMSON DATASTR.pdf}
}

@article{ismailfawazDeepLearningTime2019,
  title = {Deep Learning for Time Series Classification: A Review},
  shorttitle = {Deep Learning for Time Series Classification},
  author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  year = {2019},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {4},
  pages = {917--963},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-019-00619-1},
  abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
  langid = {english},
  keywords = {üíé},
  annotation = {963 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\VRCFYWMU\\Ismail Fawaz et al. - 2019 - Deep learning for time series classification a re.pdf}
}

@misc{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  number = {arXiv:1803.05407},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better.pdf;C\:\\Users\\Markus\\Zotero\\storage\\M2VR4L3P\\1803.html}
}

@inproceedings{jacobGroupLassoOverlap2009,
  title = {Group Lasso with Overlap and Graph Lasso},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}} - {{ICML}} '09},
  author = {Jacob, Laurent and Obozinski, Guillaume and Vert, Jean-Philippe},
  year = {2009},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1145/1553374.1553431},
  abstract = {We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of covariates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.},
  isbn = {978-1-60558-516-1},
  langid = {english},
  annotation = {325 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Jacob et al. - 2009 - Group lasso with overlap and graph lasso.pdf}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\James et al. - 2013 - An Introduction to Statistical Learning.pdf}
}

@inproceedings{jinDatadrivenApproachPredict2015,
  title = {A Data-Driven Approach to Predict Default Risk of Loan for Online Peer-to-Peer ({{P2P}}) Lending},
  booktitle = {2015 {{Fifth International Conference}} on {{Communication Systems}} and {{Network Technologies}}},
  author = {Jin, Yu and Zhu, Yudan},
  year = {2015},
  month = apr,
  pages = {609--613},
  doi = {10.1109/CSNT.2015.25},
  abstract = {Online Peer-to-Peer (P2P) lending has achieved explosive development recently, which could be beneficial to both sides of individual lending. In this study, a data mining (DM) approach to predict the performance of P2P loan before funded is proposed. Using data from the Lending Club, we explore the characteristics of loan and its applicant and use random forest to do the feature selection in the modeling phase. The Difference from other risk prediction models is that the prediction is classified into three or four categories, rather than just two the default and not default classes. Then we compare five DM models: two decision trees (DTs), two neural networks (NNs) and one support vector machine (SVM) and use two metrics: average percent hit rate and area of the lift cumulative curve to evaluate the prediction results. The Empirical result shows that the term of loan, annual income, the amount of loan, debt-to-income ratio, credit grade and revolving line utilization play an important role in loan defaults. And SVM, Classification and Regression Tree (CART) and Multi-layer perceptron (MPL)'s prediction performance are almost equal.},
  keywords = {Accuracy,Artificial neural networks,Biological system modeling,CART,Data mining,Default risk,Peer-to-peer computing,Peer-to-Peer lending,Predictive models,Random forest,Support vector machines,SVM},
  annotation = {23 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Jin und Zhu - 2015 - A Data-Driven Approach to Predict Default Risk of .pdf;C\:\\Users\\Markus\\Zotero\\storage\\63WKC8CG\\7279990.html}
}

@misc{josseConsistencySupervisedLearning2020,
  title = {On the Consistency of Supervised Learning with Missing Values},
  author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = {2020},
  month = jul,
  number = {arXiv:1902.06931},
  eprint = {1902.06931},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imputation with learning directly with a model that accounts for missing values, we analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the "missing incorporated in attribute" method as it can handle both non-informative and informative missing values.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Josse et al. - 2020 - On the consistency of supervised learning with mis.pdf;C\:\\Users\\Markus\\Zotero\\storage\\9E3XWI2S\\1902.html}
}

@article{jurkatisInferringTradeDirections2022,
  title = {Inferring Trade Directions in Fast Markets},
  author = {Jurkatis, Simon},
  year = {2022},
  month = mar,
  journal = {Journal of Financial Markets},
  volume = {58},
  pages = {100635},
  issn = {13864181},
  doi = {10.1016/j.finmar.2021.100635},
  langid = {english},
  keywords = {üíé},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Jurkatis - 2022 - Inferring trade directions in fast markets.pdf}
}

@article{kadanBoundExpectedStock2020,
  title = {A {{Bound}} on {{Expected Stock Returns}}},
  author = {Kadan, Ohad and Tang, Xiaoxiao},
  editor = {Van Nieuwerburgh, Stijn},
  year = {2020},
  journal = {The Review of Financial Studies},
  volume = {33},
  number = {4},
  pages = {1565--1617},
  doi = {10.1093/rfs/hhz075},
  abstract = {Abstract             We present a sufficient condition under which the prices of options written on a particular stock can be aggregated to calculate a lower bound on the expected returns of that stock. The sufficient condition imposes a restriction on a combination of the stock's systematic and idiosyncratic risk. The lower bound is forward-looking and can be calculated on a high-frequency basis. We estimate the bound empirically and study its cross-sectional properties. We find that the bound increases with beta and book-to-market ratio and decreases with size and momentum. The bound provides an economically meaningful signal about future stock returns. (JEL G11, G12)             Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
  langid = {english},
  annotation = {19 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Kadan and Tang - 2020 - A Bound on Expected Stock Returns.pdf}
}

@inproceedings{kadraWelltunedSimpleNets2021,
  title = {Well-{{Tuned Simple Nets Excel}} on {{Tabular Datasets}}},
  booktitle = {{{NeurIPS}} 2021},
  author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  year = {2021},
  volume = {34},
  pages = {23928--23941},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for MLPs in a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Kadra et al. - 2021 - Well-tuned Simple Nets Excel on Tabular Datasets.pdf}
}

@misc{karndeepsinghAutoEncodersFeatureExtractor2021,
  title = {{{AutoEncoders}} as Feature Extractor or Dimensionality Reduction Network - Machine Learning},
  author = {{Karndeep Singh}},
  year = {2021},
  month = jan,
  abstract = {Video demonstrates AutoEncoders and how it can be used as Feature Extractor which Learns non-linearity in the data better than Linear Model such as PCA, which is also used as Feature Extractor. Following are the links: Notebook Link: https://github.com/karndeepsingh/Auto... FOLLOW ME ON: LinkedIn: https://www.linkedin.com/in/karndeeps... Github : https://www.github.com/karndeepsingh/ \#datascience \#machinelearning \#deeplearning \#geekeedatascience \#Autoencoders}
}

@article{kellyCharacteristicsAreCovariances2019,
  title = {Characteristics {{Are Covariances}}: {{A Unified Model}} of {{Risk}} and {{Return}}},
  shorttitle = {Characteristics {{Are Covariances}}},
  author = {Kelly, Bryan T. and Pruitt, Seth and Su, Yinan},
  year = {2019},
  journal = {Journal of Financial Economics},
  volume = {134},
  number = {3},
  pages = {501--524},
  issn = {0304405X},
  doi = {10.1016/j.jfineco.2019.05.001},
  langid = {english},
  annotation = {121 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Kelly et al. - 2019 - Characteristics are covariances A unified model o.pdf}
}

@article{keskarLargeBatchTrainingDeep2017,
  title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  journal = {arXiv: 1609.04836},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Keskar et al. - 2017 - On Large-Batch Training for Deep Learning General.pdf;C\:\\Users\\Markus\\Zotero\\storage\\CY882ZFZ\\1609.html}
}

@misc{khanBootstrappingMultipleImputation2019,
  title = {Bootstrapping and {{Multiple Imputation Ensemble Approaches}} for {{Missing Data}}},
  author = {Khan, Shehroz S. and Ahmad, Amir and Mihailidis, Alex},
  year = {2019},
  month = oct,
  number = {arXiv:1802.00154},
  eprint = {1802.00154},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Presence of missing values in a dataset can adversely affect the performance of a classifier. Single and Multiple Imputation are normally performed to fill in the missing values. In this paper, we present several variants of combining single and multiple imputation with bootstrapping to create ensembles that can model uncertainty and diversity in the data, and that are robust to high missingness in the data. We present three ensemble strategies: bootstrapping on incomplete data followed by (i) single imputation and (ii) multiple imputation, and (iii) multiple imputation ensemble without bootstrapping. We perform an extensive evaluation of the performance of the these ensemble strategies on 8 datasets by varying the missingness ratio. Our results show that bootstrapping followed by multiple imputation using expectation maximization is the most robust method even at high missingness ratio (up to 30\%). For small missingness ratio (up to 10\%) most of the ensemble methods perform quivalently but better than single imputation. Kappa-error plots suggest that accurate classifiers with reasonable diversity is the reason for this behaviour. A consistent observation in all the datasets suggests that for small missingness (up to 10\%), bootstrapping on incomplete data without any imputation produces equivalent results to other ensemble methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Khan et al. - 2019 - Bootstrapping and Multiple Imputation Ensemble App.pdf;C\:\\Users\\Markus\\Zotero\\storage\\KSR9NL3D\\1802.html}
}

@article{khorramEndtoendCNNLSTM2021,
  title = {End-to-End {{CNN}} + {{LSTM}} Deep Learning Approach for Bearing Fault Diagnosis},
  author = {Khorram, Amin and Khalooei, Mohammad and Rezghi, Mansoor},
  year = {2021},
  month = feb,
  journal = {Applied Intelligence},
  volume = {51},
  number = {2},
  pages = {736--751},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-020-01859-1},
  abstract = {Fault diagnostics and prognostics are important topics both in practice and research. There is an intense pressure on industrial plants to continue reducing unscheduled downtime, performance degradation, and safety hazards, which requires detecting and recovering potential faults in its early stages. Intelligent fault diagnosis is a promising tool due to its ability to rapidly and efficiently processing collected signals and providing accurate diagnosis results. Although many studies have developed machine leaning (M.L) and deep learning (D.L) algorithms for detecting the bearing fault, the results have generally been limited to relatively small train and test datasets and the input data has been manipulated (selective features used) to reach high accuracy. In this work, the raw data, collected from accelerometers (time-domain features) are taken as the input of a novel temporal sequence prediction algorithm to present an end-to-end method for fault detection. We use equivalent temporal sequences as the input of a novel Convolutional Long-Short-Term-Memory Recurrent Neural Network (CRNN) to detect the bearing fault with the highest accuracy in the shortest possible time. The method can reach the highest accuracy in the literature, to the best knowledge of the authors of the present paper, voiding any sort of pre-processing or manipulation of the input data. Effectiveness and feasibility of the fault diagnosis method are validated by applying it to two commonly used benchmark real vibration datasets and comparing the result with the other intelligent fault diagnosis methods.},
  langid = {english},
  annotation = {32 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RHMV6G7M\\Khorram et al. - 2021 - End-to-end CNN + LSTM deep learning approach for b.pdf}
}

@article{kichererSeamlesslyPortableApplications2012,
  title = {Seamlessly Portable Applications: Managing the Diversity of Modern Heterogeneous Systems},
  shorttitle = {Seamlessly Portable Applications},
  author = {Kicherer, Mario and Nowak, Fabian and Buchty, Rainer and Karl, Wolfgang},
  year = {2012},
  month = jan,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {8},
  number = {4},
  pages = {1--20},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/2086696.2086721},
  langid = {english},
  annotation = {10 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Kicherer et al. - 2012 - Seamlessly portable applications Managing the div.pdf}
}

@misc{klingenbrunnTransformerImplementationTimeseries2021,
  title = {Transformer Implementation for Time-Series Forecasting},
  author = {Klingenbrunn, Natasha},
  year = {2021},
  month = jul,
  journal = {MLearning.ai},
  abstract = {This article will present a Transformer-decoder architecture for forecasting time-series data. This paper is a follow-up on a previous\ldots},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\ED4RPP5K\\transformer-implementation-for-time-series-forecasting-a9db2db5c820.html}
}

@inproceedings{kossenSelfAttentionDatapointsGoing2021,
  title = {Self-{{Attention Between Datapoints}}: {{Going Beyond Individual Input-Output Pairs}} in {{Deep Learning}}},
  shorttitle = {Self-{{Attention Between Datapoints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
  year = {2021},
  volume = {34},
  pages = {28742--28756},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points.},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Kossen et al. - 2021 - Self-Attention Between Datapoints Going Beyond In.pdf}
}

@article{kraussDeepNeuralNetworks2017,
  title = {Deep Neural Networks, Gradient-Boosted Trees, Random Forests: {{Statistical}} Arbitrage on the {{S}}\&{{P}} 500},
  author = {Krauss, Christopher and Do, Xuan Anh and Huck, Nicolas},
  year = {2017},
  journal = {European Journal of Operational Research},
  volume = {259},
  number = {2},
  pages = {689--702},
  doi = {10.1016/j.ejor.2016.10.031},
  abstract = {In recent years, machine learning research has gained momentum: New developments in the field of deep learning allow for multiple levels of abstraction and are starting to supersede wellknown and powerful tree-based techniques mainly operating on the original feature space. All these methods can be applied to various fields, including finance. This article implements and analyses the effectiveness of deep neural networks (DNN), gradient-boosted-trees (GBT), random forests (RAF), and several ensembles of these methods in the context of statistical arbitrage. Each model is trained on lagged returns of all stocks in the S\&P 500, after elimination of survivor bias. From 1992 to 2015, daily one-day-ahead trading signals are generated based on the probability forecast of a stock to outperform the general market. The highest k probabilities are converted into long and the lowest k probabilities into short positions, thus censoring the less certain middle part of the ranking. Empirical findings are promising. A simple, equal-weighted ensemble (ENS1) consisting of one deep neural network, one gradient-boosted tree, and one random forest produces out-of-sample returns exceeding 0.45 percent per day for k = 10, prior to transaction costs. Irrespective of the fact that profits are declining in recent years, our findings pose a severe challenge to the semi-strong form of market efficiency.},
  langid = {english},
  annotation = {260 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\9HJN7GH5\\Krauss et al. - 2017 - Deep neural networks, gradient-boosted trees, rand.pdf}
}

@article{krogerKapitelOutlierDetection,
  title = {{Kapitel 6: outlier detection}},
  author = {Kr{\"o}ger, Peer and Zimek, Arthur},
  pages = {36},
  langid = {ngerman},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\6QQNJI42\\Kr√∂ger und Zimek - Kapitel 6 Outlier Detection.pdf}
}

@article{kuhlHumanVsSupervised2020,
  title = {Human vs. Supervised Machine Learning: Who Learns Patterns Faster?},
  shorttitle = {Human vs. Supervised Machine Learning},
  author = {K{\"u}hl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik},
  year = {2020},
  month = nov,
  journal = {arXiv:2012.03661 [cs]},
  eprint = {2012.03661},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The capabilities of supervised machine learning (SML), especially compared to human abilities, are being discussed in scientific research and in the usage of SML. This study provides an answer to how learning performance differs between humans and machines when there is limited training data. We have designed an experiment in which 44 humans and three different machine learning algorithms identify patterns in labeled training data and have to label instances according to the patterns they find. The results show a high dependency between performance and the underlying patterns of the task. Whereas humans perform relatively similarly across all patterns, machines show large performance differences for the various patterns in our experiment. After seeing 20 instances in the experiment, human performance does not improve anymore, which we relate to theories of cognitive overload. Machines learn slower but can reach the same level or may even outperform humans in 2 of the 4 of used patterns. However, machines need more instances compared to humans for the same results. The performance of machines is comparably lower for the other 2 patterns due to the difficulty of combining input features.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\I5EIVREH\\K√ºhl et al. - 2020 - Human vs. supervised machine learning Who learns .pdf}
}

@article{lambertonIntroductionStochasticCalculus,
  title = {Introduction to Stochastic Calculus Applied to Finance, Second Edition},
  author = {Lamberton, Damien and Lapeyre, Bernard},
  pages = {253},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Lamberton und Lapeyre - Introduction to Stochastic Calculus Applied to Fin.pdf}
}

@article{leeInferringInvestorBehavior2000,
  title = {Inferring Investor Behavior: {{Evidence}} from {{TORQ}} Data},
  shorttitle = {Inferring Investor Behavior},
  author = {Lee, Charles M.C. and Radhakrishna, Balkrishna},
  year = {2000},
  journal = {Journal of Financial Markets},
  volume = {3},
  number = {2},
  pages = {83--111},
  doi = {10.1016/S1386-4181(00)00002-1},
  langid = {english},
  keywords = {üíé},
  annotation = {463 citations (Semantic Scholar/DOI) [2022-10-24]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Lee and Radhakrishna - 2000 - Inferring investor behavior Evidence from TORQ da.pdf}
}

@article{leeInferringTradeDirection1991,
  title = {Inferring {{Trade Direction}} from {{Intraday Data}}},
  author = {Lee, Charles M. C. and Ready, Mark J.},
  year = {1991},
  journal = {The Journal of Finance},
  volume = {46},
  number = {2},
  pages = {733--746},
  doi = {10.1111/j.1540-6261.1991.tb02683.x},
  langid = {english},
  keywords = {üíé},
  annotation = {1172 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Lee and Ready - 1991 - Inferring Trade Direction from Intraday Data.pdf}
}

@article{leePseudolabelSimpleEfficient,
  title = {Pseudo-{{Label}}: {{The Simple}} and {{Efficient Semi-Supervised Learning Method}} for {{Deep Neural Networks}}},
  author = {Lee, Dong-Hyun},
  year = {2013},
  pages = {7},
  abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With Denoising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Lee - Pseudo-Label  The Simple and Efficient Semi-Super.pdf}
}

@article{leitchEconomicForecastEvaluation1991,
  title = {Economic Forecast Evaluation: Profits versus the Conventional Error Measures},
  author = {Leitch, Gordon and Tanner, J. Ernest},
  year = {1991},
  journal = {The American Economic Review},
  volume = {81},
  number = {3},
  pages = {580--590},
  publisher = {{American Economic Association}},
  issn = {00028282},
  abstract = {Economists are often puzzled as to why profit-maximizing firms buy professional forecasts when statistics such as the root-mean-squared error or the mean absolute error often indicate that a naive model will forecast about as well. This paper argues that the reason is that these traditional summary statistics may not be closely related to a forecast's profits. Using profit measures, we find only very weak relationships between such summary error statistics and forecast value. If these results are robust, then least-squares regression analysis may not be appropriate for many studies of economic behavior.},
  keywords = {‚õî No DOI found}
}

@inproceedings{lemorvanWhatGoodImputation2021,
  title = {What's a Good Imputation to Predict with Missing Values?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Le Morvan, Marine and Josse, Julie and Scornet, Erwan and Varoquaux, Gael},
  year = {2021},
  volume = {34},
  pages = {11530--11540},
  publisher = {{Curran Associates, Inc.}},
  abstract = {How to learn a good predictor on data with missing values? Most efforts focus on first imputing as well as possible and second learning on the completed data to predict the outcome. Yet, this widespread practice has no theoretical grounding. Here we show that for almost all imputation functions, an impute-then-regress procedure with a powerful learner is Bayes optimal. This result holds for all missing-values mechanisms, in contrast with the classic statistical results that require missing-at-random settings to use imputation in probabilistic modeling. Moreover, it implies that perfect conditional imputation is not needed for good prediction asymptotically. In fact, we show that on perfectly imputed data the best regression function will generally be discontinuous, which makes it hard to learn. Crafting instead the imputation so as to leave the regression function unchanged simply shifts the problem to learning discontinuous imputations. Rather, we suggest that it is easier to learn imputation and regression jointly. We propose such a procedure, adapting NeuMiss, a neural network capturing the conditional links across observed and unobserved variables whatever the missing-value pattern. Our experiments confirm that joint imputation and regression through NeuMiss is better than various two step procedures in a finite-sample regime.},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Le Morvan et al. - 2021 - What‚Äôs a good imputation to predict with missing v.pdf}
}

@misc{levinTransferLearningDeep2022,
  title = {Transfer {{Learning}} with {{Deep Tabular Models}}},
  author = {Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C. Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  year = {2022},
  month = jun,
  number = {arXiv:2206.15306},
  eprint = {2206.15306},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\TRH7QFZ2\\Levin et al. - 2022 - Transfer Learning with Deep Tabular Models.pdf;C\:\\Users\\Markus\\Zotero\\storage\\2SS989DG\\2206.html}
}

@inproceedings{liangFactorizationMeetsItem2016,
  title = {Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-Occurrence},
  shorttitle = {Factorization Meets the Item Embedding},
  booktitle = {Proceedings of the 10th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Liang, Dawen and Altosaar, Jaan and Charlin, Laurent and Blei, David M.},
  year = {2016},
  month = sep,
  pages = {59--66},
  publisher = {{ACM}},
  address = {{Boston Massachusetts USA}},
  doi = {10.1145/2959100.2959182},
  abstract = {Matrix factorization (MF) models and their extensions are standard in modern recommender systems. MF models decompose the observed user-item interaction matrix into user and item latent factors. In this paper, we propose a cofactorization model, CoFactor, which jointly decomposes the user-item interaction matrix and the item-item co-occurrence matrix with shared item latent factors. For each pair of items, the co-occurrence matrix encodes the number of users that have consumed both items. CoFactor is inspired by the recent success of word embedding models (e.g., word2vec) which can be interpreted as factorizing the word co-occurrence matrix. We show that this model significantly improves the performance over MF models on several datasets with little additional computational overhead. We provide qualitative results that explain how CoFactor improves the quality of the inferred factors and characterize the circumstances where it provides the most significant improvements.},
  isbn = {978-1-4503-4035-9},
  langid = {english},
  annotation = {131 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Liang et al. - 2016 - Factorization Meets the Item Embedding Regularizi.pdf}
}

@article{liINVESTABLEINTERPRETABLEMACHINE,
  title = {Investable and {{Interpretable Machine Learning}} for {{Equities}}},
  author = {Li, Yimou and Simon, Zachary and Turkington, David},
  pages = {34},
  abstract = {We propose three principles for evaluating the practical efficacy of machine learning for stock selection, and we compare the performance of various models and investment goals using this framework. The first principle is investability. To this end, we focus on portfolios formed from highly liquid US stocks, and we calibrate models to require a reasonable amount of trading. The second principle is interpretability. Investors must understand a model's output well enough to trust it and extract some general insight from it. To this end, we choose a concise set of predictor variables, and we apply a novel method called the Model Fingerprint to reveal the linear, nonlinear, and interaction effects that drive a model's predictions. The third principle is that a model's predictions should be interesting, by which we mean they should convincingly outperform simpler models. To this end, we evaluate performance out-of-sample compared to linear regressions. In addition to these three principles, we also consider the important role people play by imparting domain knowledge and preferences to a model. We argue that adjusting the prediction goal is one of the most powerful ways to do this. We test random forest, boosted trees and neural network models for multiple calibrations which we conclude are investable, interpretable, and interesting.},
  langid = {english},
  keywords = {‚ùì Multiple DOI},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\IVMS854V\\Li et al. - INVESTABLE AND INTERPRETABLE MACHINE LEARNING FOR .pdf}
}

@article{liLecture11Generative2019,
  title = {Lecture 11: {{Generative Models}}},
  author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
  year = {2019},
  pages = {136},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\EPWB5VZ5\\Li et al. - 2019 - Lecture 11 Generative Models.pdf}
}

@article{limTemporalFusionTransformers2020,
  title = {Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  year = {2020},
  month = sep,
  journal = {arXiv:1912.09363 [cs, stat]},
  eprint = {1912.09363},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
  archiveprefix = {arXiv},
  keywords = {‚õî No DOI found,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\DJIRAK7G\\Lim et al. - 2020 - Temporal Fusion Transformers for Interpretable Mul.pdf;C\:\\Users\\Markus\\Zotero\\storage\\QMZCUB7Z\\1912.html}
}

@article{linnainmaaHistoryCrossSection,
  title = {The History of the Cross Section of Stock Returns},
  author = {Linnainmaa, Juhani T and Roberts, Michael},
  pages = {69},
  abstract = {Using data spanning the 20th century, we show that most accounting-based return anomalies are spurious. When we take anomalies out-of-sample by moving either backward or forward in time, their average returns decrease and volatilities increase. These patterns emerge because datasnooping works through t-values, and an anomaly's t-value is high if its average return is high or volatility low. The average anomaly's in-sample Sharpe ratio is biased upwards by a factor of three. The data-snooping problem is so severe that we would expect to reject even the true asset pricing model when tested using in-sample data. Our results suggest that asset pricing models should be tested using out-of-sample data or, if not not feasible, that the correct standard by which to judge a model is its ability to explain half of the in-sample alpha.},
  langid = {english},
  keywords = {‚ùì Multiple DOI},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\KK2DLW9X\\Linnainmaa und Roberts - The History of the Cross Section of Stock Returns.pdf}
}

@article{linWhyOptionsPrices2015,
  title = {Why Do Options Prices Predict Stock Returns? {{Evidence}} from Analyst Tipping},
  shorttitle = {Why Do Options Prices Predict Stock Returns?},
  author = {Lin, Tse-Chun and Lu, Xiaolong},
  year = {2015},
  month = mar,
  journal = {Journal of Banking \& Finance},
  volume = {52},
  pages = {17--28},
  issn = {03784266},
  doi = {10.1016/j.jbankfin.2014.11.008},
  langid = {english},
  annotation = {48 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Lin and Lu - 2015 - Why do options prices predict stock returns Evide.pdf}
}

@article{littlestoneWeightedMajorityAlgorithm,
  title = {The Weighted Majority Algorithm},
  author = {Littlestone, Nick and Warmuth, Manfred K},
  journal = {. Introduction},
  pages = {39},
  abstract = {We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and e ective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log jAj + m) mistakes on that sequence, where c is xed constant.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Littlestone und Warmuth - The Weighted Majority Algorithm.pdf}
}

@inproceedings{liuCleaningFrameworkBigData2016,
  title = {Cleaning Framework for {{BigData}}: An Interactive Approach for Data Cleaning},
  shorttitle = {Cleaning Framework for {{BigData}}},
  booktitle = {2016 {{IEEE Second International Conference}} on {{Big Data Computing Service}} and {{Applications}} ({{BigDataService}})},
  author = {Liu, Hong and Kumar T.K., Ashwin and Thomas, Johnson P. and Hou, Xiaofei},
  year = {2016},
  month = mar,
  pages = {174--181},
  publisher = {{IEEE}},
  address = {{Oxford}},
  doi = {10.1109/BigDataService.2016.41},
  abstract = {Data is a valuable resource. Proper use of high-quality data can help people make better predictions, analyses and decisions. However, no matter how much effort we put into collecting a good dataset, errors will inevitably creep into the data, making it necessary for data cleaning. This becomes a concern particularly when large-scale heterogeneous data from multiple sources are integrated for other purposes. Data cleaning can be complicated, timeconsuming, and expensive, but it is a necessary step in any data-related system since poor-quality data may not be suitable to achieve the intended purposes. The core of our data cleaning system is data association and repairing. Association aims to identify the same object and link with the most associated objects, and repairing is to make a database reliable by fixing errors in the data. For big data applications, we don't necessarily need to use all the data. In most situations, we only need a small subset of the most relevant data. So the goal of association is to convert big raw data into a small subset of the most relevant data that are most useful for a particular application. After we obtain a small amount of relevant data, we also need to further analyze the data to help people digest the data and turn the data into knowledge. We use a number of techniques to associate the data to get useful knowledge for data repairing. Our research shows that data association can effectively help with data repairing. To capture the interaction, we provide a uniform framework that unifies the association and repairing process seamlessly based on context patterns, usage patterns, metadata, and repairing rules.},
  isbn = {978-1-5090-2251-9},
  langid = {english},
  annotation = {7 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\CSWKJTQI\\Liu et al. - 2016 - Cleaning Framework for BigData An Interactive App.pdf}
}

@article{liuDataQualityProblems2020,
  title = {Data Quality Problems Troubling Business and Financial Researchers: {{A}} Literature Review and Synthetic Analysis},
  shorttitle = {Data Quality Problems Troubling Business and Financial Researchers},
  author = {Liu, Grace},
  year = {2020},
  month = oct,
  journal = {Journal of Business \& Finance Librarianship},
  volume = {25},
  number = {3-4},
  pages = {315--371},
  issn = {0896-3568, 1547-0644},
  doi = {10.1080/08963568.2020.1847555},
  abstract = {The data quality of commercial business and financial databases greatly affects research quality and reliability. The presence of data quality problems can not only distort research results, destroy a research effort but also seriously damage management decisions based upon such research. Although library literature rarely discusses data quality problems, business literature reports a wide range of data quality issues, many of which have been systematically tested with statistical methods. This article reviews a collection of the business literature that provides a critical analysis on the data quality of the most frequently used business and finance databases including the Center for Research in Security Prices (CRSP), Compustat, S\&P Capital IQ, I/B/E/S, Datastream, Worldscope, Securities Data Company (SDC) Platinum, and Bureau Van Dijk (BvD) Orbis and identifies 11 categories of common data quality problems, including missing values, data errors, discrepancies, biases, inconsistencies, static header data, standardization, changes in historic data, lack of transparency, reporting time issues and misuse of data. Finally, the article provides some practical advice for librarians to facilitate their scholarly communications with researchers on data quality problems.},
  langid = {english},
  annotation = {2 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\R84FYBQF\\Liu - 2020 - Data quality problems troubling business and finan.pdf}
}

@misc{liuMonolithRealTime2022,
  title = {Monolith: {{Real Time Recommendation System With Collisionless Embedding Table}}},
  shorttitle = {Monolith},
  author = {Liu, Zhuoran and Zou, Leqi and Zou, Xuan and Wang, Caihua and Zhang, Biao and Tang, Da and Zhu, Bolin and Zhu, Yijie and Wu, Peng and Wang, Ke and Cheng, Youlong},
  year = {2022},
  month = sep,
  number = {arXiv:2209.07663},
  eprint = {2209.07663},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Building a scalable and real-time recommendation system is vital for many businesses driven by time-sensitive customer feedback, such as short-videos ranking or online ads. Despite the ubiquitous adoption of production-scale deep learning frameworks like TensorFlow or PyTorch, these general-purpose frameworks fall short of business demands in recommendation scenarios for various reasons: on one hand, tweaking systems based on static parameters and dense computations for recommendation with dynamic and sparse features is detrimental to model quality; on the other hand, such frameworks are designed with batch-training stage and serving stage completely separated, preventing the model from interacting with customer feedback in real-time. These issues led us to reexamine traditional approaches and explore radically different design choices. In this paper, we present Monolith, a system tailored for online training. Our design has been driven by observations of our application workloads and production environment that reflects a marked departure from other recommendations systems. Our contributions are manifold: first, we crafted a collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint; second, we provide an production-ready online training architecture with high fault-tolerance; finally, we proved that system reliability could be traded-off for real-time learning. Monolith has successfully landed in the BytePlus Recommend product.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Liu et al. - 2022 - Monolith Real Time Recommendation System With Col.pdf;C\:\\Users\\Markus\\Zotero\\storage\\2PQIP48R\\2209.html}
}

@inproceedings{liuRelatedPinsPinterest2017,
  title = {Related Pins at Pinterest: The Evolution of a Real-World Recommender System},
  shorttitle = {Related Pins at Pinterest},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web Companion}} - {{WWW}} '17 {{Companion}}},
  author = {Liu, David C. and Rogers, Stephanie and Shiau, Raymond and Kislyuk, Dmitry and Ma, Kevin C. and Zhong, Zhigang and Liu, Jenny and Jing, Yushi},
  year = {2017},
  pages = {583--592},
  publisher = {{ACM Press}},
  address = {{Perth, Australia}},
  doi = {10.1145/3041021.3054202},
  abstract = {Related Pins is the Web-scale recommender system that powers over 40\% of user engagement on Pinterest. This paper is a longitudinal study of three years of its development, exploring the evolution of the system and its components from prototypes to present state. Each component was originally built with many constraints on engineering effort and computational resources, so we prioritized the simplest and highest-leverage solutions. We show how organic growth led to a complex system and how we managed this complexity. Many challenges arose while building this system, such as avoiding feedback loops, evaluating performance, activating content, and eliminating legacy heuristics. Finally, we offer suggestions for tackling these challenges when engineering Web-scale recommender systems.},
  isbn = {978-1-4503-4914-7},
  langid = {english},
  annotation = {45 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Liu et al. - 2017 - Related Pins at Pinterest The Evolution of a Real.pdf}
}

@inproceedings{liuSTAMPShorttermAttention2018,
  title = {{{STAMP}}: Short-Term {{Attention}}/{{Memory}} Priority Model for Session-Based Recommendation},
  shorttitle = {{{STAMP}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Liu, Qiao and Zeng, Yifu and Mokhosi, Refuoe and Zhang, Haibin},
  year = {2018},
  month = jul,
  pages = {1831--1839},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3219950},
  abstract = {Predicting users' actions based on anonymous sessions is a challenging problem in web-based behavioral modeling research, mainly due to the uncertainty of user behavior and the limited information. Recent advances in recurrent neural networks have led to promising approaches to solving this problem, with long short-term memory model proving effective in capturing users' general interests from previous clicks. However, none of the existing approaches explicitly take the effects of users' current actions on their next moves into account. In this study, we argue that a long-term memory model may be insufficient for modeling long sessions that usually contain user interests drift caused by unintended clicks. A novel short-term attention/memory priority model is proposed as a remedy, which is capable of capturing users' general interests from the long-term memory of a session context, whilst taking into account users' current interests from the short-term memory of the last-clicks. The validity and efficacy of the proposed attention mechanism is extensively evaluated on three benchmark data sets from the RecSys Challenge 2015 and CIKM Cup 2016. The numerical results show that our model achieves state-of-the-art performance in all the tests.},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  annotation = {306 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Liu et al. - 2018 - STAMP Short-Term AttentionMemory Priority Model .pdf}
}

@misc{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  year = {2022},
  month = sep,
  number = {arXiv:2108.02497},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\WW543SH8\\Lones - 2022 - How to avoid machine learning pitfalls a guide fo.pdf;C\:\\Users\\Markus\\Zotero\\storage\\5AI28WNQ\\2108.html}
}

@book{lopezdepradoAdvancesFinancialMachine2018,
  title = {Advances in Financial Machine Learning},
  author = {{\{L{\'o}pez de Prado\}}, Marcos},
  year = {2018},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  keywords = {Data processing,Finance,Machine learning,Mathematical models},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\L√≥pez de Prado - 2018 - Advances in financial machine learning.pdf}
}

@misc{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  month = may,
  number = {arXiv:1608.03983},
  eprint = {1608.03983},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\Z4YVX9A3\\Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf;C\:\\Users\\Markus\\Zotero\\storage\\U4NIKR6X\\1608.html}
}

@article{ludewigEvaluationSessionbasedRecommendation2018,
  title = {Evaluation of Session-Based Recommendation Algorithms},
  author = {Ludewig, Malte and Jannach, Dietmar},
  year = {2018},
  month = dec,
  journal = {User Modeling and User-Adapted Interaction},
  volume = {28},
  number = {4-5},
  eprint = {1803.09587},
  eprinttype = {arxiv},
  pages = {331--390},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-018-9209-6},
  abstract = {Recommender systems help users find relevant items of interest, for example on e-commerce or media streaming sites. Most academic research is concerned with approaches that personalize the recommendations according to long-term user profiles. In many real-world applications, however, such long-term profiles often do not exist and recommendations therefore have to be made solely based on the observed behavior of a user during an ongoing session. Given the high practical relevance of the problem, an increased interest in this problem can be observed in recent years, leading to a number of proposals for session-based recommendation algorithms that typically aim to predict the user's immediate next actions. In this work, we present the results of an in-depth performance comparison of a number of such algorithms, using a variety of datasets and evaluation measures. Our comparison includes the most recent approaches based on recurrent neural networks like GRU4REC, factorized Markov model approaches such as FISM or FOSSIL, as well as simpler methods based, e.g., on nearest neighbor schemes. Our experiments reveal that algorithms of this latter class, despite their sometimes almost trivial nature, often perform equally well or significantly better than today's more complex approaches based on deep neural networks. Our results therefore suggest that there is substantial room for improvement regarding the development of more sophisticated session-based recommendation algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {132 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ludewig und Jannach - 2018 - Evaluation of Session-based Recommendation Algorit.pdf;C\:\\Users\\Markus\\Zotero\\storage\\VS9ENB4M\\1901.04592.pdf}
}

@misc{lundbergConsistentIndividualizedFeature2019,
  title = {Consistent {{Individualized Feature Attribution}} for {{Tree Ensembles}}},
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  year = {2019},
  number = {arXiv:1802.03888},
  eprint = {1802.03888},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Lundberg et al. - 2019 - Consistent Individualized Feature Attribution for .pdf;C\:\\Users\\Markus\\Zotero\\storage\\SX8Y644M\\1802.html}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M. and Lee, Su-In},
  year = {2017},
  series = {{{NeurIPS}} 2017},
  volume = {31},
  pages = {4768--4777},
  publisher = {{Curran Associates, Inc.}},
  address = {{Long Beach, CA}},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\ZKIXRQTM\\Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@inproceedings{luoCollaborativeSelfattentionNetwork2020,
  title = {Collaborative Self-Attention Network for Session-Based Recommendation},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Luo, Anjing and Zhao, Pengpeng and Liu, Yanchi and Zhuang, Fuzhen and Wang, Deqing and Xu, Jiajie and Fang, Junhua and Sheng, Victor S.},
  year = {2020},
  month = jul,
  pages = {2591--2597},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/359},
  abstract = {Session-based recommendation becomes a research hotspot for its ability to make recommendations for anonymous users. However, existing session-based methods have the following limitations: (1) They either lack the capability to learn complex dependencies or focus mostly on the current session without explicitly considering collaborative information. (2) They assume that the representation of an item is static and fixed for all users at each time step. We argue that even the same item can be represented differently for different users at the same time step. To this end, we propose a novel solution, Collaborative Self-Attention Network (CoSAN) for session-based recommendation, to learn the session representation and predict the intent of the current session by investigating neighborhood sessions. Specially, we first devise a collaborative item representation by aggregating the embedding of neighborhood sessions retrieved according to each item in the current session. Then, we apply self-attention to learn long-range dependencies between collaborative items and generate collaborative session representation. Finally, each session is represented by concatenating the collaborative session representation and the embedding of the current session. Extensive experiments on two real-world datasets show that CoSAN constantly outperforms state-of-the-art methods.},
  isbn = {978-0-9992411-6-5},
  langid = {english},
  annotation = {15 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Luo et al. - 2020 - Collaborative Self-Attention Network for Session-b.pdf}
}

@misc{MachineLearningHow,
  title = {Machine Learning - How to Intuitively Explain What a Kernel Is?},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\5MD2YUG6\\how-to-intuitively-explain-what-a-kernel-is.html}
}

@article{maraisDeepLearningTabular,
  title = {Deep {{Learning}} for {{Tabular Data}}: {{An Exploratory Study}}},
  author = {Marais, Jan Andr{\'e}},
  pages = {144},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\GIFRU87T\\Marais - Deep Learning for Tabular Data An Exploratory Stu.pdf}
}

@book{martinEconometricModellingTime2012,
  title = {Econometric Modelling with Time Series: Specification, Estimation and Testing},
  shorttitle = {Econometric Modelling with Time Series},
  author = {Martin, Vance and Hurn, Stan and Harris, David},
  year = {2012},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139043205},
  isbn = {978-1-139-04320-5},
  langid = {english},
  keywords = {sem-stock-option},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Martin et al. - 2012 - Econometric Modelling with Time Series Specificat2.pdf}
}

@techreport{martinMarketEfficiencyAge2019,
  title = {Market Efficiency in the Age of Big Data},
  author = {Martin, Ian and Nagel, Stefan},
  year = {2019},
  month = dec,
  number = {w26586},
  pages = {w26586},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w26586},
  abstract = {Modern investors face a high-dimensional prediction problem: thousands of observable variables are potentially relevant for forecasting. We reassess the conventional wisdom on market efficiency in light of this fact. In our model economy, which resembles a typical machine learning setting, N assets have cash flows that are a linear function of J firm characteristics, but with uncertain coefficients. Risk-neutral Bayesian investors impose shrinkage (ridge regression) or sparsity (Lasso) when they estimate the J coefficients of the model and use them to price assets. When J is comparable in size to N, returns appear cross-sectionally predictable using firm characteristics to an econometrician who analyzes data from the economy ex post. A factor zoo emerges even without p-hacking and data-mining. Standard in-sample tests of market efficiency reject the no-predictability null with high probability, despite the fact that investors optimally use the information available to them in real time. In contrast, out-of-sample tests retain their economic meaning.},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\G9JVRXX8\\Martin und Nagel - 2019 - Market Efficiency in the Age of Big Data.pdf}
}

@article{martinWhatExpectedReturn2016,
  title = {What Is the Expected Return on the Market?},
  author = {Martin, Ian},
  year = {2016},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.2772101},
  langid = {english},
  annotation = {2 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Martin - 2016 - What Is the Expected Return on the Market.pdf}
}

@article{martinWhatExpectedReturn2019,
  title = {What {{Is}} the {{Expected Return}} on a {{Stock}}?},
  author = {Martin, Ian and Wagner, Christian},
  year = {2019},
  journal = {The Journal of Finance},
  volume = {74},
  number = {4},
  pages = {1887--1929},
  doi = {10.1111/jofi.12778},
  abstract = {We derive a formula for the expected return on a stock in terms of the risk-neutral variance of the market and the stock's excess risk-neutral variance relative to that of the average stock. These quantities can be computed from index and stock option prices; the formula has no free parameters. The theory performs well empirically both in and out of sample. Our results suggest that there is considerably more variation {${_\ast}$}Ian Martin (corresponding author, i.w.martin@lse.ac.uk) is at London School of Economics. Christian Wagner is at Copenhagen Business School. We thank Harjoat Bhamra, John Campbell, Patrick Gagliardini, Christian Julliard, Binying Liu, Dong Lou, Marcin Kacperczyk, Stefan Nagel (Editor), Christopher Polk, Tarun Ramadorai, Tyler Shumway, Andrea Tamoni, Paul Schneider, Fabio Trojani, Dimitri Vayanos, Tuomo Vuolteenaho, participants at the Western Finance Association Meetings 2017, the 2017 Annual Meeting of the Society for Economic Dynamics, the 2017 CEPR Spring Symposium, the BI-SHoF Conference in Asset Pricing, the AP2-CFF Conference on Return Predictability, the 2016 IFSID Conference on Derivatives, the 4nations Cup 2016, seminar participants at Arrowstreet, AQR, Banca d'Italia, BlackRock, CEMFI, the European Central Bank, the London School of Economics, MIT (Sloan), NHH Bergen, Norges Bank Investment Management the University of Maryland, the University of Michigan (Ross), and WU Vienna, and two anonymous referees for their comments. Ian Martin is grateful for support from the Paul Woolley Centre and from the ERC under Starting Grant 639744. Christian Wagner acknowledges support from the FRIC Center for Financial Frictions, grant no. DNRF102. We have read the Journal of Finance disclosure policy and have no conflicts of interest to disclose.},
  langid = {english},
  annotation = {66 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\YSI4HFBB\\Martin und Wagner - 2019 - What Is the Expected Return on a Stock.pdf}
}

@book{mccarthyApplyingPredictiveAnalytics2019,
  title = {Applying {{Predictive Analytics}}: {{Finding Value}} in {{Data}}},
  shorttitle = {Applying {{Predictive Analytics}}},
  author = {McCarthy, Richard V. and McCarthy, Mary M. and Ceccucci, Wendy and Halawi, Leila},
  year = {2019},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-14038-0},
  isbn = {978-3-030-14037-3 978-3-030-14038-0},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\DNPFQYNA\\McCarthy et al. - 2019 - Applying Predictive Analytics Finding Value in Da.pdf}
}

@article{measeBoostedClassificationTrees,
  title = {Boosted {{Classification Trees}} and {{Class Probability}}/{{Quantile Estimation}}},
  author = {Mease, David and Wyner, Abraham J and Buja, Andreas},
  pages = {31},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\SXM96IXU\\Mease et al. - Boosted ClassiÔ¨Åcation Trees and Class Probability.pdf}
}

@misc{melisStateArtEvaluation2017,
  title = {On the {{State}} of the {{Art}} of {{Evaluation}} in {{Neural Language Models}}},
  author = {Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  year = {2017},
  month = nov,
  number = {arXiv:1707.05589},
  eprint = {1707.05589},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Computation and Language},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Melis et al. - 2017 - On the State of the Art of Evaluation in Neural La.pdf;C\:\\Users\\Markus\\Zotero\\storage\\W96YCJS5\\1707.html}
}

@misc{mirzaeiHowUseDeepLearning2019,
  title = {How to Use Deep-Learning for Feature-Selection, Python, Keras},
  author = {Mirzaei, Ali},
  year = {2019},
  month = apr,
  journal = {Medium},
  abstract = {In this post, I want to present my recent idea about using deep-learning in feature selection. Nowadays, deep learning is a very\ldots},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\Z5QMTHV3\\how-to-use-deep-learning-for-feature-selection-python-keras-24a68bef1e33.html}
}

@article{mockusApplicationBayesianApproach1994,
  title = {Application of {{Bayesian}} Approach to Numerical Methods of Global and Stochastic Optimization},
  author = {Mockus, Jonas},
  year = {1994},
  journal = {Journal of Global Optimization},
  volume = {4},
  number = {4},
  pages = {347--365},
  doi = {10.1007/BF01099263},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\H6SK9Q2L\\Mockus - 1994 - Application of Bayesian approach to numerical meth.pdf}
}

@article{mogharStockMarketPrediction2020,
  title = {Stock {{Market Prediction Using LSTM Recurrent Neural Network}}},
  author = {Moghar, Adil and Hamiche, Mhamed},
  year = {2020},
  journal = {Procedia Computer Science},
  volume = {170},
  pages = {1168--1173},
  issn = {18770509},
  doi = {10.1016/j.procs.2020.03.049},
  langid = {english},
  annotation = {99 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Moghar and Hamiche - 2020 - Stock Market Prediction Using LSTM Recurrent Neura.pdf}
}

@misc{monnierCrossvalidationToolsTime2018,
  title = {Cross-Validation Tools for Time Series},
  author = {Monnier, Samuel},
  year = {2018},
  month = sep,
  journal = {Medium},
  abstract = {Developing machine learning models for time series data requires special care, mainly because the usual machine learning assumption that\ldots},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\96EDYFQX\\cross-validation-tools-for-time-series-ffa1a5a09bf9.html}
}

@article{muravyevOrderFlowExpected2016,
  title = {Order {{Flow}} and {{Expected Option Returns}}: {{Order Flow}} and {{Expected Option Returns}}},
  author = {Muravyev, Dmitriy},
  year = {2016},
  month = apr,
  journal = {The Journal of Finance},
  volume = {71},
  number = {2},
  pages = {673--708},
  doi = {10.1111/jofi.12380},
  langid = {english},
  keywords = {üíé},
  annotation = {71 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Muravyev - 2016 - Order Flow and Expected Option Returns Order Flow.pdf}
}

@article{murdochInterpretableMachineLearning2019,
  title = {Interpretable Machine Learning: Definitions, Methods, and Applications},
  shorttitle = {Interpretable Machine Learning},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and {Abbasi-Asl}, Reza and Yu, Bin},
  year = {2019},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {44},
  eprint = {1901.04592},
  eprinttype = {arxiv},
  pages = {22071--22080},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1900654116},
  abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant (PDR) framework for discussing interpretations. The PDR framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Applications},
  annotation = {454 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\WK2VIBX6\\Murdoch et al. - 2019 - Interpretable machine learning definitions, metho.pdf}
}

@article{nabiNovelApproachStock2020,
  title = {A {{Novel Approach}} for {{Stock Price Prediction Using Gradient Boosting Machine}} with {{Feature Engineering}} ({{GBM-wFE}})},
  author = {Nabi, Rebwar M. and Ab. M. Saeed, Soran and Harron, Habibollah},
  year = {2020},
  month = apr,
  journal = {Kurdistan Journal of Applied Research},
  volume = {5},
  number = {1},
  pages = {28--48},
  issn = {2411-7706, 2411-7684},
  doi = {10.24017/science.2020.1.3},
  abstract = {The prediction of stock prices has become an exciting area for researchers as well as academicians due to its economic impact and potential business profits. This study proposes a novel multiclass classification ensemble learning approach for predicting stock prices based on historical data using feature engineering. The proposed approach comprises four main steps, which are pre-processing, feature selection, feature engineering, and ensemble methods. We use 11 datasets from Nasdaq and S\&P 500 to ensure the accuracy of the proposed approach. Furthermore, eight feature selection algorithms are studied and implemented. More importantly, a feature engineering concept is applied to construct two new features, which are appears to be very auspicious in terms of improving classification accuracy, and this is considered the first study to use feature engineering for multiclass classification using ensemble methods.},
  langid = {english},
  annotation = {6 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\LEH7J9YE\\Nabi et al. - 2020 - A Novel Approach for Stock Price Prediction Using .pdf}
}

@book{nagelMachineLearningAsset2021,
  title = {Machine Learning in Asset Pricing: {{Stefan Nagel}}.},
  shorttitle = {Machine Learning in Asset Pricing},
  author = {Nagel, Stefan},
  year = {2021},
  isbn = {978-0-691-21870-0},
  langid = {english},
  keywords = {sem-stock-option},
  annotation = {OCLC: 1260147172},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\GNES9L6D\\Nagel - 2021 - Machine learning in asset pricing Stefan Nagel..pdf}
}

@misc{narkhedeUnderstandingAUCROC2021,
  title = {Understanding {{AUC}} - {{ROC}} Curve},
  author = {Narkhede, Sarang},
  year = {2021},
  month = jun,
  journal = {Medium},
  abstract = {In Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance\ldots},
  howpublished = {https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\B67SNW8V\\understanding-auc-roc-curve-68b2303cc9c5.html}
}

@misc{NetflixUpdateTry,
  title = {Netflix Update: Try This at Home},
  howpublished = {https://sifter.org/\textasciitilde simon/journal/20061211.html},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\4ME6H5RD\\20061211.html}
}

@incollection{neumannMotivatingSupportingUser2007,
  title = {Motivating and Supporting User Interaction with Recommender Systems},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Neumann, Andreas W.},
  editor = {Kov{\'a}cs, L{\'a}szl{\'o} and Fuhr, Norbert and Meghini, Carlo},
  year = {2007},
  volume = {4675},
  pages = {428--439},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-74851-9_36},
  abstract = {This contribution reports on the introduction of explicit recommender systems at the University Library of Karlsruhe. In March 2006, a rating service and a review service were added to the already existing behavior-based recommender system. Logged-in users can write reviews and rate all library documents (books, journals, multimedia, etc.); reading reviews and inspecting ratings are open to the general public. A role system is implemented that supports the submission of different reviews for the same document from one user to different user groups (students, scientists, etc.). Mechanism design problems like bias and free riding are discussed, to address these problems the introduction of incentive systems is described. Usage statistics are given and the question, which recommender system supports which user needs best, is covered. Summing up, recommender systems are a way to combine the support of library user interaction with information access beyond catalog searches.},
  isbn = {978-3-540-74850-2 978-3-540-74851-9},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Neumann - 2007 - Motivating and Supporting User Interaction with Re.pdf}
}

@article{nitscheMachineLearningAsset,
  title = {Machine Learning in Asset Pricing},
  author = {Nitsche, Maximilian},
  pages = {14},
  abstract = {This seminar utilizes various methods of machine learning (ML) to tackle the major issues of asset pricing in an empirical setting. First, characteristics providing incremental information about the cross-section of stock returns are selected by the adaptive group LASSO of Freyberger et al. (2020). Second, the chosen firm-specific characteristics are used as input-features of machine learning methods to predict cross-sectional returns of the following month. While the predetermined features of interest (FoI) enable modest ML methods like linear regression and elastic net to outperform their corresponding baseline, more complex models such as random forest and neural networks are superior regarding sharpe ratio and absolute mean return. The empirical analysis shows the importance of non-linearities and economic gains due to more complex machine learning methods.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\ZTFNRFUF\\Nitsche - Machine Learning in Asset Pricing.pdf}
}

@inproceedings{nothmanStopWordLists2018,
  title = {Stop Word Lists in Free Open-Source Software Packages},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  year = {2018},
  pages = {7--12},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-2502},
  langid = {english},
  annotation = {13 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Nothman et al. - 2018 - Stop Word Lists in Free Open-source Software Packa.pdf}
}

@misc{ntakourisTimeSeriesTransformer2021,
  title = {The Time Series Transformer},
  author = {Ntakouris, Theodoros},
  year = {2021},
  month = jan,
  journal = {Medium},
  abstract = {Attention Is All You Need they said. Is it a more robust convolution? Is it just a hack to squeeze more learning capacity out of fewer parameters? Is it supposed to be sparse? How did the original\ldots},
  howpublished = {https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\IVF9WLYP\\the-time-series-transformer-2a521a0efad3.html}
}

@inproceedings{obthongSurveyMachineLearning2020,
  title = {A {{Survey}} on {{Machine Learning}} for {{Stock Price Prediction}}: {{Algorithms}} and {{Techniques}}:},
  shorttitle = {A {{Survey}} on {{Machine Learning}} for {{Stock Price Prediction}}},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Finance}}, {{Economics}}, {{Management}} and {{IT Business}}},
  author = {Obthong, Mehtabhorn and Tantisantiwong, Nongnuch and Jeamwatthanachai, Watthanasak and Wills, Gary},
  year = {2020},
  pages = {63--71},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Prague, Czech Republic}},
  doi = {10.5220/0009340700630071},
  isbn = {978-989-758-422-0},
  langid = {english},
  annotation = {13 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\PX7R9GLW\\Obthong et al. - 2020 - A Survey on Machine Learning for Stock Price Predi.pdf}
}

@article{odders-whiteOccurrenceConsequencesInaccurate2000,
  title = {On the {{Occurrence}} and {{Consequences}} of {{Inaccurate Trade Classification}}},
  author = {{Odders-White}, Elizabeth R},
  year = {2000},
  journal = {Journal of Financial Markets},
  volume = {3},
  number = {3},
  pages = {259--286},
  doi = {10.1016/S1386-4181(00)00006-9},
  keywords = {üíé},
  annotation = {174 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Odders-White - 2000 - On the occurrence and consequences of inaccurate t.pdf}
}

@article{olbrysEvaluatingTradeSide2018,
  title = {Evaluating {{Trade Side Classification Algorithms Using Intraday Data}} from the {{Warsaw Stock Exchange}}},
  author = {Olbrys, Joanna and Mursztyn, Micha{\l}},
  year = {2018},
  publisher = {{Karlsruhe}},
  doi = {10.5445/KSP/1000085951/20},
  copyright = {Closed Access, Creative Commons Namensnennung \textendash{} Weitergabe unter gleichen Bedingungen 4.0 International},
  langid = {english},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\HI224NPR\\Olbrys and Mursztyn - 2018 - Evaluating Trade Side Classification Algorithms Us.pdf}
}

@misc{oliverRealisticEvaluationDeep2019,
  title = {Realistic {{Evaluation}} of {{Deep Semi-Supervised Learning Algorithms}}},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  year = {2019},
  month = jun,
  number = {arXiv:1804.09170},
  eprint = {1804.09170},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Oliver et al. - 2019 - Realistic Evaluation of Deep Semi-Supervised Learn.pdf;C\:\\Users\\Markus\\Zotero\\storage\\MBPQE64Y\\1804.html}
}

@article{ordoniVerifyingWorkflowModels,
  title = {Verifying Workflow Models with Data Values \textendash{} a Case Study of {{SMR}} Spectrum Auctions},
  author = {Ordoni, Elaheh and Mulle, Jutta and Bohm, Klemens},
  pages = {28},
  abstract = {Industry takes a great interest in verification techniques to improve the reliability of process designs. Providing reliable design in application domains like spectrum auctions is crucial. Spectrum auction revenue is considered as one of the principal sources for governmental income. Hence, analyzing the auction design before applying it can ensure absence of undesirable results of an auction. Those results might even be bad, if they occur with a probability of just higher than zero. Current verification approaches are mainly devoted to verify control flows only, although data values play a significant role in real life applications. Thus, these approaches are not sufficient to support data-centered workflows as spectrum auctions. We address this issue by providing a new data-centered verification approach to analyze Simultaneous Multi-Round (SMR) auction design in BPMN format. We show how to enhance a BPMN model by including important information, namely data values used in the workflow, which the standard BPMN 2.0 does not support. An example of a data value in a SMR auction is the ''auctioneer's revenue''. To enable the verification of data-centered properties, we have developed a transformation of a data-value enhanced BPMN model to Petri Nets respecting the semantics of certain data value usages. For that, we support dynamic and correlated data values. By employing a model checker and defining data-centered properties in CTL formula, we verify SMR auction models to find undesirable executions for auctioneers. For example, we can precisely detect the worst values of three important measures in auctions: efficiency, revenue, and bidder's profit. With it, we can not only find the undesirable outcomes, but also provide a counter-example to help an auctioneer to improve the auction design.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ordoni et al. - Verifying WorkÔ¨Çow Models with Data Values ‚Äì a Case.pdf}
}

@book{owenHyperparameterTuningPython2022,
  title = {Hyperparameter {{Tuning}} with {{Python}}},
  author = {Owen, Louis},
  year = {2022},
  publisher = {{PACKT PUBLISHING LIMITED}},
  address = {{S.l.}},
  abstract = {Take your machine learning models to the next level by learning how to leverage hyperparameter tuning, allowing you to control the model's finest details Key Features Gain a deep understanding of how hyperparameter tuning works Explore exhaustive search, heuristic search, and Bayesian and multi-fidelity optimization methods Learn which method should be used to solve a specific situation or problem Book Description Hyperparameters are an important element in building useful machine learning models. This book curates numerous hyperparameter tuning methods for Python, one of the most popular coding languages for machine learning. Alongside in-depth explanations of how each method works, you will use a decision map that can help you identify the best tuning method for your requirements. You'll start with an introduction to hyperparameter tuning and understand why it's important. Next, you'll learn the best methods for hyperparameter tuning for a variety of use cases and specific algorithm types. This book will not only cover the usual grid or random search but also other powerful underdog methods. Individual chapters are also dedicated to the three main groups of hyperparameter tuning methods: exhaustive search, heuristic search, Bayesian optimization, and multi-fidelity optimization. Later, you will learn about top frameworks like Scikit, Hyperopt, Optuna, NNI, and DEAP to implement hyperparameter tuning. Finally, you will cover hyperparameters of popular algorithms and best practices that will help you efficiently tune your hyperparameter. By the end of this book, you will have the skills you need to take full control over your machine learning models and get the best models for the best results. What you will learn Discover hyperparameter space and types of hyperparameter distributions Explore manual, grid, and random search, and the pros and cons of each Understand powerful underdog methods along with best practices Explore the hyperparameters of popular algorithms Discover how to tune hyperparameters in different frameworks and libraries Deep dive into top frameworks such as Scikit, Hyperopt, Optuna, NNI, and DEAP Get to grips with best practices that you can apply to your machine learning models right away Who this book is for This book is for data scientists and ML engineers who are working with Python and want to further boost their ML model's performance by using the appropriate hyperparameter tuning method. Although a basic understanding of machine learning and how to code in Python is needed, no prior knowledge of hyperparameter tuning in Python is required},
  isbn = {978-1-80324-194-4},
  langid = {english},
  keywords = {üíé},
  annotation = {OCLC: 1338299408},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\I6LTV34X\\Owen - 2022 - Hyperparameter Tuning with Python.pdf}
}

@article{panayidesBulkVolumeClassification2019,
  title = {Bulk {{Volume Classification}} and {{Information Detection}}},
  author = {Panayides, Marios A. and Shohfi, Thomas D. and Smith, Jared D.},
  year = {2019},
  journal = {Journal of Banking \& Finance},
  volume = {103},
  pages = {113--129},
  doi = {10.1016/j.jbankfin.2019.04.001},
  langid = {english},
  keywords = {üíé},
  annotation = {6 citations (Crossref) [2022-11-07]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Panayides et al. - 2019 - Bulk volume classification and information detecti.pdf}
}

@article{panayidesComparingTradeFlow2014,
  title = {Comparing {{Trade Flow Classification Algorithms}} in the {{Electronic Era}}: {{The Good}}, the {{Bad}}, and the {{Uninformative}}},
  author = {Panayides, Marios A. and Shohfi, Thomas and Smith, Jared D.},
  year = {2014},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2503628},
  langid = {english},
  keywords = {üíé},
  annotation = {1 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Panayides et al. - 2014 - Comparing Trade Flow Classification Algorithms in 2.pdf}
}

@article{panInformationOptionVolume2006,
  title = {The {{Information}} in {{Option Volume}} for {{Future Stock Prices}}},
  author = {Pan, Jun and Poteshman, Allen M.},
  year = {2006},
  journal = {Review of Financial Studies},
  volume = {19},
  number = {3},
  pages = {871--908},
  doi = {10.1093/rfs/hhj024},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Pan and Poteshman - 2006 - The Information in Option Volume for Future Stock .pdf;C\:\\Users\\Markus\\Zotero\\storage\\ZME3K29A\\Pan and Poteshman - 2006 - The Information in Option Volume for Future Stock .pdf}
}

@article{parkEffectiveHashbasedAlgorithm1995,
  title = {An Effective Hash-Based Algorithm for Mining Association Rules},
  author = {Park, Jong Soo and Chen, Ming-Syan and Yu, Philip S.},
  year = {1995},
  month = may,
  journal = {ACM SIGMOD Record},
  volume = {24},
  number = {2},
  pages = {175--186},
  issn = {0163-5808},
  doi = {10.1145/568271.223813},
  abstract = {In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a su cient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets rst and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an e ective hash-based algorithm for the candidate set generation. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods, thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to e ectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations signi cantly. Extensive simulation study is conducted to evaluate performance of the proposed algorithm.},
  langid = {english},
  annotation = {308 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\FSRG42LF\\Park et al. - 1995 - An effective hash-based algorithm for mining assoc.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  series = {{{NeurIPS}} 2019},
  volume = {32},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  address = {{Red Hook, NY}},
  keywords = {‚õî No DOI found,üíé}
}

@misc{pedregosaScikitlearnMachineLearning2018,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and M{\"u}ller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2018},
  number = {arXiv:1201.0490},
  eprint = {1201.0490},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Pedregosa et al. - 2018 - Scikit-learn Machine Learning in Python.pdf;C\:\\Users\\Markus\\Zotero\\storage\\NMSZVC69\\1201.html}
}

@article{perez-lebelBenchmarkingMissingvaluesApproaches2022,
  title = {Benchmarking {{Missing-Values Approaches}} for {{Predictive Models}} on {{Health Databases}}},
  author = {{Perez-Lebel}, Alexandre and Varoquaux, Ga{\"e}l and Le~Morvan, Marine and Josse, Julie and Poline, Jean-Baptiste},
  year = {2022},
  month = apr,
  journal = {GigaScience},
  volume = {11},
  pages = {giac013},
  doi = {10.1093/gigascience/giac013},
  abstract = {Abstract                            Background               As databases grow larger, it becomes harder to fully control their collection, and they frequently come with missing values. These large databases are well suited to train machine learning models, e.g., for forecasting or to extract biomarkers in biomedical settings. Such predictive approaches can use discriminative\textemdash rather than generative\textemdash modeling and thus open the door to new missing-values strategies. Yet existing empirical evaluations of strategies to handle missing values have focused on inferential statistics.                                         Results               Here we conduct a systematic benchmark of missing-values strategies in predictive models with a focus on large health databases: 4 electronic health record datasets, 1 population brain imaging database, 1 health survey, and 2 intensive care surveys. Using gradient-boosted trees, we compare native support for missing values with simple and state-of-the-art imputation prior to learning. We investigate prediction accuracy and computational time. For prediction after imputation, we find that adding an indicator to express which values have been imputed is important, suggesting that the data are missing not at random. Elaborate missing-values imputation can improve prediction compared to simple strategies but requires longer computational time on large data. Learning trees that model missing values\textemdash with missing incorporated attribute\textemdash leads to robust, fast, and well-performing predictive modeling.                                         Conclusions               Native support for missing values in supervised machine learning predicts better than state-of-the-art imputation with much less computational cost. When using imputation, it is important to add indicator columns expressing which values have been imputed.},
  langid = {english},
  keywords = {üíé},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Perez-Lebel et al. - 2022 - Benchmarking missing-values approaches for predict.pdf}
}

@article{perlinPerformanceTickTest2014,
  title = {On the {{Performance}} of the {{Tick Test}}},
  author = {Perlin, Marcelo and Brooks, Chris and Dufour, Alfonso},
  year = {2014},
  month = feb,
  journal = {The Quarterly Review of Economics and Finance},
  volume = {54},
  number = {1},
  pages = {42--50},
  doi = {10.1016/j.qref.2013.07.009},
  langid = {english},
  keywords = {üíé},
  annotation = {5 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Perlin et al. - 2014 - On the performance of the tick test.pdf}
}

@article{petersenMatrixCookbook,
  title = {The Matrix Cookbook},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Petersen und Pedersen - The Matrix Cookbook.pdf}
}

@article{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2207.09238},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  keywords = {üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\9X32MT2H\\Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf}
}

@article{poppeSensitivityVPINChoice2016,
  title = {The {{Sensitivity}} of {{Vpin}} to the {{Choice}} of {{Trade Classification Algorithm}}},
  author = {P{\"o}ppe, Thomas and Moos, Sebastian and Schiereck, Dirk},
  year = {2016},
  month = dec,
  journal = {Journal of Banking \& Finance},
  volume = {73},
  pages = {165--181},
  issn = {03784266},
  doi = {10.1016/j.jbankfin.2016.08.006},
  langid = {english},
  keywords = {üíé},
  annotation = {14 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\P√∂ppe et al. - 2016 - The sensitivity of VPIN to the choice of trade cla.pdf}
}

@inproceedings{prokhorenkovaCatBoostUnbiasedBoosting2018,
  title = {{{CatBoost}}: Unbiased Boosting with Categorical Features},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  year = {2018},
  series = {{{NeurIPS}} 2018},
  volume = {32},
  pages = {6639--6649},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY}},
  abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
  keywords = {‚õî No DOI found,üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Prokhorenkova et al. - 2019 - CatBoost unbiased boosting with categorical featu.pdf}
}

@article{provostTreeInductionProbabilityBased,
  title = {Tree {{Induction}} for {{Probability-Based Ranking}}},
  author = {Provost, Foster},
  pages = {17},
  abstract = {Tree induction is one of the most effective and widely used methods for building classification models. However, many applications require cases to be ranked by the probability of class membership. Probability estimation trees (PETs) have the same attractive features as classification trees (e.g., comprehensibility, accuracy and efficiency in high dimensions and on large data sets). Unfortunately, decision trees have been found to provide poor probability estimates. Several techniques have been proposed to build more accurate PETs, but, to our knowledge, there has not been a systematic experimental analysis of which techniques actually improve the probability-based rankings, and by how much. In this paper we first discuss why the decision-tree representation is not intrinsically inadequate for probability estimation. Inaccurate probabilities are partially the result of decision-tree induction algorithms that focus on maximizing classification accuracy and minimizing tree size (for example via reduced-error pruning). Larger trees can be better for probability estimation, even if the extra size is superfluous for accuracy maximization. We then present the results of a comprehensive set of experiments, testing some straightforward methods for improving probability-based rankings. We show that using a simple, common smoothing method\textemdash the Laplace correction\textemdash uniformly improves probability-based rankings. In addition, bagging substantially improves the rankings, and is even more effective for this purpose than for improving accuracy. We conclude that PETs, with these simple modifications, should be considered when rankings based on class-membership probability are required.},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\PVM54DI5\\Provost - Tree Induction for Probability-Based Ranking.pdf}
}

@inproceedings{puigMultidimensionalShrinkagethresholdingOperator2009,
  title = {A Multidimensional Shrinkage-Thresholding Operator},
  booktitle = {2009 {{IEEE}}/{{SP}} 15th {{Workshop}} on {{Statistical Signal Processing}}},
  author = {Puig, Arnau Tibau and Wiesel, Ami and Hero, Alfred O.},
  year = {2009},
  month = aug,
  pages = {113--116},
  publisher = {{IEEE}},
  address = {{Cardiff, United Kingdom}},
  doi = {10.1109/SSP.2009.5278625},
  isbn = {978-1-4244-2709-3},
  langid = {english},
  annotation = {14 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\JZZKT8DA\\Puig et al. - 2009 - A multidimensional shrinkage-thresholding operator.pdf}
}

@article{raschkaIntroductionLatestTechniques2021,
  title = {An Introduction to the Latest Techniques},
  author = {Raschka, Sebastian},
  year = {2021},
  pages = {52},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Raschka - 2021 - An introduction to the latest techniques.pdf}
}

@book{raschkaMachineLearningPyTorch2022,
  title = {Machine Learning with {{PyTorch}} and {{Scikit-Learn}}: Develop Machine Learning and Deep Learning Models with {{Python}}},
  shorttitle = {Machine Learning with {{PyTorch}} and {{Scikit-Learn}}},
  author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid and Dzhulgakov, Dmytro},
  year = {2022},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  isbn = {978-1-80181-931-2},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\HHSJ48N3\\Raschka et al. - 2022 - Machine learning with PyTorch and Scikit-Learn de.pdf}
}

@article{raschkaRecentTrendsTechnologies2021,
  title = {Recent Trends, Technologies, and Challenges},
  author = {Raschka, Sebastian},
  year = {2021},
  pages = {54},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Raschka - 2021 - Recent Trends, Technologies, and Challenges.pdf}
}

@misc{RecipeTrainingNeural,
  title = {A Recipe for Training Neural Networks},
  howpublished = {http://karpathy.github.io/2019/04/25/recipe/},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\HIHYWWEF\\recipe.html}
}

@article{ribeiroEnsembleApproachBased2020,
  title = {Ensemble Approach Based on Bagging, Boosting and Stacking for Short-Term Prediction in Agribusiness Time Series},
  author = {Ribeiro, Matheus Henrique Dal Molin and {dos Santos Coelho}, Leandro},
  year = {2020},
  journal = {Applied Soft Computing},
  volume = {86},
  pages = {105837},
  doi = {10.1016/j.asoc.2019.105837},
  langid = {english},
  annotation = {141 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ribeiro and dos Santos Coelho - 2020 - Ensemble approach based on bagging, boosting and s.pdf}
}

@article{ronenMachineLearningTrade2022,
  title = {Machine {{Learning}} and {{Trade Direction Classification}}: {{Insights}} from the {{Corporate Bond Market}}},
  shorttitle = {Machine {{Learning}} and {{Trade Direction Classification}}},
  author = {Ronen, Tavy and Fedenia, Mark A. and Nam, Seunghan},
  year = {2022},
  journal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.4213313},
  keywords = {üíé},
  annotation = {0 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\PRQK624E\\Differences.pdf;C\:\\Users\\Markus\\Zotero\\storage\\SK56ALN9\\Ronen et al. - 2022 - Machine Learning and Trade Direction Classificatio.pdf}
}

@article{rosenthalModelingTradeDirection2012,
  title = {Modeling {{Trade Direction}}},
  author = {Rosenthal, D. W. R.},
  year = {2012},
  journal = {Journal of Financial Econometrics},
  volume = {10},
  number = {2},
  pages = {390--415},
  doi = {10.1093/jjfinec/nbr014},
  langid = {english},
  keywords = {üíé},
  annotation = {4 citations (Crossref)},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\EBGQ4ZWP\\Rosenthal - 2012 - Modeling Trade Direction.pdf}
}

@article{rossiMachineLearning,
  title = {Predicting Stock Market Returns with Machine Learning},
  author = {Rossi, Alberto},
  year = {2018},
  journal = {Unpublished Working Paper},
  pages = {44},
  abstract = {We employ a semi-parametric method known as Boosted Regression Trees (BRT) to forecast stock returns and volatility at the monthly frequency. BRT is a statistical method that generates forecasts on the basis of large sets of conditioning information without imposing strong parametric assumptions such as linearity or monotonicity. It applies soft weighting functions to the predictor variables and performs a type of model averaging that increases the stability of the forecasts and therefore protects it against overfitting. Our results indicate that expanding the conditioning information set results in greater out-of-sample predictive accuracy compared to the standard models proposed in the literature and that the forecasts generate profitable portfolio allocations even when market frictions are considered. By working directly with the mean-variance investor's conditional Euler equation we also characterize semi-parametrically the relation between the various covariates constituting the conditioning information set and the investor's optimal portfolio weights. Our results suggest that the relation between predictor variables and the optimal portfolio allocation to risky assets is highly non-linear.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\FQ94H7H3\\Rossi - Machine_Learning.pdf}
}

@book{rothmanTransformersNaturalLanguage2021,
  title = {Transformers for {{Natural Language Processing}}: {{Build}} Innovative Deep Neural Network Architectures for {{NLP}} with {{Python}}, {{PyTorch}}, {{TensorFlow}}, {{BERT}}, {{RoBERTa}}, and More},
  shorttitle = {Transformers for {{Natural Language Processing}}},
  author = {Rothman, Denis},
  year = {2021},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  isbn = {978-1-80056-579-1},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\GLMXG7M9\\Rothman - 2021 - Transformers for Natural Language Processing Buil.pdf}
}

@misc{rozemberczkiAwesomeGradientBoosting2022,
  title = {Awesome {{Gradient Boosting Research Papers}}.},
  author = {Rozemberczki, Benedek},
  year = {2022},
  month = oct,
  abstract = {A curated list of gradient boosting research papers with implementations.},
  copyright = {CC0-1.0},
  keywords = {adaboost,boosting,catboost,classification-algorithm,classification-tree,classification-trees,classifier,decision-tree,deep-learning,gradient-boosted-trees,gradient-boosting,gradient-boosting-classifier,gradient-boosting-decision-trees,gradient-boosting-machine,h2o,lightgbm,machine-learning,random-forest,xgboost,xgboost-algorithm}
}

@misc{rozemberczkiShapleyValueMachine2022,
  title = {The {{Shapley Value}} in {{Machine Learning}}},
  author = {Rozemberczki, Benedek and Watson, Lauren and Bayer, P{\'e}ter and Yang, Hao-Tsung and Kiss, Oliv{\'e}r and Nilsson, Sebastian and Sarkar, Rik},
  year = {2022},
  month = may,
  number = {arXiv:2202.05594},
  eprint = {2202.05594},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Rozemberczki et al. - 2022 - The Shapley Value in Machine Learning.pdf;C\:\\Users\\Markus\\Zotero\\storage\\DA3HKJRW\\2202.html}
}

@article{rubinsteinRelationBinomialTrinomial2000,
  title = {On the Relation between Binomial and Trinomial Option Pricing Models},
  author = {Rubinstein, Mark},
  year = {2000},
  month = nov,
  journal = {The Journal of Derivatives},
  volume = {8},
  number = {2},
  pages = {47--50},
  issn = {1074-1240, 2168-8524},
  doi = {10.3905/jod.2000.319149},
  abstract = {This paper shows that the binomial option pricing model, suitably parameterized, is a special case of the explicit finite difference method.},
  langid = {english},
  annotation = {20 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Rubinstein - 2000 - On the Relation Between Binomial and Trinomial Opt.pdf}
}

@article{sametFoundationsMultidimensionalMetric,
  title = {Foundations of Multidimensional and Metric Data Structures},
  author = {Samet, Hanan},
  pages = {1022},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\5C8S3LKX\\Samet - Foundations of Multidimensional and Metric Data St.pdf}
}

@book{sandersSequentialParallelAlgorithms2019,
  title = {Sequential and Parallel Algorithms and Data Structures: The Basic Toolbox},
  shorttitle = {Sequential and Parallel Algorithms and Data Structures},
  author = {Sanders, Peter and Mehlhorn, Kurt and Dietzfelbinger, Martin and Dementiev, Roman},
  year = {2019},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-25209-0},
  isbn = {978-3-030-25208-3 978-3-030-25209-0},
  langid = {english},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Sanders et al. - 2019 - Sequential and Parallel Algorithms and Data Struct.pdf}
}

@article{savickasInferringDirectionOption2003,
  title = {On {{Inferring}} the {{Direction}} of {{Option Trades}}},
  author = {Savickas, Robert and Wilson, Arthur J},
  year = {2003},
  journal = {Journal of Financial and Quantitative Analysis},
  volume = {38},
  pages = {881--902},
  doi = {10.2307/4126747},
  abstract = {To sign option trades as buys and sells, researchers often employ stock trade classification rules including the quote, the Lee and Ready (1991), the Ellis, Michaely, and O'Hara (2000), and the tick methods. Using a proprietary CBOE dataset that reports trade direction, we find that these four rules sign correctly 83\%, 80\%, 77\%, and 59\% of all classifiable trades, respectively. These rates are based on separate classifiable samples because each of the four rules fails to classify some trades (e.g., the quote rule cannot classify midspread trades). Outside-quote and reversed-quote trades are highly misclassified by all four rules. The probability of such trades is related to trading frequency, trade size, moneyness, and maturity. Underlying asset price changes around the time of the trade improve classification precision. We find that the components of index option complex trades not executed on the Retail Automated Execution System are misclassified almost 50\% of the time by any method. The elimination of these trades (15\% of the sample) results in a success rate of over 87\% for the quote rule.},
  langid = {english},
  keywords = {üíé,options},
  annotation = {19 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\7HQLCVWI\\Savickas and Wilson - On Inferring the Direction of Option Trades.pdf}
}

@article{schaferRecommenderSystemsEcommerce1999,
  title = {Recommender Systems in {{E-commerce}}},
  author = {Schafer, J Ben and Konstan, Joseph and Riedl, John},
  year = {1999},
  pages = {9},
  doi = {10.1145/336992.337035},
  abstract = {Recommender systems are changing from novelties used by a few E-commerce sites, to serious business tools that are re-shaping the world of E-commerce. Many of the largest commerce Web sites are already using recommender systems to help their customers find products to purchase. A recommender system learns from a customer and recommends products that she will find most valuable from among the available products. In this paper we present an explanation of how recommender systems help Ecommerce sites increase sales, and analyze six sites that use recommender systems including several sites that use more than one recommender system. Based on the examples, we create a taxonomy of recommender systems, including the interfaces they present to customers, the technologies used to create the recommendations, and the inputs they need from customers. We conclude with ideas for new applications of recommender systems to E-commerce.},
  langid = {english},
  annotation = {761 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Schafer et al. - 1999 - Recommender Systems in E-Commerce.pdf}
}

@book{scholkopfLearningKernelsSupport2002,
  title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J.},
  year = {2002},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-19475-4},
  langid = {english},
  lccn = {Q325.5 .S32 2002},
  keywords = {Kernel functions,Support vector machines},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Sch√∂lkopf und Smola - 2002 - Learning with kernels support vector machines, re.pdf}
}

@article{shahriariTakingHumanOut2016,
  title = {Taking the Human out of the Loop: {{A}} Review of Bayesian Optimization},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  doi = {10.1109/JPROC.2015.2494218},
  abstract = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  langid = {english},
  keywords = {üíé},
  annotation = {1712 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\Q4DW5BWD\\Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf}
}

@article{shaniEvaluatingRecommendationSystems,
  title = {Evaluating Recommendation Systems},
  author = {Shani, Guy and Gunawardana, Asela},
  pages = {43},
  doi = {10.1007/978-0-387-85820-3_8},
  abstract = {Recommender systems are now popular both commercially and in the research community, where many approaches have been suggested for providing recommendations. In many cases a system designer that wishes to employ a recommendation system must choose between a set of candidate approaches. A first step towards selecting an appropriate algorithm is to decide which properties of the application to focus upon when making this choice. Indeed, recommendation systems have a variety of properties that may affect user experience, such as accuracy, robustness, scalability, and so forth. In this paper we discuss how to compare recommenders based on a set of properties that are relevant for the application. We focus on comparative studies, where a few algorithms are compared using some evaluation metric, rather than absolute benchmarking of algorithms. We describe experimental settings appropriate for making choices between algorithms. We review three types of experiments, starting with an offline setting, where recommendation approaches are compared without user interaction, then reviewing user studies, where a small group of subjects experiment with the system and report on the experience, and finally describe large scale online experiments, where real user populations interact with the system. In each of these cases we describe types of questions that can be answered, and suggest protocols for experimentation. We also discuss how to draw trustworthy conclusions from the conducted experiments. We then review a large set of properties, and explain how to evaluate systems given relevant properties. We also survey a large set of evaluation metrics in the context of the property that they evaluate.},
  langid = {english},
  annotation = {508 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Shani und Gunawardana - Evaluating Recommendation Systems.pdf}
}

@inproceedings{shavittRegularizationLearningNetworks2018,
  title = {Regularization {{Learning Networks}}: {{Deep Learning}} for {{Tabular Datasets}}},
  booktitle = {32nd {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Shavitt, Ira and Segal, Eran},
  year = {2018},
  pages = {11},
  address = {{Montr\'eal}},
  abstract = {Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8\% of the network edges and 82\% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at https://github.com/irashavitt/regularization\_ learning\_networks.},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\M7YM34G9\\Shavitt and Segal - 2018 - Regularization Learning Networks Deep Learning fo.pdf}
}

@article{shirkhorshidiComparisonStudySimilarity2015,
  title = {A Comparison Study on Similarity and Dissimilarity Measures in Clustering Continuous Data},
  author = {Shirkhorshidi, Ali Seyed and Aghabozorgi, Saeed and Wah, Teh Ying},
  editor = {Dalby, Andrew R.},
  year = {2015},
  month = dec,
  journal = {PLOS ONE},
  volume = {10},
  number = {12},
  pages = {e0144059},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0144059},
  langid = {english},
  annotation = {186 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Shirkhorshidi et al. - 2015 - A Comparison Study on Similarity and Dissimilarity.pdf}
}

@article{shumwayDelistingBiasCRSP1997,
  title = {The Delisting Bias in {{CRSP}} Data},
  author = {Shumway, Tyler},
  year = {1997},
  month = mar,
  journal = {The Journal of Finance},
  volume = {52},
  number = {1},
  pages = {327--340},
  issn = {00221082},
  doi = {10.1111/j.1540-6261.1997.tb03818.x},
  abstract = {I document a delisting bias in the stock return data base maintained by the Center for Research in Security Prices (CRSP). I find that delists for bankruptcy and other negative reasons are generally surprises and that correct delisting returns are not available for most of the stocks that have been delisted for negative reasons since 1962. Using over-the-counter price data, I show that the omitted delisting returns are large. Implications of the bias are discussed.},
  langid = {english},
  annotation = {645 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\UBJWL3XZ\\Shumway - 1997 - The Delisting Bias in CRSP Data.pdf}
}

@misc{shwartz-zivTabularDataDeep2021,
  title = {Tabular {{Data}}: {{Deep Learning Is Not All You Need}}},
  shorttitle = {Tabular {{Data}}},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = {2021},
  number = {arXiv:2106.03253},
  eprint = {2106.03253},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\X6DJUNW7\\Shwartz-Ziv and Armon - 2021 - Tabular Data Deep Learning is Not All You Need.pdf;C\:\\Users\\Markus\\Zotero\\storage\\R8UGVCPL\\2106.html}
}

@misc{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2017},
  number = {arXiv:1506.01186},
  eprint = {1506.01186},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\KYLZPI9D\\Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf;C\:\\Users\\Markus\\Zotero\\storage\\748K8CYB\\1506.html}
}

@inproceedings{snoekPracticalBayesianOptimization2012,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ``black art'' requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf}
}

@misc{somepalliSAINTImprovedNeural2021,
  title = {Saint: {{Improved Neural Networks}} for {{Tabular Data Via Row Attention}} and {{Contrastive Pre-Training}}},
  shorttitle = {Saint},
  author = {Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C. Bayan and Goldstein, Tom},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01342},
  eprint = {2106.01342},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners. However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems. Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Somepalli et al. - 2021 - SAINT Improved Neural Networks for Tabular Data v.pdf;C\:\\Users\\Markus\\Zotero\\storage\\F45Z8HTA\\2106.html}
}

@misc{SparseAutoencodersUsing2020,
  title = {Sparse Autoencoders Using L1 Regularization with {{PyTorch}}},
  year = {2020},
  month = mar,
  journal = {DebuggerCafe},
  abstract = {Learn how to add L1 sparsity penalty to autoencoder neural networks. Train an autoencoder neural network on the Fashion MNIST data by adding L1 penalty.},
  langid = {american},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\N8ZAPGYN\\sparse-autoencoders-using-l1-regularization-with-pytorch.html}
}

@misc{statquestwithjoshstarmerGradientBoostPart2019,
  title = {Gradient Boost Part 2 (of 4): Regression Details},
  shorttitle = {Gradient Boost Part 2 (of 4)},
  author = {{StatQuest with Josh Starmer}},
  year = {2019},
  month = feb,
  abstract = {Gradient Boost is one of the most popular Machine Learning algorithms in use. And get this, it's not that complicated! This video is the second part in a series that walks through it one step at a time. This video focuses on the original Gradient Boost algorithm used to predict a continuous value, like someone's weight. We call this, "using Gradient Boost for Regression". In part 3, we'll walk though how Gradient Boost classifies samples into two different categories, and in part 4, we'll go through the math again, this time focusing on classification. NOTE: I need to clarify what I said at 21:08. With regression trees, the sample will only go to a single leaf, and this summation simply isolates the one output value of interest from all of the others. However, when I first made this video I was thinking that because Gradient Boost is supposed to work with any "weak learner", not just small regression trees, that this summation was a way to add flexibility to the algorithm. ALSO NOTE: There is a minor error at 15:47. It should be R\_jm, not R\_ij. Also, there are minor errors at 16:18 and 24:15. At 16:18, the leaf in the script is R\_1,2 and it should be R\_2,1. At 24:15, the header for the residual column should be r\_i,2. This StatQuest assumes that you have already watched Part 1: https://youtu.be/3CC4N4z3GJc ...it also assumes that you know about Regression Trees: https://youtu.be/g9c66TUylZ4 ...and, while it required, it might be useful if you understood Gradient Descent: https://youtu.be/sDv4f4s2SB8 For a complete index of all the StatQuest videos, check out: https://statquest.org/video-index/ This StatQuest is based on the following sources: A 1999 manuscript by Jerome Friedman that introduced Stochastic Gradient Boost: https://statweb.stanford.edu/\textasciitilde jhf/ftp... The Wikipedia article on Gradient Boosting: https://en.wikipedia.org/wiki/Gradien... The scikit-learn implementation of Gradient Boosting: https://scikit-learn.org/stable/modul... If you'd like to support StatQuest, please consider... Patreon: https://www.patreon.com/statquest ...or... YouTube Membership: https://www.youtube.com/channel/UCtYL... ...a cool StatQuest t-shirt or sweatshirt:  shop.spreadshirt.com/statquest-with-j... ...buying one or two of my songs (or go large and get a whole album!) https://joshuastarmer.bandcamp.com/ ...or just donating to StatQuest! https://www.paypal.me/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https://twitter.com/joshuastarmer 0:00 Awesome song and introduction 0:00 Step 0: The data and the loss function 6:30 Step 1: Initialize the model with a constant value 9:10 Step 2: Build M trees 10:01 Step 2.A: Calculate residuals 12:47 Step 2.B: Fit a regression tree to the residuals 14:50 Step 2.C: Optimize leaf output values 20:38 Step 2.D: Update predictions with the new tree 23:19 Step 2: Summary of step 2 24:59 Step 3: Output the final prediction \#statquest \#gradientboost},
  keywords = {üíé}
}

@book{stewartCalculusEarlyTranscendentals2016,
  title = {Calculus: Early Transcendentals},
  shorttitle = {Calculus},
  author = {Stewart, James},
  year = {2016},
  edition = {Eighth edition},
  publisher = {{Cengage Learning}},
  address = {{Boston, MA, USA}},
  isbn = {978-1-285-74155-0},
  langid = {english},
  lccn = {QA303.2 .S7315 2016},
  keywords = {Calculus},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Stewart - 2016 - Calculus early transcendentals.pdf}
}

@book{strangIntroductionLinearAlgebra2016,
  title = {Introduction to Linear Algebra},
  author = {Strang, Gilbert},
  year = {2016},
  edition = {5th edition},
  publisher = {{Cambridge press}},
  address = {{Wellesley}},
  isbn = {978-0-9802327-7-6},
  langid = {english},
  lccn = {512.5},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\GFLQGDYH\\Linear_Algebra_Lang.pdf}
}

@article{stutzUnderstandingImprovingRobustness,
  title = {Understanding and {{Improving Robustness}} and {{Uncertainty Estimation}} in {{Deep Learning}}},
  author = {Stutz, David},
  pages = {74},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\HUST4G7K\\Stutz - Understanding and Improving Robustness and Uncerta.pdf}
}

@phdthesis{stutzUnderstandingImprovingRobustness2022,
  type = {{{doctoralThesis}}},
  title = {Understanding and Improving Robustness and Uncertainty Estimation in Deep Learning},
  author = {Stutz, David},
  year = {2022},
  doi = {10.22028/D291-37286},
  abstract = {Deep learning is becoming increasingly relevant for many high-stakes applications such as autonomous driving or medical diagnosis where wrong decisions can have massive impact on human lives. Unfortunately, deep neural networks are typically assessed solely based on generalization, e.g., accuracy on a fixed test set. However, this is clearly insufficient for safe deployment as potential malicious actors and distribution shifts or the effects of quantization and unreliable hardware are disregarded. Thus, recent work additionally evaluates performance on potentially manipulated or corrupted inputs as well as after quantization and deployment on specialized hardware. In such settings, it is also important to obtain reasonable estimates of the model's confidence alongside its predictions. This thesis studies robustness and uncertainty estimation in deep learning along three main directions: First, we consider so-called adversarial examples, slightly perturbed inputs causing severe drops in accuracy. Second, we study weight perturbations, focusing particularly on bit errors in quantized weights. This is relevant for deploying models on special-purpose hardware for efficient inference, so-called accelerators. Finally, we address uncertainty estimation to improve robustness and provide meaningful statistical performance guarantees for safe deployment.  In detail, we study the existence of adversarial examples with respect to the underlying data manifold. In this context, we also investigate adversarial training which improves robustness by augmenting training with adversarial examples at the cost of reduced accuracy. We show that regular adversarial examples leave the data manifold in an almost orthogonal direction. While we find no inherent trade-off between robustness and accuracy, this contributes to a higher sample complexity as well as severe overfitting of adversarial training. Using a novel measure of flatness in the robust loss landscape with respect to weight changes, we also show that robust overfitting is caused by converging to particularly sharp minima. In fact, we find a clear correlation between flatness and good robust generalization.  Further, we study random and adversarial bit errors in quantized weights. In accelerators, random bit errors occur in the memory when reducing voltage with the goal of improving energy-efficiency. Here, we consider a robust quantization scheme, use weight clipping as regularization and perform random bit error training to improve bit error robustness, allowing considerable energy savings without requiring hardware changes. In contrast, adversarial bit errors are maliciously introduced through hardware- or software-based attacks on the memory, with severe consequences on performance. We propose a novel adversarial bit error attack to study this threat and use adversarial bit error training to improve robustness and thereby also the accelerator's security.  Finally, we view robustness in the context of uncertainty estimation. By encouraging low-confidence predictions on adversarial examples, our confidence-calibrated adversarial training successfully rejects adversarial, corrupted as well as out-of-distribution examples at test time. Thereby, we are also able to improve the robustness-accuracy trade-off compared to regular adversarial training. However, even robust models do not provide any guarantee for safe deployment. To address this problem, conformal prediction allows the model to predict confidence sets with user-specified guarantee of including the true label. Unfortunately, as conformal prediction is usually applied after training, the model is trained without taking this calibration step into account. To address this limitation, we propose conformal training which allows training conformal predictors end-to-end with the underlying model. This not only improves the obtained uncertainty estimates but also enables optimizing application-specific objectives without losing the provided guarantee.  Besides our work on robustness or uncertainty, we also address the problem of 3D shape completion of partially observed point clouds. Specifically, we consider an autonomous driving or robotics setting where vehicles are commonly equipped with LiDAR or depth sensors and obtaining a complete 3D representation of the environment is crucial. However, ground truth shapes that are essential for applying deep learning techniques are extremely difficult to obtain. Thus, we propose a weakly-supervised approach that can be trained on the incomplete point clouds while offering efficient inference.  In summary, this thesis contributes to our understanding of robustness against both input and weight perturbations. To this end, we also develop methods to improve robustness alongside uncertainty estimation for safe deployment of deep learning methods in high-stakes applications. In the particular context of autonomous driving, we also address 3D shape completion of sparse point clouds.},
  langid = {english},
  school = {Saarl\"andische Universit\"ats- und Landesbibliothek},
  annotation = {Accepted: 2022-10-10T06:59:57Z},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\VDSJYHKC\\Stutz - 2022 - Understanding and improving robustness and uncerta.pdf;C\:\\Users\\Markus\\Zotero\\storage\\HPT2JEY5\\33949.html}
}

@incollection{sunAdaBoostLSTMEnsembleLearning2018,
  title = {{{AdaBoost-LSTM Ensemble Learning}} for {{Financial Time Series Forecasting}}},
  booktitle = {Computational {{Science}} \textendash{} {{ICCS}} 2018},
  author = {Sun, Shaolong and Wei, Yunjie and Wang, Shouyang},
  editor = {Shi, Yong and Fu, Haohuan and Tian, Yingjie and Krzhizhanovskaya, Valeria V. and Lees, Michael Harold and Dongarra, Jack and Sloot, Peter M. A.},
  year = {2018},
  volume = {10862},
  pages = {590--597},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93713-7_55},
  abstract = {A hybrid ensemble learning approach is proposed to forecast financial time series combining AdaBoost algorithm and Long Short-Term Memory (LSTM) network. Firstly, by using AdaBoost algorithm the database is trained to get the training samples. Secondly, the LSTM is utilized to forecast each training sample separately. Thirdly, AdaBoost algorithm is used to integrate the forecasting results of all the LSTM predictors to generate the ensemble results. Two major daily exchange rate datasets and two stock market index datasets are selected for model evaluation and comparison. The empirical results demonstrate that the proposed AdaBoost-LSTM ensemble learning approach outperforms some other single forecasting models and ensemble learning approaches. This suggests that the AdaBoost-LSTM ensemble learning approach is a highly promising approach for financial time series data forecasting, especially for the time series data with nonlinearity and irregularity, such as exchange rates and stock indexes.},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\2NQDEP3C\\Sun et al. - 2018 - AdaBoost-LSTM Ensemble Learning for Financial Time.pdf}
}

@article{tanhaSemisupervisedSelftrainingDecision2017,
  title = {Semi-{{Supervised Self-Training}} for {{Decision Tree Classifiers}}},
  author = {Tanha, Jafar and {van Someren}, Maarten and Afsarmanesh, Hamideh},
  year = {2017},
  month = feb,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {8},
  number = {1},
  pages = {355--370},
  doi = {10.1007/s13042-015-0328-7},
  langid = {english},
  keywords = {üíé},
  annotation = {134 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\XAIIBM8Q\\Tanha et al. - 2017 - Semi-supervised self-training for decision tree cl.pdf}
}

@article{theissenTestAccuracyLee2000,
  title = {A {{Test}} of the {{Accuracy}} of the {{Lee}}/{{Ready Trade Classification Algorithm}}},
  author = {Theissen, Erik},
  year = {2000},
  journal = {Journal of International Financial Markets, Institutions and Money},
  volume = {11},
  pages = {1416--5},
  abstract = {: We analyze the accuracy of the Lee / Ready (1991) trade classification algorithm and the simpler tick test. Our definition of true trade classification is based on whether the Makler (the equivalent of the specialist on the Frankfurt Stock Exchange) bought or sold shares. The Lee / Ready method classifies only 72.8\% of the transactions correctly. The simpler tick test performs almost equally well. We document that misclassification of trades may systematically bias the results of empirical microstructure research. Finally, we show that estimation of the bid-ask spread from transactions data results in a reasonably accurate estimate of the relative liquidity of our sample stocks. This is an important finding because quote data for the German stock market is not available on a regular basis.  JEL classification: G10  Keywords: Lee / Ready method, tick test, bid-ask spread  *  Financial support from the TMR grant "Financial Market Efficiency and Economic Efficiency" is gratefully acknow...},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\2XMIU8NA\\Theissen - 2000 - A Test of the Accuracy of the LeeReady Trade Clas.pdf;C\:\\Users\\Markus\\Zotero\\storage\\9KJZGCAP\\download.html}
}

@article{tobekDoesSourceFundamental,
  title = {Does the Source of Fundamental Data Matter?},
  author = {Tobek, Ondrej and Hronec, Martin},
  pages = {78},
  doi = {10.2139/ssrn.3150654},
  abstract = {We study the role of the choice of a fundamental database on the portfolio returns of a set of 74 fundamental anomalies. We benchmark Compustat by comparing it to Datastream in the US and find systematic differences in the raw financial statements across the databases. These differences only have a small effect on the returns of anomalies when they are constructed on stock-months existing in both databases. Different stock coverage across the databases, however, leads to large statistically and economically significant disparities in the returns. Profitability anomalies yield negative returns on the Datastream universe.},
  langid = {english},
  annotation = {9 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\9QJMSATL\\Tobek und Hronec - Does the Source of Fundamental Data Matter.pdf}
}

@misc{TransformerArchitecturePositional,
  title = {Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog},
  howpublished = {https://kazemnejad.com/blog/transformer\_architecture\_positional\_encoding/},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\8NDJ2I4T\\transformer_architecture_positional_encoding.html}
}

@misc{TransformersLucasBeyer,
  title = {Transformers with {{Lucas Beyer}}, {{Google Brain}} - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=EixI6t5oif0},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\DC8MAT5V\\watch.html}
}

@article{tsaiPredictingStockReturns2011,
  title = {Predicting Stock Returns by Classifier Ensembles},
  author = {Tsai, Chih-Fong and Lin, Yuah-Chiao and Yen, David C. and Chen, Yan-Min},
  year = {2011},
  journal = {Applied Soft Computing},
  volume = {11},
  number = {2},
  pages = {2452--2459},
  doi = {10.1016/j.asoc.2010.10.001},
  abstract = {The problem of predicting stock returns has been an important issue for many years. Advancement in computer technology has allowed many recent studies to utilize machine learning techniques such as neural networks and decision trees to predict stock returns. In the area of machine learning, classifier ensembles (i.e. combining multiple classifiers) have proven to be a method superior to single classifiers. In order to build a better model for predicting stock returns effectively and efficiently, this study aims at investigating the prediction performance that utilizes the classifier ensembles method to analyze stock returns. In particular, the hybrid methods of majority voting and bagging are considered. Moreover, performance using two types of classifier ensembles is compared with those using single baseline classifiers (i.e. neural networks, decision trees, and logistic regression). These two types of ensembles are `homogeneous' classifier ensembles (e.g. an ensemble of neural networks) and `heterogeneous' classifier ensembles (e.g. an ensemble of neural networks, decision trees and logistic regression). Average prediction accuracy, Type I and II errors, and return on investment of these models are also examined. Our results indicate that multiple classifiers outperform single classifiers in terms of prediction accuracy and returns on investment. In addition, heterogeneous classifier ensembles offer slightly better performance than the homogeneous ones. However, there is no significant difference between majority voting and bagging in prediction accuracy, but the former has better stock returns prediction accuracy than the latter. Finally, the homogeneous multiple classifiers using neural networks by majority voting perform best when predicting stock returns.},
  langid = {english},
  annotation = {102 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RN8ASEI9\\Tsai et al. - 2011 - Predicting stock returns by classifier ensembles.pdf}
}

@article{tunstallNaturalLanguageProcessing2022,
  title = {Natural {{Language Processing}} with {{Transformers}}},
  author = {Tunstall, Lewis},
  year = {2022},
  month = feb,
  pages = {409},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\TVF29AAM\\Tunstall - Natural Language Processing with Transformers.pdf}
}

@misc{turnerBayesianOptimizationSuperior2021,
  title = {Bayesian {{Optimization Is Superior}} to {{Random Search}} for {{Machine Learning Hyperparameter Tuning}}: {{Analysis}} of the {{Black-Box Optimization Challenge}} 2020},
  author = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  year = {2021},
  number = {arXiv:2104.10201},
  eprint = {2104.10201},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Turner et al. - 2021 - Bayesian Optimization is Superior to Random Search.pdf}
}

@inproceedings{ucarSubTabSubsettingFeatures2021,
  title = {{{SubTab}}: {{Subsetting Features}} of {{Tabular Data}} for {{Self-Supervised Representation Learning}}},
  shorttitle = {{{SubTab}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ucar, Talip and Hajiramezanali, Ehsan and Edwards, Lindsay},
  year = {2021},
  volume = {34},
  pages = {18853--18865},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Self-supervised learning has been shown to be very effective in learning useful representations, and yet much of the success is achieved in data types such as images, audio, and text. The success is mainly enabled by taking advantage of spatial, temporal, or semantic structure in the data through augmentation. However, such structure may not exist in tabular datasets commonly used in fields such as healthcare, making it difficult to design an effective augmentation method, and hindering a similar progress in tabular data setting. In this paper, we introduce a new framework, Subsetting features of Tabular data (SubTab), that turns the task of learning from tabular data into a multi-view representation learning problem by dividing the input features to multiple subsets. We argue that reconstructing the data from the subset of its features rather than its corrupted version in an autoencoder setting can better capture its underlying latent representation. In this framework, the joint representation can be expressed as the aggregate of latent variables of the subsets at test time, which we refer to as collaborative inference. Our experiments show that the SubTab achieves the state of the art (SOTA) performance of 98.31\% on MNIST in tabular setting, on par with CNN-based SOTA models, and surpasses existing baselines on three other real-world datasets by a significant margin.},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Ucar et al. - 2021 - SubTab Subsetting Features of Tabular Data for Se.pdf}
}

@article{vandermaatenVisualizingDataUsing2008,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\TWW49G78\\van der Maaten and Hinton - 2008 - Visualizing data using t-SNE.pdf}
}

@article{vanengelenSurveySemisupervisedLearning2020,
  title = {A {{Survey}} on {{Semi-Supervised Learning}}},
  author = {{van Engelen}, Jesper E. and Hoos, Holger H.},
  year = {2020},
  month = feb,
  journal = {Machine Learning},
  volume = {109},
  number = {2},
  pages = {373--440},
  doi = {10.1007/s10994-019-05855-6},
  langid = {english},
  keywords = {üíé},
  annotation = {530 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\van Engelen and Hoos - 2020 - A survey on semi-supervised learning.pdf;C\:\\Users\\Markus\\Zotero\\storage\\763TFB6D\\van Engelen and Hoos - 2020 - A survey on semi-supervised learning.pdf}
}

@misc{vasuImprovedOneMillisecond2022,
  title = {An {{Improved One Millisecond Mobile Backbone}}},
  author = {Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04040},
  eprint = {2206.04040},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Efficient neural network backbones for mobile devices are often optimized for metrics such as FLOPs or parameter count. However, these metrics may not correlate well with latency of the network when deployed on a mobile device. Therefore, we perform extensive analysis of different metrics by deploying several mobile-friendly networks on a mobile device. We identify and analyze architectural and optimization bottlenecks in recent efficient neural networks and provide ways to mitigate these bottlenecks. To this end, we design an efficient backbone MobileOne, with variants achieving an inference time under 1 ms on an iPhone12 with 75.9\% top-1 accuracy on ImageNet. We show that MobileOne achieves state-of-the-art performance within the efficient architectures while being many times faster on mobile. Our best model obtains similar performance on ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3\% better top-1 accuracy on ImageNet than EfficientNet at similar latency. Furthermore, we show that our model generalizes to multiple tasks - image classification, object detection, and semantic segmentation with significant improvements in latency and accuracy as compared to existing efficient architectures when deployed on a mobile device.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Vasu et al. - 2022 - An Improved One millisecond Mobile Backbone.pdf;C\:\\Users\\Markus\\Zotero\\storage\\DML92KRW\\2206.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}L\{\}ukasz and Polosukhin, Illia},
  year = {2017},
  series = {{{NeurIPS}} 2017},
  volume = {30},
  pages = {6000--6010},
  publisher = {{Curran Associates, Inc.}},
  address = {{Long Beach, CA}},
  keywords = {‚õî No DOI found,üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\YYIZYQ2T\\Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@inproceedings{verscheldeGPUAccelerationNewton2014,
  title = {{{GPU}} Acceleration of Newton's Method for Large Systems of Polynomial Equations in Double Double and Quad Double Arithmetic},
  booktitle = {2014 {{IEEE Intl Conf}} on {{High Performance Computing}} and {{Communications}}, 2014 {{IEEE}} 6th {{Intl Symp}} on {{Cyberspace Safety}} and {{Security}}, 2014 {{IEEE}} 11th {{Intl Conf}} on {{Embedded Software}} and {{Syst}} ({{HPCC}},{{CSS}},{{ICESS}})},
  author = {Verschelde, Jan and Yu, Xiangcheng},
  year = {2014},
  month = aug,
  pages = {161--164},
  publisher = {{IEEE}},
  address = {{Paris, France}},
  doi = {10.1109/HPCC.2014.31},
  abstract = {In order to compensate for the higher cost of double double and quad double arithmetic when solving large polynomial systems, we investigate the application of the NVIDIA Tesla K20C graphics processing unit (GPU). The focus on this paper is on Newton's method, which requires the evaluation of the polynomials, their derivatives, and the solution of a linear system to compute the update to the current approximation for the solution. The reverse mode of algorithmic differentiation for a product of variables is rewritten in a binary tree fashion so all threads in a block can collaborate in the computation. For double arithmetic, the evaluation and differentiation problem is memory bound, whereas for complex quad double arithmetic the problem is compute bound. With acceleration we can double the dimension and get results that are twice as accurate in about the same time.},
  isbn = {978-1-4799-6123-8},
  langid = {english},
  annotation = {4 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Verschelde und Yu - 2014 - GPU Acceleration of Newton's Method for Large Syst.pdf}
}

@article{vijhStockClosingPrice2020,
  title = {Stock {{Closing Price Prediction}} Using {{Machine Learning Techniques}}},
  author = {Vijh, Mehar and Chandola, Deeksha and Tikkiwal, Vinay Anand and Kumar, Arun},
  year = {2020},
  journal = {Procedia Computer Science},
  volume = {167},
  pages = {599--606},
  issn = {18770509},
  doi = {10.1016/j.procs.2020.03.326},
  langid = {english},
  annotation = {73 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Vijh et al. - 2020 - Stock Closing Price Prediction using Machine Learn.pdf}
}

@article{wangAttentionbasedTransactionalContext,
  title = {Attention-Based Transactional Context Embedding for next-Item Recommendation},
  author = {Wang, Shoujin and Hu, Liang and Cao, Longbing and Huang, Xiaoshui and Lian, Defu and Liu, Wei},
  pages = {8},
  doi = {10.1609/aaai.v32i1.11851},
  abstract = {To recommend the next item to a user in a transactional context is practical yet challenging in applications such as marketing campaigns. Transactional context refers to the items that are observable in a transaction. Most existing transactionbased recommender systems (TBRSs) make recommendations by mainly considering recently occurring items instead of all the ones observed in the current context. Moreover, they often assume a rigid order between items within a transaction, which is not always practical. More importantly, a long transaction often contains many items irreverent to the next choice, which tends to overwhelm the influence of a few truely relevant ones. Therefore, we posit that a good TBRS should not only consider all the observed items in the current transaction but also weight them with different relevance to build an attentive context that outputs the proper next item with a high probability. To this end, we design an effective attentionbased transaction embedding model (ATEM) for context embedding to weight each observed item in a transaction without assuming order. The empirical study on real-world transaction datasets proves that ATEM significantly outperforms the state-of-the-art methods in terms of both accuracy and novelty.},
  langid = {english},
  annotation = {35 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Wang et al. - Attention-Based Transactional Context Embedding fo.pdf}
}

@article{wangForecastingMethodStock2020,
  title = {Forecasting Method of Stock Market Volatility in Time Series Data Based on Mixed Model of {{ARIMA}} and {{XGBoost}}},
  author = {Wang, Yan and Guo, Yuankai},
  year = {2020},
  month = mar,
  journal = {China Communications},
  volume = {17},
  number = {3},
  pages = {205--221},
  issn = {1673-5447},
  doi = {10.23919/JCC.2020.03.017},
  annotation = {40 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Wang and Guo - 2020 - Forecasting method of stock market volatility in t.pdf}
}

@incollection{wangPerceivingNextChoice2017,
  title = {Perceiving the next Choice with Comprehensive Transaction Embeddings for Online Recommendation},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Wang, Shoujin and Hu, Liang and Cao, Longbing},
  editor = {Ceci, Michelangelo and Hollm{\'e}n, Jaakko and Todorovski, Ljup{\v c}o and Vens, Celine and D{\v z}eroski, Sa{\v s}o},
  year = {2017},
  volume = {10535},
  pages = {285--302},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-71246-8_18},
  isbn = {978-3-319-71245-1 978-3-319-71246-8},
  langid = {english},
  keywords = {enge_auswahl},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Wang et al. - 2017 - Perceiving the Next Choice with Comprehensive Tran.pdf}
}

@article{wangSurveySessionbasedRecommender2020,
  title = {A Survey on Session-Based Recommender Systems},
  author = {Wang, Shoujin and Cao, Longbing and Wang, Yan and Sheng, Quan Z. and Orgun, Mehmet and Lian, Defu},
  year = {2020},
  month = dec,
  journal = {arXiv:1902.04864 [cs]},
  eprint = {1902.04864},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recommender systems (RSs) have been playing an increasingly important role for informed consumption, services, and decision-making in the overloaded information era and digitized economy. In recent years, session-based recommender systems (SBRSs) have emerged as a new paradigm of RSs. Different from other RSs such as content-based RSs and collaborative filtering-based RSs which usually model long-term yet static user preferences, SBRSs aim to capture short-term but dynamic user preferences to provide more timely and accurate recommendations sensitive to the evolution of their session contexts. Although SBRSs have been intensively studied, neither unified problem statements for SBRSs nor in-depth characterization of SBRS characteristics and challenges are available. It is also unclear to what extent SBRS challenges have been addressed and what the overall research landscape of SBRSs is. This comprehensive review of SBRSs addresses the above aspects by exploring in depth the SBRS entities (e.g., sessions), behaviors (e.g., users' clicks on items) and their properties (e.g., session length). We propose a general problem statement of SBRSs, summarize the diversified data characteristics and challenges of SBRSs, and define a taxonomy to categorize the representative SBRS research. Finally, we discuss new research opportunities in this exciting and vibrant area.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {‚õî No DOI found,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Wang et al. - 2020 - A Survey on Session-based Recommender Systems.pdf}
}

@article{waszczukAssemblingInternationalEquity2014,
  title = {Assembling International Equity Datasets \textendash{} Review of Studies on the Cross-Section of Returns},
  author = {Waszczuk, Antonina},
  year = {2014},
  journal = {Procedia Economics and Finance},
  volume = {15},
  pages = {1603--1612},
  issn = {22125671},
  doi = {10.1016/S2212-5671(14)00631-5},
  abstract = {This paper discusses the data sources used in the international research on the cross-section of stock returns. Covering the wide range of internationally focused papers I give the overview of the applied data, sample coverage, classification schemes and data cleaning methods. I address the quality concerns in case of the non-U.S. data and methodologically relevant specifics of international data analysis providing references to available solutions. In regards to data cleaning I give an overview of applied screens, pointing out their diversity across studies. On that way I offer the first structured insight into challenges and specifics of rapidly increasing amount of papers discussing the cross-section of common stocks in both single-country and multiple-country frameworks.},
  langid = {english},
  annotation = {10 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\IWDUG79C\\Waszczuk - 2014 - Assembling International Equity Datasets ‚Äì Review .pdf}
}

@article{welchComprehensiveLookEmpirical2008,
  title = {A {{Comprehensive Look}} at {{The Empirical Performance}} of {{Equity Premium Prediction}}},
  author = {Welch, Ivo and Goyal, Amit},
  year = {2008},
  month = jul,
  journal = {Review of Financial Studies},
  volume = {21},
  number = {4},
  pages = {1455--1508},
  doi = {10.1093/rfs/hhm014},
  langid = {english},
  annotation = {2053 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\AUF5D2NU\\Welch und Goyal - 2008 - A Comprehensive Look at The Empirical Performance .pdf}
}

@misc{wengLearningNotEnough2021,
  title = {Learning with {{Not Enough Data Part}} 1: {{Semi-Supervised Learning}}},
  shorttitle = {Learning with {{Not Enough Data Part}} 1},
  author = {Weng, Lilian},
  year = {2021},
  month = dec,
  abstract = {When facing a limited amount of labeled data for supervised learning tasks, four approaches are commonly discussed.  Pre-training + fine-tuning: Pre-train a powerful task-agnostic model on a large unsupervised data corpus, e.g. pre-training LMs on free text, or pre-training vision models on unlabelled images via self-supervised learning, and then fine-tune it on the downstream task with a small set of labeled samples. Semi-supervised learning: Learn from the labelled and unlabeled samples together.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2021-12-05-semi-supervised/},
  langid = {english},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RCIKLH89\\2021-12-05-semi-supervised.html}
}

@book{wittenDataMiningPractical2017,
  title = {Data Mining: Practical Machine Learning Tools and Techniques},
  shorttitle = {Data Mining},
  editor = {Witten, I. H. and Witten, I. H.},
  year = {2017},
  edition = {Fourth Edition},
  publisher = {{Elsevier}},
  address = {{Amsterdam}},
  isbn = {978-0-12-804291-5},
  langid = {english},
  lccn = {QA76.9.D343 W58 2017},
  keywords = {Data mining},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Witten und Witten - 2017 - Data mining practical machine learning tools and .pdf}
}

@misc{xieAdanAdaptiveNesterov2022,
  title = {Adan: {{Adaptive Nesterov Momentum Algorithm}} for {{Faster Optimizing Deep Models}}},
  shorttitle = {Adan},
  author = {Xie, Xingyu and Zhou, Pan and Li, Huan and Lin, Zhouchen and Yan, Shuicheng},
  year = {2022},
  month = sep,
  number = {arXiv:2208.06677},
  eprint = {2208.06677},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  abstract = {Adaptive gradient algorithms borrow the moving average idea of heavy ball acceleration to estimate accurate first- and second-order moments of gradient for accelerating convergence. However, Nesterov acceleration which converges faster than heavy ball acceleration in theory and also in many empirical cases is much less investigated under the adaptive gradient setting. In this work, we propose the ADAptive Nesterov momentum algorithm, Adan for short, to speed up the training of deep neural networks effectively. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation (NME) method, which avoids the extra computation and memory overhead of computing gradient at the extrapolation point. Then Adan adopts NME to estimate the first- and second-order moments of the gradient in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an \$\textbackslash epsilon\$-approximate first-order stationary point within \$O(\textbackslash epsilon\^\{-3.5\})\$ stochastic gradient complexity on the nonconvex stochastic problems (e.g., deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan surpasses the corresponding SoTA optimizers on both vision transformers (ViTs) and CNNs, and sets new SoTAs for many popular networks, e.g., ResNet, ConvNext, ViT, Swin, MAE, LSTM, Transformer-XL, and BERT. More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT and ResNet, e.t.c., and also shows great tolerance to a large range of minibatch size, e.g., from 1k to 32k. We hope Adan can contribute to the development of deep learning by reducing training cost and relieving engineering burden of trying different optimizers on various architectures. Code is released at https://github.com/sail-sg/Adan.},
  archiveprefix = {arXiv},
  keywords = {üíé,Computer Science - Machine Learning},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Xie et al. - 2022 - Adan Adaptive Nesterov Momentum Algorithm for Fas.pdf;C\:\\Users\\Markus\\Zotero\\storage\\SWSP28ET\\2208.html}
}

@article{yangStockPricePrediction2021,
  title = {Stock {{Price Prediction Based}} on {{XGBoost}} and {{LightGBM}}},
  author = {Yang, Yue and Wu, Yang and Wang, Peikun and Jiali, Xu},
  editor = {Wen, F. and Ziaei, S.M.},
  year = {2021},
  journal = {E3S Web of Conferences},
  volume = {275},
  pages = {01040},
  issn = {2267-1242},
  doi = {10.1051/e3sconf/202127501040},
  abstract = {Stock trading, as a kind of high frequency trading, generally seeks profits in extremely short market changes. And effective stock price forecasting can help investors obtain higher returns. Based on the data set provided by Jane Street, this paper makes use of XGBoost model and LightGBM model to realize the prediction of stock price. Since the given training set has a large amount of data and includes abnormal data such as missing value, we first carry out feature engineering processing on the original data and take the mean value of the missing value, so as to obtain the preprocessed data that can be used in modeling.             The experimental results show that the combined model of XGBoost and LightGBM has better prediction performance than the single model and neural network.},
  annotation = {4 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Yang et al. - 2021 - Stock Price Prediction Based on XGBoost and LightG.pdf}
}

@inproceedings{yanMachineLearningStock2007,
  title = {Machine Learning for Stock Selection},
  booktitle = {Proceedings of the 13th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining  - {{KDD}} '07},
  author = {Yan, Robert J. and Ling, Charles X.},
  year = {2007},
  pages = {1038},
  publisher = {{ACM Press}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/1281192.1281307},
  isbn = {978-1-59593-609-7},
  langid = {english},
  annotation = {5 citations (Crossref) [2022-10-18]}
}

@inproceedings{yarowskyUnsupervisedWordSense1995,
  title = {Unsupervised {{Word Sense Disambiguation Rivaling Supervised Methods}}},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Yarowsky, David},
  year = {1995},
  pages = {189--196},
  publisher = {{Association for Computational Linguistics}},
  address = {{Cambridge, MA}},
  doi = {10.3115/981658.981684},
  langid = {english},
  keywords = {üíé},
  annotation = {745 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\BJB2UFED\\Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling su.pdf}
}

@article{yiWhyNotUse2020,
  title = {Why {{Not}} to {{Use Zero Imputation}}? {{Correcting Sparsity Bias}} in {{Training Neural Networks}}},
  author = {Yi, Joonyoung and Lee, Juhyuk and Kim, Kwang Joon and Hwang, Sung Ju and Yang, Eunho},
  year = {2020},
  pages = {27},
  abstract = {Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output of a predictive model largely varies with respect to the rate of missingness in the given input, and show that it adversarially affects the model performance. We first theoretically analyze this phenomenon and propose a simple yet effective technique to handle missingness, which we refer to as Sparsity Normalization (SN), that directly targets and resolves the VSP. We further experimentally validate SN on diverse benchmark datasets, to show that debiasing the effect of input-level sparsity improves the performance and stabilizes the training of neural networks.},
  langid = {english},
  keywords = {‚õî No DOI found},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\WY8W2JH7\\pdf.pdf}
}

@inproceedings{yoonVIMEExtendingSuccess2020,
  title = {Vime: {{Extending}} the {{Success}} of {{Self-}} and {{Semi-Supervised Learning}} to {{Tabular Domain}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and {van der Schaar}, Mihaela},
  year = {2020},
  series = {{{NeurIPS}} 2020},
  volume = {34},
  pages = {11033--11043},
  publisher = {{Curran Associates, Inc.}},
  address = {{Red Hook, NY}},
  abstract = {Self- and semi-supervised learning frameworks have made significant progress in training machine learning models with limited labeled data in image and language domains. These methods heavily rely on the unique structure in the domain datasets (such as spatial relationships in images or semantic relationships in language). They are not adaptable to general tabular data which does not have the same explicit structure as image and language data. In this paper, we fill this gap by proposing novel self- and semi-supervised learning frameworks for tabular data, which we refer to collectively as VIME (Value Imputation and Mask Estimation). We create a novel pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning. We also introduce a novel tabular data augmentation method for self- and semi-supervised learning frameworks. In experiments, we evaluate the proposed framework in multiple tabular datasets from various application domains, such as genomics and clinical data. VIME exceeds state-of-the-art performance in comparison to the existing baseline methods.},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Yoon et al. - 2020 - VIME Extending the Success of Self- and Semi-supe.pdf}
}

@article{yuanModelSelectionEstimation2006,
  title = {Model Selection and Estimation in Regression with Grouped Variables},
  author = {Yuan, Ming and Lin, Yi},
  year = {2006},
  month = feb,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {1},
  pages = {49--67},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2005.00532.x},
  abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
  langid = {english},
  annotation = {3896 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\8XB5CRW5\\Yuan und Lin - 2006 - Model selection and estimation in regression with .pdf}
}

@article{zhangDiveDeepLearning2021,
  title = {Dive into Deep Learning},
  author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year = {2021},
  journal = {arXiv:2106.11342},
  eprint = {2106.11342},
  eprinttype = {arxiv},
  pages = {979},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Zhang et al. - Dive into Deep Learning.pdf}
}

@article{zhangUptodateComparisonStateoftheart2017,
  title = {An Up-to-Date Comparison of State-of-the-Art Classification Algorithms},
  author = {Zhang, Chongsheng and Liu, Changchang and Zhang, Xiangliang and Almpanidis, George},
  year = {2017},
  month = oct,
  journal = {Expert Systems with Applications},
  volume = {82},
  pages = {128--150},
  issn = {09574174},
  doi = {10.1016/j.eswa.2017.04.003},
  langid = {english},
  annotation = {222 citations (Crossref) [2022-10-18]},
  file = {C\:\\Users\\Markus\\OneDrive\\Documents\\05 - Wissen & Weiterbildung\\paper\\Zhang et al. - 2017 - An up-to-date comparison of state-of-the-art class.pdf}
}

@article{zhengFeatureEngineeringMachine,
  title = {Feature {{Engineering}} for {{Machine Learning}}},
  author = {Zheng, Alice and Casari, Amanda},
  pages = {217},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\TBEXXQDV\\Zheng and Casari - Feature Engineering for Machine Learning.pdf}
}

@article{zhuSemiSupervisedLearningLiterature,
  title = {Semi-{{Supervised Learning Literature Survey}}},
  author = {Zhu, Xiaojin},
  pages = {60},
  langid = {english},
  keywords = {‚õî No DOI found,üíé},
  file = {C\:\\Users\\Markus\\Zotero\\storage\\RWRNIZHI\\Zhu - Semi-Supervised Learning Literature Survey.pdf}
}

