\section{Expos√©}
\label{sec:expose}

Determining whether a trade is buyer or seller-initiated is ubiquitous for many problems in option research. Typical applications include the study of option demand \autocite{garleanuDemandBasedOptionPricing2009} or the informational content of option trading \autocites{huDoesOptionTrading2014}{panInformationOptionVolume2006}. Despite the overall importance for empirical research, the true initiator of the trade is often missing in option data sets and must be inferred using trade classification algorithms \autocite{easleyOptionVolumeStock1998}. 

Among the most prevailing variants to sign option trades are the tick rule, quote rule \autocite{hasbrouckTradesQuotesInventories1988}, and hybrids thereof such as the \gls{LR} algorithm \autocite{leeInferringTradeDirection1991}, the \gls{EMO} algorithm \autocite{ellisAccuracyTradeClassification2000}, and the \gls{CLVN} method \autocite{chakrabartyTradeClassificationAlgorithms2007}. These algorithms have initially been proposed and tested for the stock market.

The work of \textcites{grauerOptionTradeClassification2022}{savickasInferringDirectionOption2003} raises concerns about the applicability of standard trade signing algorithms to the option market due to deteriorating classification accuracies. Against this backdrop, the question is, can an alternative, machine learning-based classifier improve upon standard trade classification rules? 

Approaching this concern with machine learning is a logical choice due to the ability to deal with high-dimensional data and learn arbitrary decision functions. Thus, we benchmark wide tree-based ensembles and deep neural networks against standard trade classification rules. The analysis is conducted on a data set of option trades recorded at the \gls{CBOE} and \gls{ISE}.

The thesis follows the following structure:

\textbf{Introduction}

In the introduction, we provide motivation and present our key findings. The contributions are four-fold: (I) We employ state-of-the-art machine learning algorithms i.~e., gradient-boosted trees and transformer networks, for trade classification. \textcolor{darkblue}{(...)} outperforms the state-of-the-art trade classification rules by at least \textcolor{darkblue}{(...~\%)}. (II) As part of semi-supervised approaches, we study the impact of incorporating unlabelled trades into the training procedure on trade classification accuracy. (III) We consistently interpret feature contributions to classical trade classification rules and machine learning models with a game-theoretic approach.

\textbf{Related Work}

While classical trade classification algorithms are extensively tested in the stock markets \autocites[e.~g.,][]{chakrabartyTradeClassificationAlgorithms2012}{odders-whiteOccurrenceConsequencesInaccurate2000}, few works have examined trade classification in option markets \autocites{grauerOptionTradeClassification2022}{savickasInferringDirectionOption2003}.

For option markets, the sole focus is on classical classification rules. Even in stock markets, machine learning has hardly been applied to trade classification. An early work of \textcite{rosenthalModelingTradeDirection2012} incorporates standard trade classification rules into a logistic regression model and achieves outperformance in the stock market. Similarly, \textcites{fedeniaMachineLearningCorporate2021}{ronenMachineLearningTrade2022} improve upon classical rules with a random forest, a tree-based ensemble. Albeit their work considers a broad range of approaches, the selection leaves the latest advancements in artificial neural networks and ensemble learning aside. Even if the focus is on standard techniques, the unclear research agenda with regards to model selection, tuning, and testing hampers the transferability of their results to the yet unstudied option market. 

\textbf{Methodology}

We start by introducing the basic quote rule, the tick test, the reverse tick test, the depth rule \autocite{grauerOptionTradeClassification2022}, and the trade size rule \autocite{grauerOptionTradeClassification2022} and derive popular hybrids thereof. Namely, the \gls{LR} algorithm, the \gls{EMO} algorithm, and the  \gls{CLVN} method. We discuss deviations from the original algorithm, like the offset in the \gls{LR} algorithm. Optionally, we include Rosenthal's method \autocite{rosenthalModelingTradeDirection2012}, which incorporates the tick test, \gls{LR}, and \gls{EMO} algorithm into a logistic regression model. Our focus is on the features used within the rules and their economic intuition. We also stress the link between hybrid rules and ensemble techniques studied in machine learning. Classical trade classification rules serve as a benchmark in our study.

Data sets of option trades adhere to a tabular format.  Thus we begin with reviewing state-of-the-art algorithms for classifying tabular data in terms of accuracy. Possible models must support both categorical features e.~g., exercise style of the option and continuous features, e.~g., the option's $\Delta$. Most often, the true label i.~e., indicator if trade is buyer-initiated, can only be inferred for fractions of the data set \autocites{grauerOptionTradeClassification2022}{savickasInferringDirectionOption2003}. Large portions remain unlabelled. Leaving the unlabelled data aside, option trade classification can be viewed as a supervised classification task. Recent research \autocites{arikTabNetAttentiveInterpretable2020}{huangTabTransformerTabularData2020}{yoonVIMEExtendingSuccess2020} indicates, however, that leveraging unlabelled data can further improve classifier performance. Thus, we also frame the problem of trade classification in option markets as a semi-supervised classification task, whereby unlabelled and labelled data is incorporated into the learning procedure. 

Our selection will likely consider wide ensembles in the form of gradient-boosted trees and deep, transformer-based neural networks, such as \textit{TabNet} \autocite{arikTabNetAttentiveInterpretable2020} or \textit{TabTransformer} \autocite{huangTabTransformerTabularData2020}, due to their superior performance in large scale comparisons \autocites{borisovDeepNeuralNetworks2022}{gorishniyRevisitingDeepLearning2021}{grinsztajnWhyTreebasedModels2022}{shwartz-zivTabularDataDeep2021}. Also, both model classes can naturally be enhanced to profit from partially-unlabelled data and are interpretable locally and globally. 

Thereafter, we thoroughly introduce the models for the supervised setting. We start with the notion of classical decision trees, as covered by \textcite{breimanClassificationRegressionTrees2017}. Trees are inherent to tree-based boosting approaches as weak learners. Thus, emphasis is put on the selection of features and the splitting process of the predictor space into disjoint regions. We motivate the use of ensemble approaches, such as gradient-boosted trees, with the poor variance property of decision trees. The subsequent chapter draws on \textcite{hastietrevorElementsStatisticalLearning2009} and \textcite{friedmanGreedyFunctionApproximation2001} with a focus on gradient boosting for classification. Therein, we introduce necessary enhancements to the boosting procedure to support probabilistic classification and discuss arising stability issues. Further adjustments are necessary for the treatment of categorical variables. Therefore, we draw on the ordered boosting by \textcite{prokhorenkovaCatBoostUnbiasedBoosting2018}, which enhances the classical gradient boosting algorithm.

Next, we focus on transformer networks for tabular data. We begin with the classical transformer architecture of \textcite{vaswaniAttentionAllYou2017}. We put our focus on introducing central concepts like the encoder-decoder structure, attention, embeddings, or point-wise networks. These chapters lay the basis for the subsequent tabular-specific architectures like \textit{TabNet} or \textit{TabTransformer}. As the classical transformer is tailored to sequence-to-sequence modelling, it can not be directly applied to tabular data.

Specialized for tabular data is the \textit{TabTransformer} of \textcite{huangTabTransformerTabularData2020}. The architecture utilizes stacked transformers to learn contextual embeddings of categorical features, whereas continuous features are directly input into a standard, feed-forward network.

Another alternative is \textit{TabNet} \autocite{arikTabNetAttentiveInterpretable2020}, which fuses the concept of decision trees and transformers. Similar to growing a decision tree, several subnetworks are used to process the input in a sequential, hierarchical fashion. Sequential attention, a variant of attention, is used to decide which features to use in each step. The output of \textit{TabNet} is the aggregate of all subnetworks. Despite its difference, concepts like the encoder or attention can be transferred from the previous variants. 

Next, we demonstrate how the models from above can be enhanced for the semi-supervised setting. We provide a short discussion on different alternatives. For gradient-boosted trees, self-training \autocite{yarowskyUnsupervisedWordSense1995} is used to obtain pseudo labels for unlabelled parts of the data set. The ensemble itself is trained on both true and pseudo labels. For the neural networks, the scope is limited to separate pre-training procedures to maintain consistency with the supervised counterparts. Thus, for \textit{TabNet}, we use unsupervised pretraining of the encoder as propagated in \textcite{arikTabNetAttentiveInterpretable2020}. Equally, for the \textit{TabTransformer}, we pre-train the transformer layers and column embeddings through masked language modelling or replaced token detection as popularized in \textcite{devlinBERTPretrainingDeep2019} and \textcite{clarkELECTRAPretrainingText2020}, respectively. 

\textbf{Empirical Study}

In our empirical analysis, we introduce the data sets, the generation of true labels, and the applied pre-processing. The data sets contain option trades executed at either the \gls{CBOE} or the \gls{ISE} with additional intraday option price and quote data, end-of-day buy and sell trading volumes, characteristics of the option, and the underlying. Yet our primary focus is on classifying \gls{ISE} trades, with a secondary emphasis on the \gls{CBOE} data set. 

Subsets of the \gls{CBOE} and the \gls{ISE} data set have been previously studied in \textcite{grauerOptionTradeClassification2022}. Thus we align the data pre-processing with their work to maintain consistency. Nevertheless, some deviations are necessary for training the machine learning models. These include the imputation of missing features, standardization, resampling, feature transformations, and feature subset selection. While all our models can theoretically handle raw tabular data without prior processing \autocites{arikTabNetAttentiveInterpretable2020}{prokhorenkovaCatBoostUnbiasedBoosting2018}{huangTabTransformerTabularData2020}, we expect to improve the model's performance with these additional steps. Features are derived through feature transformations e.~g., the relative distance of the trade from the midpoint found in the \gls{CLVN} method, to incorporate them into our models while not incorporating the rule directly. Doing so provides insights into the relationship between classical and machine learning-based approaches. Similar to \textcite{ronenMachineLearningTrade2022}, we define different subsets of data i.~e., one that includes only features found in the classical algorithms and another incorporating option characteristics as well as price and trading data. Finally, unlabelled data is kept for the training of semi-supervised models.

The data set is split into three disjoint sets for training, validation, and testing. As in \textcite{ellisAccuracyTradeClassification2000} and \textcite{ronenMachineLearningTrade2022} we perform a classical train-test split, thereby maintaining the temporal ordering within the data. We rely on labelled data to assess the performance of trade classification rules. With statistical tests, we verify that the distribution of the features and target is maintained on the test set. Due to the number of model combinations considered and the computational demand of transformers and gradient-boosted trees, we expect $k$-fold cross-validation to be practically intractable.

Next, we describe the implementation and training of the supervised and semi-supervised models, as well as classical trade classification rules. 
For a consistent evaluation, we opt to implement classical rules like the \gls{LR} algorithm as a classifier conforming to the programming interface of \textit{Scikit-learn} \autocite{pedregosaScikitlearnMachineLearning2018}.
Gradient boosting is implemented using \textit{CatBoost} by \textcite{prokhorenkovaCatBoostUnbiasedBoosting2018}.\textit{TabNet} and \textit{TabTransformer} are implemented in \textit{PyTorch} \autocite{paszkePyTorchImperativeStyle2019} and \textit{skorch} based on the original papers. Deviations from the papers are reported.
For training, we employ various model-agnostic deep learning practices like learning rate decay, drop out \autocite{hintonImprovingNeuralNetworks2012}, early stopping, ensembles \autocite{huangSnapshotEnsemblesTrain2017} or stochastic weight averaging \autocite{izmailovAveragingWeightsLeads2019} to speed up training or to obtain a better generalization. We report the loss curves to detect over- or underfitting and study learning curves to get insights into our models' bias and variance properties.  

In contrast to \textcite{ronenMachineLearningTrade2022} we emphasize a transparent hyperparameter tuning procedure. We tune with a novel Bayesian optimization based on the tree-structured parzen estimator algorithm. Compared to other approaches like a randomized search, unpromising search regions are omitted, thus requiring fewer search trails. Bayesian search is also reported to be superior over a randomized search \autocite{turnerBayesianOptimizationSuperior2021}. The search space for the parameters is based on the configurations in the corresponding papers. An implementation by \textcite{akibaOptunaNextgenerationHyperparameter2019} is used to optimize for the accuracy on the validation set. Searches may be repeated multiple times with different initializations.

We report the optimization metric on the training, validation, and test set to study the impact of different learning schemes and the learning of generalizable features. Visualization-wise, the chapter may include a study of loss surfaces. The expectation is that pre-training improves both the training and validation loss due to the larger sample size seen during training. A decline between the sets may be observed.

Subsequently, the model is evaluated. Firstly, a comparison between the selected features is conducted. \textit{TabNet}, \textit{TabTransformer}, and gradient-boosted trees are interpretable by design but rely on model-specific techniques such as feature activation masks found only in transformer-based models rendering them useless for cross-model comparisons. Still, we rely on activation masks to study trades on the transaction level. To compare all models, we suggest kernel \gls{SHAP} \autocite{lundbergUnifiedApproachInterpreting2017} or random feature permutation by \textcite{breimanRandomForests2001} for local and global interpretability. Both approaches are advantageous over logistic regression, as previously used by \textcites{savickasInferringDirectionOption2003}{chakrabartyTradeClassificationAlgorithms2012}, with their ability to capture non-linear interactions between features. Due to the implementation of the classical rules as an estimator, we can perform a fair comparison between classical and machine learning-based approaches. We back the observed results with economic intuition.

Secondly, we benchmark \textit{TabNet}, \textit{TabTransformer}, and gradient-boosted trees against the classical trade classification rules. Following a common track in literature, accuracy is the decisive metric. The analysis may be supported with additional metrics like the receiver operator characteristic, area under the curve, or confusion matrices. We expect both semi-supervised and supervised algorithms to outperform the benchmarks with additional performance gains from learning on unlabelled data.

Based on preliminary tests, \textcolor{darkblue}{(...)} outperforms \textcolor{darkblue}{(...)} on the \gls{ISE} data set in terms of classification accuracy with an accuracy of \textcolor{darkblue}{(...~\%)}. The static testing period spans from \textcolor{darkblue}{(...)} to \textcolor{darkblue}{(...)}.


Despite serious counter efforts, our models can still poorly generalize. We use rigorous robustness checks to test if the accuracy is maintained across time, trade sizes, underlyings, and exchanges, among others. The procedure follows \textcites{chakrabartyTradeClassificationAlgorithms2012}{grauerOptionTradeClassification2022}{ronenMachineLearningTrade2022}{savickasInferringDirectionOption2003}. Motivated by research of \textcite{grinsztajnWhyTreebasedModels2022}, we conduct a robustness study of our models to both informative and uninformative features.

All in all, our empirical analysis aims for reproducibility. We implement sophisticated data set versioning and experiment tracking using \textit{weights \& biases}~\footnote{Experiments are tracked at \url{https://wandb.ai/fbv/thesis}.}. The correctness of the code is verified with automated tests \footnote{Code is available at~\url{https://github.com/KarelZe/thesis}.}. 

\textbf{Discussion and Conclusion}

A discussion and a conclusion follow the presentation of the results.


